{"meta":{"title":"长征部落格","subtitle":"保持怀疑，独立思考","description":"长征部落格（CZ5H.COM）是中文科技交流博客，由TZLOOP创建于2020年，包含博主原创文章，精品分享、资源下载等内容，同时欢迎广大网友的投稿，致力于为大家提供有内涵、完整优质的技术和生活类文章。","author":"Tzloop","url":"https://www.cz5h.com","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-07-06T16:14:48.556Z","updated":"2020-02-28T00:04:55.902Z","comments":true,"path":"404.html","permalink":"https://www.cz5h.com/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"所有分类","date":"2020-07-06T16:14:48.556Z","updated":"2020-02-28T00:04:08.184Z","comments":true,"path":"categories/index.html","permalink":"https://www.cz5h.com/categories/index.html","excerpt":"","text":""},{"title":"欢迎留言互换链接哦","date":"2020-02-26T23:00:00.000Z","updated":"2020-12-08T18:27:53.647Z","comments":true,"path":"friends/index.html","permalink":"https://www.cz5h.com/friends/index.html","excerpt":"","text":"各位大佬想交换友链的话可以在下方留言，必须要有名称、头像链接、和至少一个标签哦～ 名称： 长征部落格标签： 编程, NAS, 吃喝玩乐学或者： 保持怀疑，独立思考（十字介绍）网址： https://www.cz5h.com头像： https://cdn.jsdelivr.net/gh/TianZonglin/tuchuang/img/logo.jpg"},{"title":"所有标签","date":"2020-07-06T16:14:48.558Z","updated":"2020-02-28T00:04:09.473Z","comments":true,"path":"tags/index.html","permalink":"https://www.cz5h.com/tags/index.html","excerpt":"","text":""},{"title":"我的云端生活","date":"2020-03-21T23:00:00.000Z","updated":"2020-11-26T02:03:20.681Z","comments":true,"path":"links/site.html","permalink":"https://www.cz5h.com/links/site.html","excerpt":"","text":"私有云盘 I Synology NAS NAS / DSM NAS / PLEX NAS / PHOTO NAS / KOD NAS / FILES 腾讯主机 I TxCloud CVM 小皮面板 胖五的后院 其他链接 I Personal Links WEB / BLOG WEB / RESUME WEB / MIAOPS 站点监控 I Station Monitor 监控（默认模板） 监控（自定义）！"},{"title":"评论","date":"2020-12-11T23:00:00.000Z","updated":"2020-12-20T12:25:17.398Z","comments":true,"path":"links/cmt.html","permalink":"https://www.cz5h.com/links/cmt.html","excerpt":"","text":"★ 全 站 评 论 汇 总 ★ AV.init({ appId:'WbLE88qfAcz4hSI5GsQFRlzW-gzGzoHsz', appKey:'ycqjmtEfUxuxD3IY97oRkrdO' }); //console.log(window.AV); var query = new AV.Query(\"Comment\"); query.descending(\"createdAt\");//升序 query.find().then(function(data){ //查询的结果数据 data //将data转成json格式 var jsondata = JSON.parse(JSON.stringify(data)); //console.log(data); function formatDate(time){ var date = new Date(time); var year = date.getFullYear(), month = date.getMonth() + 1,//月份是从0开始的 day = date.getDate(), hour = date.getHours(), min = date.getMinutes(), sec = date.getSeconds(); var newTime = year + '年' + month + '月' + day + '日' + hour + '点' + min + '分' ;//+ //sec; return newTime; } for(var p in data){//遍历json数组时，这么写p为索引，0,1 if(data[p].attributes.nick!=\"Administrator\"){ //console.log(data[p].attributes.url); $('#allinshow').append('' +data[p].attributes.nick +\"&nbsp;于&nbsp;\" +formatDate(data[p].attributes.insertedAt) +\"&nbsp;在&nbsp;该页面&nbsp;说：\" +data[p].attributes.comment +\"\"); } } },function(err) { //错误信息 err //console.log('err'); }); var GUEST_INFO = ['nick','mail','link']; var guest_info = 'nick,mail,link'.split(',').filter(function(item){ return GUEST_INFO.indexOf(item) > -1 }); var valine = new Valine(); valine.init({ el: '#comments', notify: true, verify: true, guest_info: guest_info, appId: \"WbLE88qfAcz4hSI5GsQFRlzW-gzGzoHsz\", appKey: \"ycqjmtEfUxuxD3IY97oRkrdO\", placeholder: \"请在这里输入你想说的话~\", pageSize:\"10\", avatar:\"retro\" })"},{"title":"感谢金主们的慷慨解囊","date":"2020-04-05T22:00:00.000Z","updated":"2020-06-21T16:55:20.034Z","comments":true,"path":"links/donate.html","permalink":"https://www.cz5h.com/links/donate.html","excerpt":"","text":"支付宝/微信打赏 支持过我的朋友们 日期 金额 交易信息 20/05/16 ¥9.90 CTJSTA**5343 20/05/13 ¥1.98 CTJSTA**7937 20/04/15 ¥1.98 CTJSTA**1838 20/04/09 ¥10.00 100004**5232 20/04/02 ¥19.80 CTJSTA**6001 20/03/17 ¥1.98 CTJSTA**4073 20/03/16 ¥1.98 CTJSTA**2112 20/03/14 ¥2.72 CTJSTA**0201 20/02/28 ¥1.98 CTJSTA**6466 20/02/27 ¥6.80 CTJSTA**0681 20/02/20 ¥1.98 CTJSTA**4968 19/07/19 ¥10.00 100004**0243"},{"title":"站点更新事件序列","date":"2020-02-26T23:00:00.000Z","updated":"2020-09-20T23:00:38.156Z","comments":true,"path":"links/logs.html","permalink":"https://www.cz5h.com/links/logs.html","excerpt":"","text":"本博客修改日志（倒序）从上往下，依次时间回溯，下方的修改内容是最近一次修改。 主题名称：Volantis最新版本：x.x当前版本：1.7.4初始版本：1.6.0最后修改：见本页更新日期 主页置顶区域（豆瓣影评）~\\Volantis\\themes\\volantis\\layout\\index.ejs123456789101112131415 &lt;%- partial('_pre') %&gt;&lt;div class='l_main&lt;%- page.sidebar == false ? ' no_sidebar' : '' %&gt;'&gt; &lt;article class=\"post white-box reveal \"&gt; &lt;section class=\"article typo\"&gt; &lt;div class=\"article-entry\" itemprop=\"articleBody\"&gt; //顶部内容 &lt;/div&gt; &lt;/section&gt; &lt;/article&gt; //从以下开始即主页文章列表 &lt;%- partial('_partial/archive') %&gt;&lt;/div&gt;&lt;%- partial('_partial/side') %&gt;&lt;%- partial('_partial/footer', null, &#123;cache: !config.relative_link&#125;) %&gt; 该部分作为豆瓣影评页在首页的载体； 亦可自定义其他非模板md文章的内容； 关于首页影评卡片，由于实现比较复杂会在近期单独写文章讲解。 基本思路： 利用 hexo-douban 插件的“已观看”部分（魔改），将缺失短影评且有长影评的记录补全，最后生成完整的豆瓣影评页面。然后利用 load 方法将影评页的部分内容（排序首位的影评）加载到首页中，最后再通过媒体查询完善一下样式自适应，即完成实现。 注意事项： 生成（hexo g）豆瓣页面时次数过多会导致IP被封禁，次日解禁（用代理/换IP可解）。另，可以不设置生成影评的部分为 built-in 模式，这样就不会影响原始正常的 hexo g 过程。 B站视频自适应样式Bilibili format1234 &lt;div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"&gt; &lt;iframe src=\"//player.bilibili.com/url\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; height: 100%; left: 0; top: 0;\"&gt;&lt;/iframe&gt;&lt;/div&gt; 首页新增近期动态幻灯片~\\Volantis\\themes\\volantis\\layout\\layout.ejs12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;%- partial('_partial/cover') %&gt;&lt;!-- 接续在cover后面... --&gt;&lt;% if (is_home()) &#123; %&gt;&lt;style&gt; @media(max-width: 1100px) &#123; .ctrl &#123;display:none; &#125; &#125;&lt;/style&gt;&lt;script type=\"text/javascript\"&gt; function changeFrameHeight()&#123; var iframe= document.getElementById(\"myiframe\"); var wid = document.getElementById('father').clientWidth; if (wid&gt;900) iframe.height = wid/2.2; else if (wid&gt;551) iframe.height = wid/2.22; else if (wid&gt;470) iframe.height = wid/2.175; else iframe.height = wid/2.26; &#125; window.οnresize=function()&#123; changeFrameHeight(); &#125;&lt;/script&gt;&lt;div class=\"ctrl1\"&gt; &lt;div class=\"l_body\"&gt; &lt;div class=\"body-wrapper\"&gt; &lt;div class=\"l_main\" style=\"width: calc(100%); padding-right: 0px;\"&gt; &lt;section class=\"post-list \"&gt; &lt;div class=\"post-wrapper\"&gt; &lt;article class=\"post white-box reveal \"&gt; &lt;div class=\"meta\" id=\"header-meta\"&gt; &lt;h2 class=\"title\"&gt; &lt;a href=\"/article/1c44.html\"&gt; 光影记忆////近期手机相机存货 &lt;/a&gt; &lt;/h2&gt; &lt;/div&gt; &lt;style&gt;iframe&#123;pointer-events: none;&#125;&lt;/style&gt; &lt;div style=\"background-color: rgb(0, 0, 0);padding-top: 7px;\" id=\"father\"&gt; &lt;iframe width=\"100%\" id=\"myiframe\" frameborder=\"0\" onload=\"changeFrameHeight()\" src=\"https://nas.cz5h.com:5443/photo/embed/embed.html?album=album_5745425f414c42554d&amp;autoplay=1&amp;lightbox=1\" photostation&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;/section&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;% &#125; %&gt; 恢复原来的标签网络原本是在旧站点挂的一个页面，更新主题后一直没恢复，现在重新搞了一下恢复了。具体思路和代码实现可以参见我之前写的以下文章。 纵览全局的文档组织模式（上） 纵览全局的文档组织模式（下） 该页面的最终效果： 文章页结束后加入评价参考自雪曦的文章哦。 步骤：去 widgetpack.com 完成注册，然后找到 Rating 在 Setting 中设置为 IP Address。 然后将 Install 中的代码块插入主题代码中。具体位置如下，不需要额外配置，代码中已经包含ID。 ~\\Volantis\\themes\\volantis\\layout\\_partial\\archive.ejs12345678910111213141516171819202122232425262728293031 &lt;% var sections = page.body ? page.body : theme.layout.on_page.body; %&gt;&lt;% sections.forEach(function(widget_id)&#123; %&gt; &lt;% if (widget_id == 'article') &#123; %&gt; &lt;article id=\"&lt;%= post.layout %&gt;\" class=\"post white-box &lt;%- theme.style.shadow.card ? 'card-shadow' : '' %&gt; &lt;%- theme.style.blur &amp;&amp; theme.style.blur.body ? 'body-blur' : '' %&gt; article-type-&lt;%= post.layout %&gt;\" itemscope itemprop=\"blogPost\"&gt; &lt;%- partial('meta', &#123;post: post, position: 'header'&#125;) %&gt; &lt;section class=\"article typo\"&gt; &lt;div class=\"article-entry\" itemprop=\"articleBody\"&gt; &lt;% (post.photos||[]).forEach(function(photo)&#123; %&gt; &lt;fancybox&gt;&lt;img src='&lt;%- url_for(photo) %&gt;'/&gt;&lt;/fancybox&gt; &lt;% &#125;) %&gt; &lt;%- post.content %&gt; &lt;!-- 五星评价代码，紧跟在Content之后 --&gt; &lt;center&gt;&lt;br&gt;&lt;br&gt; &lt;div id=\"wpac-rating\"&gt;&lt;/div&gt; &lt;script type=\"text/javascript\"&gt; wpac_init = window.wpac_init || []; wpac_init.push(&#123;widget: 'Rating', id: 24440&#125;); (function() &#123; if ('WIDGETPACK_LOADED' in window) return; WIDGETPACK_LOADED = true; var mc = document.createElement('script'); mc.type = 'text/javascript'; mc.async = true; mc.src = 'https://embed.widgetpack.com/widget.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling); &#125;)(); &lt;/script&gt; &lt;b&gt;&lt;a href=\"\" class=\"wpac-cr\"&gt;留下您对该文章的评价&lt;/a&gt;&lt;/b&gt; &lt;br&gt;&lt;br&gt;&lt;/center&gt; &lt;!-- 原内容继续... --&gt; 效果： 主题调整为灰色调主要修改一处主题样式，并且补充修改其他几个自定义页面。 12345678910111213&#x2F;* 在source\\less\\_base.less中添加 *&#x2F;html &#123; -webkit-filter: grayscale(100%) !important; -moz-filter: grayscale(100%) !important; -ms-filter: grayscale(100%) !important; -o-filter: grayscale(100%) !important; filter: grayscale(100%) !important;&#125;&#x2F;* 以下自定义页面中同样添加上述代码 *&#x2F;- theme\\volantis\\source\\csc\\index.html- theme\\volantis\\source\\monitor.html- theme\\volantis\\source\\maps.html- theme\\volantis\\source\\cv.html 效果： 修改主页下拉图标不居中 ~\\Volantis\\themes\\volantis\\source\\less\\_base.less12345678910111213141516171819202122232425262728293031323334.scroll-down&#123; width: 1.5%; // 100%; //height: 64px; position: absolute; bottom: 30px; // text-align: center; cursor: pointer; .scroll-down-effects&#123; color: white; font-size: 24px; line-height: 64px; position: absolute; text-shadow: 0 1px 2px rgba(0, 0, 0, .1); @keyframes scroll-down-effect&#123; 0%&#123; top: 0; opacity: .4; &#125; 50%&#123; top: -16px; opacity: 1; &#125; 100%&#123; top: 0; opacity: .4; &#125; &#125; animation: scroll-down-effect 1.3s infinite; transform: rotateX(20px); &#125; @media(max-width: 768px)&#123; width: 5%; &#125;&#125; 其他小修改：主页Cover背景只保留一个页面宽度，另外页面主体背景颜色改为和背景图片相连续的颜色。 添加单独的页面访问统计详见： 引入基于L-Cloud的页面访问统计 屏蔽访客插件点击以下插件（revolvermaps）是用来实时显示访客位置的，精度还可以，而且也比较直观，但唯一不足的是：任何人都可以点击插件区域进入后台IP界面（不带跳转是它的付费功能！） 巧妙的解决方式：遮罩屏蔽 新建个相同大小的div块，然后重叠放置在原插件上面（利用z-index），背景设置成透明即可。代码如下： ~\\volantis\\layout\\_widget\\qrcodeMap.ejs1234567891011 ... ...&lt;!--遮罩部分--&gt;&lt;div style=\"width:210px;height:143.5px;z-index:100;background:#fff;position:absolute;left:0;top:0;filter: alpha(opacity=0.01); -moz-opacity: 0.01;opacity: 0.01; \"&gt;&lt;/div&gt;... ...&lt;!--地图插件部分--&gt;&lt;div style=\"z-index:1;position:relative;display:inline-block;width:209px;\"&gt; &lt;!--原始插件代码--&gt; &lt;script type=\"text/javascript\" src=\"//rf.revolvermaps.com/0/0/7.js?i=51glt95x6ef&amp;amp;m=0c&amp;amp;c=ff0000&amp;amp;cr1=54ff00&amp;amp;crb1=54ff00&amp;amp;br=12&amp;amp;sx=0&amp;amp;rs=100&amp;amp;as=100&amp;amp;ds=0&amp;amp;cw=ffffff&amp;amp;cb=2196f3\" async=\"async\"&gt;&lt;/script&gt;&lt;/div&gt;... ... 内嵌页访问模式这是针对几个自定义页面祭出的访问方式，主要有几点考虑： 不跳出本站。某些网页非本站域名，且无法修改，正常情况下访问就会跳出网站； 页面访问量更准确。类似第三方监控这种页面，可以通过内嵌页方式将访问统计在本站访问量之内； 方便返回。跳出网站后极易流失访问者，内嵌方式可以强调返回功能。 实现方式：iframe 内嵌。包含页面： WEB / RESUME Page Analyse 全站点状态监控！ Logo增加文字: 使用的艺术字生成网站为 Zitiweb，此网站的优势是生成的字体可以选择透明背景。 修改相关文章的部分~\\volantis\\layout\\_widget\\related_posts.ejs12345678910111213141516 &lt;style&gt;.popular-posts&#123;margin-left: 21px !important;&#125;.popular-posts a&#123;padding-left: 0px !important;&#125;&lt;/style&gt;&lt;% if(page.layout == 'post' &amp;&amp; page.content &amp;&amp; page.path != undefined)&#123; %&gt; &lt;%- partial('_pre') %&gt; &lt;section style=\"background-color: #f2f2f2;padding-bottom:5px;\" class=\"widget &lt;%- item.class %&gt; &lt;%- page.widget_style %&gt; &lt;%- page.widget_platform %&gt;\"&gt; &lt;header style=\"background-color: #f2f2f2;padding-left: 5px; display:inline-block !important;\"&gt; &lt;i class=\"fas fa-bookmark fa-fw\" aria-hidden=\"true\"&gt;&lt;/i&gt;&lt;span class=\"name\"&gt;相关文章&lt;/span&gt; &lt;/header&gt; &lt;div class=\"content\" style=\"background-color: #f2f2f2;margin-top: 0px;\"&gt; &lt;%- popular_posts( &#123; maxCount: item.max_count , ulClass: 'popular-posts' , PPMixingRate: 0.2 &#125; , page ) %&gt; &lt;/div&gt; &lt;/section&gt;&lt;% &#125; %&gt; 更新主题版本1.7.4未完全修改原主题（1.7.4）中的以下更新： update1 update2 因为该更新去除了下拉页面时顶栏的翻转变换，我觉得这功能挺好的，故保留之。 自定义Footer样式和信息在这一次版本更新中，作者重写了Footer，由于我也在自定义修改Footer，所以就只修改了部分内容，没有更新作者的配置文件修改等内容。以下是老版本和新版本混合的结果，只是为了保留新版本的aplayer代码。 注意：如无必要一定要将上述更新全部完成（theme.aplayer.enable改为theme.footer.aplayer.enable），否则会导致aplayer出现问题。 ~\\volantis\\layout\\_partial\\footer.ejs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384 &lt;% if (theme.footer) &#123; %&gt; &lt;% var layout = theme.footer.layout; if (config.theme_config &amp;&amp; config.theme_config.footer &amp;&amp; config.theme_config.footer.layout) &#123; layout = config.theme_config.footer.layout; &#125; %&gt; &lt;footer class=\"clearfix &lt;%- theme.backstretch &amp;&amp; theme.backstretch.is_dark ? 'white' : '' %&gt;\"&gt; &lt;br&gt;&lt;br&gt; &lt;!---硬编码--&gt; &lt;div class=\"social-wrapper\"&gt; &lt;!--in--&gt;&lt;a style=\"margin-right:15px;margin-top:2.5px;\" href='https://cn.linkedin.com/in/tianzonglin' &gt;&lt;img width=\"20px\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAABmJLR0QA/wD/AP+gvaeTAAAB6klEQVRoge2YP08bMRyGn19p1aVKCkIdUYf2I3SsGKkECFakrowdgA/QhaJO+SCdkKDZyj8x9hN0KRNC/UMRyYh4GWAIF98l9iXOlfiRbjj7tf2+57N9CSQSiSAkLUk6kNTW6GlL2pe06Bvi82h9F/LJ5dlcMwFsh81jNBbNbLez4JFDtBbJTBnWswWuGWkBz6LYCadlZrXOAlcQxfMTjpnd8+56tf5LxjbIGbACTN9d74HfgzY1EHrs4XMO/dKwDw4XWR++i71uZpcZfR24CHloZSi72GcdZW/D7QwO3xn5A2wAzbv7OaABvBi8tWKyM5LOkaoxnkEsB0/NMbdn0QzwFKgBb4CPwN9ycTrw2btdbQo0vyS96zH2c0nNkHMkZpBXPQe/1T2WdOQbxGvXyr5GrjZ5Gh8kvQR+AE/69TKSxS5poqjezE6ALz59Rg0i6YOkU+BK0k9JqwXyZkFdX4MNa40s53S7kKN/HeIlRpDDnG4PcvR1Hy/RFrukc2DSUfXPzKYcegOu+/USM0ipfnu1Gc9PlCqTglSNFKRqpCBV48EESf+iVA1XkHZ0F/5cZgtcQb5HMFKWLo+uII0IRsrS5bEriJntAFtR7ISxaWZf+1ZLWpC0J6lV9EstEi1J3yTND/EBJRIPmhu/Z0SR8AyW3wAAAABJRU5ErkJggg==\"&gt;&lt;/a&gt; &lt;!--github--&gt;&lt;a style=\"margin-right:15px;margin-top:0.5px;\" href='https://github.com/TianZonglin' &gt;&lt;img width=\"22.5px\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAABmJLR0QA/wD/AP+gvaeTAAAEF0lEQVRoge2Zy29VVRTGfwvKQxQMEqAoBqMBhYkhiJHgIw6M4iNxYsSJRolBRR0aTY2JE+M/4BgnTgwJE4gBiSEaRWqQRCuJoE6Qtlhp1dry7udgn3s5Pd3rsM+9t9eS3C9pbrruenzf3ufuvfbZ0EEHHXRwLcHKvpR0FBgB9gK7zey3trC6Uv8O4GngCWCxma1vJMlGTcZlSXskPSKpdACagSTLauzNauaxoZGE78tHn6StrRSUCdgq6aeSuu81kvirkoQ1HJZ0XwtEbJLUm1DvoJcjOqLZSP8FLErgMQF8BPSY2aikbmANsBBYkH0CjALj2edxMxuUtBD4AHgNmJVQa8TMbqoi5Bbg94TEeQwBXcDiRP9h4DKwtGKdbjM7XTR2Oc5rKyaH6oSiI5uAtcAUId50LmuwSDsQHTBPyHXTSKRZzI8ZPSHeIzcTMCdm9IT8MY1EmsWfMaMnZMs0EmkWTyZ5SXo+YWP6v7GtyNsKIm4GThA2spmMMWC1mQ3UDMVH611mvgiA64GevKE+I5KWAP3A3DaTahQXgBVmNgyTZ+Qxrh0RELg+WvsnL+Tx9nNpGnXO+Y3vropJLhJ6nguETeoG0hvGGkaAf7Ncc4HlOBueg3pPmBeS0l/9TWjZPwWOmdnFooNCa76A8INcxJV25yzwD2HFGTez0UjsHGAd8Cywg6sfI5ZPsUg6f5W1+4ikVQliWwJJq7KaZTgXCxwrCTghqWqb3goxyyT9UsJrLBY0WBLwYLtF5Hg9VMKrPxZw3HE+UFLkVUmns0HYXpGgSeqRdDL7e0fOywxJXzjcfo45f+4473CSPyBpIuc3Ien+CkJeitR6wfF9w+G2r+aT30f6nJo/OvY3mdyrWWZLRWwGX6nIoc45L+QHx3nYsW9MtHlYE7Hd6fiecex1znkh+wFFnL1j79Q1HLod3xhip9B5jm+MwwSBM5ATYmangO8iASuc5FPeZACDjm8MMd/o6c/h0FvWxu+MBNztJI+Jjtk8HIzYDju+MQ4xrgGS5kvqL6wMhxzfZletzYV4SXrK8f224HdKkvcY1oO2RZa5ex3f7Qp7yICkl1NF5OJflzSksBe95fgUbwUk6cWU5Cbps0Jgr0JD11ZI6pJ0qMBlT5UESyQdKyT4RFLb3ndJmi3p4wKHPknVjgqSbo2I+VLS6mninq99m6QDERErG014o6TdhYTns9nZ0soZkjRL0sOSdkoaL9TcJan0bJJ04yTpGeBD4PbCV2eAI8D3hHahn/CWcohw6jtnZmezHPMIB67ZhBfRSwn7wzpgA3APUzfZX4G3zWxXCs8kZD+85yTtl3RJ04dLkvYpXMMlz3hDd4DZNG8CNgPrgZWE0a0dc1NQu70aIFwqHQW+Br6JHYM76KCDDgD4D6z9hVuzmv/gAAAAAElFTkSuQmCC\"&gt;&lt;/a&gt; &lt;!--ins--&gt;&lt;a style=\"margin-right:0;\" href='https://www.instagram.com/tzloop/' &gt;&lt;img width=\"24.3px\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAABmJLR0QA/wD/AP+gvaeTAAADgUlEQVRoge2Zz28WRRjHP0PFkIhWOUjgKIiJsS3wpokVjQcxJNwqxkv/A/8Clav4f9g0Xkg0ETVCo6AXf1AxWhMqKje1cPHHpaZN+XiYrdksu/vu7L5vi8n7TTbZ2ZnvPM93ZmfmmRkYYYQRRvg/IaQS1ABMA6eBx4BHgbGOfmwCt4GbwIfAUgjBjnWWQw3qK+oNh48V9eWs0QYqYq/63jYIKOID9aFBiXhQ/WYHRGxhSd3bz89dfUQEYAE4PpBWaYZLwGvAYpbuAfN2+c2MY2I7cbFgfzGXd6bO18oeyVrgzdat0A6fFtKf5N7P1fVK3a81DTzexasWeGHrJXP6ZC7vCeJvVor7aio93dGpVWAZ+D1LHwAmgf01nBfVRWJPnCQnLOfTUpIX6nyLf3wz482od/W2ukt9Rl3Iyqbi7SQRmdFLiUauq1MJ9R9Tf0y0cbGqvroxsjtB9xVgJoTwXVNCCOFb4GngswQ791dl1K4jDbECzIYQ/ixmqONqL3vGi/khhD+AWeDGAPwoh3q5QVdvqkdLuFPG8GIjV3ZDvaBOlpTv2WzMXB6WkPkS3hl1rYazpr5UwlvYSSEzBc5UHxF5MRMF7okuQrqMkVXgq8K3c8CeBtw9wFuFb18At9o600XIcgjhzlZCfRg4lcA/ZW4CyOpabutMFyG/FdKHqY8UitgNHOpTZ2N0EVIM4NpsTYuc1qF6FyEHCumfgY0E/gbwS+HbwbbOdBEyaS6eCiH8BVSGECX4OITw91ZCHQMmasrXoouQ/cQQI4+zwD8NuGvAG4VvM8QTmVboGqK8mk+EEL4H5qgXswbMhRB+qKtrYGi4IG6qx0q4E8ZwZD1Xdl19X32qpPy0eqfLgthViMZQ/JGKOsbV49lTeqyj7lN/amhrqEJUr1SJ6WNjn/p5gp1WIUrKVPo88KXa+NhInSaGOM8l2FmvyqgTsppgAOAIcNUYxZ6wfKs7pj6rvkMUcTjRRqVPdSHFzUQjEBtmLntuq8vAr8QV+yBxnWg9xXL3AvofKkOCrOu/7mB0GOiFEK6VZdQJCcRt7JFheZWIFeDJquuGyjGSEc4Oy6sWeL31nYnxTuTdhOlxWDjfuRmMdyNLOyjiqvpAZyE5MTvRM+cHJiInJhhPSFa2QcB1dTbFv7aXoT3igfIhYjg/iMvQW8TN2UfAtaFdho4wwggj3BP4F8SQMYOYcy12AAAAAElFTkSuQmCC\"&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class=\"social-wrapper\"&gt; &lt;!--图虫--&gt;&lt;a style=\"margin-right:15px;margin-top:1.5px;\" href='https://tuchong.com/5634772/'&gt;&lt;img height=\"17px\" src=\"https://s1.tuchong.com/content-image/201904/af8f36bc5c647dfd3007907d17d4e053.png\"&gt;&lt;/a&gt; &lt;!--专栏--&gt;&lt;a style=\"margin-right:15px;margin-top:1.9px;\" href='https://cloud.tencent.com/developer/column/79519'&gt;&lt;img height=\"17px\" src=\"https://cdn.jsdelivr.net/gh/TianZonglin/tuchuang/img/yunjia.png\"&gt;&lt;/a&gt; &lt;!--zhihu--&gt;&lt;a style=\"margin-right:16px;margin-top:1.8px;\" href='https://www.zhihu.com/people/tian-zong-lin-82'&gt;&lt;img height=\"18px\" src=\"https://cdn.jsdelivr.net/gh/TianZonglin/tuchuang/img/zhihu.svg\"&gt;&lt;/a&gt; &lt;a style=\"font-size:17px;margin-top:3px;\" href=\"/atom.xml\" class=\"fas fa-rss\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"&gt; &lt;/a&gt; &lt;/div&gt; &lt;br/&gt; &lt;div class=\"social-wrapper\"&gt; &lt;img width=\"20px\" height=\"12.89px\" style=\"border-radius:1.5px;\" src=\"https://cdn.jsdelivr.net/gh/TianZonglin/tuchuang/img/20200305230433.png\"/&gt; &lt;/div&gt; &lt;!---end--&gt; &lt;!---这里只留Aplyer--&gt; &lt;% layout.forEach(function(item)&#123; %&gt; &lt;% if (item == 'aplayer') &#123; %&gt; &lt;div class=\"aplayer-container\"&gt; &lt;%- partial('../_third-party/aplayer', &#123;post: null, where: 'footer'&#125;) %&gt; &lt;/div&gt; &lt;% &#125;%&gt; &lt;%&#125;) %&gt; &lt;!---end--&gt; &lt;!---保留老的footer调用--&gt; &lt;div&gt; &lt;%- __('footer.use') %&gt; &lt;%- __('footer.theme') %&gt; &lt;%- __('symbol.period') %&gt; &lt;/div&gt; &lt;div&gt; &lt;%- markdown(__('footer.license')) %&gt; &lt;/div&gt; &lt;script type=\"text/javascript\"&gt; document.write(unescape(\"%3Cspan id='cnzz_stat_icon_1278641089'%3E%3C/span%3E%3Cscript src='https://s4.cnzz.com/z_stat.php%3Fid%3D1278641089%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E\")); &lt;/script&gt; &lt;div class='copyright'&gt; &lt;br&gt; &lt;span&gt;本站已运行&lt;span id=\"showDays\"&gt;&lt;/span&gt;&lt;/span&gt; &lt;script&gt; var seconds = 1000; var minutes = seconds * 60; var hours = minutes * 60; var days = hours * 24; var years = days * 365; var birthDay = Date.UTC(2020,02,27,14,00,00); // 这里设置建站时间 setInterval(function() &#123; var today = new Date(); var todayYear = today.getFullYear(); var todayMonth = today.getMonth()+1; var todayDate = today.getDate(); var todayHour = today.getHours(); var todayMinute = today.getMinutes(); var todaySecond = today.getSeconds(); var now = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond); var diff = now - birthDay; var diffYears = Math.floor(diff/years); var diffDays = Math.floor((diff/days)-diffYears*365); var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours); var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes); var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds); document.getElementById('showDays').innerHTML=\"\"+diffYears+\"年\"+diffDays+\"天\"+diffHours+\"小时\"+diffMinutes+\"分钟\"+diffSeconds+\"秒\"; &#125;, 1000); &lt;/script&gt; &lt;%- markdown(theme.copyright) %&gt; &lt;br&gt;&lt;br&gt; &lt;/div&gt; &lt;!---end--&gt; &lt;/footer&gt;&lt;% &#125; %&gt;&lt;script&gt;setLoadingBarProgress(80);&lt;/script&gt; 除上述之外样式同样要修改： 修改位置： ~\\volantis\\source\\less\\_article.less (2 hits) Line 259: footer { ~\\volantis\\source\\less\\_footer.less (1 hit) Line 1: footer {~\\volantis\\source\\less\\_article.less12345678 footer &#123; padding: 0; text-align: justify; color: #fff;//修改 font-style: italic; margin: .6em 0; cite&#123; ~\\volantis\\source\\less\\_footer.less12345678910111213141516171819202122232425 footer &#123; position: relative; padding: 40px 10px 25px 10px; width: 100%; color: #fff;//修改 margin: 0px auto; font-size: 14px;//修改 overflow: hidden; text-align: center; ~ ~ a &#123; color: #fff;//改动 padding: 0; .enable-trans(); &amp;:hover &#123; color: @color_text_highlight; &#125; ~ ~ .copyright &#123; margin-top: @gap; font-size: 14px;//修改大小 &#125; Blogger模块也添加头部~\\volantis\\_config.yml123456789101112131415 widget: # --------------------------------------- # 博主信息小部件配置 - id: blogger class: blogger display: [desktop] # [desktop, mobile] avatar: https://cdn.jsdelivr.net/gh/TianZonglin/tuchuang/img/oiuydtsrk.gif header: icon: fas fa-user-circle title: 关于博主 # title: 关于博主 # subtitle: 副标题 jinrishici: false # 今日诗词。可以设置字符串，加载失败时会显示占位字符串。 social: false 之后，去模块源文件内插入代码。 ~\\volantis\\layout\\_widget\\blogger.ejs12345678 &lt;%- partial('_pre') %&gt; &lt;section class=\"widget &lt;%- item.class %&gt; &lt;%- page.widget_style %&gt; &lt;%- page.widget_platform %&gt;\"&gt; ---- 添加头部 ---- &lt;%- partial('header', &#123;item: item&#125;) %&gt; &lt;div class='content'&gt; ~ ~ Blogger图不必非得是方形位置： ~\\volantis\\source\\less\\_side.less (1 hit) Line 124: &amp;.blogger{~\\volantis\\source\\less\\_side.less12345678910111213141516 &amp;.blogger&#123; .enable-trans(); .content&#123; padding: 0; div.avatar&#123; display: flex; justify-content: center; &#125; img&#123; padding: 0; margin: 0; display: flex; justify-content: center; width: @width_sidebar; //height: @width_sidebar;改动 修改页头Logo的样式现在页头采用自带logo。 ~\\volantis\\source\\less\\_header.less123456789101112131415161718192021 .logo &#123; //padding: 0 @gap*1.5;//改动 font-size: @fontsize_logo; font-family: @fontfamily_logo; @media(max-width: @on_phone)&#123; padding: 0 @gap; &#125; &amp;.img&#123; padding: 0 @gap 0 0; &#125; img&#123; height: 60%;//改动 &#125; letter-spacing: 0; &#125; img.logo &#123; padding: 0px 20px 0px 0px;//改动 margin-top: 13px;margin-right: 20px; margin-left: -5px;//改动 &#125; 修改页头下边缘阴影配合不透明白色，显得更加立体。给人更快速的加载的感觉。 ~\\volantis\\source\\less\\_header.less1234567891011121314151617181920212223242526 .l_header &#123; @iconW: 32px; @iconMargin: 4px; position: fixed; z-index: 9999; top: 0; overflow: hidden; width: 100%; font-size: @fontsize_base; line-height: @height_navbar; height: @height_navbar; overflow: hidden; font-family: @fontfamily_base; padding: 0 @gap; margin-bottom: @gap; box-shadow: -9px 3px 2px 0 rgba(0,0,0,0.1);//改动 .wrapper&#123; padding: auto @gap; max-width: @width_container; margin: auto; .enable-trans(); a.logo &#123; color: @color_text_header; &#125; &#125; 底部上线PV统计使用CNZZ的统计。 修改位置： ~\\volantis\\source\\less\\_header.less (3 hits) Line 89: .nav-sub{~\\volantis\\source\\less\\_header.less12345678910111213 .nav-sub&#123; .logo &#123; font-weight: 600; ///////////////////////////// padding: 0 @gap*1.5; font-size: @fontsize_base; font-family: @fontfamily_base; @media(max-width: @on_phone)&#123; letter-spacing: -0.5px; padding-top: 1px; &#125; &#125; 页头搜索框样式修改透明度和颜色。 修改位置： ~\\volantis\\source\\less\\_header.less (3 hits) Line 176: .m_search {~\\volantis\\source\\less\\_header.less123456789101112131415161718192021222324252627282930313233343536 .input &#123; display: block; font-size: @fontsize_small; padding-top: 8px; padding-bottom: 8px; width: 100%; color: @color_text_main; background: rgba(230, 230, 230, 0.72);//改动 box-shadow: none; box-sizing: border-box; padding-left: @iconW + @iconMargin; @media(max-width: @on_phone)&#123; padding-left: @iconW + @iconMargin; &#125; border-radius: @border_radius_searchbar; border: 1px dashed transparent; .set-placeholder(&#123; color: fade(@color_text_main, 50%); &#125;); &amp;:hover&#123; color: @color_text_in_header; ~.icon&#123; color: fade(@theme_main, 80%); &#125; border: 1px solid fade(@theme_main, 60%); &#125; &amp;:focus &#123; ~.icon&#123; color: @theme_main; &#125; color: @color_text_main; background: fade(@theme_main, 15%); border: 1px solid @theme_main; &#125;&#125; 代码显示去掉行号修改位置： ~\\volantis\\source\\less\\_article.less (1 hit) Line 624: .gutter{~\\volantis\\source\\less\\_article.less123456789101112131415 .gutter&#123; width: 24px; padding: 0 12px;display: none; /////////////////////////////////////// text-align: right; border-width: 0; margin-left: 0; position: sticky; left: 0; background-color: darken(@color_bg_code_block, 5%); pre&#123; color: fade(@color_text_main, 80%); &#125; &#125; 修改目录缩进目录在从三级开始显示时，下一级会空格太多。 修改位置： ~\\volantis\\source\\less\\_toc.less (4 hits) Line 71: .toc-item.toc-level-1 { Line 77: .toc-item.toc-level-2 { Line 83: .toc-item.toc-level-3 { Line 89: .toc-item.toc-level-4 {~\\volantis\\source\\less\\_toc.less12345678910111213141516171819202122232425 .toc-item.toc-level-1 &#123; .toc-child a&#123; padding-left: 0px;//改动 font-weight: normal; &#125;&#125;.toc-item.toc-level-2 &#123; .toc-child a&#123; padding-left: 0px;//改动 font-weight: normal; &#125;&#125;.toc-item.toc-level-3 &#123; .toc-child a&#123; padding-left: 11px;//改动 font-weight: normal; &#125;&#125;.toc-item.toc-level-4 &#123; .toc-child a&#123; padding-left: 23.8px;//改动 font-weight: normal; &#125;&#125; 修改阅读更多按钮默认的感觉 padding 有点大，不协调。 修改位置： ~\\volantis\\source\\less\\_article.less (1 hit) Line 303: .readmore {~\\volantis\\source\\less\\_article.less123456789101112131415161718192021222324.readmore &#123; // display: none; // howtodesign? font-family: @fontfamily_base; font-size: .8em; letter-spacing: .1em; margin-top: @gap; a &#123; text-decoration: none; display: inline-block; vertical-align: middle; line-height: 1.8rem; //修改 font-weight: bold; background-color: @theme_main; padding: .2em .8em; //修改 color: @color_text_in_header; border-radius: @border_radius/2; .enable-trans(); &amp;:hover &#123; background: darken(@theme_main, 10%); &#125; &amp;:active &#123; // background: darken(@theme_main, 20%); &#125; &#125; Blogger组件样式修改修改圆角的程度。 修改位置： ~\\volantis\\source\\less\\_side.less (1 hit) Line 122: &amp;.blogger{ ~\\volantis\\source\\less\\_side.less (1 hit) Line 17: .widget{~\\themes\\volantis\\source\\less\\_article.less123456789101112 .content&#123; background-color:white; text-align: justify; padding: @gap/2 0; max-height: ~\"calc(100% - @&#123;height_navbar&#125; - 12.5 * @&#123;gap&#125;)\"; max-width: 100%; border-bottom-right-radius: 8px; //修改 border-bottom-left-radius: 8px; //修改 ~ ~&#125; Tab键入的code块样式修改位置： ~\\themes\\volantis\\source\\less\\_article.less (4 hits) Line 1: .article { Line 337: &amp;.article-tags { Line 723: .article { Line 858: .article-entry{~\\themes\\volantis\\source\\less\\_article.less1234567891011121314151617181920212223242526272829303132 code &#123; //Line289 font-family: @fontfamily_code; padding: 3px 3px 0px 3px; margin: 0px 1px; vertical-align: center; border-radius: 2px; border: 1px solid fade(@color_bg_code, 20%); font-size: @fontsize_base * .8; background: @color_bg_code; color: @color_text_code; @media(max-width: @on_phone)&#123; font-size: @fontsize_base * .8 * .95; &#125;&#125;~~@media (max-width: 580px).article code &#123; font-size: 13.4px;&#125;.article code &#123; font-family: Menlo,Monaco,courier,monospace,\"Lucida Console\",'Source Code Pro',\"Microsoft YaHei\",Helvetica,Arial,sans-serif,'Ubuntu'; padding: 3px 3px 0px 3px; margin: 0px 1px; vertical-align: center; border-radius: 2px; border: 1px solid rgba(255,248,240,0.2); font-size: 14px; /* background: #FFF8F0; *//改动 color: 0054be;&#125; 修改横向滚动条的样式位置： ~\\volantis\\source\\less\\_article.less (1 hit) Line 711: &amp;::-webkit-scrollbar-thumb { ~\\volantis\\source\\less\\_article.less (1 hit) Line 704: &amp;::-webkit-scrollbar-track-piece {~\\volantis\\source\\less\\_article.less123456789101112131415161718192021222324 &amp;::-webkit-scrollbar &#123; height: @border_radius_line; width: 3px; ////原始变量：@border_radius_line;&#125; ::-webkit-scrollbar-thumb &#123; background-color: #e1e1e1;&#125;::-webkit-scrollbar-track &#123; background-color: #f3f3f3;&#125;html&#123; ::-webkit-scrollbar &#123; //改动 -webkit-appearance: none !important; width: 3px !important; height: 3px !important; &#125; ::-webkit-scrollbar-thumb &#123;//改动 background-color: #aeaeae !important; &#125; ::-webkit-scrollbar-track &#123;//改动 background-color: black !important; &#125;&#125; 右侧添加地图小插件我是直接借用的 qrcode 插件的代码。直接修改以下文件： ~\\volantis\\layout\\_widget\\qrcode.ejs123456789101112&lt;%- partial('_pre') %&gt;&lt;section class=\"widget &lt;%- item.class %&gt; &lt;%- page.widget_style %&gt; &lt;%- page.widget_platform %&gt;\"&gt; &lt;%- partial('header', &#123;item: item&#125;) %&gt; &lt;div style='padding:5 5 5 5;height:104.5px;'&gt; ---- 添加地图插件 ---- ---- 注意调整宽度 ---- &lt;div style=\"display:inline-block;width:209px;\"&gt; &lt;script type=\"text/javascript\" src=\"//rf.revolvermaps.com/0/0/7.js?i=51glt95x6ef&amp;amp;m=0c&amp;amp;c=ff0000&amp;amp;cr1=54ff00&amp;amp;crb1=54ff00&amp;amp;br=12&amp;amp;sx=0&amp;amp;rs=100&amp;amp;as=100&amp;amp;ds=0&amp;amp;cw=ffffff&amp;amp;cb=2196f3\" async=\"async\"&gt;&lt;/script&gt; &lt;/div&gt; &lt;/div&gt;&lt;/section&gt; 以下是旧版博客的日志如需参考，请移步 old.cz5h.com。 搭建清单 框架：HEXO主题：Indigo模板：EJS托管：Github Pages图床：阿里云OSS(作业部落)、微博相册文档：Google Docs文件：Nginx(Local Server)云盘：可道云(Synology NAS)订阅：FeedEK弃用：APlayer、七牛云 创建记录(根目录) 1234567891011121314151617181920myblog\\source\\CNAME 域名映射myblog\\source\\robots.txt 百度、谷歌的爬虫规则myblog\\source\\plugin-shake\\harlem-shake-style.css 本地化资源文件myblog\\source\\categories\\index 开启后自动生成myblog\\source\\tags\\index 开启后自动生成myblog\\source\\pdf\\... 本地化文章显示的pdfmyblog\\source\\other\\page_log 建站日志页面myblog\\source\\other\\page_share 图片收藏页面myblog\\source\\other\\page_gif 动态图页面myblog\\source\\other\\page_bground 壁纸页面myblog\\source\\other\\page_comment 全站评论页面myblog\\source\\torrent\\neubt BT资源文件分享页面myblog\\source\\feedek\\feedread 显示订阅内容页面myblog\\source\\videos\\look 视频资源左右滑动展示indigo\\source\\js\\graph.js 获取Json数据并构造svg元素indigo\\source\\js\\barry.js 简单的拦截某些页面的访问indigo\\source\\js\\av-min.js 本地化indigo\\source\\js\\Valin.js 本地化indigo\\layout\\_partial\\post\\wordcount.ejs 字数统计 文件改动记录 12345678910111213141516171819202122232425262728293031323334353637383940indigo\\_config.yaml 配置文件|内置功能的配置indigo\\source\\img\\... 资源文件|头像和打赏图片等indigo\\layout\\_partial\\menu.ejs 左侧面板|添加各功能入口indigo\\layout\\_partial\\footer.ejs 底部样式|添加 Host byindigo\\layout\\_partial\\plugins\\site-visit 站点访问|去除数量显示indigo\\layout\\_partial\\plugins\\page-visit 页面访问|去除数量显示indigo\\layout\\_partial\\plugins\\google-analytics.ejs 谷歌分析|加入解析路径indigo\\layout\\_partial\\plugins\\baidu.ejs 百度统计|添加自定义匹配项indigo\\layout\\tag.ejs 标签显示|修改样式indigo\\source\\css\\style.less 主要样式|修改背景纹理indigo\\layout\\_partial\\post\\toc.ejs 构造目录|修改模块透明度及边框indigo\\layout\\_partial\\post\\nav.ejs 前进后退|修改模块透明度及边框indigo\\layout\\_partial\\plugins\\gitment.ejs 评论模块|修改模块透明度及边框indigo\\source\\css\\_partial\\article.less 文章样式|修改目录焦点变化的跟随高度indigo\\layout\\_partial\\script.ejs 全局添加|添加自定义JS或者CSSindigo\\layout\\page.ejs 文章页面|修改底部ABOUT模块内容indigo\\layout\\post\\copyright.ejs 文章页面|自定义底部标注模块内容indigo\\layout\\_partial\\header.ejs 顶层浮栏|下滑后修改标题居中,添加头像indigo\\layout\\post\\copyright.ejs 文章页面|自定义底部标注模块内容indigo\\layout\\_partial\\header.ejs 顶层浮栏|下滑后修改标题居中,添加头像indigo\\layout\\_partial\\plugins\\valine.ejs 评论模块|新添加Valine评论indigo\\layout\\_partial\\post\\comment.ejs 评论模块|接入扩展的Valine插件indigo\\source\\css\\_partial\\article.less 文章样式|修改最后的评论模块的Section风格indigo\\layout\\_partial\\head.less 全局头部|全部页面&lt;head&gt;添加GoogleAdsenseindigo\\layout\\page.ejs 自定页面|Pages添加底部about模块的控制变量indigo\\layout\\_partial\\menu.ejs 左侧面板|在_config中添加标签云控制变量indigo\\source\\css\\_partial\\highlight.less 代码模块|调整代码区域字体大小行间距indigo\\source\\css\\_partial\\variable.less 代码字体|更换代码块字体@font-codeindigo\\layout\\_partial\\archive.ejs 归档分类|去掉两部分显示时的tagsindigo\\layout\\_partial\\head.ejs 首页结构|规范首页meta信息及整体代码结构indigo\\source\\css\\_partial\\article.less 文章样式|修改image-bubble(-35-&gt;0)indigo\\layout\\_partial\\head.ejs 页面头部|在加载页面前加载barry.jsindigo\\source\\css\\_partial\\article.less 标注词组|修改词组标注样式indigo\\source\\css\\_partial\\highlight.less 代码模块|修改代码段样式，pre和gutterindigo\\layout\\_partial\\head.ejs 全局头部|添加自定义Js和Css代码段indigo\\layout\\_partial\\menu.ejs 全局音乐|利用frameset和catch检测跳转实现indigo\\layout\\index.ejs 主页列表|添加首项为已有全部评论 2月27日换新主题 Volantis。 2020年2月12月3日头像下方添加个人社交信息 2019年12月7月12日更新友链页面的样式 7月11日侧边栏添加多个功能页，整合 NAS和自己服务器的应用 7月2日更换新的.COM域名 2019年7月5月10日去除音乐，视频、下载页面失效（七牛云到期） 2019年5月6月21日利用多frame添加全局音乐的播放，暂不清楚对SEO的影响 6月20日添加视频播放页面，未适配移动端 6月16日使用LeanCloud实现分享下载页的下载计数 6月10日利用Hexo原生API添加生成相关文章 2018年6月5月21日添加字数统计，本地化Valine 5月18日对文件压缩上传，参见minifier，加速动画效果 5月17日添加rss加载gif及页面的简单访问拦截 5月16日去掉音乐，新增我的订阅和代码挂件 5月12日现在用QSunSync来完成本地资源云同步 2018年5月5月11日在资源分享栏添加BT资源分享页面 4月21日添加 Google 广告支持，添加留言中心栏目 4月20日停用gitment评论，新增valine评论支持，详见这里 4月15日修改侧边栏项目，汇总相关文档，添加资源分享栏目 2018年4月12月27日引入JQuery和自定义CSS代码，未发现与nodejs的兼容问题 12月19日本页面添加文件修改记录；改动了各模块的显示样式 12月15日功能异常，修复未果，故进行了一次主题重置，详见这里 12月10日添加背景纹理，美化留白 12月4日侧边栏添加图片收藏栏目，使用七牛云做图床 2017年12月11月20日侧边栏整合标签(书签)云插件，删掉原有标签选项 11月18日动次打次添加切歌，使用七牛云做音频存储 11月17日SEO优化，开启百度爬虫推送，使用google webmaster 11月7日添加Google Analysis，添加百度统计 11月4日更换托管源，托管在Coding 11月3日文章整理完毕，做第一次发布 2017年11月10月29日侧边栏添加建站日志 10月27日添加gitment评论，添加rss等基础功能 10月26日更换indigo主题，添加high一下功能 10月24日托管在Github并绑定域名，站点正式上线 10月23日站点搭建完毕，域名申请完成 10月20日正式接触到Hexo，着手搭建站点 2017年10月"},{"title":"关于（ABOUT）","date":"2020-02-26T23:00:00.000Z","updated":"2020-12-31T03:45:29.066Z","comments":true,"path":"links/about.html","permalink":"https://www.cz5h.com/links/about.html","excerpt":"","text":"事 不 三 思 终 有 悔， 人 能 百 忍 自 无 忧。 关于名称域名：CZ5H.COM 寓意“长征五号”，朗朗上口，易于记忆。长征系列运载火箭以“长征”命名，意为突出研发过程的艰辛，及整个系列对中国航天的重大意义。而长征五号，又是其中的重要一环，意义非比寻常，是我们每个关心国家航天事业的人都应该铭记的功勋火箭。 站名：长征部落格 寓意“我的长征”，长征二字取自毛体。对于我们每个人，又何尝不是行走在自己的“长征之路”上，个中曲折只有自己知道。这里就是我的记事本、我的记忆仓库，是我的长征路上的歇脚石，亦是各位路上的好风景（但愿如此）。取名“部落格”，则有复古之意，又可以在搜索引擎中独立于宽泛的内容。两者合并，即突出主题又包含寓意，同时也在搜索引擎中有一定区分度，实属难得。 以上就是对本站名称的一些解释，如有建议，还请留下您的宝贵评价。 .userK > a{ pointer-events:none; } .userK > div{ width:auto !important; } .userK img{ border-radius: 0px !important; height: 37px !important; width: 37px !important; } .userK > div{ float:left !important; border:0px solid #bbb !important; margin:1px !important; height:37px !important; font-size: 9px !important; } .userK td{ height:37px !important; } .userK > table{ margin-top:0px !important; } .userK td:first-child{ width: 37px !important; } .userK div > table{ width:198px !important; height:37px !important; } .userK tr > td{ font-size: 9px !important; } .userK table { margin-top: 0em !important; margin-bottom: 0em !important; } 关于本站博客之于我： 是平台，发泄自己的愤青言论、分享自己的日常动态； 是窗口，获取更广范围内的社交反馈； 也是回忆录，文章如流水，时光如流水，思绪如流水，一去不复返，我们能做的就是尽量靠近它们（思考），然后薅它们一把毛（记录）下来，也不枉那些我们付出的努力、激情和汗水。 我之于博客： 我无意在此对个人博客究竟应该是什么样子展开长篇大论，只能说适合自己的就是最好的，你可以重分享、也可以重评价、也可以重文学创作，前提是，（对应地）你是个外放社交、注重体验、热爱协作的人，在这方面能够匹配过后，个人觉得已经能够算是个成功的个人博客了。 剩下的所谓主题、SEO等等，都是细枝末节而已（当然技术分享类的完全可以深入），不要被浮云遮蔽了背后那灿烂的阳光，不要让这些技术性的东西影响了你输出的热情。 以上即是我对博客的看法，也是此博客已经存活两年的原因，只要我还活着，我就会有生活体验，我就可以输出文字，我就可以将此博客维继下去，正如手游部落冲突（Clash of Clans）的成功：游戏模式固化好，剩下的交给时间 —— 七年的老玩家（我）如是说。 关于本人维基赛高！ 这个用户喜欢维基百科。 Wiki-0这个用户不懂维基代码，或只用可视化编辑器。 伪基 该用户认为伪基百科是垃圾。 Baidu百度该用户反对维基百度化。 抄你妹这个用户反对百度百科抄袭维基百科。 这个用户信奉睡教。 这个用户是无神论者。 该用户支持无车运动 这个用户不吸烟。 HUAWEI这个用户使用华为的产品。 这个用户使用索尼出品的数码相机。这个用户使用小米的产品。 1.048596%这一切都是命运石之门的选择！ 这个用户很宅。 此用户反对以棱镜计划为代表的各种大规模政府监听项目，认为其严重威胁公民自由。老大哥在看着你！ 狗这个用户属狗。 小学这个用户已经小学毕业。 Ph.D.该用户高中毕业七年后，仍然在读书。 这个用户是位学生。 这个用户爱好宇宙学。 SLEEP is piddling!此用户视睡觉为浪费时间，故从不睡觉。执照该用户拥有合资格的驾驶执照 这个用户有近视。 EVA-01这个用户被选中为操纵EVA的适任者。 NEON GENESISEVANGELION这个用户喜欢新世纪福音战士，并着迷于人类补完计划的研究。 1990s这个用户是一位90后。 这个傻瓜乐于助人乐此不疲 这个用户希望有一个女朋友。 这个用户是一名好色之徒。 SEX → 这个用户惯用右手。 PC-5此用户的一切起居生活皆由电脑来完成。 PC此人喜欢并了解自己的个人电脑。 NVIDIA这个用户是NVIDIA的使用者。 这个用户是Intel的使用者。 i5 该用户正在使用酷睿i5-7200U。 HDD 该用户拥有3TB的移动硬盘空间。 这个用户使用Ubuntu对中文维基百科作出贡献。 Sogou该用户使用搜狗拼音输入法或搜狗五笔输入法为维基百科作贡献 该用户使用Windows 98对中文维基百科作出贡献。这个用户使用三星的产品。该用户使用Wi-Fi™连接到互联网。 i7 该用户正在使用酷睿i7-8700K。 NAS这个用户正在使用Synology的418play。 路由该用户使用爱快路由系统。AP自豪地使用网件R7000。 i5 该用户正在使用酷睿i5-9400F。 这个用户总是情不自禁地喜加一。这个用户使用的电脑装有一块甚至多块高性能核弹显卡。id2 HDD 该用户拥有18TB的普通存储空间。 该用户使用Google Chrome对中文维基百科作出贡献。 该用户使用骄傲地使用27寸的2560x1440分辨率的显示器。 RTX 这个用户有一块RTX2060显卡。是个英伟达的狂热爱好者 OS该用户几乎使用过全部操作系统，包括树莓派。 RTX 这个用户有一块RTX2060super显卡。是个英伟达的狂热爱好者 i7 该用户正在使用酷睿i7-6700MQ。 这个用户反对锁区。 这个用户使用惠普的产品。 SSD 该用户拥有2.8TB的固态硬盘空间。 此用户热爱和平，反对暴力。 無这位用户无党籍。 F**k大白象此用户反对政绩工程，认为其过于形式化、面子化，是对纳税人资产的浪费。 这个用户懂得太多了。 幽默这个用户自以为很有幽默感。CNC这个用户喜欢命令与征服系列游戏(包括红警、将军系列)。 这个用户喜欢观赏探索频道。 这个用户对原子能感兴趣。 公制这个维基人喜欢使用国际单位制。 这个用户使用WhatsApp通讯。 这个用户使用Telegram通讯，用户名为@Tzloop。 这个用户的Facebook用户名是：zonglin.tian.7 ヒ这位用户使用Twitter。 QQ这个用户使用腾讯QQ，账号是啥我也不知道。 WeChat这个用户使用微信账号是：忘记了 博客Blog这个用户有自己的Blog：Tzloop's Blog Email这个用户使用电子邮件：z.tian@uu.nl 这个用户喜欢YouTube 该用户使用Microsoft Office作为办公软件。 这位用户使用GitHub。 哔哩哔哩这个用户常浏览bilibili上的内容。( ゜- ゜)つロ 乾杯~ 这些用户框由长征五号创建，博客地址为cz5h.com。 奥特最高这个用户热衷关于奥特曼的一切。 此用户喜欢在Netflix上观看电视剧和电影。 这位用户使用MediaWiki建立自己的Wiki。 bash-1这个用户刚开始编写Bash脚本。 用户语文水平 查看用户语言 C-1这个用户刚开始使用C语言。 用户语文水平 查看用户语言 C++-1这个用户刚开始使用C++语言。 CSS-2这个用户能一般地使用CSS。 html-3这个用户能熟练地使用HTML。 JS-2这个用户一般地使用JavaScript。 Java-3这个用户能熟练地使用Java语言。 用户语文水平 查看用户语言 JSON-3这个用户能熟练地使用JSON。 LaTeX-3这个用户能熟练地使用LaTeX。 Matlab-1这个用户刚开始使用Matlab语言。 PHP-1这个用户刚开始使用PHP。 py-2这个用户一般地使用Python。 R-3这个用户能熟练地使用R语言。 Scala该用户会使用Scala。 SQL-2这个用户能一般地使用SQL。 svg-3这个用户是个进阶的可缩放矢量图形编写员。 xml该用户会使用XML语言。 用户语文水平 查看用户语言 该用户使用 Visual Studio Code 编辑文本文件。 这个用户使用Notepad++编辑文本文件。IEEE该用户是IEEE会员。 这位用户的GitHub账号是：TianZonglin。 .mw-parser-output blink,.mw-parser-output .blink{animation:2s linear infinite blink}@keyframes blink{0%{visibility:visible}50%{visibility:hidden}100%{visibility:hidden}}C:\\&gt;_&nbsp;此用户通晓、并曾经或者正在使用DOS。 这个用户对电脑程序感兴趣。 一中这个用户支持一个中国原则。 台独台毒这个用户反对台湾独立。 此用户支持世界大同。 此用户不太喜欢美国。 这个用户反对法西斯主义。 这个用户是中国山东人 新世界より这个用户来自新世界。神栖66町 这个用户现在或曾经住在山东。 这个用户游玩过青岛 这个用户游玩过北京 该用户现在或曾经居住于武汉。 该用户现在或曾经居住于沈阳市。 这个用户曾经出现在北戴河 这个用户现在或曾经住在广州。呢个用户而家或曾经住喺广州。( WikiProject · Portal ) 这个用户曾经不知道为什么地出现在越南河内 这个用户游玩过澳门特别行政区 这位用户居住于荷兰。 该用户所在时区及时间为：UTC+1 3月4日23:59 EOE这个用户喜欢在LCL之海中炫耀高超的泳技。 所有的地球人都知道这个用户是住在地球上的。 ubx-5这个用户简直是用户框的狂热者，在这个用户的用户页中几乎全都是密密麻麻的用户框。 zh-N此用户的母语是汉语。 用户语文水平 查看用户语言 漢字汉字这个用户认为汉字简化对汉字发展起了正面作用。 繁这个用户认为繁简中文应该共存共荣。简 F**K!这个用户是粗口精。 这个用户很♥简体字。同一语言，同一民族！一个中文维基百科为什么要去搞分裂？ 左起直写这位用户认为中日韩文字左起直写是荒谬可笑的。 方言这个用户喜欢说方言。 en-3This user is able to contribute with an advanced level of English. 用户语文水平 查看用户语言 IELTS6.5 这位用户的雅思成绩等级为6.5。为英语的有能力用者。 这个用户喜欢喝七喜汽水。 这个用户喜欢吃白饭。 这个用户喜欢喝百事可乐。 这个用户是食肉兽。这个用户喜欢吃蛋糕。 这位用户喜欢吃冰淇淋。这个用户喜欢吃面包。 这位用户喜欢吃肯德基。 这个用户喜欢吃芒果。 这个用户喜欢喝奶茶。 这个用户喜欢吃豆花。 这个用户喜欢吃蛋。 这个用户喜欢吃方便面。这个用户喜欢吃马铃薯片。 说明： 原Wiki内容可访我创建的这个页面； 以上内容包含部分本人新建的用户框，在Wiki源码页面均可引用； 欢迎和我一样在博客内使用用户框介绍自己，本页代码已共享在此Gist内； 如若使用请注明出处，感谢。 关于本人（正经版）： 点击跳转。 附：站点详情详情请参见文章：船新版本的个人博客。 基础信息 主题名称： Volantis初始版本： 1.6当前版本： 查看更新日志站点托管： Coding Pages所用图床： GitHub+jsDelivr+PicGo 注：本主题（延迟）同步更新至作者最新版本，部分内容保留自定义修改，具体更新进度请查看更新日志。 详细信息123456789101112131415161718192021222324252627282930313233343536373839&#123; \"name\": \"hexo-site\", \"version\": \"0.0.0\", \"private\": true, \"scripts\": &#123; \"build\": \"hexo generate\", \"clean\": \"hexo clean\", \"deploy\": \"hexo deploy\", \"server\": \"hexo server\" &#125;, \"hexo\": &#123; \"version\": \"4.2.0\" &#125;, \"dependencies\": &#123; \"hexo\": \"^4.0.0\", \"hexo-abbrlink\": \"^2.0.5\", \"hexo-autonofollow\": \"^1.0.1\", \"hexo-deployer-git\": \"^2.1.0\", \"hexo-generator-archive\": \"^1.0.0\", \"hexo-generator-baidu-sitemap\": \"^0.1.6\", \"hexo-generator-category\": \"^1.0.0\", \"hexo-generator-feed\": \"^2.2.0\", \"hexo-generator-index\": \"^1.0.0\", \"hexo-generator-json-content\": \"^4.1.6\", \"hexo-generator-search\": \"^2.4.0\", \"hexo-generator-seo-friendly-sitemap\": \"0.0.25\", \"hexo-generator-sitemap\": \"^2.0.0\", \"hexo-generator-tag\": \"^1.0.0\", \"hexo-helper-qrcode\": \"^1.0.2\", \"hexo-related-popular-posts\": \"^4.0.0\", \"hexo-renderer-ejs\": \"^1.0.0\", \"hexo-renderer-less\": \"^1.0.0\", \"hexo-renderer-marked\": \"^2.0.0\", \"hexo-renderer-stylus\": \"^1.1.0\", \"hexo-server\": \"^1.0.0\", \"hexo-wordcount\": \"^6.0.1\" &#125;&#125; 致谢 托管服务：Github，Coding 资源加速：Jsdesilver 社交满足：感谢 (H)EXO群、知乎、Github、哔哩哔哩、CSDN，在这些地方我认识到了非常多的好朋友，谢谢你们的陪伴！ 其他欢迎前往此页面留言互换友链…"}],"posts":[{"title":"后科技时代—赛博朋克2077","slug":"2020-12-12 后科技时代—赛博朋克2077","date":"2020-12-11T23:00:00.000Z","updated":"2020-12-14T19:23:42.454Z","comments":true,"path":"article/e6cd.html","link":"","permalink":"https://www.cz5h.com/article/e6cd.html","excerpt":"跳票多次终于还是来了，发售之日便收回成本这几乎是前无古人后无来者，这样一款现象级大作，我肯定也是忍不住滴，而且我的台式机配置还不错，一直没找到真正发挥作用的机会，终于借此机会，一赌大作风采！","text":"跳票多次终于还是来了，发售之日便收回成本这几乎是前无古人后无来者，这样一款现象级大作，我肯定也是忍不住滴，而且我的台式机配置还不错，一直没找到真正发挥作用的机会，终于借此机会，一赌大作风采！ 前言由于波兰蠢驴发行游戏时并没有采取加密，也就是说作为单机游戏的《赛博朋克2077》，盗版瞬间就满天飞了，不过蠢驴貌似也是故意的，前期的宣发加自身质量注定这游戏是要大赚的，且后期的DLC也会让他们赚的盆满钵满。那不用担心收益，还需考虑什么呢？传播度！我觉得他们的目标是制作一款《GTA》级别的游戏，不仅好玩，而且广为流传。众所周知，3A大作年年有，可真正深入人心代代流传的，可能就那么几个。当然，支持正版我们每个人责无旁贷，298元，不贵的，希望您看到这里坚持本心，购入正版。 准备工作摆脱掉上个话题，我们直奔主题：什么样的机器可以玩？说实话，我一开始被风评搞得以为我的2060s都不能畅玩，但事实不是这样的，如果你的图形参数调整到“合适”的值，再加上N卡的DLSS这种技术的加持，低配玩Cp2077也是可能的，我不是专家也没法测评，最好的办法就是下载下来试一下，64G一晚上就搞定啦。 游戏下载请移步Steam购买正版，如果您实在囊中羞涩又发誓绝不使用盗版，那么请跪求购入正版之人，认其做父，然后让其家庭分享给你即可。 如果您坚持到这里，那您一定是想白嫖的那位，我转存到天翼云了，各位自便，版本1.40。 游戏附件（Cyberpunk 2077.7z 64G）下载： 16个文件合并解压一处即可，点击bin目录内exe即可畅玩。//CZ5H.COM//以下访问码均换为数字：https://cloud.189.cn/t/mYz2y2eqAZzm ，访问码:nq三p 修改器下载：https://cloud.189.cn/t/UJbmmaQ7rIVb ，访问码:zk九五 本文链接：https://cz5h.com/article/e6cd.html 性能测试 游戏默认的全高特效，部分材质是超级，加之分辨率为2K，所以不开启DLSS的状态下，只有十几FPS，但如果开启DLSS模式，那就可以到60+FPS，可以畅玩无压力，面对静态材质，仍然具有2K的高质量画质。 通过游戏加加的监控可以发现，硬件性能被利用的不错，且FPS稳定60+，玩着非常爽。 游戏加加的下载：https://gamepp.com/download.html 初体验绝美的赛博朋克风。 赛博朋克其实是一种科技高度发展的想象风格，说的是科技水平相当之高（义体，AI遍地），但人类却依然在精神层面没怎么进化，也就是说2077的人类，和我们差不多，但拥有2077年的超强科技，这种设定本身就是讨巧的，可以让现代人很容易产生代入感，同时又用各种“高大”、“雄壮”的科技大厦来震撼每个现代人的感官。《赛博朋克2077》中这种话面比比皆是。 你会发现里面并不包含太多工业化的东西，如果有很多，那就应该归类为蒸汽朋克了，那又是一种对未来的想象了。还有没有其他的呢？有，那便是“废土”风格，类似《辐射》等游戏就是这种类型，设定为毁灭后的后人类时代，玩起来又是另一番体验（2077之中亦有此风格的部分）。 细微之处显真章。 游戏的开放程度，是超出我预期的，当然都在情理之中。 某些街边的小吃摊的锅碗瓢盆，都能被打烂。 某些工业设施可以各种攀爬跳跃。 某些街边桥下的死角也可以随意畅玩。 关于光追及技术提升带来的真实感亦很震撼。 不足之处也不少。 首先就是细微之处仍然不够细微： 大楼不少，但能进去的就那几个； NPC的举动有时依然很反人类很搞笑； 原本剧情和你主动触发的意外情节会冲突， 比如开局阶段，当你去小吃摊和NPC对话时，如果你被警察围追且被攻击，NPC会不紧不慢的重复自己的台词，即使手榴弹满地炸，NPC依然坐在小吃摊吃东西。 反射问题。在水中、在玻璃面前，你都无法看到自己，尤其是在水中/水边，你无法低头看到倒影，有点诡异。 死角BUG。前面说了，某些犄角旮旯（比如从桥上跳下去到达的那种），也能走动自如自由探索，但我发现一个地方跳进去就出不来了！位置如地图绿色箭头所示，感兴趣的各位可以去试试，那个位置有两个大垃圾箱，有个过道可以翻越围栏过去，不过那边是死路，即你跳进去就只能读档重来了，按游戏设计正常不应该有这种死角。 关于游戏的玩法 正经做任务路线； 武器大师(制造升级)路线； 摄影大师(截图)路线；自带的截图工具太给力啦！ 城市探险(犄角旮旯)路线； 为非作歹路线； 每个路线都非常可玩，由于此游戏的NPC触发剧情很多，所以讲道理可以玩很长的时间，而且可能正经玩家玩着玩着就被美轮美奂的景色吸引转职做摄影大师也未可知。 最后以一张不夜城夜景结束本文。","categories":[{"name":"Mini-ITX小钢炮","slug":"Mini-ITX小钢炮","permalink":"https://www.cz5h.com/categories/Mini-ITX%E5%B0%8F%E9%92%A2%E7%82%AE/"}],"tags":[{"name":"赛博朋克","slug":"赛博朋克","permalink":"https://www.cz5h.com/tags/%E8%B5%9B%E5%8D%9A%E6%9C%8B%E5%85%8B/"},{"name":"赛博朋克2077","slug":"赛博朋克2077","permalink":"https://www.cz5h.com/tags/%E8%B5%9B%E5%8D%9A%E6%9C%8B%E5%85%8B2077/"},{"name":"Cyberpunk 2077","slug":"Cyberpunk-2077","permalink":"https://www.cz5h.com/tags/Cyberpunk-2077/"},{"name":"附带修改器","slug":"附带修改器","permalink":"https://www.cz5h.com/tags/%E9%99%84%E5%B8%A6%E4%BF%AE%E6%94%B9%E5%99%A8/"}]},{"title":"极米Z4X观看奈飞油管完美教程","slug":"2020-11-24 极米Z4X观看奈飞油管の完美教程 - 副本","date":"2020-11-23T23:00:00.000Z","updated":"2020-11-26T02:08:04.898Z","comments":true,"path":"article/b15f.html","link":"","permalink":"https://www.cz5h.com/article/b15f.html","excerpt":"翻遍网络没找到完整的教程，有必要自己写写了。通过此方案，你可以用你的极米Z4X直接观看YouTubeTV、Netflix，完美解决极米Z4X海外体验差的痛点！本文包括全部文件的下载地址，极米Z4X完全可以照搬，对于其他型号，执行步骤相似、方案完全可行。","text":"翻遍网络没找到完整的教程，有必要自己写写了。通过此方案，你可以用你的极米Z4X直接观看YouTubeTV、Netflix，完美解决极米Z4X海外体验差的痛点！本文包括全部文件的下载地址，极米Z4X完全可以照搬，对于其他型号，执行步骤相似、方案完全可行。 基本的认知有人说YouTubeTV可以直接在原厂固件安装，是这样的，但问题是缺少谷歌框架你就无法登陆谷歌账号，无法体验Chrome投屏！所以体验差十万八千里。 也有人说用手机看youtube再投屏也可以，是这样的，但问题是如果你只是手机用小飞机，那么手机看youtube时就和电视不在一个局域网内了，也就无法投屏；即便你用路由器fq，确实是可以看，但第一画质很低，第二你的投影依旧是个显示器，并没有挖掘其本身的性能（2+8G并不差）！ 还有人说，如果你的机器安卓版本太低，那就没希望了，因为 youtubeTV 在 Andorid 5.1 以下的系统都不支持了。（实际是可以的！） 目前公认/极米官方论坛的说法是只有比较新的投影支持海外版固件的互刷，也就是说官方结论：极米Z4X及一些早起型号无法通过刷固件方式来获得海外体验，那么谷歌安装器的路子呢（软件方式）？大部分文章或帖子都没给出明确答案，不过阅读了大量的帖子之后发现，软件方式同样有几个公认的模糊地带： 极米Z4X能否安装谷歌框架？ 能否正常使用谷歌商店？ 能否安装并登陆YoutubeTV？能！ 能否不授权就登陆Netflix？能！ 其他型号同理，理论上是一样的问题，但我无法测试。总之，先说结论：以上看YT看NF全部是可行的，至少我的Z4X全部可以做到！重点就是你安装软件的版本需要和你机器安卓版本相对应！，另外，在我们开始软件安装前，我强烈建议你将你的极米Z4X刷成精简系统（在原厂包基础上的精简），这样一是可以有更多空间和性能来供后期youtube和奈飞的使用，二是官方最新固件貌似会在安装谷歌框架时出现问题。（更多详细内容请移步cz5h.com） 精简你的固件系统 这部分是通用的，因为这里面共享了大部分型号的固件，此精简固件删掉了全部（奇异果等等）第三方内置应用，只保留了基础的功能性APP，安装后会发现系统占用才300M，性能储备充足！（上图的定位是错的，没找到怎么改..） 原厂包+精简包，下载地址：链接：https://pan.baidu.com/s/1wta4E5DVbfcB4UIKFhbgkQ提取码：92ty 找到你的型号对应的文件，下载下来并解压，就是两个bin文件。 下面开始刷机： 准备：U盘 2.0 fat32格式 16G或以下（纯净U盘），刷之前格式化下！ 将原厂包的bin文件放入格式化完的U盘。 断开极米投影设备的所有网络，有线网络和无线网络（否则刷完还会有开机广告）！ 把U盘插到投影USB2.0接口，在当前（还未刷）系统下进入设置-&gt;系统升级-&gt;U盘升级，来触发刷机； 耐心等待刷机完成，大概十分钟左右。（亲测不需要重配遥控器，但如果有必要请重新匹配遥控器） 拔掉U盘，删除此时U盘内的全部文件（格式化），再拷贝精简包的bin文件到U盘； 在当前（已刷成原厂固件）系统下进入设置-&gt;系统升级-&gt;U盘升级，来触发刷机； 这次的刷机完成后，精简系统就刷完了。 默认当贝影视桌面+自带当贝市场，对后续操作十分友好！ OK！现在你已经得到了非常纯净的固件系统，此时你的投影仪应该会比之前要流畅的多，且使用过程非常舒服，自带当贝安装软件非常方便，下面我们就来装一些软件，来做下一步的准备工作。（下图是我装完谷歌套件和油管奈飞之后的文件占用情况，吓不吓人！不得不说安装4.x版本的软件是真的小啊！） 安装实用工具和谷歌框架直接在当贝搜索：3S一键ROOT、RE文件管理器（注意不是ES！）。找到后安装即可。首先用一键ROOT获取权限（必须），无所谓保修不保修，我用了三年了就没出过问题。之后，是一个可选步骤，用RE挂载局域网的共享文件夹（SMB），这样做的目的是，我用电脑将apk下载到指定文件夹内之后，直接就能用RE管理器看到并点击安装！当然，你也可以用当贝的“远程推送”功能，在浏览器页面上传文件。或者你不嫌麻烦用U盘拷贝。 以上步骤其实就是root了一下，下面就开始安装Google框架了 从此处开始，你需要到你的投影设置页面，去看你机器的安卓版本，并记住！如果你是安卓4.x，那么务必不要去下载最新版的谷歌安装器(最新版本号为4.0)，请下载3.0的谷歌安装器，一般长这样【图】。安装方式是半手动的弹出一系列谷歌软件让你安装，此刻你的网络环境应该是十分科学的，不然你会卡着不动。 谷歌安装器：https://cn.apksum.com/download/com.goplaycn.googleinstall_4.7_free 整个过程走完，谷歌框架就完美安装上了，这时大概率你会发现，你的谷歌商店会显示“从服务器检索信息出错”，没关系，不影响！ 安装匹配的海外应用原则：安卓版本&lt;=极米系统的安装版本；推荐使用 www.apkmirror.com 来检索我们所需要的软件，该网站有全部版本的apk备份，并且提供详细的对应支持的安卓版本！（比如下面这个我安卓4.3是装不上的） NOW，让我们找几个应用体验下：Netflix、YouTubeTV只要安卓版本不超出，都是可以正常安装的！如果你是Z4X（安卓4.3），那么可以直接用一下链接： Netflix：https://www.apkmirror.com/apk/netflix-inc/netflix/netflix-3-16-6-build-5382-release/netflix-3-16-6-build-5382-android-apk-download/download/ YouTubeTV：https://drive.google.com/a/ka8.hk/uc?authuser=0&amp;id=1aIeXGKyEH9H1ita9Ifj5iW-EYz_tFJw0&amp;export=download 注：这原本是解决其他低版本安卓设备问题的 Smart youtubeTV，我尝试安装在Z4X上，发现运行非常稳定。其内置四个youtubeTV模式，我选用的是第四个（LITE Alt），LITE Main模式会非常卡顿。 奈飞登陆无压力有木有！ 其他的任何软件，只要支持你设备的安卓版本，都可以顺利的安装成功！ 全部安装完之后，你会发现，虽然我们的谷歌商店是不可用的状态，但登陆youtube账号时并没有出现问题，登陆之后就和原生安卓TV的youtube体验完全相同了！实测播放画质最高可到1920x1080@30，30fps会有一点点卡顿的感觉，影响不大。 对于Netflix，同样需要注意，这里装的是安卓版，非TV应用（TV应用最低支持安卓5）。不过视频还是很清晰的，而且非常流畅（目测稳定720@30fps）。由于是安卓应用，所以遥控体验不太好，搭配个无线鼠标就非常惬意了！顺便一说，如果你不是全尺寸投屏，距离近一些的话画面会细腻的多！ 整体上观看YT和NF时，投影的发热量是比较明显的，但是Z4X屁股里有个大风扇，散热效果非常不错，所以问题不大。（更多详细内容请移步cz5h.com） 这里放一个虎牙TV版本，不是官方的云视听TV版！官方版本的直播在东八区零点到八点是没法看的，这个第三方虎牙版本还是很强的，可以登录虎牙账号，甚至可以发弹幕。（原名叫高清游戏直播TV版，进去之后会让你更新，更新完之后就是虎牙TV版了，是个非官方的第三方版本）高清游戏直播TV版：https://down.qwp365.cn/tv/gaoqingyouxizhibotvban.apk 有丰富海外软件的应用市场：https://aptoidetv-qumi-q38.en.aptoide.com/app装完这个就不用自己找应用下载了，里面应有尽有。 后记至此，你原来那台充斥着捆绑应用和广告的极米投影，摇身一变，变成了拥有 youtubeTV + Netflix 的电视投影！要知道小米、亚马逊等推出的电视棒，功能也不过如此（当然更强悍，安卓版本都是7.0起步了），能看 youtubeTV 和 Netflix 的投影，完全可以当做电视了！所以，如果你也想“全面升级”你的极米投影，那就参考本文开干吧，和从前那个鸡肋的原厂固件系统说拜拜，全新的体验在等着你！Peace. 注1：以上所有出现的软件均可正常在极米Z4X上使用，如果你的机器是Android-4.3.x以上，同样可以使用。注2：所有截图均通过极米助手完成，这也是刷官方精简包的必要所在。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"极米投影","slug":"极米投影","permalink":"https://www.cz5h.com/tags/%E6%9E%81%E7%B1%B3%E6%8A%95%E5%BD%B1/"},{"name":"刷机","slug":"刷机","permalink":"https://www.cz5h.com/tags/%E5%88%B7%E6%9C%BA/"},{"name":"YouTube","slug":"YouTube","permalink":"https://www.cz5h.com/tags/YouTube/"},{"name":"谷歌框架","slug":"谷歌框架","permalink":"https://www.cz5h.com/tags/%E8%B0%B7%E6%AD%8C%E6%A1%86%E6%9E%B6/"}]},{"title":"三界官场现形记","slug":"2020-11-17 三界官场现形记","date":"2020-11-16T23:00:00.000Z","updated":"2020-11-18T02:58:41.153Z","comments":true,"path":"article/9e2f.html","link":"","permalink":"https://www.cz5h.com/article/9e2f.html","excerpt":"十年前，时至汉东省人事更替 副省长人选尊某因破获“11.11”特大走私案件，一举打掉了本省最大的走私集团实力，并且揪出了该势力和其竞争对手-时任组织部部长魏九的密切联系，致使魏九被离职审查，后经领导班子决议，尊某成功上任汉东省副省长，由此，汉东省步入了杜绝走私的经济发展快车道。","text":"十年前，时至汉东省人事更替 副省长人选尊某因破获“11.11”特大走私案件，一举打掉了本省最大的走私集团实力，并且揪出了该势力和其竞争对手-时任组织部部长魏九的密切联系，致使魏九被离职审查，后经领导班子决议，尊某成功上任汉东省副省长，由此，汉东省步入了杜绝走私的经济发展快车道。 魏九落马，被限制在其老家清水市进一步审查，一年之后，汉东省全省筹备召开“杜绝走私研讨会暨暂1111大案表彰大会”，尊副省长亲自致电时任清水市公安局长姜涯，要求其尽快将魏九依法定罪逮捕。 姜局长终于见到了最终BOSS。一年前的“猎狐行动”他还记忆犹新，部分顽固份子拼死抵抗，致使行动过程出现一死两伤，后经查证清水市作为龙头重镇经济受到了走私势力的严重侵蚀，此刻于公于私，姜涯都想赶紧给魏九定罪，一切都在稳步推进，但就在向检察院递交材料的前夜，魏九在家中和姜涯见了一面，这“违规”的见面，竟透露给了姜局长一个秘密… 当年的走私大案，另有隐情。密会中，魏九给了一个人的资料给姜局长，她叫九妹，在1111大案中以被抓获羁押候审，卷宗显示其涉嫌多起走私和挪用公款案件，和魏九有密切联系，过了今晚的材料递交，应该会定重罪入狱，但魏九却说，她是清白的。 九妹是无辜的？案子不能结！ 姜局长力排众议，终究没能在当晚递交材料，而同一晚，魏九动用了所有关系，给九妹制造了无罪证据。当晚姜局和尊副省长告知了他的想法，或者说信念：一个人的清白都不顾，何以顾一方平安？尊副省长听罢大怒，他让姜涯冷静几天好好反思自己的立场，并让其他人暂时接替他的工作。 几天后，魏九还是锒铛入狱，姜局长被停职反省，其得力干将们也遭受到了非议和排挤，出乎意料的是，九妹因证据不足被免于起诉，不日就被释放，自此猎狐行动正式结束，姜局原本作为清水市公安局长应该被嘉奖提拔，但现在却落得个无官一身轻。 那时候，姜局每天都泡在离家不远的酒馆里，不料那一天，竟然碰到了那个女人：九妹。 作为城郊的酒馆，本该非常普通，但一年前公安部执行猎狐行动时，前线指挥部就在这里，准确的说是把这当场了临时食堂，自然有些信息会在这里被流出，老板耳濡目染，对整个“1111答案”的大小事也可以算是门儿清。九妹来这的目的，就是为了调查她父亲的事情。她一直以为他父亲兢兢业业，不理解为何会触犯法律，于是她想以一己之力查明真相。 姜涯见状，顺水推舟，承诺和他一起调查，查明真相或许也是对当年自己的一个交代。 在后来的接触中，姜涯逐渐了解了九妹的身世… 九妹的父亲是轴网科技集团的高管，轴网科技是当时清水市名噪一时的大企业，九妹大学毕业后就被安排进了公司给总裁做秘书，其父用意不言自明，后来她和总裁轴某，也确实发展为了男女朋友，不过与此同时，当时已经身为南高县县长的魏九，也把魔抓伸向了她。 当时年少青春的九妹，只把魏九当做忘年交，亦师亦友，在后来轴网科技的发展中，作为轴某的女友，在魏九的蛊惑下给轴网的发展提了很多非法“建议”，在魏九的暗中运作下，轴网集团一发不可收拾，凭借着非法获利，其在当时迅速建立了一个庞大的、以高新科技为外衣的走私帝国，霸占了当时的各大财经版面。九妹就这样，被魏九当做“影子”操控了轴网集团，而作为“领导有方”的魏九也从此发迹，人脉势力迅速扩张。以至于十年前，已经担任省委组织部长，成了接任副省长的热门人选。 帝国的崩塌非一朝一夕。 轴网集团的辉煌，只持续了两年，之后，由于轴某的资金管理不善、人事经营散乱，整个集团资金链出现严重问题，魏九早已升任组织部长，时任县长只得为其“输血”试图挽救，但随着各种集团黑幕的爆出，对轴网集团讨伐的社会舆论已经充斥了整个南高县、清水市，在不断涌现出来的大量的事实证据面前，汉东省委终于带头开展了猎狐行动。 行动波及面之广，几乎涉及了清水市的各行各业，那段时间一众上市公司停牌整顿，不少民营企业也因资金管理问题被迫吞下天价罚单，行动虽是为了打击走私违法势力，但其对整个清水市造成的负面效果，姜涯是深有体会的。而九妹，就在这大起大落中，饱受了生活的摧残，而她自始至终都不知道，她一直如棋子般任人摆布，终究不得安宁的生活。 了解到这里，姜涯终于大概明白了为何魏九说九妹是无辜的，了解到了猎狐行动对清水市尤其是南高县的负面影响。他依旧不解的问题是：从南高县一路升上去的魏九，在猎狐行动如此高调的前提下自然难逃干系，或者说，猎狐行动，就是针对魏九的一次行动？ 在整理了九妹的全部资料后，姜涯悉数递交给了纪委监委，姜不知道的是，这一举动，直接导致1111案件的主要犯罪嫌疑人魏九因为犯罪证据不实而被重审，这可谓是“放虎归山”！尊副省长获悉此事，立刻给当时负责工作的清水市领导施压，并动用自己的关系，暗地里将魏九给九妹伪造的证据“不合法”的毁掉，因此，在姜涯上报材料几天后，九妹就重新因为涉嫌违法被羁押。 姜涯隐约觉得，有人故意要把九妹和魏九“捆绑”在一起置于死地。几天后姜涯得知九妹的证据被认为改动，勃然大怒，咆哮着要揪出内鬼，在市公安局大闹一番之后，他获得了这样一个他不愿接受的消息：是尊副省长下的命令。 为什么，尊副省长如此急切的想要封口魏九，还有九妹？ 带着这个问题的疑惑，仍在停止反省的姜局长亲自到省委拜会尊副省长，当面质问此事，而尊副省长只是语重心长的说了一句：这是我做的有利于整个清水市的最好安排！谈话间，尊副省长还特别提到了九妹的状况，并对姜涯说：如果你真希望还她正常生活，就让她彻底和魏九断了联系，把她送到美国的贵婿大学读书去吧，他可以给九妹写介绍信。 从省委回来，姜涯越发不解和愤懑，但他仍然是想还九妹清白和正常生活的，所以，按尊副省长的意思，九妹顺利出狱，并且安排准备出国，在临行前的一天，九妹特地来和姜涯道谢。 姜涯说：到了美国你就可以有新的生活了。九妹说：警察不会骗人吧？！ 这段时间的相处，让姜涯感觉九妹就像自己的女儿，送她出国或许今生再无机会见面，不舍之满满，但最终，九妹还是如约离开了姜涯进入了贵婿大学。 这边温情的送走了九妹，那边“放虎归山”的魏九并没有闲着，虽然尊副省长在他被重审后的第二天就立刻吩咐了自己的亲信督办这个案子，但魏九脱离了监狱，还是如鱼得水，所有关系网立刻重启，一场魏九的“绝地反击”就要来了。 他撕掉了贵婿大学的遮羞布！ 魏九在宝贵的时间里，发动了一切关系，曝光了位于美国的贵婿大学，原来这所大学是赤裸裸的权钱交易的产物，经调查，该校就读中国学生有半数是清水市内有名企业家的子女，他们被安排在那里读书且不被允许回国！到此，舆论为之哗然，同样，姜涯也大吃一惊，因为他知道，让他送九妹过去的尊副市长，一定和这有脱不了的干系。 与此同时，尊副省长也不顾组织纪律，越级行使权力，给清水市公安局直接施压，让他们以最快速度了解魏九的案子。另一边，得知贵婿大学的内幕后，姜涯内心做了激烈斗争，难道他一直听命于一个不忠于国家不忠于人民的领导？为了证实这个可怕的想法，他总结了近期收集的全部资料，无奈的发现所有的证据都指向尊副省长，在此情形下，姜涯没有忘记自己是一名警察、是一名人民公仆，既然领导队伍中有问题，那就该改正，无奈自己身份，他只得将所有材料交给了媒体。 自此，猎狐行动的始末以及尊副省长和魏九之间的关系终于大白于天下。 时间回到十年前，由汉东省委政法委牵头，省公安厅、纪委监委等部门组成联合调查组，对清水市轴网集团以及市县级领导班子展开全面调查。公安内部行动代号“猎狐”，时值领导班子换届，尊某和魏九作为牵头小组成员，心里都知道行动成败对自己的意义。 同时尊某对魏九的所作所为都知道的非常清楚，虽然尊某也有些问题，但都没触及原则，所以在这方面尊某算是牢牢地抓住了魏某的把柄。猎狐行动开始之前，两人就曾多次私人碰面，后经调查，当时尊某就对魏九以前程为要挟，让其促成猎狐行动的成功，从而让他成功接任副省长之职。魏九无奈，只能答应，他可以“出卖”他的走私利益链，但他要求魏某接任后保留他的权力。 猎狐行动开始前夜，魔鬼契约签订了。 但后来事态的发展，是魏九始料未及的，原来尊某一早就在利用魏九，尊某打算让魏九彻底在官场出局，一是为了干掉主要竞争对手，二是为了掩盖这个“和魔鬼签订的契约”。因此，在猎狐行动后期，尊某已接任省长，但他丝毫没有停止的意思，最后居然把魏九的所有势力赶尽杀绝，这也是为什么魏九非要置尊副省长于死地的原因。 这一切经媒体报道，引发了社会的广泛热议，而姜涯被舆论推上了风口浪尖，不只是因为他的客观公正，更是因为他敢于挑战权力！省市领导班子出现严重分歧和非议，这是上级领导所不愿看到的事，纪委监委介入调查，确认了尊某当年不当竞争的事实，但在任无其他问题且工作作风扎实，暂不对其进行人事调整，而姜涯因为违反组织纪律，被调任其他部门接受进一步的工作安排。 地点：汉东省 from 电视剧《人民的名义》清水市 from 电视剧《使命》南高县 from 电视剧《阳光代表》 人物：尊副省长 - ？魏九 - ？九妹 - ？姜涯 - ？ 角色行动名称自行代入，文笔有限，不严谨处请见谅。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"瞎写","slug":"瞎写","permalink":"https://www.cz5h.com/tags/%E7%9E%8E%E5%86%99/"},{"name":"影评","slug":"影评","permalink":"https://www.cz5h.com/tags/%E5%BD%B1%E8%AF%84/"}]},{"title":"给你的Win平板安装ChromeOS系统","slug":"2020-11-9 给你的Win平板安装ChromeOS系统","date":"2020-11-08T23:00:00.000Z","updated":"2020-11-12T23:41:43.786Z","comments":true,"path":"article/b192.html","link":"","permalink":"https://www.cz5h.com/article/b192.html","excerpt":"你是否觉得你的Win10平板性能太弱、娱乐性没有（跑不动）、用触屏不爽（屏太小）、系统掉电超快、想丢掉却又不舍得？拯救你鸡肋平板的方法来了！给你的平板换装ChromeOS，你会拥有全新的平板体验，或许你会再次对你的平板爱不释手！","text":"你是否觉得你的Win10平板性能太弱、娱乐性没有（跑不动）、用触屏不爽（屏太小）、系统掉电超快、想丢掉却又不舍得？拯救你鸡肋平板的方法来了！给你的平板换装ChromeOS，你会拥有全新的平板体验，或许你会再次对你的平板爱不释手！ 我的机器配置： CPU: Intel Core m3-6Y30 Mem: 4GB 详细外貌及参数可在这里找到。 啰嗦几句Win10平板，在过去有段时间像雨后春笋般在市场上出现，以酷比魔方为代表的平板厂商，号称已经可以达到苏菲婆的体验了，我用的这个已经算是高端寨本了，但对我来说还是“小”！Windows就不应该出现在这么小的屏幕上，另外由于性能原因，娱乐性也几乎没有，不过让我欣慰的是这个本子可以流畅玩红色警戒，有很长一段时间我都是外接24寸屏幕玩红警的，当然本子会超级烫。 总之，体验就是，你能在这个小本子上满足一切Win的“日常”需求，而且还可以触屏！但是，时间一久，触屏的新鲜感过了，我就迅速对其失去兴趣了，一个重要的原因就是搭载 Windows 耗电是真的快，充电线就跟电源线一样了，拔不得。综上，在我有了台式机之后（笔记本装Ubuntu了），这个本子就彻底被打入冷宫了。 但是天不绝此本，ChromeOS来了！ Chrome OS 在今天其实已经不新鲜了，甚至在我三年前买这个平板的时候就已经出现了，但是，ChromeOS 在这几年发展了非常多的东西，包括 ARC+ 等等技术在内，直到其开始兼容安卓应用开始，其才逐渐形成比较明显的、有优势的系统特性。我会在之后写一篇 ChromeOS 使用体验的文章，这里直接列几个优点： 可以得到全系统目前最好的原生 Chrome 浏览器体验（一点也不开玩笑）！ 可以兼容 80% 安卓应用，Appstore或许没有，但可以直接安装apk！ 可以以极低的功耗运行，真的可以做“移动”平板了！ 触屏更加接近安卓体验，比win10不知道好到哪里去了！ 娱乐性剧增，办公性下降，这正是我想要的！ 开机速度巨快！ 如何知道自己的电脑能不能装呢？ 答案是基本就没有不能装的！只是你没找到对应的系统（代号），去维基的 Chromebook 页面，去找你的型号对应的厂家型号的ChromeOS代号，比如我的： 如果不幸列表内没有你的CPU型号，那你就只能找个相近的型号了（CPU代数优先），如何辨认是几代CPU？举个例子，i5-7200u是七代CPU，i7-6700hq是六代CPU，而我的 m3-6Y30 同样是六代CPU。 得到你的对应安装的版本代号，接下来就开始安装了！ 开始安装ChromeOS！一、安装启动系统目前，市面上的全部 Chromebook 都是出厂即带系统，也就是说，ChromeOS 和 OSX 一样，没有第三方正版系统，当然 ChromeOS 实际体验中根本没有正版认证的附加功能等等，也没有提示让你注册的东西。也就是说，Google 没有提供BIOS安装套件，但这并不影响骚操作安装方法，即：通过另一个系统来安装！ 1. 制作U盘启动 这里我用我现成的 UbuntuMate 的安装盘，（如果你没有，请用UtraISO写盘工具製作U盘啓动盘，默认是GPT分区） 2. 将ChromeOS相关文件同样放进启动盘 这一步当然可以在进入Ubuntu系统后再去下载，但是我们直接放进启动盘内就无需再在安装时联网下载了。 在启动盘内新建 Chrome OS（注意大小写空格）文件夹，然后放入以下两个文件（解压之后）。 Chrome 安装框架 Chrome OS 恢复镜像，按你的版本代号，下载最新版本（Recovery那列的最大的数字） 3. 平板插入U盘，装机 BIOS修改启动项等等不再赘述，关键：到Ubuntu的Grub界面，直接点击不安装进入系统，我们不需要安装UbuntuMate，我们只进去借助其环境运行我们的脚本。 进去之后，在Home文件夹内找到 cdrom-&gt;ChromeOS，在此文件夹下新建个 install.sh 将以下代码放进去，淡然你也可以一条条执行，都一样。 123456sudo apt-get updatesudo apt-get install figletsudo apt-get install pvsudo apt-get install cgptsudo bash chromeos-install.sh -src cave_recovery.bin -dst /dev/sda注意：你的recovery.bin要对应你下载的bin文件名称，可能很长，无所谓的。 即： 之后回车，开始安装，中间会卡住询问你是否继续，键入 yes 继续，出现以下信息之后，即安装完成，之后便可以点击重启，然后拔掉U盘。 4. 安装结束 再次开机后，即可进入 ChromeOS 配置引导界面。 后记如果你可以找到对应你机器CPU型号的 Chromebook 机器，那么恭喜你，百分百可以顺利安装成功，如果没有，那么也不要怕，如果你懂一点Linux知识就会发现这个安装过程是真的非常简单，这个 xxx_recovery.bin 不行那就换另一个试试，总会找到的！ 到今天为止，ChromeOS我也用了四五天了，头两天一度想要换掉这个系统，因为发现Play商店里的适配应用少的可怜，但后面发现 ChromeOS 早就可以直接安装第三方apk了！（有几率安装不成功那就是真的不适配），像QQ和微信在Play商店里是没有的，但经我测试全系列版本的移动端QQ均可以正常在ChromeOS上使用！ 下篇文章我就来写一下关于 ChromeOS 的绝佳使用体验！","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"ChromeOS","slug":"ChromeOS","permalink":"https://www.cz5h.com/tags/ChromeOS/"},{"name":"安装","slug":"安装","permalink":"https://www.cz5h.com/tags/%E5%AE%89%E8%A3%85/"}]},{"title":"重写Hexo豆瓣影评插件及npm发布","slug":"2020-10-16 重写Hexo豆瓣影评插件及npm发布","date":"2020-10-15T22:00:00.000Z","updated":"2020-11-12T23:35:37.719Z","comments":true,"path":"article/5613.html","link":"","permalink":"https://www.cz5h.com/article/5613.html","excerpt":"不知道从什么时候开始，习惯于在豆瓣找电影看，到了后来，就偶尔写个电影观后感，随着看电影看的越来越多，可比较的东西就越来愈多，所以现在看完一部新电影，写写自己的感想就成了固定环节了，作为一种“动态”，能够搬到博客上与他人共享那是再好不过的了，找了一圈Hexo的插件，发现都不怎么好用，那么索性就魔改个自己用的版本！","text":"不知道从什么时候开始，习惯于在豆瓣找电影看，到了后来，就偶尔写个电影观后感，随着看电影看的越来越多，可比较的东西就越来愈多，所以现在看完一部新电影，写写自己的感想就成了固定环节了，作为一种“动态”，能够搬到博客上与他人共享那是再好不过的了，找了一圈Hexo的插件，发现都不怎么好用，那么索性就魔改个自己用的版本！ 基于HEXO豆瓣插件 hexo-douban 的二次开发插，强烈建议先试用原插件，如果您觉得以下特性更能满足您的需要，那么再使用本插件。 原插件 hexo-douban 的不足： 书影音、大部分人就想放影评 样式不好看，字体大小的一致性即颜色 渲染全部观影记录，几百部电影会导致有几十页翻页，臃肿 单纯的构造豆瓣原页面，在“已看”列表中，只会出现短评内容，长影评是另外的部分 构造的页面目录较深，和博客其他部分关联度不够 主题兼容性问题，valine部分的缺失 移动端界面不适配/合适 对应的应对措施： 砍掉多余部分 适当的美化了CSS 设置拉取列表的长度控制 魔改原有xpath解析逻辑，拉取长影评页面内容，补全到“已看”列表 改动原模板中样式的位置，以便于无差别的插入到其他同原页面，提升关联度 插入资源文件和valine构建代码 简单的重写了移动端样式 本插件的主要特性： 原项目固有特性； 重构模板页面，支持移动适配； 补全列表影评内容，支持短评和长影评（核心）； 支持生成指定长度的列表（对于观影数量较多的用户）； 样式inline化，允许直接嵌入同源其他页面；12&lt;div id=\"dbcontent\"&gt;&lt;/div&gt;&lt;script&gt;$('#dbcontent').load('./movies/index.html .hexo-douban-item:nth-child(1)');&lt;/script&gt; 注意：本插件构建的页面完全不保证兼容IE等上古浏览器，推广使用现代浏览器，人人有责。 第一步：安装123# 如果您使用过原插件请先卸载之$ npm uninstall --save hexo-douban$ npm install --save hexo-douban-list 第二步：配置将下面的配置写入站点的配置文件 _config.yml 里(不是主题的配置文件). 12345678910douban: user: ID（数字或字幕|无需引号） builtin: true movie: title: '生成页面的标题' quote: '生成页面的内容的导语' length: 2 valine_id: WbLE88qfAcz4hSI5 valine_key: ycqjmtEfUxuxD timeout: 10000 注意：以上内容中务必确定 USER ID 的正确性！ user: 你的豆瓣ID.打开豆瓣，登入账户，然后在右上角点击 “个人主页” ，这时候地址栏的URL大概是这样：”https://www.douban.com/people/xxxxxx/&quot; ，其中的”xxxxxx”就是你的个人ID了。 builtin: 是否将生成页面的功能嵌入hexo s和hexo g中，默认嵌入（TRUE）即npm安装后无需任何操作按原命令部署博客即可生效。 title: 该页面的标题。 quote: 写在页面开头的一段话,支持html语法。 length: 默认值为2，非页数，可以自由尝试（建议取值:2-4）。 timeout: 爬取数据的超时时间，默认是 10000ms ,如果在使用时发现报了超时的错(ETIMEOUT)可以把这个数据设置的大一点。 对于 valine_id 和 valine_key，主要针对的是Volantis主题（以及其他默认渲染评论区域的主题），如果您在测试时页面没有评论区域，则可以忽略这两项，如果出现以下显示则需要填写此两项。（这两项是什么？请移步Valine官方介绍） 使用注意，通常大家都喜欢用hexo d来作为hexo deploy命令的简化，但是当安装了hexo douban之后，就不能用hexo d了，因为hexo douban跟hexo deploy的前缀都是hexo d。 升级使用 npm install hexo-douban-list --update --save 直接更新。 测试执行 hexo clean &amp;&amp; hexo generate &amp;&amp; hexo server，之后访问 localhost:4000/movies 即可访问生成的影评页面。 删除（可补回来）的内容相比较于原项目，取消或删除了以下内容： 去掉了书籍和音乐，单纯针对电影 去掉了影评页跳转的菜单按钮 去掉了以上项目涉及的配置开关 异常如果构建页面为空或404，且日志输出为 INFO 0 movies have been loaded in xx ms，这时怀疑您的IP由于多次请求豆瓣的页面而被豆瓣封禁了，一般第二天会解禁，使用代理或更改IP即可解决。","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"Node","slug":"Node","permalink":"https://www.cz5h.com/tags/Node/"},{"name":"hexo-douban-list","slug":"hexo-douban-list","permalink":"https://www.cz5h.com/tags/hexo-douban-list/"}]},{"title":"滥用jsdelivr之存储视频/m3u8","slug":"2020-9-11 滥用jsdelivr之存储视频","date":"2020-09-10T22:00:00.000Z","updated":"2020-09-17T11:21:46.618Z","comments":true,"path":"article/9cb3.html","link":"","permalink":"https://www.cz5h.com/article/9cb3.html","excerpt":"对于博客来说，媒体资源的存取方式至关重要，借助Jsdelivr加速Github上存储的图片已经是公认的方案，但对于视频来说，面对动辄几百兆的视频资源，你几乎无法找到一个免费的“视频床”，在第三方直接防盗链能力日渐完善的当下，急切需要一种折中方案。本文就借鉴前辈的尝试，将视频存放在Github之上并利用Jsdelivr实现加速，并利用DPlayer将其插入到自己的博客中。","text":"对于博客来说，媒体资源的存取方式至关重要，借助Jsdelivr加速Github上存储的图片已经是公认的方案，但对于视频来说，面对动辄几百兆的视频资源，你几乎无法找到一个免费的“视频床”，在第三方直接防盗链能力日渐完善的当下，急切需要一种折中方案。本文就借鉴前辈的尝试，将视频存放在Github之上并利用Jsdelivr实现加速，并利用DPlayer将其插入到自己的博客中。 核心问题 Github上传限制20M（网页） Jsdelivr对加速资源的限制20M Jsdelivr对MP4等视频格式的解码并不让人满意 Github网页上传限制老生常谈，网页不行就用Git工具，强烈推荐使用 Git Bash，Git下载地址。 这里推荐重新新建一个仓库来存储视频，如果被认定滥用而封禁，也只会影响这一个仓库，所以还是新建一个仓库。 然后，在本地 git clone xxxx.git，下载到本地。 在将视频资源放进文件夹内， 在文件夹内新建 push.bat 写入如下代码， 123git add -Agit commit -m\"%date:~0,4%%date:~5,2%%date:~8,2%%time:~0,2%%time:~3,2%%time:~6,2%\"git push -u origin master -f 双击bat即可强制推送本地全部内容到Github仓库。使用Git方式上传，可以摆脱20M上传大小的限制，不过即便上传成功，Jsdelivr仍然对加速的资源有所限制，要解决这个问题，就需要进入到本文的核心了，即视频分片。 如果你觉得小于20M的mp4视频就可以被加速，那你就错了（涉及第三个问题），Jsdelivr对MP4的处理好像并不好，如上图的视频（视频正常）大小满足存储和加速条件，但通过jsdelivr链接访问后，居然只剩下音轨了。。（如果你在移动端打开，又可以解析到视频，奇怪！） https://cdn.jsdelivr.net/gh/TianZonglin/Ubuntu-Installog/dd.mp4 视频切片首先，这不是简单的视频切分，这涉及到HLS技术，解释如下： HLS 的工作原理是把整个流分成一个个小的基于 HTTP 的文件来下载，每次只下载一些。当媒体流正在播放时，客户端可以选择从许多不同的备用源中以不同的速率下载同样的资源，允许流媒体会话适应不同的数据速率。在开始一个流媒体会话时，客户端会下载一个包含元数据的 extended M3U (m3u8) playlist文件，用于寻找可用的媒体流。HLS 只请求基本的 HTTP 报文，与实时传输协议（RTP）不同，HLS 可以穿过任何允许 HTTP 数据通过的防火墙或者代理服务器。它也很容易使用内容分发网络来传输媒体流。 在我们的场景中，可以理解为视频的 url 就是 playlist 的链接地址，而 playlist 可以看做是视频分片的索引，如此大的视频被拆分，即绕过存储的单位件大小限制，同时也符合Jsdelivr对资源的要求，这样存储的视频，就可以“变相”的将Github做为视频床了。 如何实现？ 获取ffmpeg工具这里我用“获取”而不是“安装”，因为对于Windows来说，其已经被构建成可用的exe了，主要是几个exe，这里我们只使用 ffmpeg.exe，下载地址在这里，下载Build压缩包之后，直接在压缩软件中查看bin目录下的内容，会发现有三个exe文件，只需要拖出 ffmpeg.exe 即可。 文件放置和测试在第一节中克隆的空白项目中，（以下文件名皆可自定义）新建个文件夹 MV-Queen，将原视频放进去，将 ffmpeg.exe 放进去，准备工作完成。 在此目录下，右键选择 Git_Bash_here（没有？请花亿分钟百度下然后重装下Git），然后将 ffmpeg.exe 直接拖入黑框中，回车，会显示ffmpeg的信息，如果你觉着这样麻烦也可以配置环境变量，这里不再赘述。 执行切片第一步：mp4转成ts格式，一对一转换，转换后大小没什么变化。 1ffmpeg -y -i 你的名字.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 你的名字.ts 第二步，按间隔分片，1对N，下面的5即“每个分片5秒”，可以自己切换。 1ffmpeg -i 你的名字.ts -c copy -map 0 -f segment -segment_list playlist.m3u8 -segment_time 5 你的名字%03d.ts 注意：这里如果直接输入ffmpeg是不能用的，我们首先拖动 ffmpeg.exe 到黑框中，这时会在 gitbash 中出现个路径，然后把上面的命令（ffmpeg后面的）复制到路径之后即可运行。 关于MP4的参数，务必满足视频编码为H264，音视频编码为AAC（YouTube下载的视频默认即此格式），如果不是请用格式工厂进行转换，具体详见王同学的文章。 最后一步： 删掉或移除原视频和转换后的ts视频（两个最大的视频文件）以及 ffmpeg.exe，都移除去，剩下的就只有playlist和一大堆分片，之后点击之前的 push.bat 即可完成上传。 使用DPlayer解析再次明确，这不是单纯的视频切分，如果你不信，可以尝试访问单独的视频分片（应该是乱码）： https://cdn.jsdelivr.net/gh/TianZonglin/bibabo/MV-5min-100m-Queen/abc001.ts 当然，直接试图访问m3u8链接也是不行的，这里需要 hls.js 。 https://cdn.jsdelivr.net/gh/TianZonglin/bibabo/MV-5min-100m-Queen/playlist.m3u8 hls.js 是一款基于 Media Source Extensions 开发的，用于实现 HTTP Live Streaming 开源JavaScript类库。它可以实现将MPEG-2 和 AAC/MP3码流变成自制的 MP4的分片。并且可以直接绑定在Video 上，实现播放。 在这里，我们使用DPlayer，只需要在引入 DPlayer.js 之前，引入 hls.js 即可，即： 1234567&lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/gh/你的路径或CDN/DP.css\"&gt;&lt;div id=\"dplayer\" class=\"dplayer-video dplayer dplayer-no-danmaku dplayer-paused\"&gt;&lt;/div&gt;&lt;script src=\"https://cdn.jsdelivr.net/gh/你的路径或CDN/jquery-3.5.min.js\"&gt;&lt;/script&gt;&lt;script src=\"https://cdn.jsdelivr.net/gh/你的路径或CDN/hls.min.js\"&gt;&lt;/script&gt;&lt;script src=\"https://cdn.jsdelivr.net/gh/你的路径或CDN/DP.js\"&gt;&lt;/script&gt; 别忘了构造代码（注意这里的type选择hls）： 123456789101112131415&lt;script&gt; $(function()&#123; const dp = new DPlayer(&#123; container: document.getElementById('dplayer'), autoplay:!0,theme:\"#42b983\",loop:true,lang:\"zh-cn\",preload:\"auto\",volume:Number(\"0\"), video: &#123; url: 'https://cdn.jsdelivr.net/gh/你的m3u8链接/playlist.m3u8', type: 'hls', defaultQuality: 0, pic: 'https://cdn.jsdelivr.net/gh/你的视频的初始未播放的画面（封面）.webp', thumbnails: 'thumbnails.jpg', &#125;, &#125;);&#125;)&lt;/script&gt; 针对自动播放问题，Chrome已经是禁止声音的自动播放了，也就是说，除非你静音，否则通常形式的video标签都无法在chrome中自动播放内容（iframe等除外），这里我的解决方式是，静音播放 + 按钮提醒，在视频下面加一行提示字符，然后利用dp的API做一下简单的控制（我把默认控制栏全部删掉了）。 12345678910111213&lt;span id=\"btnV\" style=\"color:#676767;font-size:16px;cursor:pointer;\"&gt;↑ 画面不动？没声音？点这里！&lt;/span&gt; &lt;script&gt; $(\"#btnV\").click(function()&#123; if($(\"#btnV\").html()==\"↑ 画面不动？没声音？点这里！\"||$(\"#btnV\").html()==\"↑ 没声音？点这里！\")&#123; dp.play(); dp.volume(0.4); $(\"#btnV\").html(\"↑ 太吵了，关掉！\"); &#125;else&#123; dp.volume(0); $(\"#btnV\").html(\"↑ 没声音？点这里！\"); &#125; &#125;) &lt;/script&gt; 修改Dplayer样式为了极简化播放器，我将DPlayer的全部操作区间都给删掉了（display:none），这样就让视频区显得更加的纯粹，不告诉你你都不知道是视频，样式可以直接拷贝，代码如下： 123456789101112.dplayer-controller&#123; display: none !important;&#125;.dplayer-controller-mask&#123; display: none !important;&#125;.dplayer-bezel&#123; display: none !important;&#125;.dplayer-notice&#123; display: none !important;&#125; 最终效果（jsd加速）视频画面右键统计信息-&gt;查看数据源！ .l_mainxx { padding: 8px !important; } .dplayer-video-wrap { position: relative; background: #000; font-size: 0; width: 100%; height: 100%; } .dplayer * { box-sizing: content-box; border-radius: 5px; } .dplayer { position: relative; overflow: hidden; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; line-height: 1; } .dplayer-controller{ display: none !important; } .dplayer-controller-mask{ display: none !important; } .dplayer-bezel{ display: none !important; } .dplayer-notice{ display: none !important; } ↑ 画面不动？没声音？点这里！ $(function(){ const dp = new DPlayer({ container: document.getElementById('dplayer2'), autoplay:!0,theme:\"#42b983\",loop:true,lang:\"zh-cn\",preload:\"auto\",volume:Number(\"0\"), video: { url: 'https://cdn.jsdelivr.net/gh/TianZonglin/bibabo/MV-5min-100m-Queen/playlist.m3u8', type: 'hls', defaultQuality: 0, pic: 'https://cdn.jsdelivr.net/gh/TianZonglin/tuchuang/img/20200912010715.webp', thumbnails: 'thumbnails.jpg', }, }); $(\"#btnV2\").click(function(){ if($(\"#btnV2\").html()==\"↑ 没声音？点这里！\"){ dp.play(); dp.volume(0.4); $(\"#btnV2\").html(\"↑ 太吵了，关掉！\"); }else{ dp.volume(0); $(\"#btnV2\").html(\"↑ 没声音？点这里！\"); } }) })","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://www.cz5h.com/tags/HTML/"},{"name":"Hexo","slug":"Hexo","permalink":"https://www.cz5h.com/tags/Hexo/"},{"name":"DPlayer","slug":"DPlayer","permalink":"https://www.cz5h.com/tags/DPlayer/"}]},{"title":"在群晖中优雅的使用百度网盘——baiduNetdisk+TinyManager","slug":"2020-8-28 群晖使用百度网盘","date":"2020-08-27T22:00:00.000Z","updated":"2020-08-29T13:31:33.106Z","comments":true,"path":"article/e7a6.html","link":"","permalink":"https://www.cz5h.com/article/e7a6.html","excerpt":"2020年8月，继PanDownload被搞死（我没在群晖上用过）之后，BaiduPCS-go/web也全面歇菜，最早期的版本作者也已放弃github维护，目前能在群晖Docker镜像中搜索到的镜像，搭建后几乎全部失效无法下载，于此状况下，难道在群晖上对于百度网盘真没有好的下载工具（非BT下载）了吗？不然！","text":"2020年8月，继PanDownload被搞死（我没在群晖上用过）之后，BaiduPCS-go/web也全面歇菜，最早期的版本作者也已放弃github维护，目前能在群晖Docker镜像中搜索到的镜像，搭建后几乎全部失效无法下载，于此状况下，难道在群晖上对于百度网盘真没有好的下载工具（非BT下载）了吗？不然！ 群晖-baiduNetdisk在2019年底悄悄有人开发了一款“神器”，于近期才不断完善，现在也才一百度star，基本原理就是用docker起一个Linux（推测）虚拟机，里面放个百度网盘客户端，然后配好VNC，最终通过Web访问VNC桌面，进而操作百度网盘客户端。搭配使用的 Synology-baiduNetdisk.spk 只是一个连接器，核心还是Docker的环境。 最终的界面效果如下： 安装所以，本质上该工具有两部分，一个是核心的Docker镜像，一个是spk链接器。关于Docker镜像直接在Docker中搜索 baiduNetdisk 即可找到安装（1G大小），而链接应用可以直接从Github上获取，点击下载最新的spk文件，手动安装群晖应用（不会？请百度吧）。 配置只有Docker端需要配置，而且是很重要的配置，主要有以下几部分： 配置下载根目录这部分是百度网盘可操作的最上层目录，默认是在存储Docker的空间上新建一个BDDOWNLOAD目录，如果你的群晖有多个空间，且你不想将下载内容放在系统空间/Docker空间里，那就需要修改docker配置（设置-卷）。 同理，在端口设置中，默认使用的群晖主机端口是6900，如果要改也可以改。改完之后Docker就可以正常启动了，然后这时访问群晖IP/域名:6900即可进入VNC页面，打开网盘设置窗口，可以看到在下载位置选项中，已经直接将根目录映射到了之前设置的目录。 配置VNC登录密码如果你正常完成上述步骤，那么你会发现你访问正确链接后直接就进入桌面了，没有访问拦截，没有登录验证，非常不安全，作者也想到了这个问题，可以通过手动方式设置一个登陆密码，缺点是非常简单，不是单点登录，即每次刷新或者多开页面都需要输入密码。 设置方法，在先前设置的下载根目录内新建.vnc文件夹，在里面创建一个passwd.txt文件（可在本地创建好后上传到该目录），里面写你的登录密码明文，完成后重启Docker即可出现访问控制。 其他非必要配置为了不过多的占用群晖主体性能和更好地发挥该docker镜像的性能，这里对docker再修改几处配置，包括CPU优先顺序和内存限制（4G足矣再多就不正常了），再勾选上自动重启，以便异常挂掉后可以自动恢复。 资源占用和性能总体上占用资源不低，尤其是内存，如果你的群晖内存不是很大，那就更得主动限制一下其使用内存的大小。 对于性能（下载速度），我还是比较满意的，虽然不能跑满带宽，但和桌面的网盘客户端相比也差不多了，峰值速度可观，但其实不会一直维持峰值，九成时间稳定在5-6m/s，也算很不错的表现了。 不足的一点是，由于本质上是一个VNC桌面，所以对屏幕基本没有适配，在移动端上使用比较头大，主要是按钮太小，容易误触。 Tiny File Manager安装详见文章《极简版（群晖）网盘文件管理工具——TinyManager》 和本文的工具搭配使用在 Tiny File Manager 使用的过程中需要设置一个根目录文件夹，即管理该目录下的所有文件，这里的管理目录，同样可以是百度网盘的下载目录，如此，我们可以将百度网盘的资源直接下载到 TinyManager 的管理目录内，从而下载完成后随即在 Manager 可见。 这样的好处有三个： Tiny Manager 足够轻量。该文件管理工具基于PHP，整体就一个两千行的PHP文件实现，足够的轻量不吃任何资源，同样可完成视频解析等功能。 方便转存再下载。在帮别人（没速度或网速慢）下载百度网盘资源时，可以利用自己的网盘客户端和大带宽将资源转下到群晖内，然后再通过 Manager 直接分享直链供他人下载即可。 直接解析视频音频。如果下的是百度网盘的视频资源，下载完成后即可在 Manager 目录内看到，点击预览即可播放，速度喜人，非常方便。 访问 cz5h.com 获取更多精彩内容。 后记总体上，这就是相当于在docker运行了一个百度网盘客户端（推测是基于Linux的），所以，没有封号的风险，也不会存在各种交互问题，是具有原生百度网盘体验感的方式，在众多第三方工具都失效的当下，或许这种方式反而不失为一种好方案。","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"群晖","slug":"群晖","permalink":"https://www.cz5h.com/tags/%E7%BE%A4%E6%99%96/"},{"name":"百度网盘","slug":"百度网盘","permalink":"https://www.cz5h.com/tags/%E7%99%BE%E5%BA%A6%E7%BD%91%E7%9B%98/"}]},{"title":"极简WEB文件管理工具（群晖）——TinyManager","slug":"2020-8-22 极简版网盘文件管理工具","date":"2020-08-21T22:00:00.000Z","updated":"2020-09-12T18:17:53.743Z","comments":true,"path":"article/b940.html","link":"","permalink":"https://www.cz5h.com/article/b940.html","excerpt":"在之前《导出七牛云资源到本地》的文章中，我已经将文件全部转存到了群晖中，搭配Nginx已经可以实现直链访问。前几天又把自己电脑上存在的有用的软件包全部上传到了群晖，至此我的软件包文件夹已经有三四百个文件/安装包，除了直链访问，我迫切的需要一款简洁轻量的Web文件管理工具来方便管理这些文件，最终借助 Tiny FileManager，简洁高效的实现了我的需求。","text":"在之前《导出七牛云资源到本地》的文章中，我已经将文件全部转存到了群晖中，搭配Nginx已经可以实现直链访问。前几天又把自己电脑上存在的有用的软件包全部上传到了群晖，至此我的软件包文件夹已经有三四百个文件/安装包，除了直链访问，我迫切的需要一款简洁轻量的Web文件管理工具来方便管理这些文件，最终借助 Tiny FileManager，简洁高效的实现了我的需求。 安装TinyManager该项目已在Github开源，主页地址。安装非常的简单，直接下载最新版Zip包，完成后解压，将整体目录改个名字比如“pan”，然后放入需要管理的文件的上一级，然后使用 WebStation 在该目录起一个Nginx容器+php环境，做完之后即可通过路径访问了。 配置经过上述安装，不需要配置即可通过 domain:6543/pan/index.php 访问（index可不写，6543需要路由器配置映射）登陆页面。初始的登陆密码是 admin/admin@123 和 user/12345 对应两个级别的用户。 1. 配置密码直接在群晖打开 index.php 进行编辑（config.php 中的也要对应修改），在27行，找到以下代码： 1234$auth_users = array( 'admin' =&gt; '$c9a9qa2v9W18YaYkqI5cEG/ufAviA912PDly1JWwLPgh6324231hqFG', //admin@123 'user' =&gt; '$2dasidqwioev9W18YaYkqI5cEG/uf14y1JWwLPgh6pMzzJ42412hqFG' //12345); 去到该页面，生成你想设置密码的加密字符串，然后回来将其替换到对应位置（箭头右边部分）即可完成密码修改。 2. 修改语言登陆文件管理Web页面，一定要用admin登陆，右上角下拉点击设置进入该页面选择语言，以及其他设置。 3. 配置管理根目录在 config.php 中找到以下代码，/tools 即表示 Manager 初始进入的目录只有 tools，剩下的其他目录不可见。 123// Line 53// use absolute path of directory i.e: '/var/www/folder' or $_SERVER['DOCUMENT_ROOT'].'/folder'$root_path = $_SERVER['DOCUMENT_ROOT'].'/tools'; 4. 修改文件夹权限此步骤很有必要，如果不改，默认的管理操作会无效，因为php文件没有相应的权限。需要在群晖的文件管理器中直接对 Manager 的工具文件夹进行权限修改（勾选上以下页面的“应用到子文件”，截图没截全）： 修改完之后，Web管理页面上的编辑、删除、重命名即可正常使用。 修改源码加以完善由于主体就一个php文件，所以比较容易修改，原页面主要的问题有以下几个： 文件类型对应的图标太丑 文件操作区的按钮重复 修改根目录后文件直链出错 最底部的操作区没实际意义 1. 修改文件类型对应的图标这里首先引入最新版本的 font-awsome 图标，在index中的适当位置加入（以下代码需要在官网注册后才能得到）： 1&lt;script src=\"https://kit.fontawesome.com/xxxxxx.js\" crossorigin=\"anonymous\"&gt;&lt;/script&gt; 然后在 2612 行找到 function fm_get_file_icon_class($path) 函数，全部类型对应的图标均在此修改即可。 2. 删除没用的部分对我来说，文件的下载和直链，两部分完全相同，所以操作区的下载按钮直接删掉（复制其实也没用）。还有页面最底部的操作区也没用，直接删掉，对应到源码主要是以下几部分，直接注释掉即可： 123456789101112131415161718192021222324252627282930Line 1947 - 删除对文件夹的复制&lt;!--&lt;a title=\"&lt;?php echo lng('CopyTo')?&gt;...\" href=\"?p=&amp;amp;copy=&lt;?php echo urlencode(trim(FM_PATH . '/' . $f, '/')) ?&gt;\"&gt;&lt;i class=\"fa fa-files-o\" aria-hidden=\"true\"&gt;&lt;/i&gt;&lt;/a&gt;--&gt;Line 1949 - 删除对文件夹的直链&lt;!--&lt;a title=\"&lt;?php echo lng('DirectLink')?&gt;\" href=\"&lt;?php echo fm_enc(FM_ROOT_URL . '/' . (FM_PATH != '' ? '/' . FM_PATH : '') . '/' . $f) ?&gt;\" target=\"_blank\"&gt;&lt;i class=\"fa fa-link\" aria-hidden=\"true\"&gt;&lt;/i&gt;&lt;/a&gt;--&gt;Line 2011 - 删除对文件的复制&lt;!--&lt;a title=\"&lt;?php echo lng('CopyTo') ?&gt;...\" href=\"?p=&lt;?php echo urlencode(FM_PATH) ?&gt;&amp;amp;copy=&lt;?php echo urlencode(trim(FM_PATH . '/' . $f, '/')) ?&gt;\"&gt;&lt;i class=\"fa fa-files-o\"&gt;&lt;/i&gt;&lt;/a&gt;--&gt;Line 2015 - 删除对文件的下载&lt;!--&lt;a title=\"&lt;?php echo lng('Download') ?&gt;\" href=\"?p=&lt;?php echo urlencode(FM_PATH) ?&gt;&amp;amp;dl=&lt;?php echo urlencode($f) ?&gt;\"&gt;&lt;i class=\"fa fa-download\"&gt;&lt;/i&gt;&lt;/a&gt;--&gt;Line 2054 - 删除页面底部的操作区（注意注释位置）&lt;div class=\"col-xs-12 col-sm-9\"&gt;&lt;!-- &lt;ul class=\"list-inline footer-action\"&gt; &lt;li class=\"list-inline-item\"&gt; &lt;a href=\"#/select-all\" class=\"btn btn-small btn-outline-primary btn-2\" onclick=\"select_all();return false;\"&gt;&lt;i class=\"fa fa-check-square\"&gt;&lt;/i&gt; &lt;?php echo lng('SelectAll') ?&gt; &lt;/a&gt;&lt;/li&gt; &lt;li class=\"list-inline-item\"&gt;&lt;a href=\"#/unselect-all\" class=\"btn btn-small btn-outline-primary btn-2\" onclick=\"unselect_all();return false;\"&gt;&lt;i class=\"fa fa-window-close\"&gt;&lt;/i&gt; &lt;?php echo lng('UnSelectAll') ?&gt; &lt;/a&gt;&lt;/li&gt; &lt;li class=\"list-inline-item\"&gt;&lt;a href=\"#/invert-all\" class=\"btn btn-small btn-outline-primary btn-2\" onclick=\"invert_all();return false;\"&gt;&lt;i class=\"fa fa-th-list\"&gt;&lt;/i&gt; &lt;?php echo lng('InvertSelection') ?&gt; &lt;/a&gt;&lt;/li&gt; &lt;li class=\"list-inline-item\"&gt;&lt;input type=\"submit\" class=\"hidden\" name=\"delete\" id=\"a-delete\" value=\"Delete\" onclick=\"return confirm('Delete selected files and folders?')\"&gt; &lt;a href=\"javascript:document.getElementById('a-delete').click();\" class=\"btn btn-small btn-outline-primary btn-2\"&gt;&lt;i class=\"fa fa-trash\"&gt;&lt;/i&gt; &lt;?php echo lng('Delete') ?&gt; &lt;/a&gt;&lt;/li&gt; &lt;li class=\"list-inline-item\"&gt;&lt;input type=\"submit\" class=\"hidden\" name=\"zip\" id=\"a-zip\" value=\"zip\" onclick=\"return confirm('Create archive?')\"&gt; &lt;a href=\"javascript:document.getElementById('a-zip').click();\" class=\"btn btn-small btn-outline-primary btn-2\"&gt;&lt;i class=\"fa fa-file-archive-o\"&gt;&lt;/i&gt; &lt;?php echo lng('Zip') ?&gt; &lt;/a&gt;&lt;/li&gt; &lt;li class=\"list-inline-item\"&gt;&lt;input type=\"submit\" class=\"hidden\" name=\"tar\" id=\"a-tar\" value=\"tar\" onclick=\"return confirm('Create archive?')\"&gt; &lt;a href=\"javascript:document.getElementById('a-tar').click();\" class=\"btn btn-small btn-outline-primary btn-2\"&gt;&lt;i class=\"fa fa-file-archive-o\"&gt;&lt;/i&gt; &lt;?php echo lng('Tar') ?&gt; &lt;/a&gt;&lt;/li&gt; &lt;li class=\"list-inline-item\"&gt;&lt;input type=\"submit\" class=\"hidden\" name=\"copy\" id=\"a-copy\" value=\"Copy\"&gt; &lt;a href=\"javascript:document.getElementById('a-copy').click();\" class=\"btn btn-small btn-outline-primary btn-2\"&gt;&lt;i class=\"fa fa-files-o\"&gt;&lt;/i&gt; &lt;?php echo lng('Copy') ?&gt; &lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; --&gt;&lt;/div&gt; 3. 修复链接问题在 FM_ROOT_URL 后面，将 / 改为 /tools（设置的根目录）即可。 123456// Line 2014 &lt;a title=\"&lt;?php echo lng('DirectLink') ?&gt;\" href=\"&lt;?php echo fm_enc(FM_ROOT_URL . '/tools' . (FM_PATH != '' ? '/' . FM_PATH : '') . '/' . $f) ?&gt;\" target=\"_blank\"&gt; &lt;i class=\"fa fa-link\"&gt;&lt;/i&gt;&lt;/a&gt; 将 fm_enc($file_url)改为 str_replace(&quot;6543&quot;,&quot;6543/tools&quot;,fm_enc($file_url))（把根目录加上）即可。 12345// Line 1592&lt;b&gt;&lt;a href=\"&lt;?php echo str_replace(\"6543\",\"6543/tools\",fm_enc($file_url)) ?&gt;\" target=\"_blank\"&gt;&lt;i class=\"fa fa-external-link-square\"&gt;&lt;/i&gt; &lt;?php echo lng('Open') ?&gt;&lt;/a&gt;&lt;/b&gt;// Line 1647echo '&lt;div class=\"preview-video\"&gt;&lt;video src=\"' . str_replace(\"6543\",\"6543/tools\",fm_enc($file_url)) . '\" width=\"640\" height=\"360\" controls preload=\"metadata\"&gt;&lt;/video&gt;&lt;/div&gt;'; 访问 cz5h.com 获取更多精彩内容。 4. 最终的精简效果","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"群晖","slug":"群晖","permalink":"https://www.cz5h.com/tags/%E7%BE%A4%E6%99%96/"},{"name":"文件管理","slug":"文件管理","permalink":"https://www.cz5h.com/tags/%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"},{"name":"PHP","slug":"PHP","permalink":"https://www.cz5h.com/tags/PHP/"}]},{"title":"东北亚离战争到底有多远？","slug":"2020-8-18 东北亚离战争到底有多远？","date":"2020-08-17T22:00:00.000Z","updated":"2020-08-20T11:42:47.925Z","comments":true,"path":"article/abcd.html","link":"","permalink":"https://www.cz5h.com/article/abcd.html","excerpt":"这是一部经典的“韩国式”政治剧情片，不过值得称赞和肯定的，是其又继续往前迈了一步，大胆的描绘了东北亚的一次隐晦的热战危机，虽然剧情“浮夸”经不起推敲，但当世界局势真的变得波云诡谲之时，谁又能保证世界不是朝着远离和平的方向前进？","text":"这是一部经典的“韩国式”政治剧情片，不过值得称赞和肯定的，是其又继续往前迈了一步，大胆的描绘了东北亚的一次隐晦的热战危机，虽然剧情“浮夸”经不起推敲，但当世界局势真的变得波云诡谲之时，谁又能保证世界不是朝着远离和平的方向前进？ 1. 整体观感战争迷表示吃到饱，将近一大半的篇幅都花在“热战”情节，其中又有一大半集中在朝鲜的核潜艇内，不同势力、不同思想聚集在这小小的潜艇内，在外部超强台风“钢胚”的映衬下，显得格外的渺小，但就是这渺小的一叶小舟，孱弱的维系着美、朝、韩三方的和平对峙，中间又夹杂着中、日两大外部势力的干预，真可谓是战争的“风暴眼”。 全篇的主旨，个人觉得就是片尾韩国总统对人民的发问：你们想统一吗？这句话的深刻含义是，朝韩的半岛统一，决定因素不是中美日外部因素，而是朝韩两国人民。 此主题隐晦的另一面，表现在对“中朝同盟”的否定，这个意图，在电影中表现的很明显，反叛的总督长不止一次的强调“同中国的血盟”，即便临死仍然坚信这一点，为此不惜在计划曝光后仍然（按中方意图）向日本发射核弹，当然“勇敢的”韩国总统跳出来阻止了这一切，他的行为同时也间接影响了美朝领导人的想法，并最终促成了最后的 happy ending. 2. 半岛的排他性在无数次说完佩服韩国电影的脑洞之后，这里必须得再说一次。当然，如果你看多了韩国的战争片，即便是朝韩内战，他们也拍了一大堆精彩的宣扬“朝韩一家亲”的电影。所以把朝鲜领导人“美化”到一定高度，并且和韩国领导人思维在同一层次，这是一种“现实无法达到”的剧情，即便如此，韩国电影依旧热衷于这样做，目的我觉得有二：一是在韩国内部进行“内宣”，维持韩国人的统一情节；二是对全世界说“朝韩不是敌人、是朋友”，这在当下正和文在寅的一些执政思路相吻合。由此种“朝韩一家亲”的情怀推延出来的，是一种“半岛排他性”，其核心就是： 朝鲜半岛的事情由朝鲜半岛自己解决。 这当然是个美好的梦，不仅对面的朝鲜办不到（中国不答应），就连韩国自己也办不到（韩美共同防御条约）。最现实的是：《韩美共同防御条约》存在一天，韩国就不配说这句话，当然在电影情节中是可以说的，不过，在《铁雨2》中，我们可以发现导演故意弱化了美国驻韩司令部的作用，电影里是美国副总统直接远程指挥，到最后是韩国总理给军队下令，这在现实是不可想象的。可以参照一些别的电影剧情比如《白头山》里美驻韩司令部的强势，战时基本可以凌驾在韩国国防部之上。 3. 东北亚离战争到底有多远东北亚包括中日朝韩俄美（驻军）六方势力，这也是所谓“朝核六方会谈”的成员，各个国家内部都有所谓的“鹰派/极右翼”分子，当六方权力均掌握在温和派手中，那自然是地区稳定，但只要一方内部权力斗争，极右翼掌握权力，在各方的制衡拉扯中就会失去平衡，从而导致不可预知的危险举措，进而影响区域和平。这样看来，东北亚局势的核心，目前就在朝鲜半岛，而朝鲜半岛局势的核心，就在失衡的朝鲜一侧，前段时间朝鲜单方面炸毁开城工业园的朝韩联络办公室，就是个危险举动，朝鲜内部的真实斗争我们不得而知，但如果你关注朝鲜的政治，通过支离破碎的报道，也能发现金正恩上台之后对下层官员的清洗，和对权力组织结构的改革，所谓的“先军政治”正在在金正恩的领导下朝“党国体制”过渡，姑且算作是朝鲜的一种改革开放，金正恩掌权下正在力图弱化军队对国家治理的干预，重新将劳动党回归到应有的党的地位，这是一种不走老路的“创新”，势必会有内斗，时至今日，金正恩已经连续多个月没有公开露面，对于他的健康状态的猜测一度成为新闻焦点。 在如此现实下，你还能笑着说，朝鲜很稳定吗？现实是：朝鲜动，则韩国动，韩国动，则美国动，美国动，则中俄动，则日本动，这是种不可停止的“链式反应”，为什么要六方会谈促成“半岛无核化”，就是为了将此反应链的不确定性降到最低，然而事实呢，任重而道远。 影片中，朝美签署和平协议是在2021年7月左右，目前来看，几无可能，当下中美对抗直接硬碰硬，朝鲜在这种情况下已经失去“平衡作用”，美国目前可见的招数基本都打在台湾，或许美国更希望用“台湾牌”这张更“直接”的杀手锏来制衡中国，在这种情况下，朝鲜半岛无核化、以及朝韩和平的日程都要被延后，这就是现实的无情和残酷。 4. 朝鲜无核后何去何从这也是《铁雨2》中朝鲜内部产生矛盾的根本问题，放弃核武，朝鲜能换来什么？片中给出的答案实在过于“理想化”，韩国有部分人认为，最好的状态就是达成一种“南北帮扶”，即回归到“朝鲜半岛的事情朝鲜半岛自己解决”，他们认为韩国有能力帮助朝鲜过渡到正常国家，事实如此，但这种情况并不会出现，原因就是中美日俄的影响。 半岛和平稳定自然是好事一桩，但对于周边大国，朝鲜半岛已然成为一个“准”代理人战争场地，朝鲜之于中俄，更多的是一种和“美帝同盟”的“缓冲带”，很不幸，作为一个国家，就是这样无奈，也正是如此，不甘于做“护城河”的朝鲜走了一招险棋，即发展核武，当然这不是一蹴而就，也不能说这违背或顺从了中国的意愿，各方的权力都在变化，不同时代做出的决策也不尽相同，不论如何，历史走到今天，朝鲜已经不单单是“缓冲带”这么简单了，其有了更多的“独立国格”，虽然依旧站在中俄一遍，但却需要更多的束缚，一旦把控不好，其引起的地区动荡会比以往更加严重。 说到朝鲜的核武发展，离不开其假想敌美国的存在，如果美国领导权更替，势必对朝政策会有改变，这对于韩国来说也是一样，政权更迭导致半岛局势变化，这是个不争的事实。目前的状态是：亲北的文在寅+虽然狠但不寻常的川普+有革新思想的金正恩，这是一个历史的契机，共同作用下促成了“金川会”等等一系列“高难度”政治操作，然鹅即便如此，朝鲜也只是做了些许退让，并没有做出什么实质性的操作。可以预见的是，当下没做到的事情，在下一个美国总统任期，也基本是下一个韩国总统任期内，依然很难做到。（目前看川普和文再演都很难连任） 5. 反思我完全赞同将这种“意淫”剧情划为政治宣传的范畴，虽然Low了一点，但只少能让看的人明白一些事情，剧情再怎么YY，只要你看到最后，如果你是个韩国人，那句“你想统一吗”肯定会直击你的内心，这可以算得上是一种高级内宣了吧，对此我们需要多加学习！ 政治剧情片算是韩国电影的一大特色了，而更进一步包含战争因素的也有很多了，以往都是着重在朝韩内战，到后来的“朝韩间谍”情节，再到“反美情节”，例如《战疫》，再到将“中美日”因素全部糅杂在一起，以朝鲜半岛为中心，创造出区域热战甚至是世界大战的剧情，这部《铁雨2三方会谈》不是第一次（《白头山》也是中美朝韩），当然也不会是最后一次，韩国已经成功探索出一条以现实为基础，演绎夸大为手段的政治剧情电影的路线，可以预见类似电影（中美频频出镜）以后会越来越多。 在这方面，我们的表现呢？战争片九成都是抗战或内战题材，发人深思即有内涵的并不多，到了偏剧情的战争片，更是缺乏剧情张力。我觉得诸如《战狼2》《红海行动》等就是不错的探索，虽然更加偏重战争，但总归是包含一些剧情的。港片的《赤道》等也是一种方向，不过其可能需要更加宽松的审核环境。感叹一声，不再赘述。 后记总而言之，这部的三方会谈，铺垫够足、剧情张力够大、结局够完美，作为一部标准的“韩式”YY剧情片，包含有足够量的政治、战争、人文情节，剧情紧凑不拖拉，如果你忘掉《铁雨1》，抛弃现实情节来观影，这绝对可以算一部上乘佳作。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"影评","slug":"影评","permalink":"https://www.cz5h.com/tags/%E5%BD%B1%E8%AF%84/"},{"name":"胡乱评论","slug":"胡乱评论","permalink":"https://www.cz5h.com/tags/%E8%83%A1%E4%B9%B1%E8%AF%84%E8%AE%BA/"}]},{"title":"群晖硬盘过热导致的关机保护","slug":"2020-8-4 群晖开不了机啦？别担心、看这里","date":"2020-08-08T22:00:00.000Z","updated":"2020-08-22T11:14:47.771Z","comments":true,"path":"article/fed3.html","link":"","permalink":"https://www.cz5h.com/article/fed3.html","excerpt":"从七月末开始，我的NAS就突然不能正常开机了，今天尝试排查了一下错误，结果能够正常启动且正常运行，但正常运行两个小时之后，群晖就又宕机无法开机了，如此反复气死我了，最后简单搞了一下暂时可用了，但依旧没有找到病根。","text":"从七月末开始，我的NAS就突然不能正常开机了，今天尝试排查了一下错误，结果能够正常启动且正常运行，但正常运行两个小时之后，群晖就又宕机无法开机了，如此反复气死我了，最后简单搞了一下暂时可用了，但依旧没有找到病根。 检查硬件 拆卸下来 - 有内存无硬盘状态开机 - 检查是否系统有问题 拆卸内存 - 擦拭插槽和插片，插回去后开机 - 检查内存是否有问题 装回硬盘 - 开机，看是否正常登入 - 检查硬盘是否有问题 以上是通用的步骤，对于具体的出错位置和原因，可以在我们重新登录群晖之后，通过查看日志中心来发现！ 我的故障机器通过拔插硬盘和内存，获得了短暂的复活时间，其实当时我以为是修复了，就在我截图准备写此文时，它居然又挂了，虽然这次我知道是什么问题了，但还是很懵，基本上在靠 UptimeRobot 的监控邮件在确认群晖的死亡，为了更加及时、清晰的知道群晖的宕机时间和原因，我推荐大家把群晖异常的邮件推送给配置上。 配置邮件通知非常简单，在控制面板的通知设置里，把信息填上即可，做完后记得发测试邮件试一试。注意：这里的密码不是QQ账号密码，而是要去QQ邮箱的设置中获取的 16位授权码！ 定位日志 在通过硬件插拔后，我顺利进入了系统，然后通过查看日志这个APP，可以发现很明显的、触发关机的异常，这里是硬盘温度超过保护温度而触发关机保护。 可以看到日志说是drive4的问题，在磁盘管理这个APP中，我们可以找到那块磁盘，不幸的是这块磁盘是我RAID阵列中的一块，可以看到截图时的温度还是很正常的，就在截图后的半小时内，它就又宕机了，如果还是硬盘温度超限，那我就很不解了，因为在此期间我并没有访问什么服务，也就开启了Web端的DSM桌面。 再仔细看日志的日期，或者直接看状态监控，可以发现最早的异常出现在7月22号（准确的说是7.21的23:58），要知道之前的时间跨度内我的群晖是百分百可用的，如果是硬盘老化，照理不会“坏”的这么突然。 可能的推测在上个月的那段时间我并不记得自己有任何对其的异常操作，所有操作均在七月22号之前很久就做了，我推测这次的硬盘升温并不是由于我的误操作引起的，可能还需要进一步找错误，硬盘4和另一块2T盘是同一时间同时从厂家买的，批号都很相似，讲道理不会有一块突然坏掉吧。。 如果真的是天气热导致的散热问题而引发的，那估计问题不大，天气不热了就没事了，不过还需要进一步观察。","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"群晖","slug":"群晖","permalink":"https://www.cz5h.com/tags/%E7%BE%A4%E6%99%96/"},{"name":"异常","slug":"异常","permalink":"https://www.cz5h.com/tags/%E5%BC%82%E5%B8%B8/"},{"name":"维护","slug":"维护","permalink":"https://www.cz5h.com/tags/%E7%BB%B4%E6%8A%A4/"}]},{"title":"快速建站“新玩具”—glitch.me","slug":"2020-8-6 快速建站“新玩具”—glitch.me","date":"2020-08-05T22:00:00.000Z","updated":"2020-08-06T21:49:14.905Z","comments":true,"path":"article/6868.html","link":"","permalink":"https://www.cz5h.com/article/6868.html","excerpt":"今天在 YouTube 看了一个谷歌的开发视频，里面主讲者用 Glitch.me 做了代码的演示平台，所以自然地通过视频下方的演示链接，接触到了 Glitch，初步使用过后，我觉得这和 now.sh 等功能差不多，专为Web静态页面提供快速发布服务。目前我看百度里对其介绍的文章基本没有，那我就来介绍一下它吧！","text":"今天在 YouTube 看了一个谷歌的开发视频，里面主讲者用 Glitch.me 做了代码的演示平台，所以自然地通过视频下方的演示链接，接触到了 Glitch，初步使用过后，我觉得这和 now.sh 等功能差不多，专为Web静态页面提供快速发布服务。目前我看百度里对其介绍的文章基本没有，那我就来介绍一下它吧！ 部分内容翻译自官方的说明，可不是我瞎编的！ Glitch到能干啥？创建Web应用的工具 其可以让所有人都能够使用此“开发工具”，因此其足够简单、易用。完善的代码补全提醒、健全的团队管理功能是其基本功能。除此之外，Glitch其实是一个协作式编程环境，只需要使用浏览器的开发页面，就可以构建从静态网页到结合全栈Node框架的所有应用。 第二个令我惊讶的特性是“多人协作开发”的功能，Glitch在这里真正做到了实时多人在线开发，这个功能还是很吸引人的。 第三点则是应用的“即时上线”功能，这里要吐槽下国内的Coding，使用静态部署一小时只有十次配额，对于Glitch，这将是无限的、实时的、无需我们干预的过程，Glitch本质上没有部署或发布的概念，修改后即生效。 第四点黑科技则是“和已有IDE的无缝结合”，例如可以在VSCode中使用插件来将Glitch的web开发页面搬到VSCode中，其余操作完全无影响。再比如结合Github，直接导入已有的代码进行构建。 友好、创造性的社区 Glitch已经包含了非常多的有趣的项目，包含了各种分类的各种内容，以游戏为例，其允许我们直接看到、使用到游戏渲染后的内容，这也算是Glitch的黑科技吧，如果你对某个项目感兴趣，那么将其内化并加以完善在Glitch上也是非常容易就能实现的！ Glitch怎么用？详细讲解其用法是不现实的，这里就两个关键功能来对Glitch做个简单的讲解。 快速发布静态页有多快速呢？基本上就是“新建项目-&gt;编辑index.html-&gt;获得页面链接”就结束了。具体登陆时可以直接用Github登陆，其不会再有附加账号的要求。新建项目后会发现有个双栏的开发页面，所有工作都可以在这个页面完成，而且重点是左侧部分，功能基本都在这块。 以 iamchinese 项目为例，修改完index.html后，随即打开左侧头像旁边的Share下拉框，选择LiveAPP，即可复制当前的页面链接，即：https://iamchinese.glitch.me，如果对默认名称不满意，可以直接修改一个可用的名称（即修改最左上角的项目名称）。 绑定域名 左下角，点击 Tools -&gt; Custom Domains，输入你的域名，即可以获得进一步的绑定信息。你会发现以下内容，至此应该有域名的朋友就知道怎么做了。 1234At your domain registrar you&#39;ll need to create a CNAME record for ihave.diedin.icu pointing to peyk2z9el29xvm93.preview.edgeapp.net 需要注意的是，Glitch已说明如果你想直接访问preview.edgeapp.net这个链接，那么你只会得到Not Found。其只能在CNAME下生效。这里我用Dnspod，只需要添加一个CNAME即可。解析后即可访问 ihave.diedin.icu。 至此，其主要功能“快速发布静态页面”就基本介绍完毕，各种扩展、功能大家可以自己去慢慢探索。 不过据不可靠消息称，Glitch在国内无法访问，未验证。","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://www.cz5h.com/tags/%E6%95%99%E7%A8%8B/"},{"name":"Glitch","slug":"Glitch","permalink":"https://www.cz5h.com/tags/Glitch/"}]},{"title":"修改自定义站点监控页面的样式","slug":"2020-7-25 修改自定义站点监控页面的样式","date":"2020-07-24T22:00:00.000Z","updated":"2020-08-29T12:17:27.143Z","comments":true,"path":"article/cf94.html","link":"","permalink":"https://www.cz5h.com/article/cf94.html","excerpt":"许久之前就开始使用 UptimeRobot 来监控站点的状态了，不过一直是使用默认的方式，即绑定域名后使用官方的默认模板，使用体验还算可以，但令人不爽的是只有最近一周的运行状态，而且感觉状态刷新频率也不高，时值近日 NAS 极不稳定（现在还没弄好），我非常想看看是有多不稳定，但官方的监控页面信息实在不足，因此换用基于官方API的自定义监控页面。","text":"许久之前就开始使用 UptimeRobot 来监控站点的状态了，不过一直是使用默认的方式，即绑定域名后使用官方的默认模板，使用体验还算可以，但令人不爽的是只有最近一周的运行状态，而且感觉状态刷新频率也不高，时值近日 NAS 极不稳定（现在还没弄好），我非常想看看是有多不稳定，但官方的监控页面信息实在不足，因此换用基于官方API的自定义监控页面。 官方默认模板示例页面：绑定的二级域名 不足之处： 只有七天状态信息可见 刷新频率未知（迟滞） 必须绑定域名，也就意味着无法作为网站内页（之前用iframe硬加进来的，其实监控页面是up.cz5h.com） 无法自由更改页面内容 自定义模板（原版）示例页面：原项目DEMO页 利用 UptimeRobot 的API开发的监控页面，很好的解决了上述不足： 其实现方式意味着我们可以随心所欲的安排监控页面出现的位置，网站内页或者嵌入到某页面都是可以实现的； 其API允许查看90天（据说）的状态信息，简直不要太开心； 实现方便，只需引入几个文件夹即可； 搭建步骤： 申请 UptimeRobot 账号，配置自己的站点并监控，其网站做的很人性化，步骤不再赘述，至此我们在其网站上就可以查看服务/站点的状态了； 下载自定义模板的代码，Github地址，并将其解压到Hexo文件夹内，具体可以放在 theme -&gt; yourTheme -&gt; source -&gt; folderName文件夹内； 在 UptimeRobot 官网，通过 MySettings - API Settings -&gt; Monitor-Specific API Keys -&gt; Show/hide 中的搜索框，找到你已创建的监控站点，点击生成 API kEY 并复制; 将 Key 复制到 config.js 中的对应位置，每个监控任务对应一个 Key，比如你监控了十个站点/服务，那需要十个 Key； 部署 Hexo，此时访问 yourSite/folderName（例如我的[cz5h.com/up}(https://www.cz5h.com/up)），就可以看到监控信息了。 不足之处： 样式太丑了，和我博客的风格很不搭，别的都很好，致敬作者。 简单修改后样式示例页面：本站内页 修改过程非常简单，直接将以下CSS代码写入 index.html 中即可。注意几点： 原页面有 footer 信息，这里直接用JS隐藏掉，之后你可以再覆写 footer，这样即使不修改源码也可以快速得到相要的样式； 样式名称还是很规范的，利用F12检查元素可以很快就完成修改； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;script&gt; document.getElementById('footer').style.display = 'none'; // 隐藏选择的元素&lt;/script&gt;&lt;style&gt;body &#123; background-color: #010012; background-image: linear-gradient(#010012, #111f4c, #010012);&#125;#header .navi &#123; font-size: 15px; font-weight: 600;&#125;#uptime .timeline i.ok &#123; background: #2195f3bb;&#125;#uptime .timeline i.down &#123; background: #f34539bb;&#125;#uptime .timeline i.none &#123; background: #2196f326;&#125;#uptime .meta .status.down &#123; background-image: url(); color: #f34539bb; font-weight: 600;&#125;#uptime .timeline i &#123; border-radius: 10px;&#125;#uptime .meta .status.ok &#123; background-image: url(); color: #2195f3bb; font-weight: 600;&#125;#uptime .meta .name &#123; font-weight: 600;&#125;#uptime .foot &#123; color: #757a80; font-weight: 600;&#125;#uptime .timeline &#123; height: 55px; display: flex; justify-content: space-between; margin-bottom: 10px;&#125;#footer a &#123; color: #999fa6; font-weight: 700;&#125;#header .logo &#123; font-size: 18px; font-weight: 700; color: #2195f3;&#125;#uptime .item &#123; padding: 18px 20px; border-bottom: 1px solid #c5def6;&#125;#uptime &#123; border: 0px solid #e3e4e6;&#125;#uptime .item &#123; border-bottom: 15px solid #060923;&#125;&lt;/style&gt; 最终效果：","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.cz5h.com/tags/Hexo/"},{"name":"UptimeRobot","slug":"UptimeRobot","permalink":"https://www.cz5h.com/tags/UptimeRobot/"},{"name":"CSS","slug":"CSS","permalink":"https://www.cz5h.com/tags/CSS/"}]},{"title":"快速重置乌班图和GL开发环境","slug":"2020-7-23 快速重置乌班图和GL开发环境","date":"2020-07-22T22:00:00.000Z","updated":"2020-08-20T10:57:14.990Z","comments":true,"path":"article/483a.html","link":"","permalink":"https://www.cz5h.com/article/483a.html","excerpt":"这是一份完整的Ubuntu安装手册，可以帮助你快速初始化系统到可用状态，此手册包含两部分，包括Ubuntu系统的安装及配置、及GL相关开发环境的初始化。为了使行文更加清晰，文中插图全部以链接形式呈现，该手册仍然在完善中并已开源在Github。","text":"这是一份完整的Ubuntu安装手册，可以帮助你快速初始化系统到可用状态，此手册包含两部分，包括Ubuntu系统的安装及配置、及GL相关开发环境的初始化。为了使行文更加清晰，文中插图全部以链接形式呈现，该手册仍然在完善中并已开源在Github。 Go to Github version，中英版本。 0. 机器信息 1. 安装系统 1.1 准备USB启动盘 1.2 在BIOS中关闭 Secure Boot 1.3 按正常步骤安装完系统 1.4 第一次进入桌面后更新系统，然后重启 2. 安装英伟达驱动 2.1 添加软件源并查看 2.2 通过软件和更新直接安装 2.3 不要设置 nouveau 黑名单或者设置 grub 启动项 3. 安装 cuda-toolkit 3.1 选择版本，这里我用的 cuda-10_*.run 3.2 执行 3.3 跳过显卡安装 3.4 测试 Samples 3.5 其他重要的配置 4. 安装 GL 相关环境、库 4.1 GL/gl.h 4.2 GL/glu.h 4.3 others 4.4 以上直接一次性安装 5. 安装项目所需软件并重构 5.1 基础工具 5.2 重新构建 6. 安装日程必备软件 6.1 截屏工具： flameshot 6.2 开发工具：vscode 7. 使用 Tweaks 修改主题 7.1 安装 Tweaks 及其扩展 extensions 7.2 更改顶部菜单栏样式 7.3 修改壁纸 7.4 修改面板到底部，并把图标放到桌面 8. 不重要的配置 8.1 添加中文支持 8.2 更改副屏分辨率 8.3 添加一些未适配的应用 8.4 Tweaks的一些不重要的美化设置 8.5 其他实用的配置及软件 8.5.1 屏幕亮度控制 8.5.2 Vscode配置 8.5.3 picgo 8.5.4 安装并配置 git 8.5.5 远程桌面软件 X. Ubuntu使用注意事项 X.1 对于更新 X.2 对于死机 0. 机器信息 MSI GE62 490, 显卡英伟达960M（笔记本） CPU: intel i7-6700HQ，带核显 单系统安装（ubuntu-18.04-LTS） 1. 安装系统1.1 准备USB启动盘注意：本文使用的官方镜像为 ubuntu-18.04.4-desktop-amd64.iso。 1.2 在BIOS中关闭 Secure Boot1.3 按正常步骤安装完系统注意：如果你可以正常走完安装流程进入桌面，那就不需要修改 grub 的任何信息（例如nomodeset）。正常情况下，安装完后拔出USB启动盘，应该可以顺利进入桌面（此时系统运行在核显驱动下）。 1.4 第一次进入桌面后更新系统，然后重启注意：当你第一次进入Ubuntu桌面时，可能会弹出系统及软件的更新，请同意更新，否则会影响后续操作。不过，当我们在安装完全部所需环境、软件之后，就不再需要更新了，尤其是尽量不进行系统级的更新，否则我们稍后安装的显卡驱动可能会因此出现异常。 2. 安装英伟达驱动2.1 添加软件源并查看123$ sudo add-apt-repository ppa:graphics-drivers&#x2F;ppa$ sudo apt-get update$ sudo ubuntu-drivers devices 注意：这里会输出你的机器适配的显卡驱动，并且会有一个 recommend 推荐项，我们稍后就安装这个推荐版本。 2.2 通过软件和更新直接安装1234Software and Updates -&gt; additional drivers -&gt; select the recommandation version -&gt; apply 注意：安装显卡驱动的方式有很多，较新版本的Ubuntu完全可以在“软件和更新”中直接进行显卡驱动的安装。无需下载安装文件，无需通过命令安装。 2.3 不要设置 nouveau 黑名单或者设置 grub 启动项注意：使用此种方式安装显卡驱动，配置完全由系统完成，我们不用做任何设置。在‘软件与更新’内完成显卡安装后，重启系统。 重启后进入桌面（此时正常情况下是会顺利启动的），此时用 nvidia-smi 可以测试显卡信息，截图：http://i.imgur.com/GgfSqCM.png，此外还可以直接通过系统信息来查看是否已加载英伟达显卡，截图：http://i.imgur.com/Euj6tQy.png。 3. 安装 cuda-toolkit（第3、4、5节和我自己的开发需求有关，一般使用可以跳过这几节！） 3.1 选择版本，这里我用的 cuda-10_*.run（截图： http://i.imgur.com/6xPtxju.png） 3.2 执行12$ sudo chmod 777 cuda-10_\\*.run$ sudo sh cuda-10_\\*.run 3.3 跳过显卡安装注意：我们在前面已经安装过显卡了，这里务必要回答 NO。 12Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 410.48?(y)es&#x2F;(n)o&#x2F;(q)uit: n 3.4 测试 Samples12345$ cd &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;samples$ make (10 minutes later)$ cd &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;samples&#x2F;bin&#x2F;x86_64&#x2F;linux&#x2F;release$ .&#x2F;deviceQuery （截图： http://i.imgur.com/jJ7vpNw.png） 3.5 其他重要的配置注意：别忘了添加环境变量到 /etc/profile，如果不添加的话，后面使用Cuda会出现 libcudart.so.10.0: cannot open shared object file: No such file or directory 这种错误。 12export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin:$PATH export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;lib:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F; 到此为止，英伟达显卡驱动和Cuda已经全部安装完毕。并且也通过测试来确保了安装的正确。 4. 安装 GL 相关环境、库（第3、4、5节和我自己的开发需求有关，一般使用可以跳过这几节！） 4.1 GL/gl.h1sudo apt install mesa-common-dev 4.2 GL/glu.h1sudo apt install libglu1-mesa-dev freeglut3-dev 4.3 others1libglfw3-dev libgles2-mesa-dev libglew-dev libeigen3-dev 4.4 以上直接一次性安装注意：这些库对我都是必须的，但这是取决于项目需求。 1$ sudo apt install mesa-common-dev freeglut3-dev libglfw3-dev libgles2-mesa-dev libglew-dev libeigen3-dev 5. 安装项目所需软件并重构（第3、4、5节和我自己的开发需求有关，一般使用可以跳过这几节！） 5.1 基础工具12$ sudo apt install vim$ sudo apt install cmake 5.2 重新构建12345678910$ cd ProjectionExplain&#x2F;LIBRARY&#x2F;glui-master$ rm CMakeCache.txt$ make clean$ mkdir build &amp;&amp; cd build$ cmake ..$ make install$ cd ProjectionExplain&#x2F;$ make clean$ make $ .&#x2F;projwiz -f DATA&#x2F;segmentation lamp 此时项目已经构建完毕并可以成功运行！ （截图： http://i.imgur.com/rDCtEId.png） 6. 安装日程必备软件6.1 截屏工具： flameshot使用 sudo apt-get install flameshot 完成安装，并如截图所示进行配置（截图： http://i.imgur.com/id2PPYj.png）。 6.2 开发工具：vscode..更多相关配置在 8.5 小节.. 直接在 Ubuntu 的‘软件与更新’内安装Vscode（截图： http://i.imgur.com/W971ERc.png）。 然后，对于 Cmake 项目，Vscode更多的充当编辑器的作用，搭配 terminal 进行构建即可。以前文项目为例： 在Vscode中打开项目文件夹 (ProjectionExplain) 在Vscode中打开终端，Terminal-&gt;new terminal 在Vscode代码区编辑代码，在终端内构建-&gt;make &amp;&amp; ./projwiz -f DATA/segmentation lamp 7. 使用 Tweaks 修改主题7.1 安装 Tweaks 及其扩展 extensions1$ sudo apt install gnome-shell-extensions gnome-shell-extension-dash-to-panel gnome-tweaks adwaita-icon-theme-full 在这之后需要注销或者重启生效。 7.2 更改顶部菜单栏样式扩展名称：applications menu。注意：开启此扩展可以使应用菜单栏变得和Windows风格一致。扩展-&gt;设置内的选项都可能会被用到，可以自己探索一下。 7.3 修改壁纸7.4 修改面板到底部，并把图标放到桌面Ubuntu 的dock默认的风格更像是 OSX 系统，Tweaks允许我们修改其成为 Windows 风格（在底部）。另外，像Windows那样，我们把图标也放在桌面上。 首先对 dock 样式进行修改： 12345678910111213141516打开 Tweaks -&gt; Extensions -&gt; 打开 &#39;applications menu&#39; -&gt; 打开 &#39;Dash to panel&#39; -&gt; 打开设置 -&gt; Location and Style -&gt; panel location -&gt; bottom -&gt; panel size -&gt; 45px -&gt; icons margin -&gt; 5px -&gt; indicator -&gt; open settings -&gt; height -&gt; 0px (we hide it by this) -&gt; clock location -&gt; right -&gt; task panels -&gt; left -&gt; Behaviors -&gt; 关闭 &#39;favoriate applications&#39; -&gt; 关闭 &#39;applications icon&#39; -&gt; 打开 &#39;show desktop&#39; -&gt; 关闭 &#39;cancel applications&#39; group&#39; 对于图标问题： Ubuntu 默认的图标路径有好几个，下面是几个可能包含图标的路径，我们只需要将这些图标复制到桌面即可。注意：你会发现复制之后图标变成了 *.desktop 文件，我们只需要双击它们，然后点击‘信任并启动’，之后它们就会变回图标的样子了。 123&#x2F;usr&#x2F;share&#x2F;applications&#x2F;var&#x2F;lib&#x2F;snapd&#x2F;desktop&#x2F;applications~&#x2F;.local&#x2F;share&#x2F;applications 至此，基础的主题修改已经完成，目前为止，你的 Ubuntu 看起来应该非常接近 Windows 了。（截图： http://i.imgur.com/y7safc9.png） 8. 不重要的配置8.1 添加中文支持 第一步给系统添加中文 123456789打开 Settings -&gt; Region &amp; Languages -&gt; manage installed language -&gt; 如果弹出 &#39;installed not complete&#39;，那么先完成安装 -&gt; 点击 &#39;Install&#x2F;Remove Languages&#39; -&gt; 选择 Chinese 然后安装（可能需要双击） -&gt; 然后在 &#39;Language for menu and windows&#39; -&gt; 拖拽 Chinese 到排名第一的位置-&gt; 现在在 &#39;Region &amp; Language&#39;中，语言已经自动的变为中文了 注意：上述操作完并重启后，系统才会变成中文。一般会弹出一个 ‘change public folder names’ 询问弹框，就是问你要不要修改那几个公共文件夹的名字，强烈建议不要修改，即保持默认英文名称，这对于很多 bash 操作是很有利的。 安装搜狗输入法 注意：从官网下载安装包 Linux_64-bit.deb.，如果你用的是比较老的 Ubuntu 系统，请参照 此页面 进行安装。 安装 fcitx 并修改输入源为 SogouPinyin 123456789101112$ sudo apt install fcitx然后打开 Settings -&gt; manage installed language -&gt; 修改 &#39;Input System&#39;(默认是IBus) 为 fcitx -&gt; 然后点击 &#39;apply system-wild&#39;-&gt; 现在最好是重启一次，然后 -&gt; 在任务栏找到一个小键盘图标， -&gt; 点击&#39;configure current input method&#39; -&gt; 然后应该能发现搜狗输入法，还有些别的输入法 我们可以删掉其余的输入法，只保留搜狗输入法和英文输出现在就完成全部输入法配置了，默认的切换键是 ‘单次Shift’ 注意： 以上三部分是不同的，请一步步来，并且在过程中如果没发现新增项，请尝试重启系统。有了中文支持，我们可以进一步对 Tweaks 进行配置。 （截图： http://i.imgur.com/l5sLZwZ.png） 8.2 更改副屏分辨率 连接你的副屏，我的副屏是2k屏，笔记本是1080的，所以输出分辨率最大只有1080，我们需要手动修改它； 新建个 sh 文件，里面包含重置分辨率的命令，这样只需要每次重启后设置自动执行，即可自动变为2k的输出。 下面需要做些准备工作来确定最后写入 sh 文件的三行命令的参数。 123456789101112首先是得到本机指定分辨率的渲染参数$ cvt 2560 1440&#x2F;&#x2F;下面的&quot;2560x1440_55.00&quot;就是从这里获得的$ xrandr -q&#x2F;&#x2F;这里查看副屏对应的显示器编号，下面的&quot;HDMI-1-2&quot;就是从这来的。 写入文件的三行：xrandr --newmode &quot;2560x1440_55.00&quot; 284.00 2560 2744 3016 3472 1440 1443 1448 1489 -hsync +vsyncxrandr --addmode HDMI-1-2 &quot;2560x1440_55.00&quot;xrandr --output HDMI-1-2 --mode &quot;2560x1440_55.00&quot; --output eDP-1-1 --off 文件写好后，你可以手动去执行，当然如前所述，使之开机自动运行是最好的。 123quick way : -&gt; 将这三行命令放入 &#x2F;etc&#x2F;profile 文件内的最末尾； -&gt; 重启后会发现可以自动得到 2k 的分辨率。 注意：这样写的弊端在于，你必须保证2k副屏后再重启，如果你不用副屏了，你必须从 profile 里去掉这三行命令，否则会出错的。 8.3 添加一些未适配的应用像是微信、QQ等聊天工具，或者红警等游戏，对于这些软件，默认是没有支持的，但仍然可以通过一些方法安装。 方法一：在‘软件与更新’中安装 Wine，然后通过 Wine 来安装 Windows 应用。 方法二：安装安卓模拟器，通过模拟器来使用QQ等软件，这里我推荐 这个模拟器，渲染速度和稳定性都还是不错的。（截图： img/Cache_32799f853a0e21fe..jpg） 方法三：墙裂推荐！这里直接下载封装了精简 Wine 环境的 QQ 软件包，即可以使用QQ，无需多余安装 Wine 也无需其他预先配置，只需要下载对应的 AppImage 文件，然后启动即可使用！ 方法三步骤：（Github地址） 123456&#x2F;&#x2F;首先去release下载文件$ chmod a+x TIM-x86_64.AppImage&#x2F;&#x2F;通过命令直接启动$ .&#x2F;TIM-x86_64.AppImage.AppImage 注意，尽量选择TIM，QQ在我机器上测试发现有点问题。 如果你想通过图标点击来启动之，那么可以自己新建一个图标文件来完成，然后将此图标文件放到~/.local/share/applications，之后，在系统软件搜索界面你就能搜到你刚才添加的图标了（也就是个应用启动器），如果你想把它放到桌面，只需要将其复制到桌面，双击一下（选信任并启动）就行了。下面是制作 QQ 图标的步骤： 首先，获取图片（qq.png）然后创建 desktop 图标文件。 12$ wget https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;TianZonglin&#x2F;tuchuang&#x2F;img&#x2F;qq.png$ vim QQ.desktop 然后，复制并保存以下内容到你的图标文件中，注意修改路径。 123456[Desktop Entry]Name&#x3D;QQExec&#x3D;&#x2F;home&#x2F;tzloop&#x2F;Downloads&#x2F;TIM-x86_64.AppImageIcon&#x3D;&#x2F;home&#x2F;tzloop&#x2F;Downloads&#x2F;qq.pngType&#x3D;ApplicationStartupNotify&#x3D;true 然后右键这个文件，在权限页面，勾选“允许作为应用执行”，最后将这个图标文件复制到指定路径。 1$ sudo cp &#x2F;home&#x2F;tzloop&#x2F;Downloads&#x2F;QQ.desktop ~&#x2F;.local&#x2F;share&#x2F;applications 8.4 Tweaks的一些不重要的美化设置 主题 -&gt; Adwaita-dark 指针 -&gt; Adwaita-default 字体设置，默认的字体及大小为 Ubuntu/11, Ubuntu Regular/11, Sans Regular/11, Ubuntu Mono Regular/13这里我把前三项设置为了 Ubuntu Medium 字号不变，然后把整体的 zoom ratio 设置为了 1.3. 8.5 其他实用的配置及软件8.5.1 屏幕亮度控制我使用 RedShift 软件来控制，相同的软件还有很多，像是 F.lux 等等也是很棒的。安装 RedShift 的步骤： 12345-&gt; install it with &#96;sudo apt-get install redshift-gtk&#96;, gtk means visual version.-&gt; open location service: Settings -&gt; Privacy -&gt; Location service -&gt; open. (this step is ESSENTIAL)-&gt; open redshift, if you can&#39;t find the icon, just search it in &#39;applications&#39;.-&gt; then, brightness will be changed-&gt; finally, with its menu, you can set it open with your system. 8.5.2 Vscode配置@主题 File -&gt; Preferences -&gt; Color Theme，我选择暗蓝色的 Tomorrow Night Blue 主题。 @字体 File -&gt; Preferences -&gt; Settings，通过search （搜索 font）来定位。我推荐直接用JSON进行编辑，编辑内容如下： 12345678910&#123; &quot;terminal.integrated.fontFamily&quot;: &quot;monospace&quot;, &quot;editor.fontWeight&quot;: &quot;600&quot;, &quot;editor.fontFamily&quot;: &quot;monospace&quot;, &quot;editor.fontSize&quot;: 15.5, &quot;terminal.integrated.fontSize&quot;: 12, &quot;workbench.colorTheme&quot;: &quot;Tomorrow Night Blue&quot;&#125;以上组合在我的2K高分屏上工作的很好，推荐使用。 如果你想让你的编码字体和 Windows 上的VSCode字体一致，那么你得首先安装 Windows 使用的字体，安装字体的步骤如下： 123456789$ wget https:&#x2F;&#x2F;down.gloriousdays.pw&#x2F;Fonts&#x2F;Consolas.zip$ unzip Consolas.zip$ sudo mkdir -p &#x2F;usr&#x2F;share&#x2F;fonts&#x2F;consolas$ sudo cp consola*.ttf &#x2F;usr&#x2F;share&#x2F;fonts&#x2F;consolas&#x2F;$ sudo chmod 644 &#x2F;usr&#x2F;share&#x2F;fonts&#x2F;consolas&#x2F;consola*.ttf$ cd &#x2F;usr&#x2F;share&#x2F;fonts&#x2F;consolas$ sudo mkfontscale &amp;&amp; sudo mkfontdir &amp;&amp; sudo fc-cache -fv&#x2F;&#x2F;check the installed fonts$ fc-list @终端的位置 在设置中搜索 location，找到 Workbench › Panel: Default Location ，将其修改为 Right，因为我想让代码区有更高的视野，同时代码也不会很长也就是不需要很大的宽度，即右侧部分可以来放置终端。 @在左侧文件树视图内文件的排序规则 在设置中搜索 explorer.sortOrder，并将其设置为按 type 排序，这对于混杂不同类型文件的项目来说很方便，相同种类的文件被放在一起。 最后修改完配置的Vscode截图长这样：http://i.imgur.com/g7OehEL.png。 8.5.3 picgo如果你希望方便的上传、分享你的截图或图片，那么一定要试试 PicGo，其在 Ubuntu 上的安装使用和之前的 TIM.AppImage 非常类似，具体如下： 12345678910111213141516首先获得图标 picgo.png 并创建你的图标文件$ wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;TianZonglin&#x2F;tuchuang&#x2F;master&#x2F;img&#x2F;opic.png$ vim QQ.desktop然后复制以下内容到图标文件内，改成你的路径。[Desktop Entry]Name&#x3D;PicGOExec&#x3D;&#x2F;home&#x2F;tzloop&#x2F;Downloads&#x2F;PicGo.AppImageIcon&#x3D;&#x2F;home&#x2F;tzloop&#x2F;Downloads&#x2F;opic.pngType&#x3D;ApplicationStartupNotify&#x3D;true然后右键-&gt;权限页-&gt;允许作为应用执行最后复制到所需要的路径中 如果你想直接上传你剪切板内的图片，那么在Ubuntu上你还需要安装个软件，通过 sudo apt install xclip 直接安装即可，下面是 PicGo 配置 Github 图床的步骤： 12345repository name -&gt; GITHUBACCOUNT&#x2F;tuchuangbranch name -&gt; mastertoken -&gt; get a new one from Github (Settings -&gt; Developer Settings -&gt; Personal access tokens)storage path -&gt; img&#x2F;custom domain name -&gt; https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;GITHUBACCOUNT&#x2F;tuchuang 8.5.4 安装并配置 git settings 安装非常简单，直接用 sudo apt install git 进行安装，然后配合 Github，接下来设置免密。注意：Github只是一种 git server，其他的诸如 gitlab 等同理。 12$ ssh-keygen -t rsa -C &quot;YOUREMAIL&quot;$ vim ~&#x2F;.ssh&#x2F;id_rsa.pub 然后去 Github -&gt; Settings -&gt; SSH and GPG keys -&gt; New SSH key，然后把 rsa.pub 的内容拷贝到 key 这里。 12$ git config --global user.name &quot;YOURUSERNAME&quot;$ git config --global user.email &quot;YOUREMAIL&quot; 至此，我们已完成免密设置，之后我们提交代码就不用进行账号验证了。（这只是ssh方式，https需要额外的设置） 同步代码的案例（我的方法） 在 Github 新建一个仓库并命名为你项目名称，然后通过 git clone git@xx.git 将此空项目克隆到你的电脑，最后，将原项目的内容全部移动到此空文件夹内，然后执行以下语句，即可完成第一次提交。 123git add -Agit commit -m &quot;&#96;date +%Y-%m-%d,%H:%m&#96;&quot; git push -u origin master -f 现在你的代码应该已经可以在 Github 上可见了，如果你想控制提交的部分，你还需要修改/创建 .gitignore 文件。 我通常将其放在 sh 文件内，并在文件内的第一行添加 #!/bin/sh，这样就可以通过一行命令直接运行这三行命令，当然如果你想自动地、周期性的备份你的代码，你可以用 crontab 来将这个 sh 设置为周期性执行，这样你就无需手动备份啦！ 8.5.5 远程桌面软件对于Ubuntu用户来说，好的远程桌面软件也是十分重要的，这里我强烈推荐 AnyDesk，这是个非常轻量。易安装的远程桌面软件，没有 Vnc 那种额外的设置，只需要下载、安装，即可使用。 使用 Anydesk 首先要去官网（here ）下载 ，之后安装之，安装后即可使用。 X. Ubuntu使用注意事项X.1 对于更新在安装完所有驱动软件环境之后，尽量不进行系统更新，或者推迟它，不要尝试在设置中停止更新，因为我没找到，而且实际上我把软件更新给停了，给我造成好大不便，所以只需要记住关掉每次的更新弹窗即可。（不要动截图这里的东西：http://i.imgur.com/w7Kvc7X.png） X.2 对于死机对 Ubuntu 来说，死机更多时候是桌面卡死，也就是假死，系统并没有死机，tty仍然可以进入，这时候切记不要断电重启，这种硬重启会给内核带来不明影响，说不定这样之后你系统就启动不了啦！ 正确的方法是：Ctrl+Alt+F2/3/4，进入 tty2/3/4，然后 restart gdm/lightdm，或者你如果记得你先前的错误操作，那么在此处回滚即可，亦或是等着什么也不做，有些情况下系统会在一段时间后从死机中恢复的。","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://www.cz5h.com/tags/Ubuntu/"},{"name":"显卡","slug":"显卡","permalink":"https://www.cz5h.com/tags/%E6%98%BE%E5%8D%A1/"},{"name":"CUDA","slug":"CUDA","permalink":"https://www.cz5h.com/tags/CUDA/"}]},{"title":"十分钟搭建自己的Q&A问答社区","slug":"2020-6-27 十分钟搭建自己的Q&A问答社区","date":"2020-06-27T22:00:00.000Z","updated":"2020-08-23T19:55:09.297Z","comments":true,"path":"article/614d.html","link":"","permalink":"https://www.cz5h.com/article/614d.html","excerpt":"开源问答社区一直是一种社区形式，而且已经形成了诸如 Stack Overflow 和国内 SegmentFault 等一众问答社区。这些问答型社区的初衷都是旨在围绕特定的主题（编程）来对相关知识进行发问和探讨，区别于 Quarta 和知乎这类问答，其目标更明确，答案更清晰，整体上更像一个特定的知识库，通过问题来了解某一特定领域我认为也是个不错的选择。那么现在，围绕前段时间的CSC申请，在专栏已经积累了大量问答的材料，已经有必要专为其构建一个清晰、可用的问答自主系统了，Question2Answer 无疑对此非常合适！","text":"开源问答社区一直是一种社区形式，而且已经形成了诸如 Stack Overflow 和国内 SegmentFault 等一众问答社区。这些问答型社区的初衷都是旨在围绕特定的主题（编程）来对相关知识进行发问和探讨，区别于 Quarta 和知乎这类问答，其目标更明确，答案更清晰，整体上更像一个特定的知识库，通过问题来了解某一特定领域我认为也是个不错的选择。那么现在，围绕前段时间的CSC申请，在专栏已经积累了大量问答的材料，已经有必要专为其构建一个清晰、可用的问答自主系统了，Question2Answer 无疑对此非常合适！ 注意：类似的框架/网站，还有不少，选择 Q2A 的主要原因还是因为它的易用性，搭建下来确实是非常的简单。 安装教程安装 Question2Answer 系统是非常非常简单（dead easy）的，官方教程比较啰嗦，这里直接总结干货。 环境需求首先选择自己的 Server，这里直接以 Apache 为例；然后还需要 PHP 和 Mysql，相关的软件环境版本的要求如下。更具体来说，直接安装 phpstudy（6.5以上版本均可）就完事儿了，全都是符合要求的。 123Apache or Nginx 无版本要求PHP &gt;&#x3D; 5.2 需要带MySQLi扩展（默认不用管）MySQL &gt;&#x3D; 5.x 安装完 phpstudy 后，启动 Apache+Mysql，然后用其自带的 mysql-front 工具打开数据库，创建一个数据库，或者选择一个默认的数据库也可以，设置用户名密码（如果你需要密码），到此基本就完成配置了，后面需要给其提供数据库地址（localhost）、登录用户名、登录密码、数据库名称，即可。 下载Q2A文件首先，去其Github的release下载最新版本，点击前往。下载之后，将这个压缩包解压，然后全部放入 Apache 的网站根目录（WWW目录）。注意，在此处需要修改两个文件，qa-config-example.php 和 .htaccess-example，分别修改成 qa-config.php 和 .htaccess，对于后着可能会出现无法修改文件名的问题，自己想办法吧哈。（至此就能访问了，但我们还要引入一个中文包） 然后，在 这里 下载其中文语言包。将解压出的语言文件（就一个zh文件夹），放入 WWW 目录下的 qa-lang 文件夹，和初始的英语语言文件夹并列。到此就结束了，后面在网页上可以自动识别并加载语言包。 最后，打开浏览器，访问 localhost:80 可以看到一个初始页面，上面有个 很长的按钮（初始化的按钮），点击一下稍作配置（设置超级管理员、语言）即可进入 Q2A 系统。 修改系统的配置项初始系统的主题叫 SnowFlat，样子还是不错的，整体的论坛功能分区也很简单，所有的操作都集中在顶栏，点击其中的管理，进入配置页面，在此页面的内页顶栏又分了许多子项（大概十几项）。没必要每个都拿出胡来说一遍，如果你正确的配置了中文环境， 那么其实每一页的功能就很明了了，你需要做的就是阅读每一页，并且尝试修改这些功能，然后查看效果（都是修改提交后即可生效的）。 在我这段时间的使用过程中，觉得主要的修改集中在以下几个方面： 普通：主体配置。 布局：修改各位置的html内容。 列表：关于显示条数。 权限：关于提问或者回答问题。 页面：关于导航栏的显示。 插件：开启标签云、快速提问等。 再次强调，都是很简单的配置项，基本上自己尝试一遍就可以迅速的把相关内容修改到理想的状态了，多试试就OK啦。 最终效果演示地址（临时）：点击前往","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"建站","slug":"建站","permalink":"https://www.cz5h.com/tags/%E5%BB%BA%E7%AB%99/"},{"name":"教程","slug":"教程","permalink":"https://www.cz5h.com/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"体验1Gbps的上下行对等网络","slug":"2020-6-26 体验100Ms的上下对等网络","date":"2020-06-25T22:00:00.000Z","updated":"2020-06-26T22:20:37.557Z","comments":true,"path":"article/cbd8.html","link":"","permalink":"https://www.cz5h.com/article/cbd8.html","excerpt":"由于新冠疫情的消退，关闭了许久的校园终于开始逐步开放，又坐回熟悉的位子，打开熟悉的电脑，一顿操作后一切如旧。也许是我心血来潮，毫无征兆的就想测一下网速，不测不知道，一测吓一跳，阔别了几个月的网络，今天一测居然达到了1Gbps（1000Mbps），而且还是上下行对等！瞬间惊掉了我的下巴…在此简单记录一下初次开车上高速的心情。","text":"由于新冠疫情的消退，关闭了许久的校园终于开始逐步开放，又坐回熟悉的位子，打开熟悉的电脑，一顿操作后一切如旧。也许是我心血来潮，毫无征兆的就想测一下网速，不测不知道，一测吓一跳，阔别了几个月的网络，今天一测居然达到了1Gbps（1000Mbps），而且还是上下行对等！瞬间惊掉了我的下巴…在此简单记录一下初次开车上高速的心情。 写在前面曾几何时，百兆光纤还是写在网吧门口的标语，作为重大卖点吸引顾客。而现在，百兆（100Mbps）网络早已飞入寻常百姓家，据我所知，下到城镇上至城市，200M光纤、500M光纤已成为大家的普遍选择。需要注意的是，这里的百兆，通通指的是下行带宽，对于上行带宽，不论国内还是国外，通常只有下行的十分之一，上下行对等网络由于费用问题，一般难以推广。随着5G时代的到来，越来越多的国内运营商开始吹捧1Gbps的超大网络带宽，但以目前能检索到的资讯来看，即便是目前试行的标称1Gbps的网络，速度往往也只有600-800Mbps，水分还是存在的。 那么，我们学校现在的网络到底是怎么回事儿呢？从原来的中等生一跃成为尖子生，到底发生了什么，我也不知道，或许是整合了华为的5G？这谁又知道呢。 1Gbps初体验话不多说了，开车，看一下风景！ 一般的网络资源下载如我所预料，看到这个106M/s的下载速度，心简直要飞出来了，活这么大还没体验过这下载速度噢。。这瞬间让我有点期待5G的来临了，虽然我不知道这个速度是不是5G来的。。 YouTube 8K 60FPS连接速度居然达到了惊人的41万kbps，要知道普通的海外百兆（100）带宽的连接速度也只有十几万，国内翻Q发烧友们看YouTube的发烧水准也只停留在20万，因为二十万左右8K视频就可以流畅（Buffer Health，超前缓冲）观看了。私以为这个速度还可以更快更恐怖，只不过我找不到合适的测试视频了，汗颜。 更新LOL客户端几个月没有打开的LOL，打开之后没注意的功夫，已经进入主界面了，真的是一种毫无迟滞感的体验。 写在最后不得不说，网络速度是这个网络时代影响人们体验网络的最最重要的因素之一，通过我的使用经历，我只能说网速的提升确实可以给我带来不一样的体验： 想到、找到，即可以下载到； 想到、上传，即可以云端看到。 这是从百兆到千兆的跳跃，也是改变人们网络使用习惯的一个转折，或许在可预见的未来，网速已失去其标量意义，超大带宽让人们忽略传输时间，那种生活的方方面面或许我们还不能够想象，但可以肯定的是，它正在向我们走来。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"联网","slug":"联网","permalink":"https://www.cz5h.com/tags/%E8%81%94%E7%BD%91/"},{"name":"初体验","slug":"初体验","permalink":"https://www.cz5h.com/tags/%E5%88%9D%E4%BD%93%E9%AA%8C/"}]},{"title":"将群晖相册嵌入到Hexo博客中","slug":"2020-6-21 将群晖相册嵌入到Hexo博客中","date":"2020-06-20T22:00:00.000Z","updated":"2020-08-01T07:39:13.861Z","comments":true,"path":"article/1c44.html","link":"","permalink":"https://www.cz5h.com/article/1c44.html","excerpt":"如何高效便捷的在博客中更新自己的动态是困扰很多人的问题，简单的方案就是另起炉灶在博客重新发布一份，但这种方法耗时耗力，因此不建议这样做。从原po平台同步到博客的某个位置应该是最理想的解决方案。对于照片来说，群晖的相册给我提供了一个来源库，那么如果我想让访客浏览我的近照，那么如何把照片从群晖相册同步到博客中就成了实现这一想法的核心问题。","text":"如何高效便捷的在博客中更新自己的动态是困扰很多人的问题，简单的方案就是另起炉灶在博客重新发布一份，但这种方法耗时耗力，因此不建议这样做。从原po平台同步到博客的某个位置应该是最理想的解决方案。对于照片来说，群晖的相册给我提供了一个来源库，那么如果我想让访客浏览我的近照，那么如何把照片从群晖相册同步到博客中就成了实现这一想法的核心问题。 iframe{ pointer-events: none; } function changeFrameHeight(){ var iframe= document.getElementById(\"myiframe\"); var wid = document.getElementById('father').clientWidth; if (wid>900) iframe.height = wid/2.19; else if (wid>551) iframe.height = wid/2.21; else if (wid>470) iframe.height = wid/2.165; else iframe.height = wid/2.26; } function changeFrameHeight2(){ var iframe= document.getElementById(\"myiframe2\"); var wid = document.getElementById('father').clientWidth; if (wid>900) iframe.height = wid/1.59; else if (wid>551) iframe.height = wid/2.01; else if (wid>470) iframe.height = wid/1.965; else iframe.height = wid/2.06; } window.οnresize=function(){ changeFrameHeight(); changeFrameHeight2(); } 拍摄设备：HUAWEI P30同步来源：群晖 PhotoStation Album扩展详情：关于如何放置在主页，详见 更新日志 实现思路可以查证的、群晖官方提供的嵌入相册的方法，主要就是使用iframe进行嵌入。注意这里说的群晖相册，特指 Photo Station，而不是Moments等套件、在使用 Photo Station 的过程中，比较好的地方在于照片的地图模式、缩略图加载速度以及丰富的配置设置项。Moments我没有用过，但由于其关联套件太多，所以不在我的考虑范围之内，要知道我是个连Drive都不用的群晖用户。 采用iframe嵌入的另一个好处是，异步加载，基本对原站加载速度没什么影响。 遇到的问题iframe的大小自适应这是个老生常谈的问题，但其实每次遇到其详细情况又都不尽相同，这里使用日常解决方案：JS控制。 123456789101112131415&lt;script type=\"text/javascript\"&gt; function changeFrameHeight()&#123; var iframe= document.getElementById(\"myiframe\"); var wid = document.getElementById('father').clientWidth; if (wid&gt;900) iframe.height = wid/2.19; else if (wid&gt;551) iframe.height = wid/2.21; else if (wid&gt;470) iframe.height = wid/2.165; else iframe.height = wid/2.26; &#125;&lt;/script&gt;&lt;div style=\"background-color: rgb(0, 0, 0);padding-top: 7px;\" id=\"father\"&gt; &lt;iframe width=\"100%\" id=\"myiframe\" frameborder=\"0\" onload=\"changeFrameHeight()\" src=\"https://nas.cz5h.com:5443/photo/embed/embed.html?album=album_5745425f414c42554d&amp;autoplay=1&amp;lightbox=1\" photostation&gt; &lt;/iframe&gt;&lt;/div&gt; 巧妙之处在于包裹在iframe之上的div，正式利用这个父级div标签来获得iframe可以“撑起来”的最大宽度，如果直接拿原主题文件的类去获取宽度，那需要复杂的选择器实现。 得到父级标签的宽度之后就很容易通过当前宽度来配置iframe的高度了（宽度就是div的宽度）。 嵌入的链接为http而被禁止 Mixed Content: The page at &#39;https://www.cz5h.com/&#39; was loaded over HTTPS, but requested an insecure frame &#39;http://asus.myds.me:5080/photo/embed/embed.html?album=album_5745425f414c42554d&amp;autoplay=1&amp;lightbox=1&#39;. This request has been blocked; the content must be served over HTTPS. 这也是个令人头痛的问题，基本上如果Server不是自己手写的代码，那就没法解决，只能换用https的安全链接。为此，我终于在群晖上把https的证书搞好了（之前一直拖着没弄）。 之后通过https访问5001是可以了，但群晖相册却怎么也打不开了，在经过了一大堆尝试（端口映射、相册设置等等）之后，终于解决了这个问题。 需要注意的是，群晖的相册默认是使用80端口，在内网是直接以 的形式访问，当然如果在外网，需要配置一个端口映射到80，在http时代，我使用5080-&gt;80，即访问地址是：http://nas.cz5h.com:5080/photo/#Albums 在https时代，相应的，应该再用一个端口映射到443（！非常重要），也就是这里的5443-&gt;443，之后访问地址应该是:https://nas.cz5h.com:5443/photo/#Albums 至此，iframe采用新的https链接，问题解决。 群晖相册的灯箱模式失效经验证，引入如下的JavaScript源是会出现错误的，而且导致允许连接到群晖相册和单机进入灯箱模式的失效，解决方法就是简单地取消第一、三项的勾选，只保留基本的幻灯片模式。 其他可能的引入方法RSS订阅这是一个比较通用的解决方案，缺点是好像群晖相册的RSS源只包含了有限张照片，并没有将同一相册的全部照片都涵盖其中，亟待寻求解决方案。 如果一切正常的话，通过RSS解析到图片，然后再通过自己的想法构造页面就可以了，这种方法是最贴合的嵌入方式，但暂时还不知道如何获得包含全部图片的订阅源。 待补充… 写在最后我已经将群晖相册做了整理并公开了部分相册，欢迎访问直接访问我的群晖相册。http://nas.cz5h.com:5080/photo/#Albums 群晖相册点开图片后还可以留言，简直不要太方便！另外，PhotoStation还允许你批量添加（显示层面的）自定义水印！","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.cz5h.com/tags/Hexo/"},{"name":"插件","slug":"插件","permalink":"https://www.cz5h.com/tags/%E6%8F%92%E4%BB%B6/"},{"name":"群晖","slug":"群晖","permalink":"https://www.cz5h.com/tags/%E7%BE%A4%E6%99%96/"}]},{"title":"使用R语言转存Excel到MySQL","slug":"2020-6-19 使用R语言转存Excel到MySQL","date":"2020-06-18T22:00:00.000Z","updated":"2020-06-27T22:26:49.584Z","comments":true,"path":"article/528.html","link":"","permalink":"https://www.cz5h.com/article/528.html","excerpt":"花了两天时间写了一个Excel数据转换脚本，原需求除了要把数据转存到Mysql中，还要对每一条数据进行拆分和重组，并不容易。最终我利用R语言完成了这个小需求，本着总结学习的想法，在此处将多余逻辑删除，抽离出了最基本的Excel转存Mysql的功能，这样也可以算一个小轮子了。（仅做学习用，Navicat等工具可以直接导入Excel）","text":"花了两天时间写了一个Excel数据转换脚本，原需求除了要把数据转存到Mysql中，还要对每一条数据进行拆分和重组，并不容易。最终我利用R语言完成了这个小需求，本着总结学习的想法，在此处将多余逻辑删除，抽离出了最基本的Excel转存Mysql的功能，这样也可以算一个小轮子了。（仅做学习用，Navicat等工具可以直接导入Excel） 本项目已开源至Github，地址：https://github.com/TianZonglin/transferExcelbyR 适用场景和使用要求（暂） 需要处理的表文件以文件夹形式存储 全部的数据表均必须包含相同的列格式，切忌无关表的污染 每个表文件中只有一个Sheet 数据表必须在第三级目录（单文件亦是如此），例如 12ecProject\\io_Input_Excel_Folder\\simples\\ORGDATA.XLS# 分别是：工作目录 -&gt; 输入文件夹(1st) -&gt; 内部自定义的文件夹(2nd) -&gt; 真正的表文件(3rd) 此脚本的特点 批量处理全部输入文件夹下二级文件夹的全部Excel表文件 自动根据所需要转换的Excel表文件在Mysql中创建表 自动检测Excel表文件的数据边界 详细的debug统计信息 合并全部Excel表文件到单一的Mysql数据表 默认不需要对数据库进行操作 使用方法使用对象tool_excel2mysql.R，这是通用的转换工具，其他脚本面向特殊的需求。推荐使用** R Studio **运行此代码。 所需要的软件和开发环境 即需要Mysql的环境+Mysql的可视化工具+R环境+R可视化开发工具，上述所有软件均可在网上找到。 注意：更新使用 Navicat 11 premium，原版本太过老旧。百度网盘：https://pan.baidu.com/s/18zg6NNogRVRHHD-fEj9UCg 提取码：cbkj 安装所需要的程序包123456# 注意这部分仅运行一次即可# near line 8#install.packages(&quot;RMySQL&quot;)#install.packages(&quot;stringr&quot;)#install.packages(&quot;readxl&quot;)#install.packages(&quot;readr&quot;) 修改工作路径12# near line 14setwd(&quot;C:\\\\Users\\\\zonglin\\\\OneDrive - Universiteit Utrecht\\\\Desktop\\\\ecProject\\\\&quot;) 修改Mysql配置1234567# 默认数据库名称: test# near line 17conn &#x3D; dbConnect(MySQL(), user &#x3D; &#39;root&#39;, password &#x3D; &#39;root&#39;, dbname &#x3D; &#39;test&#39;,host &#x3D; &#39;localhost&#39;)# 默认生成的表名称: tb_from_excel# 如果需要修改，可以直接在代码中全局替换这个字段# use editor&#39;s find&#x2F;replace function to replace it all. 选择合适的起始列1234# 默认起始列数: 1# 可以自由设置转存的起始列# near line 222tmp &#x3D; transExcel2MysqlDB(tmpPath, cnt, startmark &#x3D; 1) 设置仅测试部分数据如果你有大量的Excel文件，且你只想测试此代码或使用日志中的errinfo with finally来捕获Excel的调试信息（可以打开或打不开），则可以修改以下部分。 其只加载有限的表数据。 12# near line 85edata &lt;- edata[30:35,] 日志 (processRecord.csv) errinfo with summary这是插入SQL失败的记录。如果使用文件夹包含多个Excel，则每个Excel都可以输出单独的errinfo with summary。 使用此缓存信息，我们可以在Navicat的帮助下自动的找到错误的sql位置。 然后你可以修改 tool_excel2mysql 的代码内容来修复或者直接给我相关反馈。 errinfo with finally这部分是全局信息，包含无法读取的Excel表信息和最终的统计数据。 如果此处显示了某个excel，那你需要手动检查此文件以查找真正的问题。 有时重新保存（打开然后保存）就可以解决不能读取的问题。 基本上来说，该工具可以顺利地将数据从我的xls、xlsx文件转存到mysql，并且成功率几乎达到100％。 （上面的截图是为了演示errinfo的例子） English Version 赶紧使用一下吧…","categories":[{"name":"ML/R学习笔记","slug":"ML-R学习笔记","permalink":"https://www.cz5h.com/categories/ML-R%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"https://www.cz5h.com/tags/R%E8%AF%AD%E8%A8%80/"},{"name":"SQL插入","slug":"SQL插入","permalink":"https://www.cz5h.com/tags/SQL%E6%8F%92%E5%85%A5/"},{"name":"数据处理","slug":"数据处理","permalink":"https://www.cz5h.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}]},{"title":"那些年令人记忆深刻的电视剧","slug":"2020-4-30 那些年令人记忆深刻的电视剧","date":"2020-04-29T22:00:00.000Z","updated":"2020-06-18T12:19:02.143Z","comments":true,"path":"article/d24e.html","link":"","permalink":"https://www.cz5h.com/article/d24e.html","excerpt":"二十年不长不短，刚好积攒一点人生阅历。面对那些年看过的电视剧，彼时孩童的我们满头雾水，而现在，深刻的、内在的、隐晦的暗示，会在我们重温时不断涌现，好的作品需要时间的沉淀，需要阅历的解读。温故方可知新，当越来越多的人认可当下电视传媒的荒芜时，不妨停下脚步，回头看看，看看那些儿时看过的老剧，同样会给你深刻的感悟：好剧如酒，历久弥香。","text":"二十年不长不短，刚好积攒一点人生阅历。面对那些年看过的电视剧，彼时孩童的我们满头雾水，而现在，深刻的、内在的、隐晦的暗示，会在我们重温时不断涌现，好的作品需要时间的沉淀，需要阅历的解读。温故方可知新，当越来越多的人认可当下电视传媒的荒芜时，不妨停下脚步，回头看看，看看那些儿时看过的老剧，同样会给你深刻的感悟：好剧如酒，历久弥香。 2002《黑洞》 简介： 副市长聂大海之子、天都市龙腾集团董事长、闻名全省的青年企业家，这些都是加诸在聂明宇（陈道明 饰）身上的称呼。但他最重要的身份则是天都市最大的地下黑社会的头目。平日里一表人才、斯文有礼的聂明宇实则道貌岸然。他利用其四通发达的人际网，收买海关人员，走私货物，牟取暴利。而他的左右手也能力非凡，手段毒辣。在他们的经营下，龙腾集团的业绩步步高升。但世上没有不透风的墙。一封匿名检举信被递到了省委组织部。聂明宇及龙腾集团的勾当引起了省委方面的高度警觉，责令天都市迅速查明原因。但聂明宇的势力早已覆盖整个天都市。市委把聂明宇的义兄，刑警队队长刘振汉（陶泽如 饰）安排调查此事，想继续掩盖聂明宇的罪恶事迹。一场关于亲情和正义的较量在刘振汉身上展开。邪与正的角逐，究竟谁胜谁负… 观看： Youtube合集 短评：刘振汉和聂明宇的战争非黑即白：正义必将战胜邪恶；庞天岳和聂大海的较量却讳莫如深：胜者为王败者寇。（这是《名义》的爸爸） 2004《冬至》 简介： 齐州银行和平区支行柜台会计陈一平，妻子戴嘉和十二岁的女儿幼幼有一个不怎么富裕但是还算美满的三口之家。而身为北京市局刑侦处长的蒋寒比较起来生活可谓差强人意：虽然工作业绩突出，但是婚姻生活很不幸福。一个是平凡的银行小职员，一个是警察，他们的生活本是两条永不会相交的平行线。可是一场较量却在不知不觉中展开了。一宗看似平淡的银行库款失窃案这两位主人公的命运连在了一起… 观看： Youtube合集 短评：“好人”的堕落，往往只需要一个邪念；家破人亡，也只需要一纸支票。 1999《一级恐惧/黑菌》 简介： 村外那条大清河早在几年前就已成了干涸的河床，在布满卵石的河床上往来劳作的农民 们绝对没有想到在他们脚步下的某个地方有一只深埋了半个世纪的罪恶的铁盒，那盒子将给他们、给生活在这个地球上的所有生命带来一场巨大的灾难。不幸的是，他们的铁锹碰到了那个盒子，他们的双手打开了那个瓶口，黑色的液体象当年日本军队肮脏的黑爪一样，伸向了每个人宝贵的生命…十八里坡倾刻间面临灭顶之灾。 观看： Youtube合集 短评：人物脸谱化的登峰造极；讽刺官派的出神入化；天灾还是人祸，二十年前就已经开始了讨论。 1997《雍正王朝》 简介： 康熙皇帝驾崩，继位者四阿哥胤禛，在当政后出现的“摊丁入亩、火耗归公”、“士绅一体当差一体纳粮”、“河南罢考案”、“铁帽子亲王大殿发难逼宫”、“含泪杀亲子”等一系列旨在推行新政、抑制官绅敛财和宫廷内部党争、挤压的历史事件贯穿雍正的一生和雍正王朝。 观看： Youtube合集 短评：羸弱帝国的苦皇帝，低开高走雍正王朝。皇帝不再高高在上，而更像是一位从群众中来的无产阶级领袖。 2001《康熙王朝》 简介： 基于二月河小说《康熙大帝》改编，其背景故事是清朝世祖顺治帝的末年和圣祖康熙帝在位时的事迹。该剧从顺治皇帝哀痛爱妃董鄂妃病故时讲起，直至康熙在位61年驾崩而止。第一次以正剧的角度浓墨重彩刻画了清朝初期康熙皇帝充满传奇的一生。 观看： Youtube合集 短评：准确的演绎出了大清帝国皇帝的王道之气和阴柔诡谲的心性。在位六十载无人能敌，统一华夏功德无量。 2000《上车走吧》 简介：（这个是电影哦） 98年，山东青年刘承强（高虎 饰）和同乡高明（黄渤 饰）相约来到首都北京。成为了小巴302线的司乘人员。同在一条营运线的大头和大英子出于利益的关系，处处与强子和小明作对，双方为此曾大打出手。在此期间，高明更与同住一个院子的四川女孩小辫子日久生情，而强子则喜欢上了小巴上的常客丽娟。北京，这座城市的人和事，给了这两个外地青年终身难忘的记忆。 观看： Youtube地址 短评：见微知著，北京的丑陋面，多年过去，并没有多少改变。每个人都在纷纷扰扰中找到自己的位置，等级森严、不可僭越，即使相识相知也终将平行向前。 2005《生存之民工/春天里》 简介： 松江市，一个东北边陲的小城，一群来自全国各地的农民工为了生存，来到了这里，他们把汗水浸透在这个城市的建筑工地上。而上百名民工们的血汗工资无情地被非法挪用，民工们养家糊口的工钱被无期限地拖延，为了生计他们被迫集体追讨，可是讨来的却是暴力威胁和致命的恐吓，这群缺乏人文关怀的弱势群体的生命骤然间危在旦夕。。 观看： Youtube地址 短评：谢老大的无奈、王家才的窝囊、王家慧的没人性、杨志刚的鲁莽、陆长发的自以为是…极尽农民工之散漫、无用、甚至有些废物，但正是这种氛围的营造，让剧情无限接近真实生活，他们就是那样的无知、那样的天真、那样的愚昧，导演巧妙的插入了栓子这样一个新时代农民工的形象，隐晦的代表了希望、代表了农民工的明天。本剧音效也是一大亮点，没有浮夸的配乐，只有深沉的伴奏。 2010《外乡人》 简介： 讲述七八个外乡人，为了各自理想汇聚到上海，历经坎坷最终实现自我的悲喜人生。该剧围绕着小人物平凡琐碎的生活故事及情感纠葛，细致入微地刻画了人们熟知或陌生人们的内心世界。该剧以一种平实亲切的风格，以平民的视角，印证着都市乃至当今中国最近十年的发展脉络，是一幅反映中国社会进步、经济发展的市井风情画。 观看： Youtube地址 短评：本剧的点睛之笔是电视机，从南斯拉夫大使馆被炸开始，穿插了包括911恐怖袭击、五十年国庆、2003年非典、北京申奥成功、08年地震、雪灾、到最后全国取消农业税等等事件，把整个剧情牢牢的贴合在真实历史的轨迹之上，让每个人物的悲欢离合同祖国的发展同呼吸、共命运，把艺术源于生活演绎的淋漓尽致。 2008《国家形象》 简介： 为了纪念国际禁毒日，主创团队决定拍摄一部缉毒禁毒的电视剧。该剧取材于公安部门缉拿的刘招华、谭晓林、陈炳锡、张启生四位毒枭的案例。编剧在公安部门的协助下，才从数以万计的资料中，把他们的关系捋出来，重新创作出看点极佳的故事情节。剧组先后来到厦门、西双版纳、北京、西安及泰国等地取景拍摄。 观看： Youtube地址 短评：毒贩的尔虞我诈、卧底的步步惊心，以及缉毒战士的无所畏惧，交织在一起还原了一段精彩的缉毒故事。 2005《使命》 简介： 根据朱维坚长篇小说《使命：黑白道前传》改编的电视剧，是一部综合反黑、纪检、刑侦的电视剧。讲述了清水市公安局长林荫，铲除黑恶势力，扫除百姓心中的阴霾的故事。 观看： Youtube地址 短评：当一个城市被黑恶势力所浸淫，执法机关被外部势力所渗透时，试想公安局长如何能够扭转局势、反客为主。对金钱的贪婪是堕落的根源，守住底线的根本就是牢记自己的‘使命’ 2009《沉默》 简介： 依旧是根据朱维坚创作的小说《黑白道：终结篇·沉默》改编，不同的是林荫来到了春城市春城分局做局长，相比较清水市，这里的权力交织更为盘根错节，甚至出现了枪杀警队人员的恶性事件，真相虚无缥缈，凶手亦敌亦友，在权力面前，选择沉默还是奋起反抗，这是一个值得深思的问题。 观看： Youtube地址 短评：极度压抑的观看体验，几乎全程被升任的前任公安局长玩弄于手掌内，不同的是，除了忍气吞声外，林荫一直没有放弃对黑恶势力证据的收集，最终，拨云见日，成功的揪出了隐藏在警队内部的罪魁祸首，同时配合政法委一举肃清了执政队伍。看到结尾时，心中那种全程压抑的情感才得以集中释放。林荫再一次不负众望，凯旋而归。 未完待续。。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"生活","slug":"生活","permalink":"https://www.cz5h.com/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"你有没有「厌网」情节？","slug":"2020-4-23 你有没有“厌网”情节？","date":"2020-04-22T22:00:00.000Z","updated":"2020-04-23T17:11:59.896Z","comments":true,"path":"article/73f8.html","link":"","permalink":"https://www.cz5h.com/article/73f8.html","excerpt":"曾几何时，互联网的沟通方式是单调的，但人与人的沟通却是真诚的，但近年来网民数量直逼人口数量，互联网再也不是从前那一汪清泉，或许用波涛汹涌的汪洋大海来比喻比较贴切，太多人厌倦了、疲惫了，选择退网了，在他们眼中，当下互联网环境早已是恶臭漫天，烂的一发不可收拾，但事实如此吗？我想每个人有每个人的理解。","text":"曾几何时，互联网的沟通方式是单调的，但人与人的沟通却是真诚的，但近年来网民数量直逼人口数量，互联网再也不是从前那一汪清泉，或许用波涛汹涌的汪洋大海来比喻比较贴切，太多人厌倦了、疲惫了，选择退网了，在他们眼中，当下互联网环境早已是恶臭漫天，烂的一发不可收拾，但事实如此吗？我想每个人有每个人的理解。 先导写此文章的缘由是我看到了不止一个朋友、或朋友的朋友的退网经历，虽然我不知道事实如何，但我也理解他们的行为。互联网的膨胀，带来的是无穷无尽的话题和随之而来的讨论或争论。而恰恰是建立在互联网之上的「讨论」，是构成互联网生活的重要一环，也是影响人们体验的最重要因素。 因此，以下所述的互联网环境，特指「言论/舆论环境」，这也是导致人们对互联网产生厌烦情绪甚至退网的直接和最主要因素。 互联网环境的发展如果用一句话概括现如今的互联网环境，那就是「整体杂乱有序、个体多维度交叉」。如果从互联网诞生之时说起，那么大概可以描绘出以下这么几个阶段： 1. 混沌时代的个体觉醒：产生公知。起初的互联网由于在人们生活中的参与度非常有限，几乎不会对人们现实社会的决策产生影响。到了后来，国内第一批意见领袖出现，此时大多数人基本都是边缘OB大神输出，意见领袖/公知，很大程度上就是公信力的代名词。 2. 初生代网民和新生代网民的先后加入。这一过程从最初开始，延续到现在。看惯了神仙（公知）打架的初生代网民的加入，迅速使得互联网活跃起来，一系列优质的社区产生。由于初生代网民的骨子里是「问题导向」（公知打架）的，所以平均素质自然比较高，那时的互联网环境也自然比较好，那段时间也成了众多网友追忆的「黄金时代」。后来，新生代（尤其是00后）的加入，彻底打破了这一美梦。有人说新生代网民的素质堪忧，究其原因可能有二： 一是经历：初生代网民看着公知打架过来的，而新生代呢，是看着粉丝互怼过来的。 二是时间：初生代参与网络的（平均）年龄必然比新生代大的多，这直接导致了新生代网民过早的接触、参与、实施。 总之，以微博为载体、粉丝为高地的新型互联网舆论战场已经在新生代的影响下形成规模，现在每天都有大大小小的「战争」爆发，并且实实在在的在影响人们的现实生活。 3. 新生代力量的侵蚀和导致的群体分裂。续上。在新生代加入之后，除了上述的「自家战场」，他们还迅速洗劫其他「高龄」战场，凡无门槛可进入的地方，全部被低龄化所影响。新老的对立和互相嫌弃说到底，是老一代网民的错，错就错在自己没管好自己的娃，让他年纪轻轻（初中尤甚）就跑到网上参军。 4. 重回混沌，但是更高水平的混沌。终于说到了现在。现在是个什么形式呢，个人更倾向于重回混沌状态：没有所谓的公知领袖，即便有也通常最后会死的很惨；人人都有粉丝效应，人人都可能是舆论中心。至于这种混沌是更高质量还是更低质量，我觉得是前者，至少有思辨能力的人是越来越多的。不过恰恰是这些产生出思辨能力的人，往往是退网急先锋，因为他们受不了无脑的拥护，受不了不分青红皂白的辱骂，可转念一想，谁又不是这样过来的呢？ 前半段的最后说到了现今的互联网环境和退网人士，下面直接就他们为什么对现今的互联网环境心生厌恶做一下简要分析。 根本原因：个人觉醒和「同质化」的矛盾。 直接原因：「拥护者效应」的加害。 从「意见垄断」说起字面来看，意见垄断就是发表意见的权力掌握在少数人手里，在公知时期确实是这样，那时的公知或者大V，需要很长时间的积累、或运营，新手入场难度很大，因此那时虽然少数人掌握发言权，但环境还算说得过去。 但自新生代加入后，伴随互联网的爆炸式发展，自媒体井喷、营销平民化都在刺激着言论的扁平化蜕变，言论场从原先的群峰矗立，变成了现在的丘陵起伏，由此，言论更亲民了，但也更杂乱无章了。 不过聚焦在某一问题，其又有一番景象：站队思维越来越严重。究其原因，我认为是言论的扁平化，致使个人言论地位的提升，加之个体意识的觉醒，导致「希望他人认同自己」的想法越发强烈，从而导致「站队互喷」。久而久之，「表明立场」便成了必修课，在不断地、重复地、多次地表明立场后，必然有部分人会疲倦、会厌恶，究其原因，便是个体意识过于强烈，缺少包容度，一次次的站队，一次次的参加战斗，直到自己发现，原来在另一场战斗中，战友赫然站立在自己最恶心的阵营中。 何为「群体同质化」当上述这种需要表明立场的话题构成一类话题，那么经过如此「训练」的网民，便会出奇的言论一致（这就是所谓的同质化），例如明星的脑残粉，爱guo的粉红等等，当这种「同质化」表现在某些反常识的话题（例如对日系车打砸）中时，往往就会令有「独立思维」的人心生厌恶，长此以往，或许某天他们就会大喊一声失望，然后悻然退网。如果他们忍辱负重下来，那么等待他们的还有「拥护者」的加害。 副产物「拥护者效应」前面说的「群体同质化」本身并没有什么害处，但坏就坏在其会催生出一类「拥护者」，这又是「潜在退网用户」最为恶心的一类人。往往自身并不十分拥护，但仍会借其外衣攻击自己的「敌人」。最典型的，自然就是“乱咬人”的「脑残粉」对路人的「无差别攻击」。 参与者素质这是个老生常谈的问题，但其实却又包含鲜明的时代色彩。由第一部分可知，十年前的互联网参与者和十年后的参与者（主力），已完全不同。如今的玩家，除了传统角色，更多的还有各色的「营销力量」，从本质上说，我认为当下的爱豆和粉丝，也应归类于媒体营销的一种。如果从营销的角度来看，那么参与者素质就显得不是那么重要，而流量导向才是根本目的，所以至此，互联网的新时代特性也凸显了出来，那就是「流量为王」，在此指导思想之下，内容的质量、提问者的素质、言论的真实性等等因素的重要性急剧下降，自然地，互联网的环境也随之越来越恶化。这也是构成部分人厌网情节的重要因素。 综上，可以发现，「潜在退网用户」的伤心地大多都是舆论场，是他们自己过于认真，从而让自己饱受「真实伤害」，实际呢，网络上的互相伤害比比皆是，不让自己深陷其中，才是面对如此环境的根本方法。 后记在现如今的互联网（言论环境），如果事事较真，那么你将寸步难行。自己接受不了的就远离，自己热爱的就去捍卫；不要让互联网世界影响现实中的自己，也不要把现实世界的事物过多的带入到互联网中去，保持合适的远距离，才能让互联网为我们所用。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"生活","slug":"生活","permalink":"https://www.cz5h.com/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"来自千里之外祖国的关爱","slug":"2020-4-21 来自千里之外祖国的关爱","date":"2020-04-20T22:00:00.000Z","updated":"2020-04-22T20:03:42.328Z","comments":true,"path":"article/c24c.html","link":"","permalink":"https://www.cz5h.com/article/c24c.html","excerpt":"盼星星盼月亮，终于把“健康包”给盼来了，大约一周前就在网上填了表格，没想到今天才送到手里，更没想到的是居然用的是顺丰快递，原来荷兰也是有顺丰的。虽然远在千里之外，但此健康包还是领自己深深的感受到了来自祖国的关爱，特此记录一下。","text":"盼星星盼月亮，终于把“健康包”给盼来了，大约一周前就在网上填了表格，没想到今天才送到手里，更没想到的是居然用的是顺丰快递，原来荷兰也是有顺丰的。虽然远在千里之外，但此健康包还是领自己深深的感受到了来自祖国的关爱，特此记录一下。 健康包根据荷兰大使馆的说法，这一次荷兰国内发来的健康包一共有四千份，内有消毒湿巾、口罩以及湖北1号方、湖北2号方等防疫药品。通过全荷学联组织发放，顺丰荷兰负责投递，整个申请过程总计有4750人报名申领。对于首次没有领到的750人，大使馆后续又筹措了750份健康包。也就是说，只要成功完成申请的留学生，全都被包含在此次健康包的投递对象中。 申请一波三折由于一开始被告知的是健康包数量有限，并不能覆盖全部的申请者，所以一周前的那天晚上，我们全都准备好了准备抢填，结果到点一登录，表格居然崩溃了。事后我们分析，是因为学联使用了谷歌多人协作表格的锅。众所周知，这类协作表格，对并发的支持很差，几百人就不行了。由第一节可知当天总共有接近五千人申请，使用谷歌协作表格，那妥妥的是不行的，因此在等待了一个多小时之后，学联才又重新开通了申请渠道，这次采用的是调查问卷的方式，这种方法就好的多了，从技术上说，这种方式的并发支持要好的多，只需要一般性能的数据库，就可以轻松的支持几百上千的并发提交，所以后面我们就轻轻松松完成了表格填写。值得高兴的是，隔天的第一批确认名单中，就找到了我的名字！（田氏家族ohaha） 送货送到天荒地老就在出名单的隔天，也就是填完表格后的第二天，就有同学晒出收到的健康包了，结果呢，我是等到了一周之后，也就是今天，才收到的，期间我还一度认为自己的健康包被放到了暂存点（因为收到了张暂存的单子），事后才知道那是室友的快递。 晒晒里面的东东卫生湿巾和防疫小册子： KN95和一般医用口罩（2+20），以及中药预防方（六剂）： 还有一封“家信”： 写在最后 到此为止已经度过了一个多月的居家隔离生活，疫情也逐渐的从每日生活的主旋律变成了配菜，虽然情势不容乐观，尤其是害人的天气（先前没隔离的时候天天下雨，我甚至因为淋雨都感冒过，但自从开始了居家隔离的政策，这天气就变得异常的好），这蓝天白云直接导致出去一趟外面全是出来玩的。但终究整体社会氛围还比较稳定，这些Coronavirus在乐观的荷兰人眼里都不是事儿。 不过即便如此，在此时节，能收到祖国寄来的救助包裹，心里也是热乎乎的，轻轻的包裹载着重重的情谊，不远万里从国内送到我的手中，除了说句感谢，更多的还有对祖国认同感和自豪感的提升。这让我不由的想起了《战狼2》结尾的那句话（也是护照反面的那句话）： “中华人民共和国公民：当你在海外遭遇危险，不要放弃！请记住，在你身后，有一个强大的祖国！”","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"生活","slug":"生活","permalink":"https://www.cz5h.com/tags/%E7%94%9F%E6%B4%BB/"},{"name":"荷兰","slug":"荷兰","permalink":"https://www.cz5h.com/tags/%E8%8D%B7%E5%85%B0/"},{"name":"CSC","slug":"CSC","permalink":"https://www.cz5h.com/tags/CSC/"}]},{"title":"站点访问概况（近两月 2.29-4.29）","slug":"2020-4-1 站点访问概况（近两月 2.29-4.29）","date":"2020-03-31T22:00:00.000Z","updated":"2020-04-30T21:33:34.779Z","comments":true,"path":"article/ac94.html","link":"","permalink":"https://www.cz5h.com/article/ac94.html","excerpt":"马上五一了，数据已更新至建站来的两个月。 以下为原文：终于告别三月，迎来了四月，在二月的最后一天，我把主题换成了Volantis，并且最近一段时间一直在对其进行美化和微调，现在是四月的第一天，整整一个月过去了，博客运行的如何呢？让我来借助统计分析来给大家康康。","text":"马上五一了，数据已更新至建站来的两个月。 以下为原文：终于告别三月，迎来了四月，在二月的最后一天，我把主题换成了Volantis，并且最近一段时间一直在对其进行美化和微调，现在是四月的第一天，整整一个月过去了，博客运行的如何呢？让我来借助统计分析来给大家康康。 日均访客由下图可以看出，本博客还是个微型小站，每天基本也就几十个访客/IP，网页被浏览一百次左右（已排除本地调试干扰）。而且对于流量比较大的几天，其实都是知乎带来的流量。剩下的基本盘流量，基本是在 Hexo 水群时群友访问的，当然，现在不排除已经有部分老用户了，数据显示的是现在的访问量有 30% 是老用户/访客。 重要的一点是，从后面的分析可以得知，本博客站点的 UV : PV &lt; 1 : 3，个人认为这是十分正常的数据，如果 PV 是 UV 的四倍以上，那我一般认为就有水分了。 第一个月（2.29-3.29）： 第二个月（3.29-4.29）： 总量和来源地区可以看出，部署在 Coding 的新加坡机房，基本全球访问畅通，主要流量都来源于亚洲、欧洲和美国。这其中的国外访问量，我猜也基本是知乎带过来的，后面的对知乎的分析也会提及。对于中国的部分，我比较困惑的1是好像有大量访问的来源都是重叠在一起的，比如河南的访问量是最高的，我怀疑这些来源地区有运营商的根路由。关于具体来源的访客数，可以参照以下图示。 第一个月（2.29-3.29）： 第二个月（3.29-4.29）： 下面可以更直观的看出来访问来源地的总访问时长。可以看出大部分访问并不是蜻蜓点水，停留时间比未改版以前的旧博客要好了不少。 第一个月（2.29-3.29）： 第二个月（3.29-4.29）： 高光时刻下面是昨天下午的时候，博客被访问的情况，同时（五分钟内）被 8 人访问，可以称得上是高光时刻了。 来源分析下面可以看出来，主要的来源除了直接访问，就三个：知乎、十年之约和 Zonelyn.com。 最后的域名是由于在知乎留的链接没及时换成新域名导致的访问跳转，到后面更换了就降下来了； 十年之约就是个博客汇总网站，贡献了小部分访问； 最后就是知乎了，可以说域名那部分也基本是知乎来的，可能直接的访问贡献了 570+ 的浏览。这基本算六分之一的访问量了。请记住蓝色折线的曲线，基本和知乎上的文章阅读量保持相同的趋势。 第一个月（2.29-3.29）： 第二个月（3.29-4.29）： 受众最多的页面非工具页莫属了，这个小工具创建于三月初，此页面九成的访客来自于知乎的文章，下面顺带把知乎的近两个月的数据分析一下。由上面分析，剩下 100+ 的流量应该来自直接访问，因为这个链接被我放在主页顶部按钮中了，十分明显。 第一个月（2.29-3.29）： 第二个月（3.29-4.29）： 知乎近俩月的分析自从二月中旬开始，我陆陆续续的整理了下自己去年的申请材料，放到了知乎上，还搞了个专栏，事实证明，我这次的无心插柳，已经产生不小的影响了。主要的几篇文章，收藏率超过 10%，并且专栏的关注人数已经四百多人，传播效应已经开始，预计在近一个月还会继续增多，因为前天又把剩余的几份材料整理了一下放到专栏了，申请正好用到。 第一个月（2.29-3.29）： 第二个月（3.29-4.29）： 对于这几篇介绍经验的文章，在前期（那时只有两篇）曾经一度异常火爆，每天的单篇收藏都能一两百，评论都有十几二十条。不过现在稳定了，基本上每天就几个提问，不会占据我太多时间。 第一个月（2.29-3.29）： 第二个月（3.29-4.29）： 目前的状态是专栏里的东西基本齐全了，我已经想不到还有什么能分享的了，借助这个专栏和这几篇文章，我也收获了几百关注，但就是没有爆款回答就是了（没精力回答那种细致的问题），赞同数还是少的可怜的，基本都来自专栏文章。后续的想法是，把我最近他们问我的问题和回答，大概得有两三百个问题（知乎文章和私信以及QQ上问的），搞一个问答系统，这样我就不用每次都回答了，构思中。 第一个月（2.29-3.29）: 第二个月（3.29-4.29）: 结识朋友在文章数据统计中的大约 400 条的留言里，大约有一半是我的回复。大概就是解答一下疑惑而已，而且大部分问题都可以被解决掉，但也有部分人加了我的QQ好友，为此，近一个月的时间里我收获了四十多位新朋友，当然最终有部分人问完问题之后还成了好朋友，里面又有几个是可能会来我这里的，到时候请我吃饭也说不定（偷笑。 总之，虽然是我给他们解答，但也非常感谢他们问我，他们基本上全都是很优秀的，除了一些TOP的大佬，还有些“明星”，比如里面竟然有一位是“90后马克 ·吐温”的老哥，我当时都惊到了，他们倒还是都很平易近人的，和他们交流起来也没什么，光环褪去，大家都是普通人。 第一个月（44人）: 第二个月（188人）: 写在最后今天是愚人节，但却没心情愚人，还是希望疫情快点过去，好让正常的学习和生活重新回归。不过目前看来还是遥遥无期啊，什么时候出门大街上有人戴口罩了，疫情估计就快结束了。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"知乎","slug":"知乎","permalink":"https://www.cz5h.com/tags/%E7%9F%A5%E4%B9%8E/"},{"name":"站点统计","slug":"站点统计","permalink":"https://www.cz5h.com/tags/%E7%AB%99%E7%82%B9%E7%BB%9F%E8%AE%A1/"}]},{"title":"引入基于LCloud的页面访问统计","slug":"2020-3-25 引入基于LCloud的页面访问统计","date":"2020-03-24T23:00:00.000Z","updated":"2020-03-28T04:24:14.757Z","comments":true,"path":"article/4e53.html","link":"","permalink":"https://www.cz5h.com/article/4e53.html","excerpt":"由于本主题（Volantis）使用的 Valine 暂时仍然无法正常开启 Counter 页面计数，所以，在单一页面上展示出该页面的访问量就成了一个问题。不蒜子是个解决方式，但其加载巨慢而且仍属于个人维护、并不稳定。所以综合到最后，选择采用 LeanCloud 来完成这一功能。其主要思路就是访问页面时到对应的 LeanCloud 数据表中更新访问次数。不过暂时只能记录PV值。","text":"由于本主题（Volantis）使用的 Valine 暂时仍然无法正常开启 Counter 页面计数，所以，在单一页面上展示出该页面的访问量就成了一个问题。不蒜子是个解决方式，但其加载巨慢而且仍属于个人维护、并不稳定。所以综合到最后，选择采用 LeanCloud 来完成这一功能。其主要思路就是访问页面时到对应的 LeanCloud 数据表中更新访问次数。不过暂时只能记录PV值。 从零开始引入计数代码第一步新增leancloud代码主体（lc_visitors.ejs），主要实现的是访问页面时leancloud访问数+1，并且返回该页面已有的访问量。 ~\\volantis\\_third-party\\lc_visitors.ejs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778 &lt;!--由于主题包含valine.js，所以只引入av.js--&gt;&lt;script src=\"//cdn1.lncld.net/static/js/2.5.0/av-min.js\"&gt;&lt;/script&gt;&lt;script type=\"text/javascript\"&gt;if(true)&#123; var leancloud_app_id = 'WbLE88***********gzGzoHsz'; //y=偷懒直接硬写了 var leancloud_app_key = 'ycqjmt***********RkrdO'; AV.init(&#123; appId: leancloud_app_id, appKey: leancloud_app_key &#125;); var pageViewsLength = $(\".pageViews\").length; var isIndex = $(\".pageViews\").length &gt; 1 ?true:false; function showTime() &#123; var Pageview = AV.Object.extend(\"Pageview\"); if(isIndex)&#123; $(\".pageViews\").each(function()&#123; showPageViewsNum($(this), Pageview); &#125;); &#125;else&#123; addPageViewsNum($(\".pageViews\")); &#125; &#125; //仅显示阅读量 function showPageViewsNum(ele, Pageview)&#123; var id = ele.attr('id').trim(); var query = new AV.Query(\"Pageview\"); query.equalTo(\"post_id\", id); query.descending('createdAt'); query.limit(1); query.find().then(function (results) &#123; $(document.getElementById(id)).text(results &amp;&amp; results.length &gt; 0? results[0].get(\"count\") : '0'); &#125;, function (error) &#123; $(document.getElementById(id)).text('0'); &#125;); &#125; //追加并显示阅读量 function addPageViewsNum(ele)&#123; var id = ele.attr('id').trim(); var title = ele.attr('data-flag-title').trim(); var query = new AV.Query(\"Pageview\"); query.equalTo(\"post_id\", id); query.descending('createdAt'); query.limit(1); query.find().then(function (results) &#123; if (results.length == 0) &#123; saveNewPageview(id, title); return; &#125; var pageview = results[0]; var count = pageview.get(\"count\"); count++; pageview.set(\"count\", count); pageview.set(\"title\", title); pageview.save().then(function (pageview) &#123; $(document.getElementById(id)).text(count); &#125;) &#125;, function (error) &#123; saveNewPageview(id, title); &#125;) &#125; function saveNewPageview(id, title) &#123; var Pageview = AV.Object.extend(\"Pageview\"); var query = new Pageview; query.save(&#123; post_id: id, title: title, count: 1 &#125;).then(function (pageview) &#123; $(document.getElementById(id)).text(pageview.count); &#125;, function (error) &#123; &#125;); &#125; if(pageViewsLength)&#123; //此处判断等于 1 在执行 可去除循环 showTime(); &#125;&#125;&lt;/script&gt; 第二步将leancloud代码主体（lc_visitors.ejs）引入到原来的主题模板中，紧跟在jquery的后面。 ~\\volantis\\layout\\_partial\\scripts.ejs12345678 &lt;%- js(theme.plugins.jquery) %&gt;&lt;!--引入代码--&gt;&lt;%- partial('_third-party/lc_visitors') %&gt;&lt;% if (theme.search &amp;&amp; theme.search.enable) &#123; %&gt; ... ...&lt;% &#125; %&gt;... ... 第三步在原页面中插入显示访问量的代码段，这里我直接紧跟在meta信息的阅读时长统计后面了。 ~\\volantis\\_meta\\wordcount.ejs12345678910111213141516171819202122 &lt;% if (theme.plugins.wordcount) &#123; %&gt; &lt;div class=\"new-meta-item wordcount\"&gt; &lt;a class='notlink'&gt; &lt;i class=\"&lt;%- theme.meta.wordcount.icon_wordcount %&gt;\" aria-hidden=\"true\"&gt;&lt;/i&gt; &lt;p&gt;&lt;%- __('post.wordcount', wordcount(post.content))%&gt;&lt;/p&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class=\"new-meta-item readtime\"&gt; &lt;a class='notlink'&gt; &lt;i class=\"&lt;%- theme.meta.wordcount.icon_duration %&gt;\" aria-hidden=\"true\"&gt;&lt;/i&gt; &lt;p&gt;&lt;%- __('post.duration', min2read(post.content))%&gt;&lt;/p&gt; &lt;/a&gt; &lt;/div&gt; &lt;!--添加显示访问量的代码（火焰图标+℃）--&gt; &lt;div class=\"new-meta-item readtime\"&gt; &lt;a class='notlink'&gt; &lt;i class=\"fas fa-fire\" aria-hidden=\"true\"&gt;&lt;/i&gt; &lt;p&gt;&lt;span id=\"&lt;%= post.path %&gt;\" class=\"pageViews\" data-flag-title=\"&lt;%= post.title || post.path %&gt;\"&gt;&lt;/span&gt;&lt;/p&gt; &lt;/a&gt; &lt;/div&gt;&lt;% &#125; %&gt; 注意：以上只有部署后访问才有作用，在本地访问无效果。 LeanCloud并发查询429错误这是第二次遇到了，每次鼓捣LeanCloud都会出现这个问题，主要还是其对访问做了限制。错误主要出现在列表页面，因为会同时查询多篇文章的访问量，所以会导致触发429错误。 我上次的解决方案：应对LeanCloud对于查询性能的限制 具体只需修改以下代码： ~\\volantis\\_third-party\\lc_visitors.ejs123456789101112131415161718 &lt;!--循环间隔地控制查询的触发--&gt;function showTime() &#123; if(isIndex)&#123; var cnt = $(\".pageViews\").length; var i = 0; var interval = setInterval(function()&#123; if(i &gt;= cnt) clearInterval(interval); else showPageViewsNum($($(\".pageViews\")[i++]), AV.Object.extend(\"Pageview\")); &#125;,500); &#125;else&#123; addPageViewsNum($(\".pageViews\")); &#125;&#125;... ... 最终效果：","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://www.cz5h.com/tags/JavaScript/"},{"name":"JQuery","slug":"JQuery","permalink":"https://www.cz5h.com/tags/JQuery/"},{"name":"LeanCloud","slug":"LeanCloud","permalink":"https://www.cz5h.com/tags/LeanCloud/"}]},{"title":"使用ValineAdmin完善评论系统","slug":"2020-3-21 使用ValineAdmin完善邮件系统","date":"2020-03-20T23:00:00.000Z","updated":"2020-03-28T03:50:51.625Z","comments":true,"path":"article/269.html","link":"","permalink":"https://www.cz5h.com/article/269.html","excerpt":"在更换主题后的这一段时间里，博客运行良好，内容我也在不断完善，评论自然是继承原来的 Valine 也运行正常，但是，其实从上个主题开始，我就没有配置邮件的评论系统，所以对于评论的查看，我一直都是登陆 LeanCloud 后台来完成的。当然，默认的邮件提醒我是配置了的，但那个太简陋了，几乎没有使用价值。关于 ValineAdmin，群友早就配置过了，这次终于轮到自己来配一下了。","text":"在更换主题后的这一段时间里，博客运行良好，内容我也在不断完善，评论自然是继承原来的 Valine 也运行正常，但是，其实从上个主题开始，我就没有配置邮件的评论系统，所以对于评论的查看，我一直都是登陆 LeanCloud 后台来完成的。当然，默认的邮件提醒我是配置了的，但那个太简陋了，几乎没有使用价值。关于 ValineAdmin，群友早就配置过了，这次终于轮到自己来配一下了。 为什么我现在才配因为我一直有个误区，那就是完成该配置需要在LeanCloud完成实名认证，但实际是不需要的！配置 ValineAdmin 是不需要 LeanCloud 实名认证的！（目前我仍然没有进行实名认证） 而且依托于Valine使用的应用/仓库即可，完全没有其他要求。 添加 ValineAdmin 步骤明确： 这里用的是“添加”，也就是说，ValineAdmin 和原来的 Valine 完全可以共存，互不影响，不需要重新配置。 1. 遵循 Valine 官方教程完成 Valine 的配置1https:&#x2F;&#x2F;valine.js.org&#x2F;quickstart.html 2. 进入配置页面 3. 源码部署填写以下仓库1https:&#x2F;&#x2F;github.com&#x2F;DesertsP&#x2F;Valine-Admin.git 4. 填写16个自定义变量 注意事项： 务必确认已开通SMTP服务，以QQ为例需要发送短信来获得授权码； SMTP_USER 和 SENDER_EMAIL 务必正确 AKISMET_KEY 请到akismet.com注册并选择免费版获得秘钥，否则在LeanCloud实例日志内会出现“Akismet key 异常!”的错误。 5. 给博主通知的邮件模板 BLOGGER_EMAIL 就是博主的邮箱； MAIL_SUBJECT_ADMIN 邮件标题； MAIL_TEMPLATE_ADMIN 邮件正文，注意变量位置，其他完全是HTML标签，可自己重写： 12345678910111213141516&lt;html&gt; &lt;head&gt;&lt;/head&gt; &lt;body&gt; &lt;div style=\"border-top:2px solid #12ADDB;box-shadow:0 1px 3px #AAAAAA;line-height:180%;padding:0 15px 12px;margin:50px auto;font-size:12px;\"&gt; &lt;h2 style=\"border-bottom:1px solid #DDD;font-size:14px;font-weight:normal;padding:13px 0 10px 0px;\"&gt;您在&lt;a style=\"text-decoration:none;color: #12ADDB;\" href=\"$&#123;SITE_URL&#125;\" target=\"_blank\"&gt;$&#123;SITE_NAME&#125;&lt;/a&gt;上的文章有了新的评论&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;$&#123;NICK&#125;&lt;/strong&gt;回复说：&lt;/p&gt; &lt;div style=\"background-color: #f5f5f5;padding: 10px 15px;margin:18px 0;word-wrap:break-word;\"&gt; $&#123;COMMENT&#125; &lt;/div&gt; &lt;p&gt; 您可以点击&lt;a style=\"text-decoration:none; color:#12addb\" href=\"$&#123;POST_URL&#125;\" target=\"_blank\"&gt;查看回复的完整內容&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;评论页面为&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;$&#123;POST_URL&#125;&lt;/strong&gt;&lt;/p&gt; &lt;br /&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 效果： 6. 给评论者通知的邮件模板 MAIL_SUBJECT 邮件标题，注意措辞； MAIL_TEMPLATE 邮件正文，需要美化！ 不同于站长的通知邮件，这是给评论者看的，必须花里胡哨，这是面子工程！ 1234567891011121314151617181920212223242526272829303132333435&lt;html&gt; &lt;head&gt;&lt;/head&gt; &lt;body&gt; &lt;div style=\"border-radius: 10px 10px 10px 10px;font-size:13px; color: #555555;width: 666px;font-family:'Century Gothic','Trebuchet MS','Hiragino Sans GB',微软雅黑,'Microsoft Yahei',Tahoma,Helvetica,Arial,'SimSun',sans-serif;margin:50px auto;border:1px solid #eee;max-width:100%;background: #ffffff repeating-linear-gradient(-45deg,#fff,#fff 1.125rem,transparent 1.125rem,transparent 2.25rem);box-shadow: 0 1px 5px rgba(0, 0, 0, 0.15);\"&gt; &lt;div style=\"width:100%;background:#49BDAD;color:#ffffff;border-radius: 10px 10px 0 0;background-image: -moz-linear-gradient(0deg, rgb(67, 198, 184), rgb(255, 209, 244));background-image: -webkit-linear-gradient(0deg, rgb(67, 198, 184), rgb(255, 209, 244));height: 66px;\"&gt; &lt;b&gt;&lt;p style=\"font-size:15px;word-break:break-all;padding: 23px 32px;margin:0;background-color: hsla(0,0%,100%,.4);border-radius: 10px 10px 0 0;color:#257db9\"&gt;您在&lt;a style=\"text-decoration:none;color:#257db9\" href=\"$&#123;SITE_URL&#125;\"&gt;「长征部落格」&lt;/a&gt;上的留言有新回复啦！&lt;/p&gt; &lt;/b&gt; &lt;/div&gt; &lt;div style=\"margin:40px auto;width:90%\"&gt; &lt;b&gt;&lt;p&gt;尊敬的 &lt;span style=\"color:#7777ff\"&gt;$&#123;PARENT_NICK&#125;&lt;/span&gt; 您好，您曾在该页面/文章：&lt;/p&gt; &lt;/b&gt; &lt;div style=\"background: #fafafa repeating-linear-gradient(-45deg,#fff,#fff 1.125rem,transparent 1.125rem,transparent 2.25rem);box-shadow: 0 2px 5px rgba(0, 0, 0, 0.15);margin:20px 0px;padding:15px;border-radius:5px;font-size:14px;color:#555555;\"&gt; &lt;b&gt;$&#123;POST_URL&#125;&lt;/b&gt; &lt;/div&gt; &lt;b&gt;&lt;p&gt;发布了如下评论：&lt;/p&gt; &lt;/b&gt; &lt;div style=\"background: #fafafa repeating-linear-gradient(-45deg,#fff,#fff 1.125rem,transparent 1.125rem,transparent 2.25rem);box-shadow: 0 2px 5px rgba(0, 0, 0, 0.15);margin:20px 0px;padding:15px;border-radius:5px;font-size:14px;color:#555555;\"&gt; &lt;b&gt;$&#123;PARENT_COMMENT&#125; &lt;/b&gt; &lt;/div&gt; &lt;b&gt;&lt;p&gt;刚刚，用户 &lt;span style=\"color:#7777ff\"&gt;$&#123;NICK&#125;&lt;/span&gt; 给您的回复如下：&lt;/p&gt;&lt;/b&gt; &lt;div style=\"background: #fafafa repeating-linear-gradient(-45deg,#fff,#fff 1.125rem,transparent 1.125rem,transparent 2.25rem);box-shadow: 0 2px 5px rgba(0, 0, 0, 0.15);margin:20px 0px;padding:15px;border-radius:5px;font-size:14px;color:#555555;\"&gt; &lt;b&gt;$&#123;COMMENT&#125;&lt;/b&gt; &lt;/div&gt; &lt;b&gt;&lt;p&gt;您可以点击 &lt;a style=\"text-decoration:none; color:#d25353\" href=\"$&#123;POST_URL&#125;#comments\"&gt;查看回复的完整內容&lt;/a&gt;，欢迎再次光临&lt;a style=\"text-decoration:none; color:#257db9\" href=\"$&#123;SITE_URL&#125;\"&gt; $&#123;SITE_NAME&#125;&lt;/a&gt;。&lt;/p&gt; &lt;/b&gt; &lt;style type=\"text/css\"&gt;a:link&#123;text-decoration:none&#125;a:visited&#123;text-decoration:none&#125;a:hover&#123;text-decoration:none&#125;a:active&#123;text-decoration:none&#125;&lt;/style&gt; &lt;/div&gt; &lt;/div&gt; &lt;center&gt; &lt;b&gt;由LeanCloud强力驱动&lt;/b&gt; &lt;br /&gt; &lt;b&gt;Copyright &amp;copy; 2017-2020 &lt;a href=\"https://www.cz5h.com\" style=\"color:auto;\"&gt;CZ5H.COM&lt;/a&gt;&lt;/b&gt; &lt;br /&gt; &lt;a href=\"https://www.cz5h.com\"&gt;&lt;img style=\"height:25px !important;\" src=\"https://i.loli.net/2020/03/25/NbrlqVpzLBj8Xta.png\" /&gt;&lt;/a&gt; &lt;/center&gt; &lt;br /&gt; &lt;br /&gt; &lt;/body&gt;&lt;/html&gt; 收到邮件的评论者看到的效果： 7. 变量填写完毕后部署 完成之后的问题以上全部完成，基本可以实现比较友善的评论系统，但仍然存在一个大问题： LeanCloud 的强制休眠！ 该休眠会导致有很大概率用户在评论时实例是休眠状态，从而导致邮件提醒的不可触发，即以上整个邮件系统并不是 100% 可用，迫切需要一种比较好的休眠策略。（好像每天必须休眠6小时） 可能的解决措施： 使用定时器激活：*/20 7-23 * * * 附录，文字版自定义环境变量： NAME VALUE 备注 NO. SITE_NAME 长征部落格(cz5h.com) 基本信息 01 SITE_URL https://www.cz5h.com 基本信息 02 SENDER_NAME 胖五 SMTP配置 03 SENDER_EMAIL tian*****@qq.com SMTP配置 04 SMTP_USER tian*****@qq.com SMTP配置 05 SMTP_PASS rekyxiqasdo***** SMTP配置 06 SMTP_SERVICE QQ SMTP配置 07 SMTP_HOST smtp.qq.com SMTP配置 08 SMTP_PORT 465 SMTP配置 09 SMTP_SECURE true SMTP配置 10 BLOGGER_EMAIL tian*****@qq.com 站长通知 11 MAIL_SUBJECT_ADMIN ${SITE_NAME}上有新评论！ 站长通知 12 MAIL_TEMPLATE_ADMIN 详见第五段代码 站长通知 13 MAIL_SUBJECT 尊敬的『${PARENT_NICK}』，您在${SITE_NAME}上的评论刚刚收到了一条新回复！ 用户通知 14 MAIL_TEMPLATE 详见第六段代码 用户通知 15 AKISMET_KEY 4b02d0d***** 评论过滤 16","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Valine","slug":"Valine","permalink":"https://www.cz5h.com/tags/Valine/"},{"name":"LeanCloud","slug":"LeanCloud","permalink":"https://www.cz5h.com/tags/LeanCloud/"},{"name":"评论系统","slug":"评论系统","permalink":"https://www.cz5h.com/tags/%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/"}]},{"title":"在中文维基百科新建用户框","slug":"2020-3-18 在中文维基百科新建用户框","date":"2020-03-17T23:00:00.000Z","updated":"2020-03-19T18:45:48.656Z","comments":true,"path":"article/27ff.html","link":"","permalink":"https://www.cz5h.com/article/27ff.html","excerpt":"目前中文维基百科仍然由于某些原因无法在大陆被直接访问，但仍然有很多用户喜欢使用Wikipedia，关于Wiki，我的理解就是个协作的知识系统，可以多人编辑并且多人维护，这也是维基百科实际的壮大方式，非常浅显的方式但却在当时引领了一种新的内容组织方式，所以维基百科也就不断壮大。中文维基百科和原本的英文维基并不是镜像，属于交集模式，目前有专门的翻译小组做互相翻译的工作。","text":"目前中文维基百科仍然由于某些原因无法在大陆被直接访问，但仍然有很多用户喜欢使用Wikipedia，关于Wiki，我的理解就是个协作的知识系统，可以多人编辑并且多人维护，这也是维基百科实际的壮大方式，非常浅显的方式但却在当时引领了一种新的内容组织方式，所以维基百科也就不断壮大。中文维基百科和原本的英文维基并不是镜像，属于交集模式，目前有专门的翻译小组做互相翻译的工作。 由以上我们也可以得知，其实维基百科的模式“可以被复制”，但这是个市场占有问题，你可以自建自己的Wiki服务，但不太可能壮大至维基百科的规模，但也有例外，比如 萌娘百科，但很显然，这是和普通百科几乎无交集的专业百科。扯远了… 对于用户框，就是种维基百科内自定义的显示框，可以直观的以词条的方式来展示用户的各方面，主要用来描述用户的属性，可以包罗万象，因为维基百科允许自建用户框，由此，各式各样的用户框被用户创造出来，一般你可以找到你想要的用户框。 搜索现有用户框方式一注意：以下若出现Wiki或者维基百科均特指“中文维基百科” 1. 使用[Category:]来当种子 这样可以直接找到一类人，然后通过类别可以打开该类别下的全部用户 2. 拷贝该用户页面内的用户框 点击编辑找到对应内容即可，如果你偷懒，完全可以全部拷贝一份，然后再去掉你不需要的词条。 搜索现有用户框方式二通过维基百科用户框汇总类别进行搜寻。 那么如果某个用户框并不存在怎么办呢？当然可以自建，因为现存的用户框都是不同的用户自建的，当然你也可以创建你自己想展示的用户框。 在创建的过程中，需要注意几点： 图片资源必须是维基百科的内部资源，也就是说，你必须将图片传到维基百科上，使用维基百科的内链才可以。 创建后的用户框可以被全部用户所搜索到并使用 创建新用户框的简单版一图片+文字模式 学习简单例子“非吸烟者”Template:User 非吸烟者 以上是用户框的固定格式，Template:User 后跟空格，再跟用户框名称，填入搜索框就可以搜出该用户框详细内容： 点击编辑，即可看到这个用户框的源代码： 1234567891011&#123;&#123;Userbox|id&#x3D; [[File:No smoking symbol.svg|45px]]|id-c&#x3D; #FFFFFF|info&#x3D; 这个用户&#39;&#39;&#39;[[吸烟|&lt;span style&#x3D;&quot;color:#0000FF;&quot;&gt;不吸烟&lt;&#x2F;span&gt;]]&#39;&#39;&#39;。|info-c&#x3D; #789CFF|info-s&#x3D; 10|info-fc&#x3D; #2E2E2E|border-c&#x3D; #000000&#125;&#125;&lt;includeonly&gt;[[Category:不吸烟的维基人]]&lt;&#x2F;includeonly&gt;&lt;noinclude&gt;&#123;&#123;Documentation&#125;&#125;&lt;&#x2F;noinclude&gt; 代码解读现在我们就可以得出构建方法了，对应上述内容每条的意义如下。 File:No smoking symbol.svg|45px 左侧贴图和贴图大小，一般是45×45，这个图片必须是属于维基百科站内的图片链接，如果你要用自己的图片，可以先上传到维基百科上，但一般偷懒可以直接搜索维基百科内已有的图片，直接复制图片链接改为 File:XX 的形式即可。 id-c= #FFFFFF 左侧贴图图片的背景颜色（如果背景透明）。 info= AAA&#39;&#39;&#39;[[BBB|CCC]]&#39;&#39;&#39; AAA：正常文本。 ‘’’[[BBB|CCC]]’’’：这是维基百科的超链接语法，写BBB即表明超链接为 zh.wikipedia.org/wiki/BBB，竖线（|）之后的CCC是超链接的文本，可以写入html代码，比如用控制文字的颜色。 info-c= #789CFF 右侧文本部分的背景颜色。 info-s= 10 右侧文本字体大小，一般是10。 info-fc= #2E2E2E 右侧普通文本字体颜色，使用html样式控制的字体的大小和颜色不受影响。 border-c= #000000 整个用户框的边框颜色，一般是黑色。 12345678&#123;&#123;Userbox ...&#125;&#125;&lt;includeonly&gt; [[Category:不吸烟的维基人]]&lt;&#x2F;includeonly&gt;&lt;noinclude&gt;&#123;&#123;Documentation&#125;&#125;&lt;&#x2F;noinclude&gt; 以上内容是整体的必要的用户框结构。 以此新建用户框比如我现在新建“这个用户喜欢使用Hexo搭建博客”这个用户框，基本步骤有以下几步： 1. 获取图片链接 懒得找，直接上传一张Hexo图片，按照引导上传即可。 选择本地上传即可，即只上传至中文维基。 上传成功我们就可以拿到文件链接了，此处文本链接就是 https://zh.wikipedia.org/wiki/File:Hexo-logo.png，我们构造用户框只需要使用最后的那部分，即 File:Hexo-logo.png。 2. 新建用户框词条 比如我们的用户框名称就叫Template:User 静态博客，那么就直接去搜索框搜索，然后进入新建页面。 3. 构造代码 1234567891011&#123;&#123;Userbox|id&#x3D; [[File:Hexo-logo.png|45px]]|id-c&#x3D; #FFFFFF|info&#x3D; 这个用户喜欢使用&#39;&#39;&#39;[[Github|&lt;span style&#x3D;&quot;color:#0000FF;&quot;&gt;HEXO&lt;&#x2F;span&gt;]]&#39;&#39;&#39;搭建博客。|info-c&#x3D; #66ccff|info-s&#x3D; 10|info-fc&#x3D; #2E2E2E|border-c&#x3D; #000000&#125;&#125;&lt;includeonly&gt;[[Category:使用hexo的维基人]]&lt;&#x2F;includeonly&gt;&lt;noinclude&gt;&#123;&#123;Documentation&#125;&#125;&lt;&#x2F;noinclude&gt; 注意：以上内容的Category并非我新建的，这是已有的分类，当然也可以新建分类，但最好的选择是讲我们新建的用户框归入其他已有分类，这样可以使更多人发现我们的用户框。 4. 填入词条新建页面 预览无误，点击发布即可获得最终的新鲜的用户框：Template:User_静态博客。 5. 使用 在所有的Wiki页面中，使用 {{User 静态博客}} 即可插入上述用户框了！ 创建新用户框的简单版二文字+文字模式 挖坑。 进阶挖坑。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"中文维基","slug":"中文维基","permalink":"https://www.cz5h.com/tags/%E4%B8%AD%E6%96%87%E7%BB%B4%E5%9F%BA/"},{"name":"花里胡哨","slug":"花里胡哨","permalink":"https://www.cz5h.com/tags/%E8%8A%B1%E9%87%8C%E8%83%A1%E5%93%A8/"},{"name":"编程","slug":"编程","permalink":"https://www.cz5h.com/tags/%E7%BC%96%E7%A8%8B/"}]},{"title":"船新版本的个人博客","slug":"2020-2-27 船新版本的个人博客","date":"2020-02-26T23:00:00.000Z","updated":"2020-06-18T22:33:10.406Z","comments":true,"path":"article/6f83.html","link":"","permalink":"https://www.cz5h.com/article/6f83.html","excerpt":"终于花了些时间，把博客整体迁移到新主题Volantis了，原主题Indigo我还是非常喜欢，但感觉有点朴素，自己又瞎改主题改到无法更新最新版本，所以换个好看的主题就成了待办事项之一。","text":"终于花了些时间，把博客整体迁移到新主题Volantis了，原主题Indigo我还是非常喜欢，但感觉有点朴素，自己又瞎改主题改到无法更新最新版本，所以换个好看的主题就成了待办事项之一。 此外，原先的博客一直托管在Github Pages，在百度是0索引的，这就意味着国内搜索引擎是无法找到我的，所以换到Coding托管，也成了待办事项之一；最后，我也一直在物色好的域名，现在都快成米商了，不过还是找到了一个不错的域名，更新解析，也是待办事项之一。最终，昨天把三件事全做了，在这里记录下，顺便说一下博客一般需要的一些辅助显示模块。 使用Volantis主题注意：我这里并不是单纯的“更新”，而是保留原站点的前提下用一个新主题而已，因为 Indigo 主题确实非常好用，并且自己也对其做了很多改动，已经比较稳定了，没必要丢弃不用，所以后面我用了二级域名 old.cz5h.com 来解析到原来的 Github 空间，对于新主题，我也只是把_post内的文章搞过来了而已，其他的小模块还需要慢慢完善，至于有哪些小模块，在文章最后会详细说明。 这次我换的主题是 Volantis，貌似是原先的 Materi-X （可能拼错了）主题，本来想用 butterfly 的，但是 bf 还是有点花哨，最终我感觉目前的这个主题比较不错，而且是有主页的主题哦！ 新建个博客文件夹，hexo init，然后 clone 新主题，然后把 _post 复制过来，然后安装必要的主题插件，然后安装 deploy 插件，最后开始配置 yml 站点配置文件和主题配置文件（两个）： 比对原站点的配置文件先修改一部分； 看到无关链接（主题自带的链接）或文字，要么注释掉要么改成你自己的； 修改 yml 配置文件后要 hexo s 本地测试下，如此反复； 填写一些和旧站点不一样的配置项，比如 deploy。 完成之后，还要注意搜索并替换一下 post 文章文件夹中的旧域名，现在要统一换成新域名，虽然不换也可以访问到（做了重定向），但既然准备用这个新域名了，那就得做好逐渐清退旧域名的措施，首先就是文章内的链接内容。旧站点由于用 old.cz5h.com 来绑定了，所以也要把旧站点内（notepad++全局搜索替换）的 zonelyn.com 全部换成 old.cz5h.com。最终的解析托管示意图如下： 换域名和旧域名重定向目前手里的域名有六个，时间分先后，要是有看上的，欢迎联系我啊。 whereareyou.site（301-&gt;zonelyn-&gt;cz5h） z-tian.cn（未解析） tzloop.cn（未解析） zonelyn.com（301-&gt;cz5h） cxmoe.com（挂了篇文章） cz5h.com（主站） 只用Github/Coding托管如何完成301重定向呢？答案是借助Pages服务+404页面，具体可以参见我之前写的 Github托管站点的域名更换。 为了偷懒，我并没有更新最老的 whereareyou.site 这个域名的重定向，它还是被重定向到 zonelyn.com，因为我给 zonelyn 做了重定向，所以，最终连续两跳，whereareyou.site 还是能重定向到 cz5h.com。是的，你没猜错，两个域名重定向，我注册了两个Github小号。还有，虽然是从Github-&gt;重定向到了Coding的空间，但我实际体验完全感觉不出来，果然是单向墙。 Coding的域名绑定就很简单了，解析个A记录和WWW记录就OK了。 二级域名 old.cz5h.com ：本来是不想动原来的域名的（因为用了新域名），但想了一下还是正规一点吧，而且也方便日后管理，所以就把 Github 托管的旧版本绑定了新域名的二级域名，最终域名的解析如下： 完成之后还要去 Github Pages 修改绑定的域名，同时注意需要将之前旧域名的解析一并删除，否则 Pages 服务会提醒报错的。 博客模块之我见本次也算是从零开始新建一个博客站点，当然对博客的美化也要从零开始，那么到底需要改动什么呢，简单写一下我的想法。（基本仅针对目前的Volantis主题） 主题色调 背景图：深色调还是浅色调，决定着前景标签内容的浅色调还是深色调。鄙人非常喜欢深色调的背景色，更可以突出文章主体的文字部分。 各种语法的颜色搭配：例如 tab 缩进的 code 段落，不同的主题样式肯定不同，我又很喜欢用这种方式写，所以要修改下对应的颜色搭配。 滚动条的样式：又宽又粗的滚动条实在是太丑了，必须要换掉。 整个页面 content 的宽度：目前很多主题的文章页宽度很窄，在台式机上看着更严重，所以一定要调宽一些，还更利于图片的展示。 访问统计这是一个重要的功能，但比较好加入。一般添加个不蒜子就可以了，但我喜欢追踪自己网站的访客，所以，我添加了 Google Analyse、百度统计、CNZZ的统计，并且加入了 revolvermaps 可视化小插件来做实时显示。 其他：注意SEO用的sitemap需要重新生成，还有robots.txt也要重新搞。 Posts属性类显示博客必备的一个部分，这个包括标签云、分类统计等，可以方便的放在这个主题右侧的插件部分。 文章页的署名信息这个要根据自己的调整，最好要包含这部分。 评论部分思来想后，还是决定采用Valine做评论吧，Gitalk得登陆啊，可有人连Github是什么都不知道，我的博客又不是正经的技术博客，所以…Valine真香。 站点的头脚前置index：虽然不是纯首页，但基本起到首页的作用了，而且效果还不错，准备把一些社交链接放在首页上。 Header：这里对应旧站点的自定义页面，由于原先旧站点有些主题的自定义语法，所以直接拿过来会解析出错误，得花时间重新写一下这些自定义页面。 Footer：由于用了深色背景，目前的页脚全黑的基本看不到东西，准备还是类似旧站点那样，在页脚写上基本信息，以及友链的链接，然后放上访问人数统计和全站文字统计的文本。 SEO完善默认的新建站点是没有这些SEO相关的文件的，需要借助第三方插件在渲染时生成 sitemap.xml 和 baidu_sitemap.xml，然后还需要自己新建一个 robots.txt 文件，因为是 hexo 站点，所以在生成xml文件之后，这个 robots.txt 文件的内容基本上是固定的。 12345678910111213User-agent: *Allow: &#x2F;Allow: &#x2F;archives&#x2F;Allow: &#x2F;categories&#x2F;Allow: &#x2F;tags&#x2F;Allow: &#x2F;about&#x2F;Disallow: &#x2F;vendors&#x2F;Disallow: &#x2F;js&#x2F;Disallow: &#x2F;css&#x2F;Disallow: &#x2F;fonts&#x2F;Disallow: &#x2F;fancybox&#x2F;Sitemap: https:&#x2F;&#x2F;你的域名&#x2F;sitemap.xmlSitemap: https:&#x2F;&#x2F;你的域名&#x2F;baidusitemap.xml 注意：以上文件放在 hexo 的source文件夹或者主题的source文件夹里都可以；另外，这里如果有自定义的路径，比如 /friends/（友链），也需要将其加入进去（添加新的 Allow 项），否则蜘蛛是不能爬到相关页面的。 恢复卡片的tags这是后续的改进了，现在的首页文章卡片没有标签信息，但我想把标签信息加上，这样会更加详细一点，初步想法就是在卡片的底部加个footer，然后把当前文章的tags读出来放到上面，再把footer贴到原来的卡片最下面。 进阶的功能相关文章：貌似这个主题支持。热门文章：调用统计的接口把。 还有很多功能可能主题内就带着，毕竟只接触了这个主题一天而已，后续慢慢探索，初步的感觉是这个主题比较完善，支持非常多的扩展功能（已经写好了），而且貌似作者还在更新这个项目。 写在最后这也算半个新的开始吧，目前自己已经在博客里写了不少东西了，原先的主题也看厌烦了，本来是想用 typecho 的，但想了下还得备案，所以干脆不用了，还是用回老本行 hexo 吧，而且现在各种操作非常熟练了，写作模式也基本固定了（自从前几天确定用Github+jsdeliver当图床后）。搬回 coding 后，以后在百度就能见到我了，而且我在外网访问 coding 的站点没任何卡顿，希望 coding 的 pages 服务比以前提升了，可不要让我失望啊！","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.cz5h.com/tags/Hexo/"},{"name":"Indigo","slug":"Indigo","permalink":"https://www.cz5h.com/tags/Indigo/"},{"name":"Volantis","slug":"Volantis","permalink":"https://www.cz5h.com/tags/Volantis/"},{"name":"重定向","slug":"重定向","permalink":"https://www.cz5h.com/tags/%E9%87%8D%E5%AE%9A%E5%90%91/"}]},{"title":"优雅的备份博客内的外链图片","slug":"2020-2-25 优雅的备份博客内的外链图片","date":"2020-02-24T23:00:00.000Z","updated":"2020-02-29T18:43:55.578Z","comments":true,"path":"article/509f.html","link":"","permalink":"https://www.cz5h.com/article/509f.html","excerpt":"这是我早就想做的一件事情了，但没想到不得不做这件事的这一天这么快就来临了。其实从一开始，我就对免费图床并不放心，在国内一直使用的是付费的作业部落的图片存储（阿里云的对象存储），但到外面以后作业部落已经实质性歇菜了，只得找其他的替代产品，这时我才真正意义上的接触并使用到了免费图床。先后用过sm.ms、微博图床、即刻图床（聚合性质的单一图床），直到有一天…","text":"这是我早就想做的一件事情了，但没想到不得不做这件事的这一天这么快就来临了。其实从一开始，我就对免费图床并不放心，在国内一直使用的是付费的作业部落的图片存储（阿里云的对象存储），但到外面以后作业部落已经实质性歇菜了，只得找其他的替代产品，这时我才真正意义上的接触并使用到了免费图床。先后用过sm.ms、微博图床、即刻图床（聚合性质的单一图床），直到有一天… 我的几篇文章的图片打不开了！于此，我是非常惊恐的，因为这些图片往往是信手拈来，上传即丢，本地并没有备份，这样显示突然显示不出来，你让我怎么找补，太难受了，遂将备份图片的事情提上日程。 备份Hexo博客的全部图片这是我感觉还算可以的、比较傻瓜式的一种解决方案。不需要安装任何软件和环境，全部借助Chrome插件完成。思路简单来说，就是先把全部的博客页面打开，然后使用图片抓取插件把打开的页面中的图片全部下载下来。很疯狂吧！ Linkclump 这是个批量打开链接的插件，可以在页面上框选，会自动识别网页链接（很棒的一点是可以过滤掉中转页面），然后同时在Chrome中打开，接下来就要用到图片抓取插件了。 Fatkun图片批量下载 这个插件可以直接搜刮浏览器内打开的页面内的全部图片，然后我们就可以下载这些图片了，注意这个工具可以按照像素值过滤图片，这样对一些很小的图片（比如恶心的百度统计的加载图hm）就可以排除在外。下载后的图片被放置在以页面命名的文件夹内，还是比较人性化的。 总结来说，其实上述操作相当于冷备份，可能我并不会用到这些图片，但当某些外链坏掉时，那么我就有本地备份可以替换，不然就像我这次一样，图片丢了找都找不回来了。 找补缺损图片这真的很难，我只能有部分重新截图了，还有小部分是存在微博相册的，这部分图片还在相册里，只是微博防盗链了。丢的那部分基本是用即刻图床传到阿里云的图床上了，但目前那批图片全挂了，涉及的文章可能有五六篇。 目前使用的稳妥的方案目前所使用的是一款聚合图床，这个不同于“聚合性质的单一图床”，而应该是“分发多地的聚合图床”，即其会将图片分发到多个图床之上（没错，还是多个免费图床），这样虽然单一图床有挂掉的风险，但三个（其目前的分发数量）图床同时挂掉的风险基本很小，而且其还提供上传记录来允许用户进行追溯，当然也可以定期输出备份。 我不确定这个图床的存活时间，但理论上其不是个图床，只是个聚合索引平台，所以无论他倒不倒闭，只要把我上传历史的列表给我保留就OK。 使用痛点：图片会压缩，上传慢，仍然看不到图片的原文件。 GitHub+jsDelivr+PicGo先马一个教程。 貌似这是我所追求的终极方案：我可以忍受图片挂掉（jsDelivr过滤了），但同时全部图片在GitHub的仓库里，图片的安全性不用担心，备份+外链一举两得。 但目前我的使用痛点是：PicGo貌似只能以桌面客户端的形式进行配置，然后才能进行图床的搭建，但我目前用了两台台式+一台笔记本三个电脑，且Linux和Windows混用，或者说，我想在任何一台机器上不用配置就完成上传，显然目前好像是实现不了的。所以暂时观望一下，不过仍然是未来的尝试方向，Githuh仓库存图片太适合我这种小博客了，图不多，又安全。 后记不用纠结了，直接用上了，花了十几分钟就配置好了，好像很简单。具体步骤如下： 下载安装PicGO， 新建个Github仓库，专门用来存储图片， 新建token，在 Setting/Developer Setting 里， 打开安装好的PicGo，选择上传图床为 Github图床，然后进行配置， 完毕。 实验了一下，简直不要太好用，海外党的福音啊，上传非常快，而且比较易用，最关键的是不压缩！！没有质量损失！！下面这张图片就是放在Github图床的。 感受一下 Github + jsDelivr 的绝佳体验吧 下图就是放在Github的19.8M的长幅： 第二个例子：带文字的QQ截图： 放在我用的聚合图床的体验如下 所使用的图床给他压缩成了3.9M： 第二个例子：带文字的QQ截图： 直观的比较（肉眼其实看不太出来，但某些图片差异会比较明显） 总结：从今天开始，Github + jsDelivr 就是我的御用图床了，除非它俩死一个，否则我不会再换图床了！还有个漫长的工作，就是把图片全部迁移到 Github 上！","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.cz5h.com/tags/Hexo/"},{"name":"图床","slug":"图床","permalink":"https://www.cz5h.com/tags/%E5%9B%BE%E5%BA%8A/"},{"name":"Chrome插件","slug":"Chrome插件","permalink":"https://www.cz5h.com/tags/Chrome%E6%8F%92%E4%BB%B6/"}]},{"title":"构建CUDA项目二周目翻车记录","slug":"2020-2-22 构建CUDA项目二周目翻车记录","date":"2020-02-21T23:00:00.000Z","updated":"2020-02-29T18:43:55.576Z","comments":true,"path":"article/c85a.html","link":"","permalink":"https://www.cz5h.com/article/c85a.html","excerpt":"继上次的翻车之后，我算是有了些经验，同时机器上也装了些共通的依赖库，由于上项目最后的错误解决不了就放那里了，开始搞一下这个项目，这和上一个项目的目的是一样的，都是借助GPU进行加速计算的可视化工具，但此项目是用netbeans开发的，在文件结构上要比上一个复杂的多，而且采用的是CMakeList.txt的方式，应该算比较正式的C++项目了吧。","text":"继上次的翻车之后，我算是有了些经验，同时机器上也装了些共通的依赖库，由于上项目最后的错误解决不了就放那里了，开始搞一下这个项目，这和上一个项目的目的是一样的，都是借助GPU进行加速计算的可视化工具，但此项目是用netbeans开发的，在文件结构上要比上一个复杂的多，而且采用的是CMakeList.txt的方式，应该算比较正式的C++项目了吧。 题外话：nvcc和nvidia-smi显示的cuda版本不一样这个问题最终被确定和程序运行的错误无关，但碰巧发现了，总觉得不一样怪怪的，而不一样的原因其实很简单，可以理解为一个版本是程序用的软件运行的Cuda版本，而另一个是开发用的Cuda版本。更多内容可以参见这个帖子 具体来说，就是确保Cuda安装后添加PATH：添加Cuda Bin的$PATH（即将以下行添加到您的~/.bashrc） 1export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;NsightCompute-2019.1$&#123;PATH:+:$&#123;PATH&#125;&#125; 注意：确保首先存在以下两个路径：/usr/local/cuda-10.1/bin和/usr/local/cuda-10.1/NsightCompute-2019.1（NsightCompute路径的结尾可能略有不同，具体取决于安装的Nsight计算版本。然后，更新$LD_LIBRARY_PATH（即添加以下行，在~/bashrc内）。 1export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;lib64\\$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125; 在此之后，两者nvcc和nvidia-smi（或nvtop）就应该打印相同版本的Cuda了 尝试运行项目构建这个项目，首先需要新建个cmake文件夹，然后进入该文件夹内，执行cmake命令，结果报错了，找不到Cuda（found 9.1），隐约感到这不是简单的路径问题，后来得到证实，这是在编译时没有指定版本导致的。 解决方案：在cmake时添加编译参数（参见这里），如下所示： 1cmake -DCUDA_TOOLKIT_ROOT_DIR&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.2 常规的配置修改修改arch参数，改为compute_50，由于是CMakeList.txt构建的，所以这次在这里修改。之后，修改对应的模块路径，修改简单的引用错误。 make系列错误做完以上操作后，cmake命令就基本没问题了，这里我们需要继续make，但是make到[66%]左右时会出现错误如下，我以为又是什么难以解决的问题，结果在make之前clean一下就ok了。 注意make之前一定要make -clean，不然会出错，详见这里，修改玩上述报错后会有新错误： 链接时的recompile with -fPTC 可以看到已经编译完成了，但链接linking出错，在此，我推测是原有的静态链接文件（源代码带的）并不能在我的机器上正常工作，我需要重新编译生成一遍，之后我重新编译了ANN、Wordcloud、Cubu模块，发现大部分错误消失了，但又出现新错误，如下： 恼人的Cubu模块 定位：graphdrawing 有问题，进而定位到Cubu，非常怀疑是libcubu.a这个静态链接库有问题，但我重新编译后生成的新文件去替换它之后，会出现以下错误， 怀疑是Cubu的版本不对应，编译都不通过。用原来Cubu的include文件夹+新的libcubu.a，错误依旧： 用新的include文件夹＋原来的libcubu.a，错误又会复现。基本确定是cubu重新编译的问题。原来的libcubu.a需要重新编译，但我现在编译出的静态链接文件libcubu.ａ又和原先的Cubu的版本好像不一样，所以导致上述错误。 卡壳了。","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://www.cz5h.com/tags/C-C/"},{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"Cuda","slug":"Cuda","permalink":"https://www.cz5h.com/tags/Cuda/"},{"name":"CMakeList","slug":"CMakeList","permalink":"https://www.cz5h.com/tags/CMakeList/"}]},{"title":"重新编译运行C++/Cuda混编项目","slug":"2020-2-13 重新编译运行C++Cuda混编项目","date":"2020-02-12T23:00:00.000Z","updated":"2020-02-29T18:43:55.574Z","comments":true,"path":"article/38ca.html","link":"","permalink":"https://www.cz5h.com/article/38ca.html","excerpt":"由于需要，最近得重新运行一个CUDA项目，但我苦于没有经验，只能从编译开始入门一下，不过还是不算难的，难的是原项目代码不保证质量，而且有若干无关文件，且运行环境未知、各模块的运行版本也不是很清楚，导致搞了一大堆操作（应该是正确的）最后却没跑起来，是的，这是一篇翻车笔记。","text":"由于需要，最近得重新运行一个CUDA项目，但我苦于没有经验，只能从编译开始入门一下，不过还是不算难的，难的是原项目代码不保证质量，而且有若干无关文件，且运行环境未知、各模块的运行版本也不是很清楚，导致搞了一大堆操作（应该是正确的）最后却没跑起来，是的，这是一篇翻车笔记。 从最简单的CMAKE示例开始为了打印文件结构，我们要先装一个tree来做准备。 1tzloop@tzloop-GE62-6QC:~&#x2F;Desktop&#x2F;test1$ sudo apt install tree 当前的例子的文件结构如下所示，这不是最简单的形式，但却是最接近项目结构的形式。具体的文件内容在这里可以找到，在此仅做示例。 12345678910111213tzloop@tzloop-GE62-6QC:~&#x2F;Desktop&#x2F;test1$ tree -C -L 2.├── bin├── build├── CMakeLists.txt├── include│ ├── testFunc1.h│ └── testFunc.h└── src ├── CMakeLists.txt ├── main.cpp ├── testFunc1.cpp └── testFunc.cpp 构建项目的步骤如下： 123456tzloop@tzloop-GE62-6QC:~&#x2F;Desktop&#x2F;test1$ cd build&#x2F;tzloop@tzloop-GE62-6QC:~&#x2F;Desktop&#x2F;test1&#x2F;build$ cmake ..... ... -- Configuring done-- Generating done-- Build files have been written to: &#x2F;home&#x2F;tzloop&#x2F;Desktop&#x2F;test1&#x2F;build 会发现： 在build文件夹下已经出现一众的Cmake文件的src文件夹 并生成了Makefile文件，这是我们接下来进行cmake的保证 对于CmakeCache.txt，如果我们要重新cmake，则需要删除之 1234567tzloop@tzloop-GE62-6QC:~&#x2F;Desktop&#x2F;test1&#x2F;build$ makeScanning dependencies of target main[ 25%] Building CXX object src&#x2F;CMakeFiles&#x2F;main.dir&#x2F;main.cpp.o[ 50%] Building CXX object src&#x2F;CMakeFiles&#x2F;main.dir&#x2F;testFunc.cpp.o[ 75%] Building CXX object src&#x2F;CMakeFiles&#x2F;main.dir&#x2F;testFunc1.cpp.o[100%] Linking CXX executable ..&#x2F;..&#x2F;bin&#x2F;main[100%] Built target main 在make（有百分比阶段）完成后，会发现编译完的o文件，被link（链接）完成，最后输出可执行的文件到bin目录，我们进入bin目录运行main会打印出正常结果。 1234tzloop@tzloop-GE62-6QC:~&#x2F;Desktop&#x2F;test1&#x2F;build$ cd ..&#x2F;bintzloop@tzloop-GE62-6QC:~&#x2F;Desktop&#x2F;test1&#x2F;bin$ .&#x2F;maindata is 100data is 200 开始调试项目项目简介：一个CUDA项目，使用GLUI构建的操作界面，使用OPENGL来利用GPU加速计算，整体上是C++混编CUDA程序，采用Makefile构建。按道理直接make即可，但遇到的错误没想到一大堆！ Error1: clang++: not found解决方法，安装clang即可： 12tzloop@tzloop-GE62-6QC:sudo apt-get updatetzloop@tzloop-GE62-6QC:sudo apt-get install clang Error2: 依赖的头文件找不到分析下Cmake文件，这里主要有三个模块需要引入到源文件中，分别是：ANN、TRIANGLE、CUBU。 解决方法：重新安装。要注意查看项目的源代码，是直接调用的，还是依赖编译环境的。前者需要将模块构建完成后放到项目中，而后者只需要在系统中安装相应模块，程序运行时会调用系统的库。 12345678910tzloop@tzloop-GE62-6QC:~&#x2F;Desktop&#x2F;ProjectionExplain$ makeclang++ -c -I. -Iinclude -O3 -g -m64 -I..&#x2F;CUBu -I..&#x2F;Geometry&#x2F;tsANN&#x2F;include -I..&#x2F;Geometry&#x2F;Triangle -I&#x2F;Developer&#x2F;NVIDIA&#x2F;CUDA-6.0&#x2F;include -I&#x2F;Developer&#x2F;NVIDIA&#x2F;CUDA-6.0&#x2F;samples&#x2F;common&#x2F;inc -stdlib&#x3D;libstdc++ -o main.o main.cppIn file included from main.cpp:2:In file included from .&#x2F;include&#x2F;vis.h:9:.&#x2F;include&#x2F;pointcloud.h:4:10: fatal error: &#39;ANN&#x2F;ANN.h&#39; file not found#include &quot;ANN&#x2F;ANN.h&quot; ^~~~~~~~~~~1 error generated.Makefile:32: recipe for target &#39;main.o&#39; failedmake: *** [main.o] Error 1 相关链接：Ubuntu中c++项目引用第三方包库的方法 粗体文本Error3: Cubu cmake failed粗体文本Cubu的安装要先下载，然后手动构建完后放到项目中，下载地址。 出现错误： missing: GLUI_INCLUDE_DIR GLUI_LIBRARY 123456789101112131415161718192021222324252627tzloop@tzloop-GE62-6QC:~&#x2F;Desktop&#x2F;rmmartins-cubu-ee034cafdaa2$ mkdir -p build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make install-- The C compiler identification is GNU 7.4.0-- The CXX compiler identification is GNU 7.4.0-- Check for working C compiler: &#x2F;usr&#x2F;bin&#x2F;cc-- Check for working C compiler: &#x2F;usr&#x2F;bin&#x2F;cc -- works-- Detecting C compiler ABI info-- Detecting C compiler ABI info - done-- Detecting C compile features-- Detecting C compile features - done-- Check for working CXX compiler: &#x2F;usr&#x2F;bin&#x2F;c++-- Check for working CXX compiler: &#x2F;usr&#x2F;bin&#x2F;c++ -- works-- Detecting CXX compiler ABI info-- Detecting CXX compiler ABI info - done-- Detecting CXX compile features-- Detecting CXX compile features - done-- Found OpenGL: &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libOpenGL.so -- Found GLUT: &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libglut.so CMake Error at &#x2F;usr&#x2F;share&#x2F;cmake-3.10&#x2F;Modules&#x2F;FindPackageHandleStandardArgs.cmake:137 (message): Could NOT find GLUI (missing: GLUI_INCLUDE_DIR GLUI_LIBRARY)Call Stack (most recent call first): &#x2F;usr&#x2F;share&#x2F;cmake-3.10&#x2F;Modules&#x2F;FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE) cmake&#x2F;FindGLUI.cmake:106 (find_package_handle_standard_args) CMakeLists.txt:8 (find_package)-- Configuring incomplete, errors occurred!See also &quot;&#x2F;home&#x2F;tzloop&#x2F;Desktop&#x2F;rmmartins-cubu-ee034cafdaa2&#x2F;build&#x2F;CmakeFiles&#x2F;CmakeOutput.log&quot;. 解决方法：Cubu有其他模块的依赖，最好是先安装其他模块。对于只需要在系统安装的模块，可以直接一条命令安装： 1tzloop@tzloop-GE62-6QC:~$ sudo apt-get install libtriangle-dev libann-dev libann-dev libfreeimage-dev libglew-dev libboost-all-dev libftgl-dev libfreetype6-dev libeigen3-dev 粗体文本Error4: glui install failed**解决方法：我这里出现了安装 libglui-dev失败的提示，搜索后发现是Ubuntu不提供这个模块了，所以只能去github下载源代码手动构建。下载版本为glui-2.35，这里是下载链接 下载完成后进入目录，因为也是Makefile构建的，所以直接make： 12345tzloop@tzloop-GE62-6QC:~&#x2F;Desktop&#x2F;glui-2.35$ makemkdir -p binmkdir -p libg++ -O0 -Wall -pedantic -I.&#x2F; -I.&#x2F;include -I&#x2F;usr&#x2F;X11R6&#x2F;include -c glui_add_controls.cpp… … ANN模块同理，下载源文件tar.gz，在这里下载 ，然后注意编译的方式，使用linux-g++ 12tzloop@tzloop-GE62-6QC:~&#x2F;Desktop&#x2F;ann-1.1.2+doc$ make linux-g++… … Triangle模块同理，在这里下载 ，最后Cubu也要放进来（项目lib目录），然后改下名字（随意），这里把版本号去掉了。然后修改Malefile的对应的模块路径。 穿插GLUT教程教程在，这里，按照教程，我成功的运行了所给的demo，这表明我的Glut模块是安装正常的。 Error5: Unsupported gpu architecture ‘compute_12’ 问题的详细 纠错指导 解决方法：这里需要修改Makefile中的arch参数，看自己的Cuda参数而定，这里我使用compute_50即正常。 Error6: skelf.cu: pointcloud not found解决方法：修改skelf.cu内部的头文件引用名称。 Error7: cuda_ruantime.h not found 解决方法：这是Cuda路径配置错误导致的，查看makefile中对应cuda的路径CUDAROOT，修改成本地环境对应的cuda路径。相应的也要修改CUDALIB路径。 Error8: undeclared identifier of xxEXT 因为上面是vis.app报错，所以找到vis.h，修改其头文件的引用方式： 之后，出现找不到glui.h 拷贝libglui.a和glui.h到usr相应目录，之后再运行仍然又重复undeclared错误。 总结重新构建的步骤首先是添加依赖模块，然后修改Makefile中对应的依赖模块的路径和Cuda的相关路径，顺带把arch的参数改为合适自己的compute_xx，之后修改一些头文件的引用错误（有的可能单纯是头文件名称错了），有的在不同系统下的路径不同，要改成对应的引用路径（比如glui在mac可能是GLUI/glui但在ubuntu是GL/glui），最后就到达这个错误了。。 12345678910111213141516vis.cpp:343:2: error: use of undeclared identifier &#39;glGenFramebuffersEXT&#39; glGenFramebuffersEXT(2,framebuffers); &#x2F;&#x2F;Make t... ^vis.cpp:347:2: error: use of undeclared identifier &#39;glBindFramebufferEXT&#39; glBindFramebufferEXT(GL_FRAMEBUFFER_EXT,framebuffer_lum); ^vis.cpp:352:2: error: use of undeclared identifier &#39;glFramebufferTexture2DEXT&#39; glFramebufferTexture2DEXT(GL_FRAMEBUFFER_EXT,GL_COLOR_ATTACHMENT... ^vis.cpp:354:2: error: use of undeclared identifier &#39;glBindFramebufferEXT&#39; glBindFramebufferEXT(GL_FRAMEBUFFER_EXT,framebuffer_rgba); ^vis.cpp:359:2: error: use of undeclared identifier &#39;glFramebufferTexture2DEXT&#39; glFramebufferTexture2DEXT(GL_FRAMEBUFFER_EXT,GL_COLOR_ATTACHMENT... ^... ... 貌似是某模块的EXT扩展引用错误，但目前暂时不知道怎么改，难顶…","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://www.cz5h.com/tags/C-C/"},{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"Cuda","slug":"Cuda","permalink":"https://www.cz5h.com/tags/Cuda/"},{"name":"Makefile","slug":"Makefile","permalink":"https://www.cz5h.com/tags/Makefile/"}]},{"title":"Ubuntu的基本配置及界面美化","slug":"2020-2-1 Ubuntu的基本配置及界面美化","date":"2020-01-31T23:00:00.000Z","updated":"2020-02-29T18:43:55.575Z","comments":true,"path":"article/a441.html","link":"","permalink":"https://www.cz5h.com/article/a441.html","excerpt":"本文介绍针对Ubuntu系统的一般操作，包括一些基本配置和界面美化，以及中间可能出现的一些问题。可能以后会比较多的用笔记本来做实验，虽然台式机性能更好，但是毕竟不能背来背去，所以还是用笔记本会好很多，正因为如此，所以把Ubuntu整的好看一点还是很重要的，因为实在找不到比较好的Win10美化主题，所以最终还是决定向MacOS方向美化，网上已经有很多资料了，这里只是记录下自己系统的配置及美化过程。","text":"本文介绍针对Ubuntu系统的一般操作，包括一些基本配置和界面美化，以及中间可能出现的一些问题。可能以后会比较多的用笔记本来做实验，虽然台式机性能更好，但是毕竟不能背来背去，所以还是用笔记本会好很多，正因为如此，所以把Ubuntu整的好看一点还是很重要的，因为实在找不到比较好的Win10美化主题，所以最终还是决定向MacOS方向美化，网上已经有很多资料了，这里只是记录下自己系统的配置及美化过程。 基本配置和软件修改root密码为了方便的执行命令，不用每次都sudo，我们可以直接修改root账户的密码，继而切换到root账户执行相关操作，并且在后续过程中，一旦出现开机异常，在具有root账户密码的情况下，我们可以方便的通过Ctrl+Alt+F4（或其他F1~F6）进入tty4通过命令行的方式修复相关的配置文件，从而修复开机失败的情况。 12tzloop@tzloop-GE62-6QC:~$ sudo passwd root# 输入你的新密码 注意：初始密码是随机的，只能通过重置来给定一个新密码。 QQ for Linuxhttps://im.qq.com/linuxqq/download.html Sogou for Linuxhttps://pinyin.sogou.com/linux/ 在使用过程中有个问题，那就是使用输入法输入数字和英文字母的时候会出现异常的字符，“ｌｉｋｅ ｔｈｉｓ”。 重启报错的问题如果每次开机重启都会出现“System program problem detected”的问题，我们可以通过配置忽略这个弹窗，其本身系统运行没有影响。 12tzloop@tzloop-GE62-6QC:~$ sudo gedit /etc/default/apport # 将 enabled=1 改为 enabled=0 主题和Dock这里包括两部分内容： 一个是在Chrome插件中配置使用gnome插件，这可以调节一些细微的细节 一个是材质包的下载（包括主题资源、光标、图标）以及配合gnome-tweaks工具完成主要的主题配置 Chrome插件，修改细节下载gnome管理插件，可以方便的下载并启用相关的gnome插件。下载地址在这 ，之后可以在Chrome中打开插件主页进行配置。 这里列举几个用到的插件： Top bar script executer：顶栏快捷方式运行自定义命令 Keys indicator：顶栏显示当前按下了Caps Lock, Num Lock, Ctrl, Shift, Alt中的哪些键 Hide activities button：隐藏顶栏活动按钮 Launch：替换顶栏活动按钮为所有应用程序（9个点） Hide top bar：自动隐藏顶栏 No title bar：直接将应用的标题栏和顶栏结合为一体，窗口操作按钮也被放在顶栏上（不推荐使用） 美化Dock也就是底部程序托盘第一步： 1234# 添加软件源tzloop@tzloop-GE62-6QC:~$ sudo add-apt-repository ppa:noobslab/macbuntu# 安装Planktzloop@tzloop-GE62-6QC:~$ sudo apt-get install plank 将Plank固定在Dock上，可以进入文件夹/usr/share/applications，找到相关软件，拖到Dock上就OK。 第二步： 下载Frost：在解压后的文件夹里找到名为Frost的文件夹，将他复制到/.local/share/plank/themes里（没有就创建）下载Mac OS Themes：解压后进入文件夹，你会看到一大堆主题，根据名字复制你需要的文件夹到/.local/share/plank/themes里（没有就创建）最后在Plank上Ctrl+右键，点击首选项，选择相关主题。 第三步：隐藏原Dash 安装Dash to Dock 安装好之后进入GNOME Tweaks，进入左侧栏插件，将Dash to Dock打开，之后关闭之，如果发现没有变回Ubuntu默认dash，那么就完成关闭了。 点击Dash to Dock的设置按钮，打开自动隐藏，把两个开关都关掉，你会发现你找不到Dash了 设置Dock开机启动：打开GNOME Tweaks，左侧栏进入开机启动程序， 其他配置 安装Slingscold，Slingscold是一个比GNOME原生启动器更轻量的全屏启动器。 1tzloop@tzloop-GE62-6QC:~$ sudo apt-get install slingscold 将Slingscold固定在Dock上，可以进入文件夹/usr/share/applications，找到相关软件，拖到Dock上就OK。 使用Tweaks修改主题1tzloop@tzloop-GE62-6QC:~$ sudo apt-get install macbuntu-os-icons-v1804 macbuntu-os-ithemes-v1804 macbuntu-os-plank-theme-v1804 通过以上命令安装Macbuntu的主题、图标和Plank主题，打开GNOME Tweaks，将左侧栏外观中的应用程序 光标 图标设成Macbuntu即可。 最终的美化效果 修改字体Ubuntu系统的英文字体很好看，但中文字体有时会觉得不是那么好看，所以可以自己替换相应的字体。这里首先需要检查是否安装了中文字体，在终端上输入下面指令查看已安装字体 1tzloop@tzloop-GE62-6QC:~$ fc-list :lang=zh 如果没有中文字体可以到网络中下载，或者直接到windows系统中拷贝。中文字体安装完成后，需要修改默认的字体设置文件，修改默认字体找不到的时候的默认替代字体，进入下面路径找到配置文件： 1tzloop@tzloop-GE62-6QC:~$ vim /etc/fonts/conf.d/64-language-selector-prefer.conf 修改（添加）文件中的字体配置信息，每个后面的第一行都是添加的默认新字体。 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE fontconfig SYSTEM \"fonts.dtd\"&gt;&lt;fontconfig&gt; &lt;alias&gt; &lt;family&gt;sans-serif&lt;/family&gt; &lt;prefer&gt; &lt;family&gt;Microsoft YaHei&lt;/family&gt; &lt;family&gt;Noto Sans CJK JP&lt;/family&gt; &lt;family&gt;Noto Sans CJK KR&lt;/family&gt; &lt;family&gt;Noto Sans CJK SC&lt;/family&gt; &lt;family&gt;Noto Sans CJK TC&lt;/family&gt; &lt;family&gt;Noto Sans CJK HK&lt;/family&gt; &lt;/prefer&gt; &lt;/alias&gt; &lt;alias&gt; &lt;family&gt;serif&lt;/family&gt; &lt;prefer&gt; &lt;family&gt;Microsoft YaHei&lt;/family&gt; &lt;family&gt;Noto Serif CJK JP&lt;/family&gt; &lt;family&gt;Noto Serif CJK KR&lt;/family&gt; &lt;family&gt;Noto Serif CJK SC&lt;/family&gt; &lt;family&gt;Noto Serif CJK TC&lt;/family&gt; &lt;/prefer&gt; &lt;/alias&gt; &lt;alias&gt; &lt;family&gt;monospace&lt;/family&gt; &lt;prefer&gt; &lt;family&gt;Microsoft YaHei&lt;/family&gt; &lt;family&gt;Noto Sans Mono CJK JP&lt;/family&gt; &lt;family&gt;Noto Sans Mono CJK KR&lt;/family&gt; &lt;family&gt;Noto Sans Mono CJK SC&lt;/family&gt; &lt;family&gt;Noto Sans Mono CJK TC&lt;/family&gt; &lt;family&gt;Noto Sans Mono CJK HK&lt;/family&gt; &lt;/prefer&gt; &lt;/alias&gt;&lt;/fontconfig&gt; 使用sudo权限保存后重启即可生效，原来的字体就会变为中文字体默认的繁体字体就变成了新的简体中文。 调节Ubuntu亮度这部分比较麻烦，可能出现很多种情况，下面分别讲解一下。 如果是Fn+亮度调节按钮可以出现亮度调节图标，但调节无效，那么可以使用如下方法： 1234567tzloop@tzloop-GE62-6QC:~$ sudo gedit /etc/default/grub# 修改文件中的字段：GRUB_CMDLINE_LINUX=\"\" 改成 GRUB_CMDLINE_LINUX=\"acpi_backlight=vendor\"# 更新tzloop@tzloop-GE62-6QC:~$ sudo update-grub# 重启tzloop@tzloop-GE62-6QC:~$ reboot 如果想设置亮度的初始值，可以直接修改文件内的数值。 123tzloop@tzloop-GE62-6QC:~$ sudo gedit /etc/rc.local# 在打开文件里增加以下代码（加在exit 0之前）echo 500 &gt; /sys/class/backlight/intel_backlight/brightness 2. 如果/sys/class/backlight/目录下是空的 可以通过安装第三方软件的方法来调节亮度，安装brightness-controller或者brightness-controller-sample这两个软件， 1234tzloop@tzloop-GE62-6QC:~$ sudo add-apt-repository ppa:apandada1/brightness-controllertzloop@tzloop-GE62-6QC:~$ sudo apt-get updatetzloop@tzloop-GE62-6QC:~$ sudo apt-get install brightness-controllertzloop@tzloop-GE62-6QC:~$ brightness-controler 左侧是正常版的调节软件，右侧是simple版本。 3. 如仍然无效可以采用以下危险方案 以下修改可能造成重启后无法进入系统，因为新增的驱动配置可能导致显示不正常，所以慎用，不过如果重启后进不去系统，也不要害怕，通过Ctrl+Alt+F4（或者F1~F6）进入tty4把新增的配置文件s删除即可，这里直接要用root用户登录，然后删除conf文件重启即可恢复。 进入/etc/X11/路径，如果没有xorg.conf文件的话，就自己新建一个，输入下面的内容，保存退出。 12345678910111213Section \"Screen\" Identifier \"Default Screen\" DefaultDepth 24EndSectionSection \"Module\" Load \"glx\"EndSectionSection \"Device\" Identifier \"Default Device\" Driver \"nvidia\" Option \"NoLogo\" \"True\" Option \"RegistryDwords\" \"EnableBrightnessControl=1\"EndSection 如果有这个文件的话在终端中输入： 12345678tzloop@tzloop-GE62-6QC:~$ sudo gedit /etc/X11/xorg.conf# 把fuiying对应内容改成以下内容Section \"Device\"Identifier \"Device0\"Driver \"nvidia\"VendorName \"NVIDIA Corporation\"Option \"RegistryDwords\" \"EnableBrightnessControl=1\"EndSection 然后保存，退出，重启之后，或许正常，如果进不去系统了，那么请看本段开头的解决方案，命令行进入系统删除conf文件即可。 4. 如果还是没有效果 合理的怀疑是因为显卡驱动的问题，我们知道在双显卡（独显加集显）的电脑上安装显卡驱动时，需要修改grub文件，添加nomodeset字段在slpash之后，这是为了禁止系统使用GTX的N卡驱动，具体在前一篇文章中详细说明，这里如果仍然无法调节亮度，请务必参看一下文件中是否仍然存在nomodeset字段。要将其删掉。 1234tzloop@tzloop-GE62-6QC:~$ sudo vim /etc/default/grub# 删除nomodeset字段，保存退出tzloop@tzloop-GE62-6QC:~$ sudo update-grubtzloop@tzloop-GE62-6QC:~$ reboot 这是我的最终错误定位，去掉nomodeset之后，其实才是真正使用独显显示，重启后会发现一些变化，比如打开程序会出现动画，点击程序菜单也会出现动画，亮度随意调节，网页或程序渲染速度也变得更快，而且上述的两个亮度调节软件也可以使用了。 参考资料https://zimocp.github.io/2019/05/20/Ubuntu-18-04-LTS-%E7%BE%8E%E5%8C%96/https://www.jianshu.com/p/4c95b708ae1dhttps://blog.csdn.net/redstone0001/article/details/17042011https://blog.csdn.net/u013406197/article/details/80773820https://blog.csdn.net/TianliangQiu/article/details/51585398https://blog.csdn.net/HedWater/article/details/75465110https://blog.csdn.net/weixin_41490463/article/details/80987654https://blog.csdn.net/gengyuchao/article/details/101215243","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://www.cz5h.com/tags/Ubuntu/"},{"name":"Linux","slug":"Linux","permalink":"https://www.cz5h.com/tags/Linux/"},{"name":"系统设置","slug":"系统设置","permalink":"https://www.cz5h.com/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE/"}]},{"title":"双显卡笔电安装N卡驱动及CUDA","slug":"2020-1-29 双显卡笔电安装N卡驱动及CUDA","date":"2020-01-28T23:00:00.000Z","updated":"2020-06-09T22:20:24.543Z","comments":true,"path":"article/891.html","link":"","permalink":"https://www.cz5h.com/article/891.html","excerpt":"由于工作需要，必须换操作系统了，一想到笔记本已经冗杂不堪，所以就索性重装成Linux系统，虽然显卡性能不如实验室的机器，但完全可以当做试验机，同时本身机子性能也不差，所以装个乌班图应该体验还不错。以上是我开始时的想法，后来装完了之后呢，体验总体也不错，但总归是有写麻烦，我总结一下放在开头。","text":"由于工作需要，必须换操作系统了，一想到笔记本已经冗杂不堪，所以就索性重装成Linux系统，虽然显卡性能不如实验室的机器，但完全可以当做试验机，同时本身机子性能也不差，所以装个乌班图应该体验还不错。以上是我开始时的想法，后来装完了之后呢，体验总体也不错，但总归是有写麻烦，我总结一下放在开头。 安装和使用Ubuntu的感受 初始安装时要修改配置，针对双显卡机器； 安装系统的过程非常快，这得益于我的固态硬盘； 系统的初始环境比Win10全，比如自带Python环境等等； 操作起来整体感觉圆滑，但比Win10要卡； 随便的小操作就可能导致开机黑屏，不过修复也比较容易； 开机关机要快点，因为不存在Win10那种更新； 系统的自定义程度高，基本哪里都可以改； 注意：微星笔电开启独显后会导致风扇一直运行在高转速； 注意：真不需要占用太多硬盘空间，后悔没装双系统！ 总体体验还不错，除了娱乐性差一点，我基本可以适应从Win10到Ubuntu的过度。有关于系统设置和美化的内容请转到 这篇文章。 下面是安装系统及显卡驱动的步骤和需要注意的地方。 安装系统及Nvidia驱动双显卡的系统安装及驱动安装的准备工作 Bios改成uefi，关闭安全启动； 插入启动U盘，启动安装，在try ubuntu ..install Ubuntu 的这个界面，按e，进入grub的调整界面； 把里边的quiet splash修改为nomodeset idle=nomwait，修改完成以后Ctrl + X或F10，使之生效； 然后继续安装，这样安装完成以后，应该可以顺利进入桌面； 正常进入桌面后，编辑 /etc/default/grub 这个文件，同样添加nomodeset字段到splash后面； 此时重启，应该一切正常可以进入桌面，这时其实系统使用的集显，独显的显卡还没装，下面开始。 123456789101112131415161718192021222324252627282930313233343536# 自动探测显卡版本并列出可使用的显卡驱动编号tzloop@tzloop-GE62-6QC:~$ ubuntu-drivers devices… …vendor : NVIDIA Corporationmodel : GM107M [GeForce GTX 960M]driver : nvidia-driver-440 - third-party free recommendeddriver : nvidia-driver-415 - third-party freedriver : nvidia-driver-410 - third-party freedriver : nvidia-driver-430 - third-party freedriver : nvidia-driver-390 - third-party freedriver : nvidia-driver-435 - distro non-freedriver : xserver-xorg-video-nouveau - distro free builtin# 查看内核版本（高内核会引发显卡驱动的不兼容）tzloop@tzloop-GE62-6QC:~$ uname -r 5.3.0-26-generic# 开始安装驱动时，先卸载之前的驱动tzloop@tzloop-GE62-6QC:~$ sudo apt-get remove --purge nvidia*… …INFO:Disable nvidiaDEBUG:Parsing /usr/share/ubuntu-drivers-common/quirks/put_your_quirks_hereupdate-initramfs: deferring update (trigger activated)Processing triggers for initramfs-tools (0.130ubuntu3.8) ...update-initramfs: Generating /boot/initrd.img-5.3.0-26-generic# 添加驱动的软件源tzloop@tzloop-GE62-6QC:~$ sudo add-apt-repository ppa:graphics-drivers/ppa… … Fetched 88,7 kB in 0s (186 kB/s) Reading package lists... Done# 更新下载器tzloop@tzloop-GE62-6QC:~$ sudo apt-get update… … Reading package lists... Done 选择安装驱动的版本列出适用该笔记本电脑显卡的具体驱动版本。这里有三点需要注意： 尽量选择高版本的显卡驱动，比如这里选择安装440，如果选择安装410，则会出现与内核不兼容的情况，我们面会谈到。 第二点就是，可能你运行下面的命令不能自动识别出显卡版型号，那么你就要去手动查询对应的Nvidia显卡型号和Ubuntu驱动编号的关系，手动选择一个版本安装。 最后，如果可以在&lt;软件和更新&gt;中找到相对应的驱动版本，也可以直接鼠标选择后应用即可。这是18.04及以后都支持的Nvidia驱动安装方法。（一般初始驱动是不完整的） 如果错误的在高版本内核下安装了低版本驱动，会安装失败的： 123456789tzloop@tzloop-GE62-6QC:~$ sudo apt install nvidia-driver-410 … …Building initial module for 5.3.0-26-genericERROR: Cannot create report: [Errno 17] File exists: '/var/crash/nvidia-kernel-source-410.0.crash'Error! Bad return status for module build on kernel: 5.3.0-26-generic (x86_64)Consult /var/lib/dkms/nvidia/410.104/build/make.log for more information.Setting up nvidia-driver-410 (410.104-0ubuntu0~18.04.1) ...Processing triggers for initramfs-tools (0.130ubuntu3.8) ...update-initramfs: Generating /boot/initrd.img-5.3.0-26-generic 所以我们要安装高版本，直接安装其recommended的版本即可。安装过程中会有基佬紫对话框，可能会被要求输入密码。 12345tzloop@tzloop-GE62-6QC:~$ sudo apt install nvidia-driver-440 … …Processing triggers for initramfs-tools (0.130ubuntu3.8) ...update-initramfs: Generating /boot/initrd.img-5.3.0-26-genericProcessing triggers for libc-bin (2.27-3ubuntu1) 显卡驱动安装完成的判定：，重启后输入nvidia smi，如果正常的显示出以下信息则正常。 可能出现的问题 重启后nvidia-smi通信失败如果安装非常顺利，但重启后nvidia-smi命令无效，切记要在重启后进入蓝色方框的界面输入安装驱动时的密码，再重启就可以啦。 首次安装过程中，登录账户后桌面黑屏原因：安装时的grub配置没有修改，这时要重新安装系统，按照上述步骤，一定要注意修改grub文件。请确认，这是在首次安装过程中出现的，我们可以看见登录界面，但登录进去后就是黑屏，可能有鼠标也可能没有。 安装显卡驱动之后，重启黑屏原因：没有在grub配置文件内添加nomodeset字段。请确认，这是在我们安装完成系统之后出现的问题，那么接下来的操作就与安装盘没有任何关系，所有操作都不要插着安装盘。只需要使用Ctrl+Alt+F4（也可能是F1~F6），进入tty4也就是黑框（微型Ge62系列是在出现MSI图标后就按下ESC进入，时机比较难把握，多试几次就行了），登录root账户后，找到/etc/default/grub文件，将里面的quiet splash后面添加上nomodeset这个字段，保存重启即可。 安装CUDA为什么重新安装Cuda，安装完成显卡驱动后的输出里已经能发现Cuda10.2了，这个问题我的理解是，我们要一个可控的开发环境，而不是运行环境，所以我们自己要重新装个Cuda，在Cuda官网可以找到对应显卡驱动的Cuda安装包，安装过程直接照官网来。 那么如何知道我们安装成功了呢，这里可以通过编译Cuda自带的例子来看其是否通过测试。 123456789101112131415161718# 首先测试下运行环境是否正常，主要是GCC是否正常tzloop@tzloop-GE62-6QC:~$ cat /proc/driver/nvidia/versionNVRM version: NVIDIA UNIX x86_64 Kernel Module 440.48.02 Tue Jan 14 06:22:51 UTC 2020GCC version: gcc version 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) # 进入sample所在的目录tzloop@tzloop-GE62-6QC:~$ cp -r /usr/local/cuda-10.2/samples/ ~tzloop@tzloop-GE62-6QC:~$ cd ~/samples# 编译自带的例子tzloop@tzloop-GE62-6QC:~/samples$ make… … arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o cudaNvSci.o -c cudaNvSci.cppIn file included from cudaNvSci.cpp:12:0:cudaNvSci.h:14:10: fatal error: nvscibuf.h: No such file or directory #include &lt;nvscibuf.h&gt; ^~~~~~~~~~~~compilation terminated. 注意：这里我们直接使用 make 的话会无法通过对例子的编译，参见 这里 的解释，这是一个已知的错误，我们可以通过 make -k 来忽略过程中的错误。 1234567891011121314151617181920# 使用 -k 参数来跳过错误tzloop@tzloop-GE62-6QC:~/samples$ make -k… …mkdir -p ../../bin/x86_64/linux/releasecp cuSolverRf ../../bin/x86_64/linux/releasemake[1]: Leaving directory '/home/tzloop/samples/7_CUDALibraries/cuSolverRf'make: Target 'all' not remade because of errors.# 最终的测试，如果是PASS则Cuda运行正常tzloop@tzloop-GE62-6QC:~/samples/bin/x86_64/linux/release$ ./deviceQuery./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking)Detected 1 CUDA Capable device(s)Device 0: \"GeForce GTX 960M\" CUDA Driver Version / Runtime Version 10.2 / 10.2 … … Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.2, CUDA Runtime Version = 10.2, NumDevs = 1Result = PASS 参考资料https://bbs.deepin.org/forum.php?mod=viewthread&amp;tid=41469https://bbs.deepin.org/forum.php?mod=viewthread&amp;tid=41469http://askubuntu.com/questions/162075/my-computer-boots-to-a-black-screen-what-options-do-i-have-to-fix-ithttps://www.jianshu.com/p/fc5edbd6f480https://www.jianshu.com/p/e476e5ad6b56","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"双显卡","slug":"双显卡","permalink":"https://www.cz5h.com/tags/%E5%8F%8C%E6%98%BE%E5%8D%A1/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://www.cz5h.com/tags/Ubuntu/"},{"name":"Nvidia","slug":"Nvidia","permalink":"https://www.cz5h.com/tags/Nvidia/"},{"name":"Cuda","slug":"Cuda","permalink":"https://www.cz5h.com/tags/Cuda/"}]},{"title":"家中为什么要用软路由","slug":"2020-1-21 家中为什么要用软路由","date":"2020-01-20T23:00:00.000Z","updated":"2020-02-29T18:43:55.567Z","comments":true,"path":"article/e1b1.html","link":"","permalink":"https://www.cz5h.com/article/e1b1.html","excerpt":"这原本是我在知乎上的回答，偶然发现被折叠了，改了N次也无济于事，还是放在这里吧。 软路由能做的但硬路由不能做的 插件。不仅仅只是网络插件的差别，拿LEDE举例，网络插件恰恰被从酷软中心拿掉了，哈哈，求生欲很强。但剩下的那些插件可不是一无是处，比如ftp就很有用，还有很多插件，自己探索吧。","text":"这原本是我在知乎上的回答，偶然发现被折叠了，改了N次也无济于事，还是放在这里吧。 软路由能做的但硬路由不能做的 插件。不仅仅只是网络插件的差别，拿LEDE举例，网络插件恰恰被从酷软中心拿掉了，哈哈，求生欲很强。但剩下的那些插件可不是一无是处，比如ftp就很有用，还有很多插件，自己探索吧。 小飞机。单独说下这个插件，对于软路由系统比如LEDE其支持更高级的Clash/Surge方式，这才是软路由使用网络查件的正确姿势。没错，这就是国内要想从TV看油管4K的必备，基本没什么其他解决方案。 平台扩展。要搞清楚软路由是什么，它一般依托于工控机平台（MiniPC），拿我自己的软路由I57200U来说，我可以物理机装LEDE、装爱快；但当然，我也可以装PVM、装ESXi，然后以虚拟机的方式运行我的软路由系统，在网卡直通的条件下软路由性能基本不受影响。所以如果以ESXi的方式，你完全可以安装其他的虚拟机，打造自己的本地云主机。你可以装Linux挂脚本、可以装Win10/7挂机器人，总之，你可以有若干台24小时运行的虚拟机，结合公网IP或者DDNS，可以获得和阿里云主机基本一样的效果，说到这里，想想4核16G200M带宽（上行5M/s）的阿里云主机要多少钱一年。 性能。终于说到性能了，我的看法，200M宽带及以上直接上软路由不用考虑其他（土豪钱多可以买高端路由）；其他的想全家网络番茄的，可以软路由，也可以硬路由刷梅林固件（梅林的插件的性能要差得多，但一般视频需求也足够了）。另外，看硬件，硬路由其实处理器性能很弱（比较同价位的软路由），外加需要同时支持无线网卡，这其实会影响整体的路由处理能力，但软路由+AP的模式，其实更适合高带宽场景。总之，小水管无所谓性能，大水管才看的出来。 一些言论 关于路由系统，真不需要听别人的安利，LEDE是小白专属； 关于异地访问，动态公网IP现在并不难得到，相信我，在国外的我可以透过路由自由的访问到家中的每一台设备（只要被分配了内网IP），这种体验非常棒，尤其是搭配我的群晖NAS。当然这已经不太属于软路由的范畴了。 所谓优化，别被唬住，设备到位优化就到位了，说需要技巧的都是瞎扯； 所谓稳定，我上次的软路由完美运行了快半年了，其实是三个系统同时运行，Winserver2016、Ubuntu和爱快，爱快是我的主路由。最后为什么崩了？因为停电了！ 所谓带机量，只要你家不是电影院，不来个百十号人，我相信中端路由（例如R7000）就完全可以满足。 所谓门槛，真没什么门槛，只要你认得汉字，都可以按照教程来，没人让你自己研究，网上的奶妈级教程多如牛毛~ 所谓玩法，玩法真的太多了，硬路由功能相比之下太少了。 最后，推荐几个土鳖上的软路由相关的频道：Vedio Talk、Big东东、翼王 多看看这些实际操作的视频，也可以有个初步了解，尤其推荐VedioTalk，有很多很实用的入门知识和实际操作教程，如果对软路由跃跃欲试，那就赶快去看看吧。 注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"组网","slug":"组网","permalink":"https://www.cz5h.com/tags/%E7%BB%84%E7%BD%91/"},{"name":"路由","slug":"路由","permalink":"https://www.cz5h.com/tags/%E8%B7%AF%E7%94%B1/"}]},{"title":"纵览全局的文档组织模式（下）","slug":"2020-1-8  纵览全局垂直打击的组织模式（下）","date":"2020-01-07T23:00:00.000Z","updated":"2020-03-07T04:07:45.015Z","comments":true,"path":"article/99a8.html","link":"","permalink":"https://www.cz5h.com/article/99a8.html","excerpt":"本文详细记录了如何在Hexo博客中实现用图组织内容的方法，但是，请注意：以下内容并非操作教程，仅表明相信思路以供参考，或许您可以实现出更好的版本，但仅依照下文内容并不保证一定能重现，一些尝试和debug的细节过于繁琐并未列出，如有疑问欢迎留言。","text":"本文详细记录了如何在Hexo博客中实现用图组织内容的方法，但是，请注意：以下内容并非操作教程，仅表明相信思路以供参考，或许您可以实现出更好的版本，但仅依照下文内容并不保证一定能重现，一些尝试和debug的细节过于繁琐并未列出，如有疑问欢迎留言。 代码实现hexo.extend.helper.register文档说明，借助该函数，可以在Hexo渲染生成页面文件之前，完成用户的自定义JavaScript代码。 其实，在Hexo的框架内，ejs（或其他类型的）模板中的代码就是渲染生成html的代码，在这些页面中，借助Hexo内建的对象，比如.post对象和.achieves对象，可以访问到其中保存的全部文章信息及关联信息。例如： 123let posts = hexo.locals.get('posts');let Xtags = posts.data[x].tagslet tagsY = Xtags.data[y].name 上述内容，可以最终得到第X篇文章（POST）中的第Y个标签的文本。类似的方法同样可以得到某篇文章的Categories的信息。这就是构造可视化数据的基本方法。（在渲染前构造、借助.post对象）关于位置，在ejs模板中放置构造代码当然可以，但是不优雅，Hexo中建议的插入方式是： 在专门放置自定义JavaScript处理逻辑的文件中（plugin.js）放入代码，并使用内建函数。 在ejs（或其他）模板的相关位置，使用&lt;%%&gt;方式调用上述内建函数 使用console.log在渲染html时（hexo generate时的黑框）输出至Console里，拿到输出数据，放入到可视化的页面中即可。 或者一气呵成，直接将可视化的代码写入ejs模板中，即第一次渲染结束时产生的html就已经完成可视化页面的生成。 由于处在尝试阶段，所以这里使用步骤3 的方法，这样各模块相对独立，对主题源代码入侵小。 可视化页面这里采用的是 D3.js 进行的可视化呈现，基本上是复用的 d3 的官方模板，但将文本信息一并和节点进行可视化展示。这段代码首先需要被抽取出来，这对于 d3 来说非常简单，只需注意引入的JavaScript库以及使用的json文本数据。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110&lt;svg width=\"1000\" height=\"1000\"&gt;&lt;/svg&gt; //d3绘制的内容全部放置在该画布上&lt;script src=\"https://d3js.org/d3.v4.min.js\"&gt;&lt;/script&gt; &lt;script&gt; var sss = 'JSON字符串'; //这就是整个代码所可视化的数据 var abc = parseInt($(\".card\").css(\"width\").replace(\"px\",\"\")); if(abc&gt;1080) abc=1050; else if(abc&gt;1040) abc=1020; else abc=abc-40; $(\"svg\").css(\"width\",abc); $(\"svg\").css(\"height\",abc); //此部分将画布大小跟随文章页宽度变化 var svg = d3.select(\"svg\"), width = abc, height = abc; var color = d3.scaleOrdinal(d3.schemeCategory20); var simulation = d3.forceSimulation() .force(\"link\", d3.forceLink().id(function(d) &#123; return d.id; &#125;)) .force(\"charge\", d3.forceManyBody().strength(-180).distanceMin(10).distanceMax(300).theta(1)) .force(\"center\", d3.forceCenter(width / 2 - 40, height / 2 - 30)); var graph = JSON.parse(sss); var link = svg.append(\"g\") .attr(\"class\", \"links\") .selectAll(\"line\") .data(graph.links) .enter().append(\"line\") .attr(\"stroke-width\", function(d) &#123; return Math.sqrt(d.value); &#125;); var node = svg.append(\"g\") .attr(\"class\", \"nodes\") .selectAll(\"g\") .data(graph.nodes) .enter().append(\"g\") var circles = node.append(\"circle\") .attr(\"r\", function(d) &#123; if(d.group&gt;=100) return d.group/100*(10.00/48.00)+1; //取整 else return d.group+1; &#125;) .attr(\"fill\", function(d) &#123; if(d.group&gt;=100) return \"#ff4081\"; else return \"#3f51b5\"; &#125;) .call(d3.drag() .on(\"start\", dragstarted) .on(\"drag\", dragged) .on(\"end\", dragended)); var lables = node.append(\"text\") .html(function(d) &#123; if(d.group&gt;=100) &#123; var p = d.group/100*(10.00/48.00)+10; return \"&lt;a style='font-size:\"+p+\"px;font-weight:600;color:red' href='/categories/\"+d.id.replace(\"_\",\"-\")+\"'&gt;\"+d.id+\"&lt;/a&gt;\"; &#125;else&#123; var q = d.group+10; return \"&lt;a style='font-size:\"+q+\"px;' href='/tags/\"+d.id+\"'&gt;\"+d.id+\"&lt;/a&gt;\"; &#125; &#125;) .attr('x', function(d) &#123; if(d.group&gt;=100) return d.group/100*(10.00/48.00)+5; //取整 else return d.group+3; &#125;) .attr('y',function(d) &#123; if(d.group&gt;=100) return d.group/100*(3.00/48.00)+5; //取整 else return 5; &#125;); node.append(\"title\") .text(function(d) &#123; return d.id; &#125;); simulation .nodes(graph.nodes) .on(\"tick\", ticked); simulation.force(\"link\") .links(graph.links); function ticked() &#123; link .attr(\"x1\", function(d) &#123; return d.source.x; &#125;) .attr(\"y1\", function(d) &#123; return d.source.y; &#125;) .attr(\"x2\", function(d) &#123; return d.target.x; &#125;) .attr(\"y2\", function(d) &#123; return d.target.y; &#125;); node .attr(\"transform\", function(d) &#123; return \"translate(\" + d.x + \",\" + d.y + \")\"; &#125;) &#125;function dragstarted(d) &#123; if (!d3.event.active) simulation.alphaTarget(0.3).restart(); d.fx = d.x; d.fy = d.y;&#125;function dragged(d) &#123; d.fx = d3.event.x; d.fy = d3.event.y;&#125;function dragended(d) &#123; if (!d3.event.active) simulation.alphaTarget(0); d.fx = null; d.fy = null;&#125;&lt;/script&gt; 构造数据格式需要匹配示例的输入格式，这样才能最大化的复用代码。上述内容的官方示例中使用的格式是： 123456789101112&#123; \"nodes\": [ &#123;\"id\": \"Myriel\", \"group\": 1&#125;, ... ... &#123;\"id\": \"Mme.Hucheloup\", \"group\": 8&#125; ], \"links\": [ &#123;\"source\": \"Napoleon\", \"target\": \"Myriel\", \"value\": 1&#125;, ... ... &#123;\"source\": \"Mme.Hucheloup\", \"target\": \"Enjolras\", \"value\": 1&#125; ]&#125; 即，需要在可视化页面被渲染出来之前就得到上述格式的数据，这便是要借助Hexo的辅助函数来完成，将构造数据的代码封装成一个函数，然后在适当的ejs模板中调用一下，即可在 hexo generate 之后，从Console中拿到构造好的数据。 在此，构造规则是：类别永远单向的指向标签，类别不互连，标签不互连，同时，还需要计算的是类别和标签出现的次数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566hexo.extend.helper.register('getPostData', () =&gt; &#123; var posts = hexo.locals.get('posts'); var tagsMap = new Map(); //counter // 利用posts对象获取类名和标签名 for(var i = 0; i&lt; posts.length; i++)&#123; var nameCS; posts.data[i].categories.forEach(function(k, v) &#123; nameCS = k.name; return; &#125;) for(var j = 0; j&lt; posts.data[i].tags.length; j++)&#123; var pname = posts.data[i].tags.data[j].name; var pval = tagsMap.get(pname); if(pval != null)&#123; // 将类名和标签名压制在一起 tagsMap.set(nameCS+\"&gt;\"+pname, parseInt(tagsMap.get(pname))+1); &#125;else&#123; // tagsMap.set(nameCS+\"&gt;\"+pname, 1); &#125; &#125; &#125; //由此开始，构造符合特定格式的JSON字符串 let obj= []; let setss = new Map(); for (let[k,v] of tagsMap) &#123; var st = k.split(\"&gt;\"); var str = &#123;&#125;; str.source = st[0]; str.target = st[1]; str.value = v; obj.push(str); if(setss.get(st[0]) != null)&#123; // 类节点 每次加100 setss.set(st[0], parseInt(setss.get(st[0]))+100); &#125;else&#123; // setss.set(st[0], 100); &#125; if(setss.get(st[1].trim()) != null)&#123; // 标签节点 每次加1 setss.set(st[1], parseInt(setss.get(st[1]))+1); setss.set(st[0], parseInt(setss.get(st[0]))+100); &#125;else&#123; // setss.set(st[1], 1); setss.set(st[0], parseInt(setss.get(st[0]))+100); &#125; &#125; let obk= []; for (let [k,v] of setss) &#123; var str = &#123;&#125;; str.id = k.trim(); str.group = v; //通过数量分类 obk.push(str); &#125; let d3str = &#123;&#125;; d3str.nodes = obk; d3str.links = obj; console.log(JSON.stringify(d3str).trim()); //按第三步说的，可以手动放置数据到可视化页面 return JSON.stringify(d3str).trim(); //或按第四步，将数据返回至ejs模板中，直接渲染出可视化页面 &#125;); 注意上述代码中的注释，这里利用了类节点和标签节点出现的次数，来分辨两种节点的种类，因为绘制时类节点和标签节点都是一视同仁的被绘制。如何分辨呢？在可视化页面中有以下代码： 12345var circles = node.append(\"circle\") .attr(\"r\", function(d) &#123; if(d.group&gt;=100) return d.group/100*(10.00/48.00)+1; //取整 else return d.group+1; &#125;) 按照不同的次数计算步长，得到的类节点的次数一定是100的倍数，而标签节点的次数一定小于100，这个值可以设的很大，从而让两者不可能出现交集。在判断时“如果次数大于100”，那么就是类节点，取整百的好处是，归一化方便。例如上述代码需要给定节点的大小，类节点的次数统计可能是100-4800（1-48次），而标签节点的次数却是1-10（1-10次），如是，两者应绘制的一样大。这就需要归一化，只需要缩放100倍再乘比例系数即可。 最终调用上文中hexo.extend.helper.register(‘getPostData’, () =&gt; {}）的“getPostData”即注册的函数名，在ejs（或其他）模板中直接调用即可。但由于我希望把这个可视化模块放在我的评论页或者关于页面，而这两个页面都不是渲染出来的，所以就只能采用先前第三步的做法，只构造出数据，再手动放入可视化页面。 12// 在 index.ejs 内添加：&lt;% var arr = getPostData(); %&gt; 所以，需要做的就是找一个渲染页面的ejs，调用下该函数即可，这里放在index.ejs里，注意由于分页可能该模板会构造很多次，所以就会重复输出很多遍JSON数据。 总结基本上还是抓住代码执行的输入输出做文章。从待改造代码的输入找格式，然后从原代码的框架中构造出该格式的数据（输出），就像适配一样，如此便可以利用Hexo可以获得的数据，借助D3.js等可视化库，把自己的博客（知识系统）做一个梳理和呈现，从而更好的帮助自己管理和维护，也给了自己二次挖掘自己知识的机会。","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"力导向布局","slug":"力导向布局","permalink":"https://www.cz5h.com/tags/%E5%8A%9B%E5%AF%BC%E5%90%91%E5%B8%83%E5%B1%80/"},{"name":"图数据","slug":"图数据","permalink":"https://www.cz5h.com/tags/%E5%9B%BE%E6%95%B0%E6%8D%AE/"},{"name":"D3","slug":"D3","permalink":"https://www.cz5h.com/tags/D3/"},{"name":"主题扩展","slug":"主题扩展","permalink":"https://www.cz5h.com/tags/%E4%B8%BB%E9%A2%98%E6%89%A9%E5%B1%95/"}]},{"title":"纵览全局的文档组织模式（上）","slug":"2020-1-6  纵览全局垂直打击的组织模式（上）","date":"2020-01-05T23:00:00.000Z","updated":"2020-03-07T04:07:37.215Z","comments":true,"path":"article/65a9.html","link":"","permalink":"https://www.cz5h.com/article/65a9.html","excerpt":"对知识系统（eg.博客）而言，良好的组织结构是极为重要的，尤其是当内容增多，关联复杂后显得尤为重要。传统的“分类(Categories)+标签(Tags)”的二级模式虽足以应付大部分用户的需求，但本质上其还是需要用户对已有分类和标签有良好的组织，这对很多用户来说是根本做不到，因为我们往往缺的就是这种“纵览全局”的能力。","text":"对知识系统（eg.博客）而言，良好的组织结构是极为重要的，尤其是当内容增多，关联复杂后显得尤为重要。传统的“分类(Categories)+标签(Tags)”的二级模式虽足以应付大部分用户的需求，但本质上其还是需要用户对已有分类和标签有良好的组织，这对很多用户来说是根本做不到，因为我们往往缺的就是这种“纵览全局”的能力。 分类往往越分越多，标签也是随意放置，久而久之，不仅已有的分类和标签杂乱无章，更为甚者是新增内容时根本不知从何下手，往往需要遍历过往的标签和分类，才能做出最终定夺。现在，通过图布局的方式，可以以一种近乎完美的方式对复杂的内容进行组织，详细效果请查看 该页面。 纵览全局对于知识系统（之后均以博客代指）而言，传统的模式只是简单的分支，或者称其为树形结构，在探索过程中，用户就如同“蚂蚁”一样，只得选择先从哪进入，然后再进入到哪里。对于单篇内容而言并无影响，但当需要感知全局时，往往这种模式就会出现问题。 分级/树形组织方式的不足 用户开始便直接希望查阅某些内容，且不确定分类时，无法定位（局部要求）可以通过搜索功能完成该需求。 新增分类和标签时，缺少对已有项的感知能力（全局要求）尤其对于标签，会更加的随意和杂乱，会出现重复、同义等等问题，在每次打标签时都要头疼一番。 对于所打的标记，没有评价方法，永远不知道分类和标签是否匹配（全局要求）对于已存在的标签或分类，这样打标签是否合理，由于标签的“松散”特性，不同分类中可以出现同一标签，这样在传统分级模式下，分类和标签的契合程度如何，系统的维护者无从知晓。 天然的解决方案：图布局分级/树形标记模式本身就是一个分类过程，自己的知识内容（博客文章）是对象，维护者将其放置在不同的类别下。标签(Tags)则更像是分类过程中的副产物，更贴近文章内容，但又言简意赅，通过分级的思考方式，分类和标签和文章的关系是： 分类-标签-文章（1:M:N） 对于上述关系，分别用A、B、C表示的话，则整个系统其实就是一个“Ai-Bi-Ci”的三元组集合。该集合的好坏（即质量）就是其在语义上的契合程度，例如： 123分类:军事 -&gt; 标签:爆炸 -&gt; 文章:伊拉克遭遇恐怖袭击分类:娱乐 -&gt; 标签:爆炸 -&gt; 文章:阿富汗遭遇恐怖袭击 当抽象为网络/图之后，军事类别和娱乐类别会通过“爆炸”这一标签相连，如是，明显的会发现“爆炸”位置不对。（虽然例子很蠢，但当语义区分模糊、标签数量繁多时，极易出现该情况）。下面直接拿已完成的布局来解释： 粉红色为分类、蓝色为标签、节点半径为被使用的次数 语义不符的连接点（异常的跨类标签），如果连接点对某一方语义不匹配，那么很可能该文章是特殊的，或者该标签不应该出现在该文章。（下图里可视化的文章在这儿，属于特殊文章，正常“生活分类”和“可视化”的语义并不匹配） 合格的连接点（跨分类的标签）：虽然标签出现在不同分类中是非常正常的，例如“总结”，可以出现在任何分类中。但类似“总结”这类标签往往数量很多，即多次的出现在不同的类别中，那我们就说这是一个合格的跨分类标签。 对于分类点，以本博客为例，由于是对已存在数据进行分析，所以如果某分类下属节点不足，那么高度怀疑该分类不合理，除非是需要日后扩充的分类。这一需求在图布局的视图下非常容易分辨出来，合格的类别应该有众多叶节点，当叶节点不足，则应考虑将其降级至标签。（例如下图中的“朴素贝叶斯”，可将其降级为标签，并归类到“研究方向”中） 值得注意的一点是： 这里使用的图布局使用力导向(Force-directed)布局算法，相关则相近，无关则疏远，又完美的给布局结果以语义上的解释，即： 当两个类别及其叶子节点距离很远时，其两者基本无关 当两个类簇距离很近时，其高度相关 反推设计上节中的分析看似很有道理，布局结果的使用也非常方便，那么如何从无到有将其构建出来？主要有以下几个方面： 天然的三元组集合：文章的特性（篇幅长）决定了其不能参与整个构建和评价过程，那么剩下的二元组是天然的“关系数据”，对于关系数据的可视化，图布局算法/模式首当其冲。 分析需要呈现的维度：对于任意节点（布局时类别和标签并无分别）来说，主要有以下维度信息： 自己（如果是类别）包含哪些标签； 自己是什么类型的节点（类别？标签？）； 自己被使用了几次； 对应的可视化要素：a. 图中节点的邻节点（点、线）b. 类别为粉色标签为蓝色（颜色）c. 次数与节点的半径成比例（圆面积） 还可以附着信息（扩展维度）的要素： 节点的形状（三角形、圆、方） 连线的颜色（红、蓝） 连线的线型（虚线、实线） 上述过程中，确定“图布局”模式是基础，剩下的无非是将信息绑定到可视化元素上，例如，已实现的布局将“类别/标签”用颜色区分，其实用形状等其他可视化元素区分也完全可以。 垂直打击到此为止，只是上层结构，类似数据库存储，搞了半天只是在搞索引，并没有触碰到数据，所以目前为止该网络并没有直通最底层（文章内容）的能力，这个问题恰好被Hexo的文件结构所解决，Hexo给每个标签和每个分类都渲染了单独的页面，关联的文章被放置在页面中，在此，直接通过节点的文本信息构造访问地址，将其绑定到文本上，即可点击后跳转到相关页面，虽然不是直接跳转文章，但也可以说具备相当的垂直打击能力了。 进阶版本：变的更强 简单粗暴的加入之前三元组被抛弃掉的文章信息，但由于加入后过于散乱，所以有必要将文章信息固定，以便于视觉呈现。如下图（d3.js实现的、用于可视化编程概念的可视化模型）： 上图就是简单的带固定节点的力导向布局，但其实现代码比较复杂，目前处在构造数据阶段。一般的可视化模型套用的步骤： 阅读原站代码 -&gt; 从原站抽离可视化部分 -&gt; 搞清调用数据的方法及格式 -&gt; 构造同样的数据 -&gt; 独立运行 -&gt; 放回自己的站点内 问题迎刃而解到此，对于分级/树形分类的三点不足，可以发现很轻松就可以解决。既有全局视角，又可以同时具备直达的能力，对于组织内容数量较高（超过50）的站点非常适合该模式的导航、或辅助探索。 扯犊子完毕，下一篇（分为上下两篇）将详细说明一下如何遵循上节中的套用步骤、借助Hexo的辅助函數（Helper）来一步步实现的该可视化功能的。 传送门，请用PC查看","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"力导向布局","slug":"力导向布局","permalink":"https://www.cz5h.com/tags/%E5%8A%9B%E5%AF%BC%E5%90%91%E5%B8%83%E5%B1%80/"},{"name":"图数据","slug":"图数据","permalink":"https://www.cz5h.com/tags/%E5%9B%BE%E6%95%B0%E6%8D%AE/"},{"name":"D3","slug":"D3","permalink":"https://www.cz5h.com/tags/D3/"},{"name":"主题扩展","slug":"主题扩展","permalink":"https://www.cz5h.com/tags/%E4%B8%BB%E9%A2%98%E6%89%A9%E5%B1%95/"}]},{"title":"Summary and Review of 2019, B","slug":"2019-12-31 Summary and Review of 2019, B","date":"2019-12-30T23:00:00.000Z","updated":"2020-02-29T18:43:55.546Z","comments":true,"path":"article/9c47.html","link":"","permalink":"https://www.cz5h.com/article/9c47.html","excerpt":"This article is a set of memories, belonging to the list of RMBATS. *August *From July, after I finished my master’s defense, I had been modifying my paper to try to publish it. It’s a long and boring time, which time I have to see the long papers and try to shorten it to a condensed version. So, in the head of this month, I do this paperwork every day with the hot summer weather. To against the summer, my friend and I started to sleep in the lab. Because the lab has an air-condition that we can let it open all night to keep low temperatures. It’s a funny period, my friend slept with the foldable bed he bought. I didn’t have that bed, so I put the chairs together and slept on them. I have to admit that it’s not a good place to sleep, we have to wake up early in case someone else finds us. Anyway, at least it’s cold enough, we can overcome any problems but the hot temperature at that time.","text":"This article is a set of memories, belonging to the list of RMBATS. *August *From July, after I finished my master’s defense, I had been modifying my paper to try to publish it. It’s a long and boring time, which time I have to see the long papers and try to shorten it to a condensed version. So, in the head of this month, I do this paperwork every day with the hot summer weather. To against the summer, my friend and I started to sleep in the lab. Because the lab has an air-condition that we can let it open all night to keep low temperatures. It’s a funny period, my friend slept with the foldable bed he bought. I didn’t have that bed, so I put the chairs together and slept on them. I have to admit that it’s not a good place to sleep, we have to wake up early in case someone else finds us. Anyway, at least it’s cold enough, we can overcome any problems but the hot temperature at that time. After the main tasks finished, the day I have to leave finally come to see me. It’s time to say goodbye to NEU, although I want to say that many times before that time. No feelings about sadness, but I felt that I hadn’t prepared for that. I knew that I have wasted too much time here, continue to stay here could help me nothing. So, on August 16, I asked my supervisor that I want to go, he didn’t try to keep me anymore. I left my university in that afternoon, and my two friends come to see me off. I arrived home at about 11:00 pm. It’s a remarkable day cause it’s my birthday and I farewelled to my school on this unique day. A few days later, I started to prepare the materials needed for going abroad. The first thing I need to do is to submit the contract to the scholarship council office in Beijing. So, father and I spent almost one week in Beijing and visited many attractions and scenic areas. But, unfortunately, we didn’t buy a ticket to enter the Forbidden City. Of course, we climbed the Great Wall. In those days, we lived in different hotels every night, and one day, we lived near. Most of the time we take the subway and ride the shared bikes. I need to say that is a wonderful way to travel around Beijing with this kind of bikes, not too expensive, but you can go anywhere you want. SeptemberThe time came to September, the longest summer vacation was coming. It could be more than three months, which means I need to stay at home for about 100 days. I supposed to finish many things, but actually, I wasted most of the time. The environment and atmosphere are not good for focusing. Sometimes, I watched TV all day and didn’t want to do anything. Generally speaking, it’s not a healthy period of time. At the beginning of this month, I spent a lot of time to install the new router and build the new home network. Then, I tried to let my NAS work in the local network and connect it with the smart TV. The process was complex, I need to test them, then improve them. Finally, every component worked well. And with the DDNS service of Synology, I could let the whole system access to the Internet. Now, this system is still working very well. I can check and fix all problems with the remote connection, even though I stay on the other side of the earth! Then, one interesting experience is that I joined an activity about assisting-poverty. People come from the local educational department went inside the countryside by themselves. With the stuff of rice, eating-oil or white flour, they can help these poor people try to get rid of poverty. This is a very meaningful and useful measure to help poverty, but the question is obvious. These things could help them through away the current hard time, but can’t have any benefits on their hard life in the long run. Anyway, it’s a good trend for poverty, maybe we could find a better way to help them. At the end of September, I received the email to get the visa of the Netherlands from the embassy in Beijing. So, I came to Beijing again, after that, I came to Wuhan to meet my bachelor supervisor as I promised before. What I didn’t expect was that my classmate invited me to play together. So after leaving Wuhan, I went to Qinhuangdao with him and spent two days there. These things all happened before the National Day, the last week of September, which left me a deep impression. I met and talked with my supervisor again. I can’t imagine the uncertain way of my future we discussed last year incredible came true at that time. Although we shared a simple lunch together, we talked about many things. I can feel his happiness from his face. After saying goodbye to HZAU, I had a very enjoyable two days with my other friend. Then, I came to Tianjin right away and met with another friend in Qinhuangdao, and spent two days to visit Beidaihe. OctoberOn the National Day, of course, I stayed at home and watched the celebration show on the TV. The main theme for this month is watching TV and eating with others. I spent some good days with much meaningful time shared with my family members. Eating lunch or dinner with them actually is a way to say goodbye. After all, I need to leave them for more than one year, which means we can’t see each other face to face anymore. As I have a big family, so I ate many times with them outside this month. Besides that, I also visited some local interesting place and took some beautiful photos. This is my first serious attempt to explore my hometown. I admit that I supposed to do that early, but only then did I discover the beauty of my hometown. NovemberSince October, time passed very fast, and every day I worried about the coming of the day I need to leave. When the time came to November, I prepared everything and wait the final day. On November 6, I spent the last night at home, I have forgotten the feelings but that was definitely a hard time for me. Although it likes a normal leaving from home, I knew this time I would leave for a long time. The fight was not feeling well cause it covered the whole sleeping time. After solving some troubles, I finally arrived in the Netherlands. I finally touched the ground far away from my home. Not have too many strange feelings, I adapt to the foreign environment very quickly. With the help of an elder friend from Eindhoven, I came to my hostel smoothly. I surprised that it just about one hour from Amsterdam to Utrecht by car. In the start days, I suffered from the time difference, I spent a long time to adjust the sleeping time. Now I have fully adapted the life here. The most important thing then was the meeting with my supervisor. Before that, we didn’t meet each other, so it was nervous when I first met him, but he is a nice person and a good guide man. Actually we talked a lot, and he told me a lot of related things I need to learn. Then the long time of self-study came, till now, I still in the first step. Alex, my supervisor, said my horizon of this area could extend bigger and bigger, but I need to start from the smallest circle. This is a big difference compared with the speed of study in China. This month, the first remarkable time was to visit my friend in Eindhoven. The friend and I all come from the same place in China so that she is almost like a relative for me. We had a warmful dinner time and took part in the GLOW 2019 lighting show together. One week later, I followed my colleagues to come to Groningen to meet with some other friends. They are all my supervisor’s Ph.D. candidates. Because Alex worked in Groningen before, so some of his students still lived there, not moved to Utrecht with him. We ate hot-pot and played a table game together. What a good time! I even first meet a friend come from Taiwan, she is a nice girl and study in Groningen for her master degree. Compared with Utrecht, I like Groningen more, because of the low housing rent and the low cost of living. DecemberAfter Black Friday, I spent one week to pick up my delivery. At the end of the first week of December, I received them all, then I started to build my own mini-ITX computer. Now, the machine works very well, and thanks to the small pc, I can get rid of the feeling of alone. After the half of this month, people slow down to prepare the Christmas day and the New year. Although we Chinese don’t care about these two festivals too much, we still happy to join the different activities for them. I joined the final borrel drinks party of my faculty. And also joined the Christmas party of the Chinese Utrecht Christian Church. Besides, we Chinese colleagues also spent some special party, like making dumplings together in the winter solstice. And these days, I also developed a good relationship with my hoster, a nice man comes from Pakistan. He invited me to his home and I had a wonderful daytime with him and his son. What’s more, I also made some Chinese friends here, Mr. Wei and Ms. Wang are a Chinese couple here. I came to their home and ate dinner together many times. It’s Ms. Wang who takes me to the church activity, and then we visit the Castle De Haar by bike. I took some photos of her using my camera, she liked them very much. The other Chinese is the owner of a Chinese restaurant, I went to their restaurant to eat dinner for many times. One of her friends is a researcher from Maastricht University, we talked a lot about life and study. Today is December 31, the last day of 2019, I will go to Eindhoven by train to attend the party. Tomorrow is no difference between today, but, it is a truly time point to have a new start. New beginning needs old ending. A good summary and review of the past are essential for a new and bright future. Try to do that, discover and explore yourself!","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"生活","slug":"生活","permalink":"https://www.cz5h.com/tags/%E7%94%9F%E6%B4%BB/"},{"name":"Summary","slug":"Summary","permalink":"https://www.cz5h.com/tags/Summary/"},{"name":"Writing","slug":"Writing","permalink":"https://www.cz5h.com/tags/Writing/"}]},{"title":"Summary and Review of 2019, A","slug":"2019-12-27 Summary and Review of 2019, A","date":"2019-12-26T23:00:00.000Z","updated":"2020-02-29T18:43:55.547Z","comments":true,"path":"article/9d07.html","link":"","permalink":"https://www.cz5h.com/article/9d07.html","excerpt":"This article is a set of memories, belonging to the list of RMBATS. Say goodbye to 2019, the year that changed my life and destiny. The first half of 2019From the second half of 2018, I’ve been working on the application of CSC. All things happened can be found in last year’s summary. This year is the harvest time. Here, I’ll put those important things together with some good pictures, which likes the mode of last year’s summary.","text":"This article is a set of memories, belonging to the list of RMBATS. Say goodbye to 2019, the year that changed my life and destiny. The first half of 2019From the second half of 2018, I’ve been working on the application of CSC. All things happened can be found in last year’s summary. This year is the harvest time. Here, I’ll put those important things together with some good pictures, which likes the mode of last year’s summary. The part of the beginning of this year has been recorded in the summary of 2018, including the first two months. From March, I came back to school after finishing the winter vacation. This semester is a special period for me because all my classmates and roommates left school. They had finished their whole study and master’s defence before the winter vacation. But for me, I had to start the way with myself, I need to write my master’s thesis and finish the relative experiments in three months. Besides, in March, I also need to prepare all documents and fill in the online CSC applying system. And to prepare a second choice for myself, I got in touch with a professor of SDU. The professor, Mr Yunhai, gave me several hard questions that I need to take a lot of time to read his papers to make an answer. After March, I started a long time with paperwork and coding. I went to the lab every day, which is ironic to me that I hardly ever went to the lab before this semester. At that time, I got a good experience with other students. The whole lab just had one 3-grade student so that everyone else showed their respect for me and I helped them to solve some troubles. Most time I was writing my master’s thesis. It’s hard to have a good structure and start, I took much time to discuss with my professor. Time escaped fast, after April, I started to worry about the result of my application. I worried about what I should do if the application was failed. This kind of feeling around me for the whole of May. On May 31, the final result came out, I still remember that afternoon. I refreshed the browser again and again and kept my eyes on QQ to catch every message. Someone started to post good news in the QQ group, I even can’t breathe at that time. Finally, the red pages jumped out and I almost cried when I see this good news. I can’t describe the happiness after I received the good news, I went to play the badminton with my friends. That day, I realized what is relaxation, and finally, the burden in my heart was put down. JuneTime to June, the feeling of happiness almost disappeared after I came back from home. Instead, I was nervous about the master’s defence, I thought the final day would be mid of June, so I worked very hard to finish the thesis. I used April to develop the System and used two months to write, but actually, the content mostly finished in June. Finally, I finished the thesis with more than 70 pages and 30,000 Chinese characters. It’s a huge project, the system built with the cluster of 16 PCs, and the whole project contains more than 10,000 code lines. I finish them all in three months, maybe I will write one post to introduce my thesis and my research works. Besides, from that moment, I already started to pack my stuff. Because I delayed my ending time, I had moved out my original room to a new one, which means I’ve packed them once. But when I try to do this again, there were still too many things to order into my box. Finally, I used 3 big boxes to contain them, and deliver them to home with a high price. JulyBefore July, every month has one Chinese festival so that we have at least one week of free time. With these small vacations, I could come back home every month in the first half of the year. But from this month, I didn’t come back until I left school. In this final period of time, I played a lot with my friend, mostly we play badminton in the gym, we shared a good time in the lab. In the first week of July, I finally faced the day of master’s defence. To my surprise, we 3 students finished our defence with 7 professors in a small room. Almost all the respected professor attended our defence, including the leader of our faculty. The situation was more serious than the first time of defence. After that, I started to edit thesis, check the content and finally make them into one book. After finishing them all, the boring task came to me. I need to edit the long thesis to 6 or 7 pages. I did this for about one month from July to the mid of August.","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"生活","slug":"生活","permalink":"https://www.cz5h.com/tags/%E7%94%9F%E6%B4%BB/"},{"name":"Summary","slug":"Summary","permalink":"https://www.cz5h.com/tags/Summary/"},{"name":"Writing","slug":"Writing","permalink":"https://www.cz5h.com/tags/Writing/"}]},{"title":"导出七牛云的数据到本地服务器","slug":"2019-12-26 导出七牛云内对象存储的数据","date":"2019-12-25T23:00:00.000Z","updated":"2020-08-29T11:52:04.573Z","comments":true,"path":"article/f2f2.html","link":"","permalink":"https://www.cz5h.com/article/f2f2.html","excerpt":"大概半年多以前，七牛云就失效了，一个是欠费再一个是没有绑定域名，听说是七牛云被举报了然后就必须要实名认证了，而且测试域名的时间也变得只有一个月之久，基本没什么作用了。如果绑定域名，需要该域名是备案的域名，这对于大部分自建博客的人来说基本就是死路一条了，备案的个人博主还是比较少的。","text":"大概半年多以前，七牛云就失效了，一个是欠费再一个是没有绑定域名，听说是七牛云被举报了然后就必须要实名认证了，而且测试域名的时间也变得只有一个月之久，基本没什么作用了。如果绑定域名，需要该域名是备案的域名，这对于大部分自建博客的人来说基本就是死路一条了，备案的个人博主还是比较少的。 如上，我自然是没法再访问对象存储里的数据了，不过比较欣慰的是七牛云并不会删除上传的数据，数据仍然在相关的 bucket 里测试域名被回收了，现在要做的就是将数据导出，然后使用其他方案，例如阿里的oss或者自己服务器上，无论如何，将数据从七牛云导出是最重要的，但恶心人的是：七牛云的控制台里没有导出的功能，执行导出要借助额外的工具，全部工具列表如下： https://developer.qiniu.com/kodo/tools/1302/qshell 这里我们使用 qshell，在命令行完成原数据的下载 准备工作 下载 qshellqshell 是一个命令行工具，在 Win 系统下就是个 exe 可执行文件，官方教程还要配置环境变量，其实直接输入该 exe 名称运行也可以。 12需要在该文件的目录内$ .&#x2F;qshell-windows-x64-v2.4.0.exe -v 配置密钥 在七牛云的web控制台中，个人中心里的密钥管理，可以找到自己的 AK 和 SK，帐号名称即用户名。 12$ .&#x2F;qshell-windows-x64-v2.4.0.exe CBVEWIVBOI82391091231 284VSHDUAVBU98-vgyhsd 1805984583@qq.com该语句执行完无输出 库的整体移动 利用新空间的测试域名 由于没域名，用 qshell get 下载总是失败，或许不是域名的原因，但总归是卡住了，找了半天找到了可行的新方案，即： 实名认证 -&gt; 新建Bucket -&gt; 转移空间内的数据 -&gt; 用新空间的测试域名下载 还是不用备案的域名，符合我的预期，认证也比较简单，上传身份证然后支付宝搞一下就行了，说是三个工作日审核其实完成认证操作后马上就能新建Bucket了，新建完后，记住新空间的名字，下面用 batchcopy 来完成移动。 注意：新建的空间要和原空间在同一个大区内。 获得文件名列表这里使用 listbucket 命令，将输出存到文件中，利用 awk 直接取出第一列即可（强烈推荐使用 git bash 运行，大家应该都有）。 123456$ .&#x2F;qshell-windows-x64-v2.4.0.exe listbucket whereareyou &gt; tocopy.txt只有一个参数即：空间名称其会列出该空间内的全部文件，带文件夹路径$ cat tocopy.txt | awk &#39;&#123;print $1&#125;&#39; &gt; only-name.txt去掉上面输出文件中的其他无关项，只留文件名 执行 batchcopy其仍属于 qshell 内的命令，是个批量拷贝命令，输入是个文件名构成的文件，可以用 –forece 强制移动。 12345678910$ .&#x2F;qshell-windows-x64-v2.4.0.exe batchcopy --force whereareyou zonelyn -i only-name.txt参数分别是：旧空间名，新空间名，移动文件的名称列表...Copy &#39;whereareyou:image&#x2F;acger&#x2F;app.png&#39; &#x3D;&gt; &#39;zonelyn:image&#x2F;acger&#x2F;app.png&#39; successCopy &#39;whereareyou:image&#x2F;acger&#x2F;group.png&#39; &#x3D;&gt; &#39;zonelyn:image&#x2F;acger&#x2F;group.png&#39; successCopy &#39;whereareyou:image&#x2F;gif&#x2F;107659232.gif&#39; &#x3D;&gt; &#39;zonelyn:image&#x2F;gif&#x2F;107659232.gif&#39; successCopy &#39;whereareyou:image&#x2F;gif&#x2F;120094544.gif&#39; &#x3D;&gt; &#39;zonelyn:image&#x2F;gif&#x2F;120094544.gif&#39; successCopy &#39;whereareyou:image&#x2F;gif&#x2F;148607128.gif&#39; &#x3D;&gt; &#39;zonelyn:image&#x2F;gif&#x2F;148607128.gif&#39; success...移动后会打印出相关信息 上述操作后，新建的 bucket 中已经具有原空间的全部数据了，并且具有测试域名。这样就比较好办了，有多种方法，这里用 qdownload 批量进行下载，由于需要拼凑下载路径，这里可以直接通过补充配置文件完成。 最后一步在 qshell-windows-x64-v2.4.0.exe 所在的目录新建下载配置文件 dcongf.txt： 123456&#123; &quot;dest_dir&quot;: &quot;F:\\七牛&quot;, &#x2F;&#x2F;本地目录 &quot;bucket&quot;: &quot;zonelyn&quot;, &#x2F;&#x2F;新空间名 &quot;cdn_domain&quot;: &quot;q35ajtip3.bkt.clouddn.com&quot; &#x2F;&#x2F;外链默认域名 &#x2F;&#x2F;注释要删掉，不能留&#125; 批量下载最后就可以执行批量下载的命令了，输入就是上述配置文件，这样会将整个空间内的全部文件下载到本地目录内： 123456789$ .&#x2F;qshell-windows-x64-v2.4.0.exe qdownload dconf.txtWriting download log to file C:\\Users\\zonglin\\.qshell\\qdownload\\d2f9566497ad74e39755de09c8837d9b\\d2f9566497ad74e39755de09c8837d9b.log...Downloading image&#x2F;gif&#x2F;165263167.gif [25&#x2F;1364, 1.8%] ...Downloading image&#x2F;gif&#x2F;166739693.gif [26&#x2F;1364, 1.9%] ...Downloading image&#x2F;gif&#x2F;170320182.gif [27&#x2F;1364, 2.0%] ...Downloading image&#x2F;gif&#x2F;174817916.gif [28&#x2F;1364, 2.1%] ...... 慢慢等待上述过程完成，自此全部数据就从七牛云上弄出来了。全部数据大小在10G内是肯定没问题的，下载可能很慢，等着吧。 开倒车 转存到自己的本地服务器 使用自用服务器，用 nginx 驱动，直接把数据放在自己的机器上，也不用什么cdn加速，因为并没有多少下载量，让 资源分享面 可用才是最重要的。 没有 CDN 加速，也没有任何优化，怎么说呢，一个是本身下载的就基本没有，再一个家里的上行速度还可以，如果在国内的话下载体验应该不差，在国外亲自体验下载速度大约在 700kb/s 左右，够用了。 七牛云的 BUG对于带有空格文件名的文件，正常上传到七牛云是可以下载和访问的，但是：使用 qshell 读出文件列表时，会按空格分割并只能获取第一部分，这明显是个 大BUG，怎么说呢，七牛云的服务本身也就那样，照理说 qshell 这些东西完全可以放在前端页面上，总之令人感觉七牛云是个半成品，反正也不用了，在咋咋地吧。","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"七牛云","slug":"七牛云","permalink":"https://www.cz5h.com/tags/%E4%B8%83%E7%89%9B%E4%BA%91/"},{"name":"脚本","slug":"脚本","permalink":"https://www.cz5h.com/tags/%E8%84%9A%E6%9C%AC/"},{"name":"Nginx","slug":"Nginx","permalink":"https://www.cz5h.com/tags/Nginx/"}]},{"title":"2019圣诞节 — 徒步看风景","slug":"2019-12-25 2019圣诞节 — 徒步看风景","date":"2019-12-24T23:00:00.000Z","updated":"2020-02-29T18:43:55.544Z","comments":true,"path":"article/b8d5.html","link":"","permalink":"https://www.cz5h.com/article/b8d5.html","excerpt":"今天是正经的圣诞节，应朋友的邀请和他一起去徒步散步，这位朋友来自巴基斯坦，他的荷兰妻子早早就回父母家欢度圣诞去了，显然他和他害羞的小儿子并不喜欢混在一群荷兰人中，所以他说今天我们可以一起去徒步，我自然是没什么事的，就干脆地答应了。当时他建议我要穿一双 nice shoes，先前并不晓得他的用意，结果后来走进步道的泥巴中，才明白确实得要一双“硬货”啊，往来的人群都穿着大皮靴，真是心疼我的运动孩。","text":"今天是正经的圣诞节，应朋友的邀请和他一起去徒步散步，这位朋友来自巴基斯坦，他的荷兰妻子早早就回父母家欢度圣诞去了，显然他和他害羞的小儿子并不喜欢混在一群荷兰人中，所以他说今天我们可以一起去徒步，我自然是没什么事的，就干脆地答应了。当时他建议我要穿一双 nice shoes，先前并不晓得他的用意，结果后来走进步道的泥巴中，才明白确实得要一双“硬货”啊，往来的人群都穿着大皮靴，真是心疼我的运动孩。 全程概览这次是第一次体验荷兰当地人是如何步行锻炼的，路线是这位朋友经常走的，他有点糖尿病所以每天都要通过走路来消耗掉摄入的多余糖分，自然路线的选择是非常棒的，沿途风景很赞，但美中不足的是，最近天天下雨所以有段路程道路泥泞，非常让人恼火。 全程包含：公路、林中车道、林中小道、火车铁轨旁道、沿河小道 整体基本没有海拔起伏，其实荷兰全境的海拔都比较平均。还有全程会发现几乎没有行人，因为是圣诞节！当地人都过节去了，可能非欧美的外国人才这么闲吧，路上还碰见过几次中国人（一看就看出来了）。 愉快的开始 令人恼火的泥巴 河畔的美景 小镇中的小路 路边的足球场在路上我们边走边聊，当然大部分时间是在“作比较”，同一件事在中国会怎样、在巴基斯坦会怎样、在荷兰会怎样，我挺喜欢谈论这些的，因为有些事情非常有趣，比如在荷兰当地： 如果步行时迎面走来陌生人，互相之间要问好，即使不认识 和荷兰人当面说话时不要看手机，他们会非常在意 没路灯的路并不是没人走的路，反而可能很多人走 基本看不见摄像头，女性单独跑步或散步很常见 不准酒驾，但喝一点点可以开车，基本没人查，靠自觉 苍穹下的农场 写在最后在结束这次长途跋涉之后，还在这位朋友家里吃了午饭，巴铁是真的热情，因为孩子妈没在家，所以他亲自下厨，鼓捣了一些黑暗料理，其实味道还不错了。 说起来这是我第二次拜访他家了，还和他的小儿子聊了半天游戏，果然游戏是无国界的，我想这会是年轻人之间最容易想到的共同话题了，LOL、PUBG、Steam等等，虽然他高中还没毕业，但看来他真的是很喜欢玩游戏啊，没事基本就在家玩 XBOX，他爹也不管他，认为只要他高中毕业，那他就“自由”了，这可能也是一种东西文化的差异吧。 总之，这次圣诞节是真的体验到了一些不同的东西，当然也非常感谢这位巴铁朋友，中巴友谊万岁！","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"荷兰","slug":"荷兰","permalink":"https://www.cz5h.com/tags/%E8%8D%B7%E5%85%B0/"},{"name":"节日","slug":"节日","permalink":"https://www.cz5h.com/tags/%E8%8A%82%E6%97%A5/"},{"name":"运动","slug":"运动","permalink":"https://www.cz5h.com/tags/%E8%BF%90%E5%8A%A8/"}]},{"title":"自己动手丰衣足食","slug":"2019-12-22 自己动手丰衣足食","date":"2019-12-21T23:00:00.000Z","updated":"2020-02-29T18:43:55.543Z","comments":true,"path":"article/e749.html","link":"","permalink":"https://www.cz5h.com/article/e749.html","excerpt":"古老的传统虽然冬至是中国古代流传至今的一个节气之一，但其可不是一个简单的节日，本文目的并不是探究其有趣之处，所以就不展开叙述。总之，在全国范围内，人们都比较看中这个节气，北方吃饺子，南方吃汤圆，或许各地习俗有别（北方还有冬至上坟的习俗），但演变至今日，已然已经成了一个家庭团聚，朋友会面的重要契机，使得人们在忙碌的工作之余，吃口饺子/汤圆，借着热气回味下久违的亲情和友情。","text":"古老的传统虽然冬至是中国古代流传至今的一个节气之一，但其可不是一个简单的节日，本文目的并不是探究其有趣之处，所以就不展开叙述。总之，在全国范围内，人们都比较看中这个节气，北方吃饺子，南方吃汤圆，或许各地习俗有别（北方还有冬至上坟的习俗），但演变至今日，已然已经成了一个家庭团聚，朋友会面的重要契机，使得人们在忙碌的工作之余，吃口饺子/汤圆，借着热气回味下久违的亲情和友情。 饺子来啦纵使远离家乡千万里，尤恋故土那人那滋味。 作为海外留学的我们，能做的不多，能做的又很多，自己动手丰衣足食。 今天，从格村远道而来的师兄师姐们，和我们乌特这本地四人小队来了个串联，七个人，挤在不小不大的房子里，叮叮当当做了一天的饭食。回来之后我甚至因为太累而睡了一觉，这一觉睡得安详，因为今天过得又充实又难忘。 聚餐必备-火锅或许这算是最容易想到的形式了吧，容易准备，当然也容易吃。 万物皆可下火锅，吃了坏肚子可不好说，要是为这停了筷，吃货的脸面往哪搁？！ 如是，中午我们就吃的火锅，准备了肉和菜，切一切就开涮了，当然，吃火锅前每个人都是做好了最坏打算的，拉肚子这种小事不足挂齿，至少我是这么认为的。席间师兄师姐们表达了要逛古堡的想法，因为他们想逃避下午的包饺子环节，当然我们怎么能让客人劳动呢，所以匆匆吃过火锅后，他们就真的去古堡玩了。在他们走后，我至少又从锅里捞出了五个虾仁，没有享受到的人真是遗憾！ 冬至必吃-饺子认真对待每个节日，留下该有的美好回忆，这就不是我们活着的动力所在吗，冬至到了，饺子自然就来报道了。 下午刚开始包的时候，由于面太软了，我主动请缨，开始了揉面的试炼。最后的体验是，虽然很累，但是看着面粉被一点点揉进面里，感觉还挺不错，下次还揉。 在九九八十一下（大概）的揉搓之后，面终于硬了，师姐擀皮也找到了感觉，我随即便加入了包饺子小组，嗯，后来发现，我这水平（上图左下部分的）还可以拿个优胜奖，简直没有想到。 在此不得不提及一下师兄强烈建议加到馅子里的好东西——马蹄。一开始我以为是马的蹄子呢，结果原来是藕一样的东西。三下五除二，剁碎了加入海带、萝卜和肉馅，完成了第一种饺子馅。第二种馅就厉害了，是碎鸡蛋加虾仁，起初我还担心那么大虾仁包的进去不，后来发现自己还是多虑了。 饺子包好，我们收工，但宇哥的工作才刚刚开始：滚烫的热油在锅里翻滚，将刚包好的水饺直接放入锅中，在这层热油的煎炸下，水饺底部很快便被炸至金黄，这时将少量热水放入其中，盖上锅盖，沸腾的热水和蒸汽会将饺子完全闷熟，待水蒸发殆尽，水煎包就制作完成了。以上便是我在旁边偷师学艺的结果，最终的水煎包，当然也是非常的好吃。嗯，是正宗的味道！ 晚上，我们等了好一会德哈尔城堡的来客，他俩好像走丢了一样很晚才回来，当然那时候我们已经解决完战斗了。师兄师姐们吃着这水饺，连连称赞美味，这或许是对我们最大的欣慰了。饭后，我们再师兄的建议下又玩了那个卡片游戏，这是自从上次在格村玩过之后的第二次完了，惊喜的是，我居然毫无悬念的大比分赢了。 离别在今日当你觉得快乐、幸福的时候，往往也是感觉时间流逝最快的时候，匆匆告别后，这段时光便只能以记忆再见了。看着收拾一新的餐桌，干干净净的厨房，这一尘不染的样子仿佛我们从未来过，或这房子里的其他租客永远不会知道，我们在这里度过了多么美好的一天。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"荷兰","slug":"荷兰","permalink":"https://www.cz5h.com/tags/%E8%8D%B7%E5%85%B0/"},{"name":"节日","slug":"节日","permalink":"https://www.cz5h.com/tags/%E8%8A%82%E6%97%A5/"},{"name":"美食","slug":"美食","permalink":"https://www.cz5h.com/tags/%E7%BE%8E%E9%A3%9F/"}]},{"title":"Hexo主题(EJS模板)自定义页面扩展","slug":"2019-12-17 Hexo主题(EJS模板)自定义页面扩展","date":"2019-12-16T23:00:00.000Z","updated":"2020-02-29T18:43:55.541Z","comments":true,"path":"article/b5cf.html","link":"","permalink":"https://www.cz5h.com/article/b5cf.html","excerpt":"自从初次接触 Hexo 到现在已经有两年多的时间了，时间过得飞快啊，关于 Hexo 的优点不再赘述，关于个人站点的优点，有必要在强调一下，那就是极高的自由度，这也是这篇文章的基础。现在有时间刚好总结一下我对于 Hexo 做的一些自定义扩展，虽然之前可能在别的文章中或多或少的涉及了，但并没有统一整理过。","text":"自从初次接触 Hexo 到现在已经有两年多的时间了，时间过得飞快啊，关于 Hexo 的优点不再赘述，关于个人站点的优点，有必要在强调一下，那就是极高的自由度，这也是这篇文章的基础。现在有时间刚好总结一下我对于 Hexo 做的一些自定义扩展，虽然之前可能在别的文章中或多或少的涉及了，但并没有统一整理过。 本人主题：Indigo，以下内容均基于此主题所写。 首先需要明白的是，Hexo 的博客内容（静态内容）均由 generate 生成，其核心是一个 node 应用，提供了一系列帮助函数，或者说调用接口；而各种主题，只不过是在其规定的框架内，以一种特定的模板（ejs/swg/pug），调用特定的 Hexo 帮助函数来完成的。在构造时，这些模板文件每次都会重新生成对应文件，例如文章页面，就是对应的模板文件将编译后的 markdown 格式的文本填入 HTML 页面，同时也会插入进去其他东西（比如题目，尾注等等）。以EJS+LESS为例： EJS中包括全部的 html标签 和 JavaScript 脚本 Less是CSS的一种使用方式，这里可以理解为样式文件，但其样式参数可以用变量来表示，这样在开发主题的过程中就可以简化和统一整个样式所涉及的颜色高度等CSS属性。 原页面修改由上面可知，对博客进行的任何修改，这里特指简单的、在已有界面上的修改，均需要找到渲染/生成该 HTML 页面（浏览器中我们可见的部分）的模板文件，在模板文件中进行我们想要的修改。如果涉及主题的CSS样式，则一般需要找到对应的 less 文件，如果里面用的变量代替，则还需要到存储变量值的文件里去修改变量的值，这样才完成样式的修改，但偷懒的方法是，直接在对应生成页面的模板文件中添加style代码段，然后用 !important 将你修改后的样式强制覆盖原先的主题样式。 总之，或许你的主题文件中包含很多的模板文件，但实际他们是有机的整体，都会在某个模板文件中被引用，从而组合成一个完整的整体，修改时要耐心的去找到最细粒度（对应html语句）的那部分。 新增页面主要有两种方式，一种是添加一个 md 文件，一种是直接放一个 html 文件，前者在渲染时会生成相应的 html 内容，两者本质上没有什么区别，取决于你添加的新页面的内容，比如一般的文本则使用 md 就可以了，但如果是复杂的 js插件，那还是直接添加 html 较好，方便修改。 这两种方式均需要将 md 文件或者 html 文件放入到 Hexo 根目录的 source 文件夹中，Hexo 的机制是 source 文件夹中的全部文件都会被原封不动的生成到 public 文件夹内（只有 md 文件会被“翻译”成 html 格式）。 基于 Hexo 参数的修改这一部分可以在某些原有页面上添加，也可以是在新增页面上添加。主要是通过借助 Hexo 的程序接口，获取像 文章数量、分类数量、各种标签下分别有多少文章等等数据，通过这些数据，可以完成一些功能，比如： 主要基于的对象：分类（Category）、标签（Tags）、文章（Article） 可以扩展的操作：过滤、匹配、重构 其实扩展内容可以由阅读原先的主题文件的代码来理解，因为包括“归档”、“分类”等页面的显示，均用的 Hexo 的帮助函数，具体的函数接口定义与说明可以参照 Hexo的API说明。 案例一：添加关联文章 扩展思路：这里使用 Tags 进行比对，相似越多的文章就越相关，在最后一步，Category 相同会优先入选，排序后只取最相关的 TOP3，不足三个则有多少显示多少； 扩展结构： 1. 找合适的文件插入实现上述功能的新函数，一般在主题的 plugin.js，之后以注册函数的形式完成代码实现 123456789101112131415161718192021222324252627282930313233343536hexo.extend.helper.register('getTagsList', (archive, size) =&gt; &#123; var posts = hexo.locals.get('posts'); var articleArr = []; var small = []; size = size || 10; var ishere = []; for(var j = 0; j&lt; archive.tags.length; j++)&#123; var contnu = true; var tname = archive.tags.data[j].name; for(var p = 0; p&lt; posts.length; p++)&#123; var tid = posts.data[p]._id; if(small.length == size || !contnu ) break; for(var k = 0; k&lt; posts.data[p].tags.length; k++)&#123; if( tname === posts.data[p].tags.data[k].name &amp;&amp; ishere.indexOf(tid) == -1 &amp;&amp; archive._id != tid)&#123; if(posts.data[p].category === archive.category &amp;&amp; archive._id != tid)&#123; small.push(posts.data[p]); contnu = false; &#125; articleArr.push(posts.data[p]); ishere.push(tid); &#125; &#125; &#125; &#125; if(small.length &gt;= 1)&#123; return small; //正常情况下返回文章数组 &#125;else&#123; var ln = articleArr.length; if ( ln &lt; size) return articleArr.slice(0, ln-1); else return articleArr.slice(ln-1-size, ln-1); &#125;&#125;); 2. 新建子模板文件（为了模块分离） 注意：这里的getTagsList(page, 3)就是对上述代码的调用，page即整个文章对象，可以从中获取到该文章的所属分类和标签信息，3则时控制显示相关文章个数的参数。可以发现每篇文章都需要这样查找一遍，所以该代码最后在第三步中放到了文章页的ejs模板中，以确保在渲染每篇文章时将相关文章找出并放进页面中。 1234567891011121314151617181920&lt;section&gt; &lt;p&gt;&lt;span&gt;「 相关文章 」&lt;/span&gt;&lt;/p&gt; &lt;ul&gt; &lt;% //页面全部内容，只有TOP3 var arr = getTagsList(page, 3); //每次生成文章都会执行 for( var i in arr) &#123; var article = arr[i]; %&gt; &lt;li&gt; &lt;a href=\"&lt;%- url_for(article.path) %&gt;\"&gt;&lt;%- article.title%&gt;&lt;/a&gt;&lt;/b&gt; &lt;!--显示文章的摘要--&gt; &lt;p&gt;&lt;%- truncate(strip_html(article.excerpt || article.content ), &#123;length: 100&#125;) %&gt;&lt;/p&gt; &lt;/li&gt; &lt;% &#125; %&gt; &lt;/ul&gt;&lt;/section&gt; 3. 将上述新模板内容插入到原主题的对应位置内 123456789101112131415161718192021222324252627282930313233343536373839&lt;!--这部分是整个文章页的汇总模板--&gt;&lt;%- partial(&#39;post&#x2F;toc&#39;, &#123; post: post&#125;) %&gt;&lt;article id&#x3D;&quot;&lt;%&#x3D; post.layout %&gt;-&lt;%&#x3D; post.slug %&gt;&quot; class&#x3D;&quot;post-article article-type-&lt;%&#x3D; post.layout %&gt; fade&quot; itemprop&#x3D;&quot;blogPost&quot;&gt; &lt;!-- post-card 即文章的主体部分，包括题目、分类、字数、及最下方的标签和分享图标等 --&gt; &lt;div class&#x3D;&quot;post-card&quot;&gt; &lt;h1 class&#x3D;&quot;post-card-title&quot;&gt;&lt;%- post.title %&gt;&lt;&#x2F;h1&gt; &lt;div class&#x3D;&quot;post-meta&quot;&gt; &lt;%- partial(&#39;post&#x2F;date&#39;, &#123;date_format: config.date_format&#125;) %&gt; &lt;%- partial(&#39;post&#x2F;category&#39;) %&gt; &lt;%- partial(&#39;post&#x2F;wordcount&#39;) %&gt; &lt;%- partial(&#39;plugins&#x2F;page-visit&#39;) %&gt; &lt;&#x2F;div&gt; &lt;div class&#x3D;&quot;post-content&quot; id&#x3D;&quot;post-content&quot; itemprop&#x3D;&quot;postContent&quot;&gt; &lt;%- post.content %&gt; &lt;&#x2F;div&gt; &lt;%- partial(&#39;post&#x2F;copyright&#39;) %&gt; &lt;%- partial(&#39;post&#x2F;reward-btn&#39;) %&gt; &lt;div class&#x3D;&quot;post-footer&quot;&gt; &lt;%- partial(&#39;post&#x2F;tag&#39;) %&gt; &lt;%- partial(&#39;post&#x2F;share-fab&#39;) %&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;!-- 下面的模块，第一个是左右导航栏，第二个是咱们的相关文章模块，第三个是评论 最后，还包括了打赏图标（reward）在文章最底部 --&gt; &lt;%- partial(&#39;post&#x2F;nav&#39;) %&gt; &lt;%- partial(&#39;post&#x2F;postlist&#39;)%&gt; &lt;%- partial(&#39;post&#x2F;comment&#39;) %&gt;&lt;&#x2F;article&gt;&lt;%- partial(&#39;post&#x2F;reward&#39;) %&gt; 案例二：Category 或 Tags 的数值展示实现思路：统计分类、标签的重复个数，并绑定其所载文章的创建日期，这样整体上就有变量值、时间值两个维度，可以借助 D3.js 等做一些复杂的分析展示模块。 假象模块： 单一维度：分类及标签的名称及数量（柱状图） 两个维度：文章发布的频率：日历图（类似github贡献图） 复杂分析：以文章类别为一组，标签为一组，出现在同一文章中则算作有关联，分析全部文章，将分类和标签的关系用 Graph 进行可视化，从而更好的指导分类和标签的匹配关系。 实现部分：与案例一类似，第二三步骤很简单，关键是上述假象模块的实现，主要包括 通过 Hexo API 得到相关数据 和 通过可视化库进行绘制。 继续更新中19-12-20 新问题：引入资源文件 例如想在 Hexo 博客中的某个页面，做成一个资源分享页面（自己用或者给别人用），这样就会涉及文件的下载。其实有以下几种方式实现： 直接将该文件放在博客文件中一起发布，这对于单个小文件来说并无大碍，比如自己的头像完全可以这样放，但（以图片举例）图片过大，或者数量过多，就一定不能将其和其他文件放在一起，原因是： Hexo 每次发布都会重新生成博客的全部静态页面，如果资源也放在其中，那么也会每次都刷新一遍，如果是部署在云端或者用Pages服务进行托管的，这样每次的上传量将非常庞大，上传时间会非常长，且浪费时间的恰恰是基本不会修改的资源文件。 将资源文件分离，放在其他地方（云空间或Github的其他仓库）。例如使用七牛云进行存储，这样的好处是： 访问速度有保证，例如七牛云就有融合CDN加速 对资源的可控性高，可以知道请求的数量等指标，但如果自己去计算这些数据，则需要复杂的代码实现。 和 Hexo 分离，加快每次的上传更新速度 其实这个问题适用于博客中所涉及的全部资源文件，包括头像、文章中插入的照片等等。七牛云是个不错的对象存储平台（包括文档、图片、媒体文件等等），如果只是图片的话，推荐使用专职的图床进行存储（例如微博图床）。","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.cz5h.com/tags/Hexo/"},{"name":"Indigo","slug":"Indigo","permalink":"https://www.cz5h.com/tags/Indigo/"},{"name":"主题扩展","slug":"主题扩展","permalink":"https://www.cz5h.com/tags/%E4%B8%BB%E9%A2%98%E6%89%A9%E5%B1%95/"},{"name":"EJS模板","slug":"EJS模板","permalink":"https://www.cz5h.com/tags/EJS%E6%A8%A1%E6%9D%BF/"}]},{"title":"记一次意义深刻的旅行","slug":"2019-12-13 记一次意义深刻的旅行","date":"2019-12-12T23:00:00.000Z","updated":"2020-02-29T18:43:55.538Z","comments":true,"path":"article/9cf5.html","link":"","permalink":"https://www.cz5h.com/article/9cf5.html","excerpt":"此内容系回忆内容，归属于RMBATS列表。 事件主题是2018年12月 8 号 到 13 号的一次短暂的出国旅行。这是我第一次出国，第一次长途跋涉这么远，第一次… 体验了很多的第一次，自然印象深刻，现在趁着还记得拎出来晒一晒，免得记忆发了酶。 这次去的是越南的首都河内，主要目的是去考雅思的，从下决心到出发时间也就一个月，非常紧张，好在越南可以落地签，所以只要有护照就简单的多了。虽然着了一条龙的服务，但那人没给我弄好，导致我原本可以提前落地签办完的结果没办完，让我自己体验了一把怎么办落地签的流程，怎么说呢，由于是凌晨到达的，而且河内时间比北京时间慢一小时，所以最后到达酒店的时间也就差不多还是凌晨。","text":"此内容系回忆内容，归属于RMBATS列表。 事件主题是2018年12月 8 号 到 13 号的一次短暂的出国旅行。这是我第一次出国，第一次长途跋涉这么远，第一次… 体验了很多的第一次，自然印象深刻，现在趁着还记得拎出来晒一晒，免得记忆发了酶。 这次去的是越南的首都河内，主要目的是去考雅思的，从下决心到出发时间也就一个月，非常紧张，好在越南可以落地签，所以只要有护照就简单的多了。虽然着了一条龙的服务，但那人没给我弄好，导致我原本可以提前落地签办完的结果没办完，让我自己体验了一把怎么办落地签的流程，怎么说呢，由于是凌晨到达的，而且河内时间比北京时间慢一小时，所以最后到达酒店的时间也就差不多还是凌晨。 考试呢，分在了几天内考，我到的第二天就有场考试，最后看结果嘛自然是晕晕乎乎考的不好。此次同考的伙伴呢，有两位女生，应该都是来自暨南大学的已毕业学姐（去年我还是应届），打算申请香港的博士但卡英语了，所以来这边考。开始我们只是微信认识的，后面也不是一起去的，但总归是住在一个酒店里，一起吃吃饭什么的，她们都是学霸类型的，到后面我就大部分时间都跑出去逛街溜达拍照去了。 前面也说了，我们找了当地的代理，但为了节省开支，我们只要求住一晚代理安排的酒店（太贵了），然后剩下的几天我们自己找地方住。也就是说基本上正经的旅程是从第二个住处开始的，从第二天起，我们在南门市场附近找到了一个小酒店（Angel Palace Hotel）住着，位置可以说是非常中心了，后来我逛了才知道，这个位置简直就是来河内游玩的最佳住处之一啊，包括下面这么多的打卡地，都能在半小时或者一个小时内步行走到。而且，晚上的东京义塾广场，简直是外国游客的天堂，整个街区基本充斥着外国人，一直到一两点都还非常热闹。 附近的景点：主要有环剑湖区域、巴亭广场区域、圣若瑟主教堂、河内火车站、统一公园、国家博物馆、军事博物馆 以上是本次到过的地方，拍了很多照片，下面就挑几张简单回忆一下。 启程这次是从学校走的，从沈阳先坐一晚上的卧铺到北京，然后其二天下午在首都国际机场起飞，先去往 澳门 中转，是中国航空的机票但实际承运是澳门航空的飞机，由于买的是往返票，所以去的时候是 北京-澳门-河内，回来还是这样折腾一遍，不过令人高兴的是，返程的时候我被安排做了头等舱，第一次离登机口这么近，坐两个人一排的大椅子，后来分析，只可能是往返票的优惠了，可能知道我是第一次做中国国际航空，而且空间有富余。 下午从北京起飞，晚上到达澳门，在澳门逗留两个小时后，八点左右起飞飞往越南，到越南好像也九十点左右（慢一小时），整体这个航线还是可以的，并不影响休息，而且比直达要便宜多了。不过行李是需要下飞机然后在上飞机的，不过时间是完全充足的。 抵达真是基本去之前什么也没准备，越南盾是在机场的取款机直接取的，越南好些个取款机都支持银联的卡，不需要换汇就可以取出越南盾来，怎么说呢取了几百块钱，瞬间变百万富翁了。下面是两百多万越南盾，实际才六七百块人民币，这些钱省着花的话能花好几天了。 怎么说呢，在河内的街头，基本上沿街房都是小商铺，这跟中国的小城镇没什么区别，只是路上的外国人是真的太多了，混在一起多了一种奇妙的感觉。虽然整体很接地气，但却又这么的国际化，这就是河内街头给人的体验。 还剑湖方向（Hồ Hoàn Kiếm）这片区域是一片商业区，说是商业区，其实全部是三四层的片楼组成的街道，西起南门市场，东到东京义塾广场，这一片充斥着各种卖衣服的、卖纪念品的、卖水果的、卖画的等等，老外们真的是一堆一堆的，很有视觉冲击，一个这么东方的地方居然遍地是西方人。 下图是夜晚的环剑湖，湖中间是玉山祠。红色的桥叫做栖旭桥，晚上非常漂亮。 环剑湖再往东方向，靠近红河的位置，就是越南国家博物馆和军事博物馆，下面是国家博物馆，里面基本介绍古代越南的历史，有趣的是其和我国的历史有很多交集，有一部分（例如元朝）内容是被美化了，元朝的骑兵止于越南边境或许只是因为气候原因，总之必然不是越南军队顽强抵抗住元军的攻击，仔细想一下是根本不现实的。 下面是越南的国家军事博物馆，军事博物馆和国家博物馆是合在一起卖票的，而且它俩就隔着一条街，军事博物馆基本是介绍近代越南建国的历史，这中间的越南共产党和中共的联系就更多了，有很多内容基本都是我们熟悉的风格。你甚至可以看到下图中那面党旗上的汉字（越南共产党）。 当然，十分微妙的是，关于上世纪中越之间的那场战争，并未出现在此，官方定调的越南近代的主轴仍然是在胡志明的领导下的反美抗争。对于其骚扰中国、亲美抗中等阶段，看不见任何记录。 巴亭广场方向（Quảng Trường Ba Đình）这片区域我觉得集中了大部分河内的行政单位，由于河内是首都，所以包括越南的国会等首都机关也在这一片区域，这附近还有有意思的越战纪念馆，里面有很多美军的飞机残骸，应该是宣扬越战时抗美成果的，时不时还可以看见河内的军人来参观。 关于巴亭广场，其实和天安门广场的结构和作用是类似的，只不过胡志明的陵墓成了绝对的主题，是整个广场的核心，而且广场大部分是草坪，不是石板，其对面就是越南的国会。 圣若瑟主教座堂附近这部分其实在我们下榻的酒店和环剑湖之间偏下的位置，虽然也可以算商业区，但不同的是这里当地人更多一些，在这里你能发现更多的越南人生活的真实情况。 下面是当时东南亚足球锦标赛、越南国家队踢比赛的那晚，年轻球迷们在当街宣传越南队和发放头绳，好像是和菲律宾踢，虽然是区域性的足球赛，但那天晚上真的是整个街区都沸腾了，晚上更是到一两点还是喧闹的气氛。 甚至可以看见在街头下棋的越南人，没错，这就是中国象棋，在当地还是很受欢迎的。 而圣若瑟主教堂，作为非常有名的打卡地点，基本上百分百能在门口遇见来自中国的旅行团。 統一公園方向（Công Viên Thống Nhất）这部分在南边方向，非常靠近雅思的考点，人口密度显然没有商业区那么密集，最好玩的就属统一公园了，这个公园本身是个长方形的湖加周围一圈的绿植组成的，转一圈估计要一个点，是非常大的。不过在体验到了河内街头的摩托车大军的喧嚣之后，就会知道这里的安静是多么难能可贵了、 摩托车大军。如果说荷兰是自行车王国，那么我真的想说越南是摩托车王国了，一到下班的时间点，嘟嘟嘟的摩托车声音真的是要把耳朵震聋了，而且在河内，Grab打车软件可以打到摩的，且可以看到绿色衣服的摩的数量是非常多的。 另外很特别的是，越南人骑摩托车非常有安全意识，一个质量好的头盔是必备的。基本人人都自觉地佩戴安全头盔。 返回北京往回走的时候呢，着实被打的滴滴（Grab）坑了一下，在下图的桥上，司机开着车轮胎竟然被爆了，无奈只好下来徒手换轮胎，我这赶着去坐飞机呢，那里还在换轮胎，无语了，这座桥应该算河内数一数二的大桥了，下面就是红河，总之耽误了我大概快一个小时。到机场的时候差点没赶上飞机，还是我在拿票的时候求助了国航的工作人员，他们带着我走了快速通道才赶上的，还有幸做了一次机场里的摆渡车。总之要吐槽一下河内机场的安检效率着实太低了。 回到北京后的那天早上。回程的时间比较蛋疼，时间是在凌晨，十二点从河内飞往澳门，然后等到早上五点飞北京，基本上那天是没怎么睡觉，还要等到晚上才能做卧铺回学校。就在北京溜达了一下，还去了CBD万达看了《海王》，简直不要太开心。 后记现在时间是 2019 年的 12 月了，距离这次美妙的旅途已经整整一年了，但一些事情仿佛还历历在目，真的像发生在昨天一样。那时的我偶尔也会幻想有一天自己的努力能换来回报，自己的尝试会有个好结果。那时候，光是这样想一下都觉得非常梦幻，感觉那是遥不可及的梦。不过人的心态是会随境遇不同而变化的，18年暑假开始学雅思的时候，自己连对过雅思都没有信心，第一次考试的结果也确实印证了我的担心，然后就是快两个月的自习室学英语，在临近毕业之际，众人皆找工作之时，我却在教室里看着枯燥的英语，做着不切实际的梦，现在想想，真的很佩服自己下决心的勇气。 从越南回来，到次年的一月，我才拿到了这次的成绩，因为是连考两次，所以分两天出成绩，第一次的分数出来时一看两个5.5，心里一下子就凉了半截。至于第二次的分数，我至今还记得查分的场景，那天是我们师门最后的一次聚餐，师弟们都参加的那种，我在校门口等着打车，结果收到了出分的消息，结果一查，只有口语5.5，其他都满足了，当时心情肯定是失落的，那天的聚会基本上就没这么说话，回到宿舍就给代理发消息，复议我的口语，当时心情是忐忑的，之前同行的那两位都拿到了自己想要的分数，而我还要等，等待对我来说已经是家常便饭了，终于等到了今年一月十五号，该给我发邮件说复议成功，结果是我所期望的，终于在春节回家前长舒了一口气，我记得那几天是真的高兴，恨不得第二天就回家等着过年。 那时我的心态稍稍好了一些，感觉着自己的短板被治好了，自己可以以一个各方面都合格的申请者申请CSC了，同事也感觉到那个梦越来越多真实了。这种感觉是一种质的变化，从胆战心惊的条件基本满足，到后来的条件完全满足，虽然对申请结果可能没什么影响，但对自己的心态影响是巨大的：从我有了英语成绩，感觉填系统填的也理直气壮，各种材料也准备的很顺利，提交完申请的那一刻，没任何感觉，只觉得就这样了，应该可以了吧。事实上当时也没有给自己找所谓的“后路”（尝试找过但不了了之），只想着成了就成了，至于不成的事，基本没想过。现在想想，其实也没什么感觉。不喜欢做预案、不喜欢PlanB可能就是我的性格了。 现在，先前的终极目标已经实现，我也体会到了理想变为现实的美好。从去年八月到今年六月，十个月的时间，我觉得非常好的诠释了什么是“延迟满足”，也让我更坚信了”有付出就有回报”这句话的正确性（但有多少付出并不一定就有多少回报）。 总之，这次旅途，非常非常的难忘且意义重大。一个是增长了见识，有了初步的国外生存经验；另一个是目标达成，拿到了成绩；最后就是增强了自己的信心，让自己在接下去的日子里坚定地熬下去，从而成就了今天的自己。每次想到考雅思这段记忆，我就想到“盘活”这个词，这就像一步棋，一步盘活死局的棋，下完之后，我不知道能不能赢的漂亮，但我知道一定不会输的难看。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"旅行","slug":"旅行","permalink":"https://www.cz5h.com/tags/%E6%97%85%E8%A1%8C/"},{"name":"回忆","slug":"回忆","permalink":"https://www.cz5h.com/tags/%E5%9B%9E%E5%BF%86/"},{"name":"雅思","slug":"雅思","permalink":"https://www.cz5h.com/tags/%E9%9B%85%E6%80%9D/"}]},{"title":"新机常用软件及环境配置清单","slug":"2019-12-8 新机常用软件及环境配置清单","date":"2019-12-07T23:00:00.000Z","updated":"2020-02-29T18:43:55.548Z","comments":true,"path":"article/2239.html","link":"","permalink":"https://www.cz5h.com/article/2239.html","excerpt":"写在前面的评价有些应用的选择需要单独说一下。","text":"写在前面的评价有些应用的选择需要单独说一下。 网易云音乐，海外有版权限制，真的是自掘坟墓，本身自己的资源就快被QQ音乐抢光了，还有这限制，9102年都过完了还有这些限制，真该学学QQ音乐啊，不然再多云村钉子户，也分分钟就弃坑了。 Chrome，我现在已经彻底是Google用户了，从Map到YouTube到GPlay，我没有理由不选择Chrome作为我的主力浏览器，Firefox已经多年不用了，不晓得体验如何（只在Linux上用用） 360安全浏览器，只是想用这个来当IE的替代品，极个别时候还必须得用IE，木的办法。 Editplus，这个我要强调，这是我见过的能够打开文本文件最大的编辑器了，比如50M、100M的文本，用记事本和NPP等会直接卡死的。 AIDA64，强大的硬件监控、查看、测试软件，其实前面说的全都能干，但我一般查看信息，里面的传感器部分很好，可以捕获全部的硬件传感器信息。 HWIDGEN脚本，Win10专业版现在找激活秘钥太难了，基本不可用，而KMS类似的激活又有时效且不稳定。基于此网上有永久激活的方法，但亲测并不适用每个人，而这个脚本可以一键傻瓜式激活（虽然也是命令行运行），效果和使用秘钥激活一样。 Anydesk，这个要说，非常轻便，真的是随下随用，安不安装都可以用，显示速度也不错，重要的是免配置，这个非常棒，就通过一个数字码连接，而且配置非常容易，适用全部的操作系统，手机、Linux、Windows全部可以互通。RealVNC可以实现相同的功效，但只是VNC分为Server和Viewer，而且都需要安装，方便性稍差一些。 PUBG_Lite，我第一次用我的Win10平板玩就惊到了，平板的配置 core m 真的很差了，但我玩PUBG_Lite依旧可以畅玩，操作和视觉上和正常版差一些，但差归差该有的一点不少，只是画质稍差，当然可以调高，而且操作非常流畅，Lite真的降低了PC吃鸡的要求，看好这个版本。 日常使用 No. 软件名称 说明 1 搜狗拼音 基础 2 WinRAR 最好用，不喜欢7z 3 Chrome 浏览器主力 4 Office365 学校给的账号，白嫖 5 HWIDGEN脚本 永久激活Win10专业版 6 360安全浏览器 用他的IE内核 7 Notepad++ 文档编辑主力 8 Editplus 亲测适用于50M以上文件 9 迅捷PDF编辑器 强大，当阅读器也不错 10 迅捷PDF转换器 各种转换一步到位 11 Cmd_Markdown 写作工具，最好无之一 12 360驱动大师 主要更新下声卡驱动 13 360安全卫士 只是想用加速球 14 TrafficMonitor 替代360加速球 15 Adobe Flash Player Flash，谷歌已弃用 16 Potplayer 4K播放环境-播放器 17 madVR 4K播放环境-渲染 18 LAVFilters 4K播放环境-解码 19 微信 社交 20 QQ 社交 21 Telegram 社交 22 QQ音乐 网易云海外体验极为差劲 23 CPU-Z 简单的看下CPU和内存 24 AIDA64 查看硬件信息必备 25 CrystalDiskMark 硬盘测速，注意版本 编码使用 No. 软件名称 说明 1 Git_for_Windows 基础环境，很重要 2 Java 基础环境 3 node 基础环境 4 python3 基础环境 5 IntelliJ_IDEA_2019 弃用eclipse了 6 VSCode 写Python用这个了 7 VS2019 可能C++会用到 8 Anydesk 远程桌面的首选 9 Real_VNC 全OS远程桌面 10 SecureCRT Linux远程SSH 11 WinSCP Linux传输文件 娱乐使用 No. 软件名称 说明 1 Nvidia_Experience 随时更新GameReady驱动 2 Steam 必备 3 WeGame 想玩一下国服，但幻想破灭了 4 LOL_EU 玩玩欧服还是可以的 5 RedAlert2战网 资深红警2菜鸡必备软件 6 PUBG_Lite 正常版本太大了 该页面持续更新中…","categories":[{"name":"Mini-ITX小钢炮","slug":"Mini-ITX小钢炮","permalink":"https://www.cz5h.com/categories/Mini-ITX%E5%B0%8F%E9%92%A2%E7%82%AE/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"装机","slug":"装机","permalink":"https://www.cz5h.com/tags/%E8%A3%85%E6%9C%BA/"},{"name":"软件","slug":"软件","permalink":"https://www.cz5h.com/tags/%E8%BD%AF%E4%BB%B6/"}]},{"title":"黑五剁手纪实 — ITX装机录","slug":"2019-12-6 黑五剁手纪实 — ITX装机录","date":"2019-12-05T23:00:00.000Z","updated":"2020-12-17T22:12:13.791Z","comments":true,"path":"article/772.html","link":"","permalink":"https://www.cz5h.com/article/772.html","excerpt":"我的上一任主力设备GE62-490已达三年的高龄，纵使宝刀未老，但也因我饱受松鼠症困扰而早早就将其存储空间挥霍一空，加之自己的新方向简直就是在显卡上跳舞，那原先的GTX960M可真的是力不从心了。于是乎，在寂寞的闲暇时间里，更新我的生产力工具就像个小雪球一样在我的脑海里越滚越大，终于滚到了黑色星期五，11月29日，就是你了！","text":"我的上一任主力设备GE62-490已达三年的高龄，纵使宝刀未老，但也因我饱受松鼠症困扰而早早就将其存储空间挥霍一空，加之自己的新方向简直就是在显卡上跳舞，那原先的GTX960M可真的是力不从心了。于是乎，在寂寞的闲暇时间里，更新我的生产力工具就像个小雪球一样在我的脑海里越滚越大，终于滚到了黑色星期五，11月29日，就是你了！ 德亚再见由于地域关系，为了免除运费和关税，只能用德亚了，怀揣着一腔热血的我，高兴而来失望而归，和电脑相关的产品，真的是硬通货啊，那价格基本就没低多少，我机智的用谷歌搜了一下，果然，本地商家的价格要良心太多了，运费基本没有而且全部支持收件网点暂存！所以，拜拜了亚马逊，本地商家搞起。 Google的产品搜索，虽然是个半广告的功能，但其实非常不错，商家的价格一目了然，直接就能知道谁家卖的最便宜，最后，显而易见 Megekko 的价格是最有竞争力的（最便宜的），类似的网站还有 Azerty，而且这两家网站其实后面的送货速度都非常快，如果仓库有货的话，荷兰境内基本三天就可以到货，且支持通过 PostNL 来暂存，这对于租房子的外来人员来说太好了，不用在家等快递。 装机配置选择攒机环境攒机，或者装机，其实最开始本就源自国外，是国外发烧友自己组装自己电脑的一种方式，但现在随着各大电脑品牌商的溢价，攒机也成了获得高性价比电脑的一种方式，在国内尤为盛行，比如在国内想要买台高性能的台式，基本上没人推荐购买整机了，性价比太低。其实后面看起来，国内外的攒机最大的区别就是 散热、机箱和电源，在主要配件上国内价格并没有明显优势，但在配件上则是天差地别，尤其是机箱方面，国内什么样的机箱都能买到还便宜，但在国外可选择的只有固定厂家的定制型号，基本是YouTube上攒机能看到的那些机箱，价格自然也非常高，虽然质量好，但着实没什么性价比，但这也没办法。 需求这是最终要的预备工作，因为攒机的配件高度相关，不确定好买来不适配那就抓瞎了，确定自己的需求可以按以下顺序完成（包括包含的类别）： 机箱（Case）：ATX、ITX、Mini-ITX 最高优先级，取决于你理想的电脑的样子，是大是小是方是圆。现在越来越多的人对台式机也追求便携，我也不例外，所以选择了 Mini-ITX 的机箱，最终主机的大小可以轻松的塞进15寸的笔记本电脑双肩包。 核心（CPU）：主要是针脚 这里和主板强相关，Ryzen 和 Core 的CPU的安装方式并不相同，即选择其中一种，就必须要选择对应的主板型号，例如 R5 3600X 对应 B450，我的 i5 9400F 对应 B360。这里我为了后续的更新空间和更强的超频能力使用了 Z390 主板，当然也是可以的。 主板（Motherboard）：ATX、ITX、Mini-ITX 主板跟随机箱的选择，比如我只能选择 Mini-ITX 主板，这里需要注意的是， ITX的机箱、主板、电源都要比ATX的更贵一些。 电源（Power）：500W、600W 电源也有大小！ITX机箱强烈推荐买 SFX 的电源，且强烈推荐买大点功率的电源，以备硬件使用。以海盗船（Corsair）为例，按我的选择，我只能选择 SF600W，而不能买 CX600W 等型号。 显卡（Graphics card）：全长、半长、单双三风扇 显卡的型号选择比较多样，视自己选择的机箱型号来看，切莫买来装不进去，我这款机箱支持全长显卡，也就是说支持三风扇的长卡，当然也可以选择半长的专为ITX设计的显卡，但其实那种卡有性能损失，所以最终我选择了比较大众的双风扇显卡。 内存（Memory）：主要是频率 内存选择比较自由，因为就算买错了高频，其实使用中还是会自动降频，并不影响使用。但为了不浪费内存性能，这里务必要搞清楚主板和CPU支持的最高频率，搞清这个问题并不容易，以我的 i5 9400F 为例，其配合 Z390 这种主板，其CPU睿频可以稳定在 3.9Ghz，就连内存频率可以轻松稳定在 3200MHZ，可见下图。 散热风扇（Cooler）：主要是尺寸 主要是针对CPU的散热，这里由于使用了ITX机箱，所以水冷就不再选择内了，而且由于使用的 9400F，发热量并不十分大，后来的使用过程中日常也就 30-40℃，所以这里使用的是 Noctua 的 NH-L9i，特别注意其散热方式，是立着的还是扁着的，安装完的厚度是多少，这个都要确认清楚，不然装上盖不上机箱盖就完蛋了。 硬盘（SSD）：注意接口 目前的主板，一般板载一个 PCIe 的SSD插槽，可以插NVMe的固态，这种固态的速度极快，但价格高，所以一般买小容量做启动盘。 可选（额外的风扇）：注意尺寸 这是机箱风扇，如果机箱空闲大，可以自己加装风扇，增强机箱内部的散热能力，机箱的风扇很简单，就是个叶片加电源线，主要有厚度和直径这两个尺寸，还要注意主板上预留的风扇电源插槽的数量，比如我的包括CPU散热在内只有三个插槽，加上原一个风扇，所以我只能再加装一个风扇。 可选（额外的硬盘）：注意尺寸 有了系统盘，一般外加单独的硬盘做存储盘。视机箱情况而定，受限于机箱，我只能加装2.5寸的薄的硬盘，最后选择了西数的蓝盘固态做存储盘。 具体落实以上之谈了大致的类型选择，并没涉及具体的型号，所以下面开始列举我最终选择的配件型号，只给型号不给价格。 机箱（Kolink Rocket Mini-ITX Minitowermodel）这是在海外能找到的为数不多的 Mini-ITX 机箱了，其他的像 DAN 等机箱根本就是缺货状态，买国内的酷鱼什么的寄过来又太慢了（一个月左右），所以这款机箱是我能够获取的最合适的一款了，其特性有以下几点： 支持全长显卡（竖着） 自带一个机箱风扇 体积9升，长宽比大（扁平） 支持两块2.5寸薄硬盘 前置电源键+两口USB CPU（Intel Core i5 9400F processor）目前阶段，如果选英特尔，那这款性价比没的说，是非常良心的CPU了，目前可以稳定超在3.9GHz，真的很稳。所以这里的问题是到底选酷睿还是锐龙，这里根据自己需要把，没法选，不过网上看的一句话是娱乐选酷睿，剪视频选锐龙，毕竟多核玩的还是溜呀。 主板（Gigabyte Z390 I AORUS PRO WIFI moederbord socket 1151）前面说了，Z390+9400F是大学生配初中生，主板过于高端，但事实是这样搭配也给了9400F充足的性能发挥空间，连原本其（9400F）支持的DDR4-2666MHz的内存频率都可以轻松拉到 3200MHz，而且还可以以后上8700K 等等这些超频高端U。其特点是： 板载WIFI，性能非常棒，自带延长天线基座 板载+扩展的 USB3.1 gen1 口有 6 个（蓝色） 板载 USB3.1 gen2 口有 1 个（红色） 板载 雷电3 接口 1 个 集成音频 5 个 + SPDIF口 1 个 板载风扇电源口 3 个 视频输出口两个（给集显的） 电源（Corsair SF600 600W SFX PSU / PC voeding）同样没什么品牌可选，就用最常见的海盗船了，选择600W 是因为需要充足的电力供应给主板，还有450W 等功率的电源，但这样在使用中或者后期扩展时可能会有电力供应不足的问题。 显卡（KFA2 GeForce RTX 2060 Super (1-Click OC) Videokaart）这里最终选用了KFA2的 RTX 2060Super，国内代号影驰的骁将，影驰还有大奖，应该对应该显卡的 GALAXY 版本，有更好看的外形和风扇灯效。这一版叫一键超频版，其实跟自动超频差不多，也就是说性能还是那些性能，但控制器可能稍微简单一点，当然价格也就更便宜，综合考量还是这一版（骁将）性价比更高。至于为什么选择 2060Super，原因主要有以下几点： 体验“入门级光追”嘛，实际买RTX显卡也赠送了《使命召唤:现代战争》 2060S 性能真的出众，比2060要好不少，改显存 6G 为 8G，性能只会非常略微低于RTX 2070 和实验室的电脑（RTX 2060）保持基本一致，后续会跑些 CUDA 代码。 对比 2060 砍掉光追的 1660ti，在其砍掉了光追也附带损失了一些性能，且 1660ti 也不便宜。 内存（Corsair DDR4 Vengeance LPX 2x8GB 3200 White Geheugenmodule）前面也说过了，虽然 9400F 标配的是 DDR4-2666MHz 但实际在 Z390 主板上的表现完全可以稳定在 3200MHz，不过要首先在 BIOS 中设置一下才能超内存频率。 CPU散热（Noctua NH-L9i）选这个散热器也是实属无奈，其本身非常薄，是压合式的散热器，不过对于 9400F 来说实际完全够了，在使用中风扇的表现还是不错的，基本听不到声音，在安装完后其实并没有到达最大的可容纳高度，所以感觉以后可以买再厚一点的散热器来压火（如果上了大U的话）。 启动盘（Samsung 970 EVO Plus 500GB - Solid state drive）这里没什么好讲的，用什么都行，但还是、、国外能选的太少了，怕翻车只能选大牌子，然后看着 Plus 和原版又没差多少，所以果断上 Plus 了。 机箱风扇（Arctic F8 PWM）这个风扇的安装位置并不是机箱预制的，我只是单纯的希望增加机箱内的对流，虽然在不盖盖子的时候看上去没什么用，但实际如果两面的机箱盖装上，那么这个风扇的作用就体现出来了，考虑到吸气不如吹气，所以这个风扇其实是内吹的，这样在装上机箱盖后的比较封闭的机体内，有个强劲的冷气输入源可以更好的加速内部的空气对流，增加散热效果（我的理解）。此外，其对吹的正是电源的插线部分，估计也能带走一些电源散发的热量。 存储盘（WD Blue 2TB SSD）非常巧妙的硬盘位设计，正好在外壳和内胆的缝隙中，所以空间很薄了，我很怀疑标准的 2.5寸HDD 能不能放进去，我这里很薄的 SSD 刚刚好能塞进去。使用的电源线是电源自带的 一拖四 的线，所以理论还能接三块，但实际空间还能塞一块。 安装过程及最终结果安装过程略，基本无难度，只是在最后接电源线的时候要好好理一下，以一个最梳顺的方式放置线材。其次是显卡延长线的整理，这个机箱需要把显卡延长线压在主板下面，所以需要提前压一下延长线。 结果如下图所示，上几张图（下图中显卡那一面还没加小风扇）。 这是实际的尺寸大小，跟 A4 纸比较很明显，想携带到处走也不是不可能。 写在最后另一方面，整机的噪音非常小，如果显卡的风扇不转，那几乎听不到声音，真的听不到声音，不过一般情况下显卡会有一个风扇在转，所以噪音还是有一些，但对比我右边的笔记本，噪音我认为更小，是一种更低沉的嗡嗡声，而不是笔电的风扇呼呼声。 整体散热上，加显卡的两个风扇，目前全机箱内共有 6 个风扇，分别是： 大颗的CPU散热风扇 × 1 显卡卡载风扇 × 2 机箱上下方向固定风扇 × 1 电源内置风扇 × 1 机箱左右方向风扇 × 1 即，显卡一侧有 3 风扇（显卡的+自己后加的），CPU一侧有 2 风扇（CPU的+电源的），底板上固定 1 个垂直方向的风扇。日常使用中仅有CPU风扇、底板风扇、后加风扇和单个显卡风扇常转，所以噪音并不大。同时如果需要散热时，这些风扇全开应该会带来非常不错的散热效果。实际使用过程中CPU温度稳定在40℃左右，后面会专门对其进行测试。 目前，已经使用我的新设备几天了，感觉非常良好，开始的时候还担心自己第一次组装电脑会出什么差错，实际过程中也确实出了些问题（在拆卸机箱时一定要用大螺丝刀，小螺丝刀没劲还容易把螺纹磨掉！），不过后面基本就比较顺利了，其实这不仅是我第一次自己动手装电脑，也是我第一次使用“大卡”（显卡），使用体验确实和笔记本的不一样！英雄联盟高画质轻松两百FPS，非常舒爽。后面还装了Photoshop等软件，等有时间写一下软件的使用体验，侧面来对整个机器的性能做个评价（包括NVMe的启动盘和西数2T固态的性能）。 备注作业部落在国外基本不可用了，编辑编辑文字还可以，但上传图片体验太差了，上传十次有一次成功就算万幸了，所以从现在起要换图床了，从 这里 的推荐看还是 sm.ms 是最好的，那就用这个啦，以后的图片都会存在上面的，只是他不支持从剪切板上传，那只能转存一下再上传，有点麻烦了。 收回以上说的话，sm.ms 也开始拜金了，我上传了几张图片之后就不让上传了，总是显示上传超过数量限制，加了电报群问了下，说是冲会员就好了，我冲个鬼，再见吧。寻寻觅觅，终于又找到了微博图床，然后还找到了 微博图床的Chrome插件，使用了一下效果非常好，即便海外访问也完全没问题。 自此，博客内的图片会全部换成微博图床，特此说明。","categories":[{"name":"Mini-ITX小钢炮","slug":"Mini-ITX小钢炮","permalink":"https://www.cz5h.com/categories/Mini-ITX%E5%B0%8F%E9%92%A2%E7%82%AE/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"荷兰","slug":"荷兰","permalink":"https://www.cz5h.com/tags/%E8%8D%B7%E5%85%B0/"},{"name":"注意事项","slug":"注意事项","permalink":"https://www.cz5h.com/tags/%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"},{"name":"装机","slug":"装机","permalink":"https://www.cz5h.com/tags/%E8%A3%85%E6%9C%BA/"}]},{"title":"初到荷兰的待办事项","slug":"2019-12-2 初到荷兰的待办事项","date":"2019-12-01T23:00:00.000Z","updated":"2020-02-29T18:43:55.542Z","comments":true,"path":"article/979e.html","link":"","permalink":"https://www.cz5h.com/article/979e.html","excerpt":"本文总结下近期在荷兰生活遇到的各种坑，以及在荷兰需要的方方面面，后续会持续更新。下文从入境荷兰之后开始介绍。此内容系回忆内容，归属于RMBATS列表。","text":"本文总结下近期在荷兰生活遇到的各种坑，以及在荷兰需要的方方面面，后续会持续更新。下文从入境荷兰之后开始介绍。此内容系回忆内容，归属于RMBATS列表。 在国内要办完的事简单提一下，主要有以下几点： 开通国际漫游：这个一定要做，而且要确认已开通了在出境，不然你的所有验证码都会收不到，对于有登陆验证的页面或应用，如果不开通国际漫游，那基本上是别想登进去了。而且银行转账一般也要手机验证，工行的有工行信使还可以，但中行的没有，所以如果没有漫游，在国外想用自己的银行卡转账也是不行的。 办理一张国内的VISA或万事达卡：如果是学生，推荐办理主副卡，家长是主卡，自己拿副卡，这样额度高而且还安全，同时主卡可以为副卡还钱，没有后顾之忧。 确认签证信息：要注意核准荷兰移民规划局（IND）给你的回执信息，看清你的逗留日期，荷兰的人办事马大哈，给你写错了很正常（比如我就-），前面没发现后面再改的话，需要耗费非常长（可能几个月）的时间。 搞定租房：国内租国外的房，两眼一抹黑，尽量按照以下几点租，应该是没问题的： 优先租中国人房东的 优先租带独卫的Studio的 优先租距离学校近的 一定要有正规的租房合同 优先租可提供注册地址的 优先租短租（几个月最好），来了你会发现其实房源不少，所以为了不因为自己的合同未到期而错过好房源，尽量第一次租的时间短一点，来了再找更好的地方。 入境后的待办事项以下事项基本都需要提前预约，有的是学校已经做了预约，有的需要自己预约。 电话卡这个真的是下飞机的第一件事，荷兰的手机运营商我也不知道好坏，但目前来看，认识的人还是用 Lebara 的多。其也分匿名卡和实名卡，匿名卡你可以在大超市或者饭店见到，就跟广告一样，匿名卡开卡后在手机APP上可以充钱，没话费了会自动扣除。注意：千万要少用流量，荷兰的流量非常贵，可能 2GB 就要上百人民币，所以尽量不要在移动流量下看视频等。关于流量，一个解决方案是使用天际通服务，比本地运营商的价格要便宜，但仍然比国内贵太多，可能一天要十块多人民币。 学校的登记这部分一般由学校的秘书完成，可能会单独约谈一次，不需要自己做什么，后续会涉及校园卡、门禁卡的发放。最主要目的就是确认你人到学校了。 个人项目的注册登记这部分是指完善学生在学校的个人信息，一般会很详细，并且和博士或硕士的毕业设计联系在一起。 大市政厅领居留卡大市政厅是指所在辖区的市中心的市政厅，荷兰的划分和我们的省市县三级差不多，比如我住在乌特勒支省的宰斯特市，那么我的居留卡应该在乌特勒支的市政厅领取，而地址的注册应该在宰斯特市正厅完成。 小市政厅注册这部分需要提供自己的“地址”，也就是俗称的“报地址”，所以在租房时要寻问清楚可不可以报地址。在居住地当地的市政厅注册后，会有个漫长的等待过程，大概三周后会收到一封纸质信，里面有自己的 BSN 码，这个码非常重要，办保险和银行卡都需要。 购买保险如果是学生，请不要购买类似 AON 的学生保险，该类保险虽然便宜，但其不属于荷兰的国民健康保险，因此不能申请荷兰的保险补助（资本主义国家的福利），把补助算进去实际保险的花费会很少，但 AON 每年六百多欧元是实打实的，所以不建议购买。注意：国民健康保险需要BSN码才能办理。 办理银行卡如果是学生，一定要办理学生卡，因为非学生卡每个月都会收点钱，每年大概十几欧，也就是非学生卡一年要交一百多块人民币的管理费。注意学生卡的办理也需要提供 BSN 码，然后需要携带相关的证明材料。最后的卡仍旧以纸质信的方式邮寄给你。 公交打折卡如果是学生，务必办理黑黄色的 OV打折卡，因为在荷兰坐公交非常昂贵，如果付现金（刷银行卡）单程每次要将近 3 欧元（不管远近）。办理打折卡同样需要 BSN 码。如果办不了打折卡，可以办理蓝色的 OV匿名卡，单程每次可能 1.5 欧元，打折卡的话会更便宜一些。如果每天公交车通勤，那必须要办一张打折卡。 NS火车票如果需要做城际火车，例如从乌特勒支去埃因霍芬，那么务必购买往返火车票，或者“天票”，如果直接用公交卡刷，费用会非常贵，往返火车票一般会便宜不少，然后天票一般是固定价格（15欧左右）单日往返随意乘坐。可见，天票是最划算的，不过天票不是想买就买的到的，其只在某些超市以特定的时间作为促销进行发售，一般有一段有效期，有效期内可以凭天票兑换的二维码，在某一天任意出入火车站，也就是说凭该票去哪都行（往返）。 自行车在自行车王国购买一辆自行车是在正常不过的了，可不要以为自行车王国自行车就便宜，反而，自行车在荷兰是相当的贵，车行里卖的车普普通通就上万人民币（2000欧），所以如果买新车，那是买不起啊，靠谱的二手车就完全可以。一百欧左右的二手车一般就非常好骑，Facebook的 Marketplace 是个很好的二手自行车交易平台。注意，买车时一定要看看轮胎的好坏，自行车最容易坏的就是轮胎，就算是漏个气，在荷兰修一次也得10欧网上了，所以如果买个破车，可能修几次还不如再买一辆。注：听说乌大有新生一千欧的自行车购买津贴，还未求证。 超级市场AH（Albert Heijn），分布非常广泛，史基浦机场里就有的超市，购买“天票”的地方，不支持VISA卡。 LiDL，德国超市，价格明显低廉，吃的用的都有，比较杂。 Action，杂货超市，吃的和水果很少，可以买鼠标、键盘、晾衣架等等东西。 Jumbo，地方性的超市，只分布在荷兰中部。 Kruidvat，专营日用品和药品，经常打折捆绑销售。 東方行，地道的中超，可以买到榨菜、泡面、老干妈等等国货，支持支付宝付款。 写在最后一般而言，从注册到收到BSN码要很长时间，且注册可能就是入境后的一段时间之后，所以BSN码可能在入境后的一个多月后才能收到，在这一个多月的空窗期，就没法办理 OV打折卡、银行学生卡、买保险。所以，开头的国内信用卡就有用处了，基本上用（工行）信用卡可以在绝大多数超市（AH不行）、全部公交车、部分餐厅使用。 补充一点，对于电子产品控，荷兰的本地商家 MediaMarket 并不便宜，即使在黑五也是如此。bol.com 更是奇贵无比。这里推荐从本地的专营电子产品的网上电商平台购买（MediaMarket有线下店，除外），Megekko.nl 和 Azerty.nl 是我用过的两个不错的网商，我从上面买到了租装机的全部零件，价格基本全网最低了（得益于Google的商家产品检索），物流三至四天，支持PostNL寄存，感兴趣的可以尝试一下。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"荷兰","slug":"荷兰","permalink":"https://www.cz5h.com/tags/%E8%8D%B7%E5%85%B0/"},{"name":"注意事项","slug":"注意事项","permalink":"https://www.cz5h.com/tags/%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"}]},{"title":"CSC申请结束到派出前的事项","slug":"2019-11-11 CSC申请结束到派出前的事项","date":"2019-11-10T23:00:00.000Z","updated":"2020-06-18T11:29:32.540Z","comments":true,"path":"article/2275.html","link":"","permalink":"https://www.cz5h.com/article/2275.html","excerpt":"最新版本已更新至知乎专栏文章。对于CSC的申请来说，得到结果后到约定派出的时间（一般是九月份）大约还有两三个月左右，虽然看起来时间很长，但实际上，有些时间节点如果不做完该做的事，就会严重影响后续的进度，下面就以我的经验（有些记不清楚了）说一下申请成功之后需要做的一些事项，顺便详细解释下关于荷兰签证（MVV，其他国家的不知道！）的申请流程。","text":"最新版本已更新至知乎专栏文章。对于CSC的申请来说，得到结果后到约定派出的时间（一般是九月份）大约还有两三个月左右，虽然看起来时间很长，但实际上，有些时间节点如果不做完该做的事，就会严重影响后续的进度，下面就以我的经验（有些记不清楚了）说一下申请成功之后需要做的一些事项，顺便详细解释下关于荷兰签证（MVV，其他国家的不知道！）的申请流程。 按时间先后对各种事项进行排列（可能有遗漏），着重注意超链接网页文本，其中包含了更为详尽的内容。 行前培训基本是得到结果几天后，学校就会办，如果不是申报单位的，关注就近学校的类似的活动，会给发些相关的手册，还是比较有用的。 注意：行前培训一般会提到“留服代办签证”的内容，你可以让留服给你办理签证，但可能和我们个人办理所需要的材料有些许出入，办理的时间有些许的不同，留服办理MVV签证可以详见这个页面。 出入境检疫局办理健康证这个证有没有用，我目前并没有确切答案（如果让留服代办，是需要这个证的），在我办理签证的流程中，并没有环节需要出示此证件，也不排除某些环节内有联机查询是否符合条件的操作，总之作为一个出国学习一年甚至是四年的学生，一次全面的身体检查还是很有必要的，就把其当做一次查体即可，免费哦！出具《留学资格证书》可以免费检查、拿证。 关于体检的要求，详情参见《出入境检疫局办理健康证流程》。 签证事宜（点击查看-&gt;官方指导流程） 开始准备的时间：拿到《留学资格证书》之后，一般是出结果后1-3周左右寄到。 3.1 联系外导拿到资格书后就联系外方的反签/学校注册事宜（没有护照的抓紧办一个）。之后需要两个月左右的时间才能收到回执，收到回执后才能到大使馆完成最后一步（步骤3.6，之前的若干步骤可以在这个空档内完成）。可见开始一定不能松懈，抓紧时间联系外导询问这些事。 一般是提供给导师/秘书：成绩单、毕业证、学位证、留学资格证书的英语扫描件即可。两件事情（递交材料给荷兰移民归化局+学校注册）都用这些材料即可。 3.2 材料公证户籍所在地公证处，公证原始材料 -&gt; 胶装成册的‘公证材料’ （一周内即可完成） 公证是‘认证’之前的必备步骤，公证的单位找位于自己家附近的公证处即可，我的经验是：小县城里的公证处也完全可以办涉外的公证，只要和他们说你是办留学签证材料的公证，一般他们就明白怎么做（对他们来说你不是第一个也不是最后一个，大胆去交流）。 可能需要的证的材料：出生证明、户口本、学位证、毕业证。我公证了四本，不确定是不是这四项，但公证这些肯定是符合要求。以上公证材料的直接目的就是办理签证，所以详细要求可以仔细阅读签证材料的要求：汗颜、找不到当时那份详细的材料要求了。。 公证材料准备好之后，应该是胶装成册，原来可能是四张纸，但现在多了公证文、翻译件、落款页等等，每张纸都变成了一本册子，也就是四本册子。 3.3 直接去代办的源头一般的签证，只需要X国驻华大使馆的认证即可，甚至像越南这种落地签，直接将审核下放到越南入境海关；但对于荷兰的签证，其需要外交部+荷兰驻华大使馆的双认证，比较麻烦。 人力/快递‘公证材料’给代办机构 -&gt; 代办时间（2-3周） -&gt; 人力/邮寄拿到双认证过的‘公证材料’。下面的机构位于(39.9165N, 116.430496E)，市面上的第三方盈利机构的代办，最后都会汇总到这里。（这里需要2-3周左右即可，比第三方代办要快一些） 注意：外交部认证是不接收个人申请的，这里必须通过“代办机构”，上一步骤的公证的费用可能是100左右/本，代办双认证的费用可能是300-500左右/本。 具体的代办机构不必须是以下这个，原则上任何北京的、大一点的代办机构都可以，不过非北京的代办不建议使用，因为你材料寄给他们后、他们还要再送到北京\\上海…时间慢，如果你想时间安排的紧凑，(又是北方人)那就选择北京的代办。 具体怎么找到这里的已经忘了 3.4 等待第1步的结果第一步审核时间可能很久-&gt; 拿到外方发给你的邮件（告知你荷兰移民规化局已经审核完成） 3.5 预约荷兰大使馆的签证申请时间预约网站-&gt;Schedule Appointment-&gt;选择城市-&gt;选择MVV签证-&gt;选择日期…. 由于疫情原因，目前预约系统是没有可选择日期的，不过大家知道这回事即可。预约是到大使馆之前的必备步骤，否则人家门都不让你进。 3.6 亲自前往荷兰驻华大使馆带好‘外交部认证过的公证材料’+护照+照片 -&gt; 按时到达位置 -&gt; 填表、交护照 -&gt; 填顺丰的邮寄单（整个过程如果不排队不会超过10分钟，注意上下班时间）。(这个审核+邮寄大概只用一周就能完成，前提是提前预约时间，否则错开的时间就浪费了) 3.7 完成等待收接收经‘大使馆认证’过的最终的签证（已经贴进护照里了） 以上整个过程花费的时间总和是： 步骤3.1(2-3个月左右，这个空档内完成3.2和3.3和3.5)+步骤3.6(1周左右)=2-3月左右，期间的空档期内把材料公证、认证、预约和最后一步无缝衔接比较困难，所以给自己留好充分的时间来办理签证，切忌迷茫怠工，错过时间。 附：我的时间节点(基本没怎么浪费时间) 05/31 出结果 06/20 收到红头文件《资格证书》 06/22 (外)导师开始帮我向学校交材料 07/02 (外)收到学院的Agreement 08/?? (内)完成材料公证 08/22 (内)材料交给代办双认证 09/11 (内)双认证结束 09/16 (外)收到IND回执 09/23 (内)材料交给大使馆出签 09/28 (内)拿到签证 注意：内，指国内办理，外，指外方协助办理。黑体字时间节点是我们不能控制的，一个是出结果到收到资格证书；再一个是双认证递交材料到完成；再一个是从六月提交材料后到收到IND的回执；再一个是出签时间。虽然我们不能准确知道这些时间点，但我们知道大概需要花费多少时间，我们要做的就是尽量穿插的准备材料，利用好时间空档，尽量不浪费时间去等待。(这些步骤都是有顺序的，比如IND回执来了，但你还没双认证，那双认证的时间就会拖慢整个流程) 提前租房开始时间：收到红喜报那天晚上就可以开始找房了（自己找房），如果是希望租住到学生房，那就得等到完成学校注册（步骤3.1中的一项）后开始。当时学校注册花了一个多月的时间，所以如果目的地是国际生很多的地方，那么像荷兰的SSH等租房网站必定存在排队现象，排队的时间时长可达四个月（学校官方推荐提前四个月开始找房），那么不要孤注一掷，自己也要积极的通过各种渠道来租房子。 关于学生房vs.自租房，各人喜好不同。 我比较讨厌住在学生房里。我所在的学校学生房基本都是集中在一起的大楼，类似国内的宿舍，但都是单间独卫，环境不错，但没朋友的话着实容易抑郁！ 自租房的范围就比较广了，也可能包含学生房，但更多的是房东直接租给你和其他人合住这种形式，一般只有单间，合用的洗浴、厨房、卫生间，有人可能觉得不卫生，但这种生活更丰富一点，可以有固定的邻居室友（强制你们交朋友，学生房的话可能互不搭理也没关系）。另外，优先选择华人房东，不要怕自己人宰自己人，宰的再狠也就那点钱，关键的好处是租住在华人房东房子里，有紧急事情可以方便的找到能无障碍沟通、且生活经验丰富的人。 一切准备就绪后，订机票这里的一切指的是： 学校注册完成 MVV签证办理完成 切记《留学协议书》交一份给留学基金委 租房手续/协议完成 在这些完成之后，距离正式开学时间至少前两周左右就要定好机票，选择好时间，选择好航班，注明学生票，不要在乎价格，万把块钱的直航都是可以的，在留服CSC网上业务办理页面提交你的意向航班信息，留服人员会给你购买后，将单据（电子客票行程单）以邮件形式发给你，打印出来，到时候去机场换登机牌即可。（一周左右即可收到回执） 之后去留服提交派出申请申请通过后，可以看到报到证，这个后续在抵达目的国之后（尽量一个月内完成）需要打印出来，和其他相关材料一起邮寄给（一般是）中国驻X国大使馆教育处，审核后完成整个派出环节，三个月后（出国前会预付三个月的奖学金，用来支付房租等）开始正常发放奖学金。 打包行李，坐飞机关于行李打包，切记不要放违禁物品，否则开包检查可就麻烦了； 切记不要超重太多，以南航为例，可以去官方 查阅行李要求，学生票的话应该是两件23KG的行李箱+?KG背包，实际体验是24公斤也不会管你的，背包多重也没人管，外加手提行李好像也没有重量检查。 我携带的重量大概是 24KG×2(皮箱)+14KG(背包)+3KG(手提)。 关于落地的海关查验，我没有被查，同一时段也没有人被查，因为那时候是凌晨5点，所以，强烈建议购买类似时间点的航班，免去很多麻烦，否则你要出具租房协议、回答问题等等等等，尽量选择时段避开之。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"Summary","slug":"Summary","permalink":"https://www.cz5h.com/tags/Summary/"},{"name":"材料准备","slug":"材料准备","permalink":"https://www.cz5h.com/tags/%E6%9D%90%E6%96%99%E5%87%86%E5%A4%87/"}]},{"title":"出入境检疫局办理健康证流程","slug":"2019-8-12 出入境检疫局办理健康证流程","date":"2019-08-11T22:00:00.000Z","updated":"2020-02-29T18:43:55.569Z","comments":true,"path":"article/b8a6.html","link":"","permalink":"https://www.cz5h.com/article/b8a6.html","excerpt":"注：转载请注明出处，以下内容均为个人总结，不保证正确性和完整性，请酌情参考","text":"注：转载请注明出处，以下内容均为个人总结，不保证正确性和完整性，请酌情参考 准备事项 地点：出入境检疫局（地市区的海关） 时间：上午、下午两点之前 事项：办理健康证（体检） 要求：空 腹 材料： 两寸照片 1-2 张 身份证原件、复印件一张 护照原件、首页复印件一张 基金委红头文件（可免体检费） 办理步骤以下针对的是浑南海关（检疫局），各地要求可能不一样，比如免费体检这项，我只被要求提供红头文件（资格证书）即可，有些地方需要提供签证（那就比较头大）。 填业务表、叫号、排队（和银行办业务差不多） 核对材料在柜台会被要求出示照片、复印件和基金委资格证书，照片复印件必须提供，核对无误后会发给你一份体检单，同时会问你要不要接种疫苗，这个一般不用，打疫苗在哪里打都可以，只需体检就行。体检单上自己看一下，人员类别一定是留学人员。 拿到体检单后去交费处缴费盖章，我只拿去盖个章就走了（免费用）。 开始体检以下是我体检的顺序，体检项目各地应该都差不多，除了抽血和尿检，基本都能马上问医生正不正常，一般都是没事的，好奇可以问问。 下午去的话，整个体检过程务必在下午三点之前搞完（尤其是验尿）。 身高、体重、血压 视力、色弱色盲 B 超、心电图 抽血、尿检 X 射线 取证上述全部流程完成后，将体验单交回前台，一天后的下午 16:00 - 16:30 拿回执单取化验结果及健康证，不同地方肯定是不一样的，但一般都支持自取和邮寄，我这里邮寄是顺风，邮费当场交上。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"Summary","slug":"Summary","permalink":"https://www.cz5h.com/tags/Summary/"},{"name":"材料准备","slug":"材料准备","permalink":"https://www.cz5h.com/tags/%E6%9D%90%E6%96%99%E5%87%86%E5%A4%87/"}]},{"title":"告别《长安十二时辰》","slug":"2019-8-11 告别《长安十二时辰》","date":"2019-08-10T22:00:00.000Z","updated":"2020-02-29T18:43:55.562Z","comments":true,"path":"article/b5dc.html","link":"","permalink":"https://www.cz5h.com/article/b5dc.html","excerpt":"在刚刚过去的几天，《长安十二时辰》迎来了谢幕，全剧剧情时长仅为一天，内容非常紧凑，主线明确，支线丰富，角色感情饱满，剧情张力十足，服饰道具异常考究，是一部最近难得的且非常有特色的古装剧。","text":"在刚刚过去的几天，《长安十二时辰》迎来了谢幕，全剧剧情时长仅为一天，内容非常紧凑，主线明确，支线丰富，角色感情饱满，剧情张力十足，服饰道具异常考究，是一部最近难得的且非常有特色的古装剧。 主线剧情细细回想，该剧情节从巳正开始，历经十二个时辰结束，期间剧情完全在长安城展开，中间穿插了关键的一些回忆情节，这些支线对主线中角色的行为起到了非常重要的解释作用，从而使整剧虽展开的时间有限，但表现的内容却很完整。 这里简单的利用Gephi对整部剧中的人物角色做个梳理，主要的角色设定及相关性如下： 上述内容并不完整，有些缺漏（比如没有何监的傻儿子） 李必和张小敬是本剧的核心，其构成了关键的人物组，包括檀棋、徐斌等 圣人和林九郎、何监则构成了互相牵扯的权力组，其中的勾心斗角。相互制衡，非常精彩 最后就是本剧的核心大反派龙波，其与闻无忌和张小敬的奇妙连结，是本剧情节升华的关键；在本剧中反派的发展经历了两个基本独立的阶段，一个是由曹破延主导的炮灰组，其中也包括何监的傻儿子，其“阙勒霍多”就是简单的闹市区自爆炸弹；再一个就是真正的幕后黑手龙波，在炮灰组吸引靖安司注意力的情况下，其才在有条不紊的实施“阙勒霍多”——将整个巨型花灯打造为一个炸药桶，并通过机关达到击杀圣人的目的。 在地点上主要涉及几个坊市、靖安司以及花萼楼。整体情节的变换紧跟反派的变换，从最初简单的炮灰组，到最后复杂的暗杀圣人阶段，主角们遇到的问题各不相同。 在反派的合理性上，本剧设计也很出彩，层层递进逐渐揭开大反派的面目，结果原来是主角最亲密的挚友，是典型的“官逼良民反”，其实就连炮灰组曹破延和何监的傻儿子，都有他们作恶的苦衷，这种好人作恶的情节很容易让人产生同情、共鸣和感慨。在龙波身上体现的尤为明显。 最终的结局，感觉是情理之中但又是意料之外，虽然知道大的情节走向是圣人并没有遇刺（本剧符合正常的历史观），但最终还是没想到，决心行刺的龙波最后会放弃，但放弃的原因也说得过去，因为他直接对话圣人了，也在侧面完成了他的终极目标，解开了他郁结在内心的心结，这点也还是得佩服圣人的嘴炮，一点也不比主角俩人差。 总体上来说，这部剧场景虽然设计简单，但设置却非常精致，没有太多的修饰感，服装设计拿捏的很棒；加之分集的巧妙，使得此剧虽篇幅不短但张力十足，一开始就引人入胜，值得推荐！","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"观后感","slug":"观后感","permalink":"https://www.cz5h.com/tags/%E8%A7%82%E5%90%8E%E6%84%9F/"},{"name":"胡乱评价","slug":"胡乱评价","permalink":"https://www.cz5h.com/tags/%E8%83%A1%E4%B9%B1%E8%AF%84%E4%BB%B7/"},{"name":"剧透","slug":"剧透","permalink":"https://www.cz5h.com/tags/%E5%89%A7%E9%80%8F/"}]},{"title":"Github托管站点的域名更换","slug":"2019-8-4 Github托管站点的域名更换","date":"2019-08-03T22:00:00.000Z","updated":"2020-02-29T18:43:55.566Z","comments":true,"path":"article/7f0e.html","link":"","permalink":"https://www.cz5h.com/article/7f0e.html","excerpt":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考 单纯的修改站点的域名","text":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考 单纯的修改站点的域名 第一步，到阿里云域名管理页面，找到旧域名的解析页，点击导出配置，选择xlsx导出即可。 第二步，打开新域名的解析设置，导入刚才的xlsx配置。由于使用 Github 托管，上述步骤不涉及托管端的修改，所以整个导入后不需要任何修改就算配置完成了。 第三步，需要将原站点内的所有硬编码旧域名的地方，全局替换为 新域名，使用 notepad++ 直接对文件夹进行全局查找替换，十秒钟搞定。 第四步，打开Hexo博客文件夹目录下的 CNAME 文件，看看是否自动替换为了为新域名，CNAME是会自动更新的： 第五步，部署上传一遍（clean + generate + deploy） 第六步，测试，至此，把旧域名完全清除干净了，无论是博客的源文件部分，还是打包部署托管到Github上的部分，均与旧域名无关，可以说此时已经完成了域名的更改。 旧站点链接的重定向首先说明，到现在我也不太理解到底 301重定向 是什么意思，但关于借助 Github Pages 服务的重定向我是整明白了，对于我这种使用 Github 托管静态资源站点的用户来说，要重定向目测只能通过这种方式（JS页面重定向），其本质是将旧域名绑定到一个新的 github.io（Github Pages） 上，再通过这个新 Pages 站点的 index.html（主页访问） 和 404.html（子页面访问） 来完成页面的重定向。原理如下： 开始配置重定向之前需要两个准备环节，第一个环节，新建一个 Github 账号（因为一个账号仅允许绑定一个 username.github.io），新建Github账号（我这里是zonelyn），并创建同名的项目，默认 Github Pages 服务会自动启动。 第二个环节，去阿里云域名解析设置页面，将 旧域名 解析到 新的.github.io 地址，主要是以下两个配置项，完成之后，即，此时访问的流程是 旧域名 → 新的.github.io。下面开始讲解使用 Github Pages 进行重定向的操作，主要分两种情况： 直接访问旧域名时，默认跳转到 新 Pages 站点的 index.html 页面，这时我们在该页面只写一句话，即：访问旧域名时完成了如下跳转： 旧域名 → 新的.github.io → index.html → 执行JS跳转到新域名 → 旧的.github.io 上述跳转对用户来讲就是 旧域名 → 旧的.github.io，也就是我们在更换了新域名后，仍然可以通过旧域名访问到原站点。 访问含域名的其他子页面时，比如先前被搜索引擎收录的链接、被好友保存的友链等等，这些我们无法去更改，同时也获取不到全部的列表，即无法做一对一的重定向映射。这时如果不做任何操作直接访问的话，由于访问的是子页面，新的.github.io 中必然没有对应项（实际只有个index.html），结果就是会显示 github 的 404页面（默认）。至此，我们需要一个访问的阀门来对所有 旧域名子链 的访问进行重定向，而 Github Pages 允许用户自定义 404.html（像index那样），如果站点中有用户自定义的 404.html 那么出现异常时就会转到用户自定义的 404，我们正是借助这一特点，来完成对所有子链的重定向。同样的，通过JS来更改被访问的子链，从而完成重定向，整个流程完成如下跳转： 旧域名子链 → 新的.github.io → 404.html → 执行JS替换为新域名子链 → 旧的.github.io的子链文件 即，对用户来说是 旧域名子链 → 旧的.github.io的子链文件，也就是在更换新域名后，旧域名子链也正常访问到了文件位置。 至此，托管于 Github Pages 服务的站点的 域名重定向 设置就全部完成了，达到的效果就是，全站均采用新域名访问，同时如果使用旧域名访问，无论是主页还是子链，均可以利用 新的.github.io（Pages服务）进行重定向到原页面。 最终效果 基本信息 旧域名：www.whereareyou.site 旧站点：TianZonglin.github.io 新域名：www.zonelyn.com 新站点：zonelyn.github.io 目的 在“单纯的修改站点的域名”这一节中，完成的是新域名绑定旧站点； 在“旧站点链接的重定向”这一节中，完成的是旧域名绑定旧站点； 新建的站点只起到中转作用（利用了一下他的 Github Pages 服务） 效果： 访问 www.zonelyn.com → 最终访问 TianZonglin.github.io； 访问 www.zonelyn.com/tags → 最终访问 TianZonglin.github.io/tags； 访问 www.whereareyou.site → … → 最终访问 TianZonglin.github.io； 访问 www.whereareyou.site/tags → … → 最终访问 TianZonglin.github.io/tags； 百度统计的更新域名更改后，还会受到影响的就是百度统计等若干统计了，这里采取的方案分以下几步： 新建统计站点（zonelyn）； 重新安装统计代码 20分钟后，会检测到代码正常 合并为百度统计中的“汇总网站” 由于仅仅是替换原站点中的统计代码，不会删除访问量等数据，原站点的访问数据同原域名一起，停止在了某个状态，重新安装完百度统计的代码后，访问数据便均为新域名的，旧域名将和新域名一起组成汇总网站，此后我们只需查看汇总网站的报告即可！ 其他相关更新 Valine评论插件在更换完域名后也得重新添加下安全域名，之前出于安全考虑，在 LeanCloud 的配置页面添加了 Web安全域名，将新域名填进去保存即可。 我的博客即将同步至腾讯云+社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=1uwztntxoqyyn","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.cz5h.com/tags/Hexo/"},{"name":"域名","slug":"域名","permalink":"https://www.cz5h.com/tags/%E5%9F%9F%E5%90%8D/"},{"name":"Github","slug":"Github","permalink":"https://www.cz5h.com/tags/Github/"}]},{"title":"网站域名到底加不加 WWW","slug":"2019-8-3 网站域名到底加不加WWW","date":"2019-08-02T22:00:00.000Z","updated":"2020-02-29T18:43:55.565Z","comments":true,"path":"article/bcad.html","link":"","permalink":"https://www.cz5h.com/article/bcad.html","excerpt":"本文系【转载】，感谢作者的总结！原作地址：知乎日报原文作者：Rio转载文本请注明出处和原作出处，下文不保证百分百正确性和完整性，请酌情参考","text":"本文系【转载】，感谢作者的总结！原作地址：知乎日报原文作者：Rio转载文本请注明出处和原作出处，下文不保证百分百正确性和完整性，请酌情参考 不加 www 的裸域名的好处和坏处 好处：主要是域名更加简短、容易记忆。 坏处： 裸域名只能绑定 DNS 的 A 记录，不能绑定 CNAME 记录。也就是说你不能把裸域设定为另外域名的别名。很多时候这对管理不是很方便，特别是使用第三方托管服务的时候。如果第三方迁移服务器导致 IP 地址变更，你必须自己去更改 DNS 的 A 记录。比如你的个人博客采用 Tumblr 的服务，如果使用裸域，你需要手动将你域名的 A 地址指向 Tumblr 指定的 IP 地址。Tumblr 如果迁移了机房，所有通过这种方式设定个人域名的用户都必须更改自己的 DNS 才能继续使用，否则服务就会中断。使用子域名的 CNAME 记录就相对简单很多，只需要将 www 子域名的 CNAME 字段指向 http://domains.tumblr.com 这个域名，之后如果 Tumblr 更改 IP 地址，他们只需要重新设置 http://domains.tumblr.com 这个域名的 A 记录，而无需要求每个用户去更改 DNS 记录。这个技术上的限制导致许多大型的第三方服务商不支持使用裸域。典型的如 Google 的服务，现在都不能使用裸域。Google 的服务用户基数大，不得不采用 DNS 级别的分布式，使用到的 IP 地址太多，而且变动大。让用户绑定 A 记录的话不利于负载均衡，维护起来也是几乎不可能完成的任务。同理，大部分 CDN 也不支持裸域。 裸域的 cookie 的作用范围太大。假如知乎也采用裸域，那么知乎所有 cookie 的作用范围就包括 http://zhihu.com 下的所有子域名。也就是说访问 http://foo.zhihu.com 和 http://bar.zhihu.com 的时候都会带上 http://zhihu.com 裸域页面设置的 cookie。从安全、隐私、可扩展性、以及管理的角度而言，这对很多大型网站来说是不可接受的。 URL 的正则匹配，如果带 www 前缀的并且以 .com/.net/.org 结尾的，通常成功的机会要大很多。这个你会在许多文本编辑器里面遇到。如果 URL 不是 www 开头，并且也不是三大顶级域名结尾的，匹配成功的概率就要小很多。这是使用过程中有时候会让人很抓狂的点，重不重要全看你的用途和场合了。 非技术上的考量：用 www 子域名的好处体现在线下的环境，比如户外广告、报纸杂志、语音广播、语音电话等使用场合，www 这个前缀（不管是视觉的还是听觉的）能够很明确的提醒受众，这个信息片段是一个网站。有人会说加上 http:// 前缀也能解决这个问题，但现在随着以 Chrome/Safari 为首的浏览器都开始在地址栏里隐藏 http:// 协议前缀了，普通用户对于 http:// 这几个字符的理解会越来越模糊，所以如果是线下的话，保留 www 这个 visual/vocal cue 还是有一定意义的。 总的来说对于大访问量或多子域名的网站来说，不建议使用裸域。小流量或子域名少的网站的话就看个人爱好了。我挺喜欢裸域的。最近几年流行起来的「单页网页应用」 ( Single Page Web App ) 也是以采用裸域的居多，Twitter 算是一例。 去掉 www 是否会影响网站的 SEO（主要是排名和收录）（前提是过去有加 www）早先裸域刚开始流行的时候确实有传闻说不利于 SEO，但现在看来似乎并无任何问题。如果有的话也是搜索引擎的 bug，给他们提一下他们应该会很乐意去改。Google 的站长工具里面有工具可以帮助你做 URL 迁移的，可以有效的解决这个问题，再配合下一部分的跳转，不用担心对 SEO 有任何负面影响。 用什么方式去跳转最好？（如 301）不管你决定使用还是不使用裸域，最好不要在同时保留 www 和非 www 前缀的 URL，这样既不方便用户的浏览器区分访问历史，也会对你做访问统计带来不少麻烦。最佳的方式是采用 301 跳转，并且跳转的时候保留 URL 里域名后的全部内容。比如，如果你决定使用裸域 http://example.com，那么请务必将 http://www.example.com/foo/bar?spam=egg 301 跳转到 http://example.com/foo/bar?spam=egg 。或者反过来，如果你决定不使用裸域，那么请务必将 http://example.com/foo/bar?spam=egg 301 跳转到 http://www.example.com/foo/bar?spam=egg，这样的跳转需要在 web 服务器里单独配置，很多 DNS 管理界面提供的简单的跳转到新域名的根目录无法实现这样的功能（仅仅跳到 http://example.com/ ) ，对用户体验和搜索引擎 SEO 而言都是非常糟糕的。 下面给出如何在 nginx 里面实现上述的跳转： 1234567891011# redirect http:&#x2F;&#x2F;www.example.com to http:&#x2F;&#x2F;example.comserver &#123; server_name www.example.com; return 301 $$scheme:&#x2F;&#x2F;http:&#x2F;&#x2F;example.com$$request_uri;&#125;# redirect http:&#x2F;&#x2F;example.com to http:&#x2F;&#x2F;www.example.comserver &#123; server_name example.com; return 301 $$scheme:&#x2F;&#x2F;www.$$host$$request_uri;&#125; 综上，总结个小结论，即加或不加，必须要确定一种，相应地，另一种形式务必通过 301 重定向到前者的地址，这样就统一定向到唯一的主打域名上。另外，最好还是加上 www 前缀显得正式和规范。 在确定使用方案后，接下来就开始操刀域名的更换了，site域名虽然便宜，但接受度较差，在QQ及微信内均已无法直接访问，考虑到长远的发展，现已购入新的 COM 域名，稍后给出详细的更改过程。 本文系【转载】，感谢作者的总结！原作地址：知乎日报原文作者：Rio","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://www.cz5h.com/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"域名","slug":"域名","permalink":"https://www.cz5h.com/tags/%E5%9F%9F%E5%90%8D/"},{"name":"建站","slug":"建站","permalink":"https://www.cz5h.com/tags/%E5%BB%BA%E7%AB%99/"}]},{"title":"LEDE+iKuai双软路由整合","slug":"2019-7-28 LEDE+iKuai双软路由整合","date":"2019-07-27T22:00:00.000Z","updated":"2020-02-29T18:43:55.563Z","comments":true,"path":"article/fe88.html","link":"","permalink":"https://www.cz5h.com/article/fe88.html","excerpt":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考 首先明确LEDE 同 iKuai 可以说是目的相近的两种不同的路由系统，其均能够完成一些路由的基本功能，在这方面来说两者并无差别，但在扩展功能上，二者各有所长：","text":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考 首先明确LEDE 同 iKuai 可以说是目的相近的两种不同的路由系统，其均能够完成一些路由的基本功能，在这方面来说两者并无差别，但在扩展功能上，二者各有所长： LEDE 具有强大的插件扩展能力，作为 Openwrt 的发行版之一，其本身有着丰富的插件工具支持，同时也非常适合有能力的玩家自己编译自己的路由系统，好用的插件（出国留学+广告拦截）和强大易用的编译扩展能力是其特色； iKuai 的特色在于其出色稳定的 单线多拨 功能和 流控分流 功能，这使之更加贴近真正的路由器设备，从而被大多数玩家应用为主路由，通过单线多拨功能，可以完成 宽带叠加 的效果，对于有大带宽需求的用户来说是非常好的选择。 综上，这里使用 iKuai 作为主路由，使用 LEDE 作为旁路由 来搭建网络，整个路由系统以虚拟机形式安装在 ESXi 上，通过网卡直通，使虚拟机的带宽损失降到最低。 承载设备安装设备：YANLING-SKUL6, 2 × i5-7200U 2.5GHz, 16GB RAM, 500GB SSD设备接口：6 × LAN, 4 × USB, COM, HDMI系统版本：VMware-VMvisor-Installer-6.7.0-8169922.x86_64-DellEMC_Customized-A01其他需求：需首先安装好 ESXi软件镜像： iKuai 网址/选择ISO,64位， 直接下载文件 LEDE 网址/选择combined,vmdk， 直接下载文件 StarWind V2V Image Converter 网址， 直接下载文件 网口的映射需要明确以下几点： 我们将拥有两层路由，第一层是 iKuai 主路由，第二层是 LEDE 旁路由； 第一层路由需要具备 WAN 口，作为整个路由系统的网络来源； 第二层路由需要具备多个 LAN 口，作为路由系统的输出（连接设备）； 若拿一般路由器举例，WAN 即为头、LAN 为脚，则 主路由 iKuai 有头无脚，旁路由 LEDE 有脚无头； 结合：iKuai 虚拟出一只脚（LAN口），LEDE 把这只脚当做头（iKuai 的这个 LAN 口会被 LEDE 当做 WAN 口）； 在虚拟映射方面： iKuai 只需要单单映射一个网卡直通的网口（WAN），给 LEDE 使用的 LAN 口不用做网卡直通，因为双软路由对外作为一体使用，中间的映射只用逻辑的 虚拟网卡即可； LEDE 需要将 WAN 口之外剩余的网口全部做网卡直通，此处，剩余的各网口和一般路由器的 LAN 口没什么区别，直通只是减少损耗； iKuai 的安装 iKuai 的初始网络设置（黑框） iKuai 的系统配置（Web端） LEDE 镜像的合并处理 LEDE 在 ESXi 上的安装 LEDE 的初始网络配置（黑框） LEDE 路由配置（Web端） 最后：配置LEDE的网卡直通","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"组网","slug":"组网","permalink":"https://www.cz5h.com/tags/%E7%BB%84%E7%BD%91/"},{"name":"路由","slug":"路由","permalink":"https://www.cz5h.com/tags/%E8%B7%AF%E7%94%B1/"}]},{"title":"虚拟化系统ESXi的安装使用","slug":"2019-7-26 虚拟化系统ESXi的安装使用","date":"2019-07-25T22:00:00.000Z","updated":"2020-02-29T18:43:55.560Z","comments":true,"path":"article/9d39.html","link":"","permalink":"https://www.cz5h.com/article/9d39.html","excerpt":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考 写在前面动机在先前的文章中已经说的很清楚了，现在直接开干，下面 Step by Step 的详细介绍 ESXi 的安装以及 LEDE、iKuai、Win7 等虚拟机的安装及使用。","text":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考 写在前面动机在先前的文章中已经说的很清楚了，现在直接开干，下面 Step by Step 的详细介绍 ESXi 的安装以及 LEDE、iKuai、Win7 等虚拟机的安装及使用。 承载设备安装设备：YANLING-SKUL6, 2 × i5-7200U 2.5GHz, 16GB RAM, 500GB SSD设备接口：6 × LAN, 4 × USB, COM, HDMI系统版本：VMware-VMvisor-Installer-6.7.0-8169922.x86_64-DellEMC_Customized-A01其他需求：普通双头网线一根，优盘一个，显示器+HDMI线+鼠标+键盘（有线即可）软件镜像： ESXI 网址， 直接下载文件 叙述别名：安装设备下文称为 &gt; 软路由/工控机 配置设备下文称为 &gt; 主机/笔记本 角色关系：ESXI为工控机主系统，iKuai、LEDE、Win7、黑裙等均为其上的虚拟机 可能的驱动驱动：链接 工控机安 ESXI 系统的注意事项： BIOS设置优盘启动 PE系统内清除硬盘分区 ESXI在工控机安装配置就两步：配置静态地址（IP1+掩码+网关），检查插网线LAN口 上述操作需要显示器+键鼠，之后便不用 主机/笔记本配置的注意事项： 在笔记本用网线连接后，笔记本需配置：静态地址（IP2+掩码+网关） 在笔记本测试连接时：只需Ping通IP1即可，然后浏览器访问 IP1 6.7.0 (Build 8169922) 许可证：0U0QJ-FR1EP-KZQN9-J1C74-23P5R 关闭Web端的视觉效果以加速 详细安装步骤截图 首先制作 U盘 镜像 然后设置 BIOS U盘引导启动 之后进入系统安装界面，选择 ESXi 系统的所在盘符 一路 “同意” 下一步 安装完成后选择 reboot 重启 进入黄色的 ESXi 系统后台界面（输入密码） 进入管配置页，选择配置网络 此时，将网线插入 LAN1 - LAN6 中的一口，另一端插入电脑/笔记本的网口，接下来就用该笔记本来登录ESXi的WEB配置页面 然后在下面页面勾选“Connected”选项，用空格键在中括号内打叉 选择静态IP地址，设置完成后保存，这是关键步骤，Web端的访问需要IPv4的地址，同时笔记本电脑的网关要设置成该网关。 以上内容需要鼠标键盘和显示器的参与，但至此，机器上的配置已经完成，接下来，使用笔记本配置IPv4的选项，需要： 使用静态IPv4地址，随意指定，但不可与上一步中的地址个重复 使用固定的网关，即上一步中的网关 配置完成后，在浏览器访问上一步中ESXi的IPv4地址，即可进入Web后台页面 注意：此时只有工控机和笔记本网口通过一根网线相连，即这时的设备均无网络连接，也不需要网络连接，待到安装好路由系统后，将外部网线插入工控机的某网口（WAN口）并在路由系统内完成配置后方可顺利上网。 对 ESXi 的远程操控： 操控内容：虚拟机的安装、虚拟机的管理 方式一：Web 后台，简单粗暴，但有时会受浏览器影响； 方式二：Vmware 软件远程，用过PC端Vmware的可以说是非常人性化了； 方法三：vSphere，没用过，正规企业级应该用这个； 小试牛刀：安装Win7需要明确的是，ESXi的虚拟机安装流程及镜像要求和PC上的Vmware基本完全一样，即，一般的装机映像无论是Linux还是Windows都可以顺利安装，需要特别注意的有以下几点： 镜像首先要上传至ESXi的文件系统，一般给每个虚拟机单独建个文件夹最好，之后在安装虚拟机时直接选择上传好的镜像； 网卡的虚拟是否可以直通，对于Win7来说，具备Wifi功能可以很好的提升使用体验，研凌的主板允许用户自己搭载一粒mini-pcie接口的网卡（半高），但安装完ESXi后，一般的无线网卡是不被ESXi识别的，这时就要用到网卡直通，单独将该无线网卡配置到Win7的虚拟机中，这样在Win7中就可以顺利使用Wifi或发射热点。 显卡的直通，研凌板载的集显是 Intel Graphics 630 对于路由系统来说毫无用处，但对于HTPC来说则是非常重要的，由于我想利用Win7来做媒体中心，自然需要其解码能力，这时显卡直通可方便地完成该需求。 对于 Win7 + Plex Server：需注意，在基础版本会缺失很多功能，导致 Plex Server 无法启动，解决方法是安装 SP1（Service Package 1） 及以上的版本，即可顺利开启 Plex Server，之后，挂载影音资料的磁盘，在 Plex 中扫描即可，和在群晖中的使用类似。 对于 Win7 的远程操控，方法比较多样，主要有： Windows 远程桌面：使用体验最好 TeamViewer：比较臃肿 AnyDesk：最轻量便捷 VNC：通用性强 本文详细的说明了 ESXi 的安装过程、注意事项及简单应用，不过，完成上述过程才只是使用了研凌工控机的一小部分功能，真正发挥功效的软路由部分还未做介绍，有关于 LEDE + iKuai 的双软路由搭建将在今后同大家见面。","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"组网","slug":"组网","permalink":"https://www.cz5h.com/tags/%E7%BB%84%E7%BD%91/"},{"name":"路由","slug":"路由","permalink":"https://www.cz5h.com/tags/%E8%B7%AF%E7%94%B1/"},{"name":"HTPC","slug":"HTPC","permalink":"https://www.cz5h.com/tags/HTPC/"}]},{"title":"如何组建高效的家庭网络","slug":"2019-7-25 如何组建高效的家庭网络","date":"2019-07-24T22:00:00.000Z","updated":"2020-02-29T18:43:55.561Z","comments":true,"path":"article/13d5.html","link":"","permalink":"https://www.cz5h.com/article/13d5.html","excerpt":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考","text":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考 在“为什么家里要有公网IP”一文中已经简要的说明了一下网络环境对于NAS的重要性，这里专门就此问题展开讨论。 首要任务首先要确定自己宽带是否具备公网IP（已办用户），一般情况下，按网络的接入情况，可将家庭网路（宽带业务）分为三种： 入户即为局域网IP 区分：路由器配置页面中，WAN口IP地址是局域网IP，例如192.168开头、10.XX开头等等 判定：直接在百度搜索上述IP地址，公网IP会有详细的运营商等信息，局域网IP的搜索结果则没有有效信息；或者电询运营商。 特点：向外访问一切正常，但向内访问无法完成，即该类型的IP无法直接在公网访问，需通过FRP（内网穿透）来中转连接，无法直连。 入户为专线（静态公网IP） 区分：路由器WAN口的地址非局域网特征的IP 判定：直接搜索会有明确的运行商名称、及大概的地理位置 特点：公网环境向内访问可实现，直接访问IP即可，由于IP固定，所以只需简单的采用DNS映射就可以满足绝大多数场景 入户为动态公网IP 区分：同上 判定：同上 特点：不定期的动态变更IP地址，但在非变更期间内，其所有特征同静态公网IP一致，但由于IP动态更新，所以通过IP直连的方式不可取（单次可直连但变更后无法追踪），所以一般需要采用DDNS（动态域名服务）来绑定动态IP，其和DNS的区别就是一个解析静态IP一个解析动态IP，对用户使用来说均是透明的，做完映射解析后，在公网环境中访问域名即可，动态变化的IP由DDNS负责动态绑定。 次要任务公网IP是第一重要的，这是硬性条件，除此之外，为了获得更好的使用体验，还还需考虑： 上行、下行的带宽大小 与所办宽带的业务直接相关，对于我们将要使用的若干功能（在前一篇文中提及的），都要有较大下行带宽的前提下，具备足够的上行带宽，个人意见上行 &gt;20Mbps 下行 &gt;50Mbps 方可流畅使用各功能。 路由器中转后的带宽损失 原因：随着带宽的变大，一般低端路由的处理芯片便会出现吃力的情况，加之长期高温会进一步降低处理器的处理性能，所以建议 200M 带宽以上的用户使用中高端路由，或使用处理性能更强的处理器，目前多是直接使用研凌等品牌的工控机或者Intel的NUC微型计算机来做软路由。 判定：找到良好的下载资源（不要仅看网速测试），用迅雷等软件下载进行实测，看一段时间内的下载速度的波动和速度值是否匹配带宽。 改善：如若确定路由器是瓶颈，那就需要升级路由器，中高端硬路由性价比没什么优势，推荐使用软路由，即将工控机装入路由系统，从而使其充当路由器，一般软路由主板的处理器性能要比中端路由器强许多，例如J1900或更强大的i5 7200U，要比 网件R7000 的博通BCM4709A双核处理器若干倍了。 对接配置在确认具备公网IP的环境之后，要想将内部的原有服务及应用暴露在公网内，还需两步设置，即 DDNS 和 端口映射，二者缺一不可（对于动态公网IP来说）。 DDNS一般直接去阿里云就有DDNS服务，但其实群晖也好、网件的路由器也好，都已经提供了相应的 DDNS 服务，例如群晖内的“外部访问”页面可以配置 DDNS，对于硬路由 R7000（实际是梅林固件的作用）来说，其也提供 DDNS 服务，实测后者较好，目前使用的是 xxx.asuscomm.com，配置完之后，对公网环境来说，只需访问该域名就可以访问到真实的物理IP，但像群晖提供的各种功能均在路由器下级，正常以“内网IP+端口号”的形式进行访问，现在 xxx.asuscomm.com 仅仅访问到了路由器，之后还需要路由器的端口映射，来指导具体的请求流向。 端口映射正常的端口有 0-65535 这么多，除了已被占用的端口和保留端口，以及被封禁的 80/443 端口，还有上万个自定义端口可以使用，即路由器可以事先约定：凡是公网的用户访问 xxx.asuscomm.com:5000，那么就转到 群晖内网IP:5000，默认DSM的端口是5000，即原来在内网环境中通过 群晖内网IP:5000 来访问DSM，但现在可以在公网环境中通过 xxx.asuscomm.com:5000 来访问，唯一的不同就是多了一步“约定”，而这个约定，就是一张端口映射表，将向内的访问分发到不同的内部网址+端口号，从而完成内部应用的“暴露”。注意：通常路由器的端口映射部分还有个 DMZ主机 选项，可以将一台内部电脑的全部端口暴露在公网内，在某些情况下适用，但在家用环境中这是十分危险的，可以给恶意攻击者当做跳板进而攻击内网设备，所以尽量适用手动配置端口映射，避免使用 DMS主机。 软路由的作用及使用有机会单独成文介绍，下面简要说一下软路由在整个家庭网络环境中的作用。（注意，当软路由接管整个家庭网络之后，原先的硬路由一般会充当 AP 进行使用，只负担 WIFI 功能，所以对应的 DDNS 和 端口映射的位置要调整，使安装情况来看是在 LEDE 还是 iKuai 中配置） 基础：如前所示，软路由的使用者一般是不希望路由成为整个网络环境的瓶颈，而事实是软路由确实做得不错。开源的路由系统Openwrt目前已经拥有了活跃的社区，衍生出了若干版本，LEDE便是其中之一；此外，还有商业化十分成功的iKuai，以及海蜘蛛、高恪等等。目前主要使用LEDE+iKuai。 LEDE：作为Openwrt的发行版之一，其拥有丰富的插件系统，可以允许用户安装ad屏蔽、出国留学等插件，从而路由器端进行过滤或代理，十分强大有效；其他还有许多功能，但出国留学是其特色和优势。 iKuai：商业化公司提供的免费版本，UI十分精美，负载均衡和单线多拨是其特色和优势。单线多拨需要特别说明，其可实现宽带叠加的效果（宽带运营商未屏蔽该操作的前提下），例如 200M 在三拨的情况下理论可达 600Mbps 的带宽。 扩展：诸如 i5 7200U 这样的路由器，直接安装 LEDE 这样的路由系统是在是对处理能力的浪费，所以，为了具有更多的可扩展性，通常是在工控机上安装 ESXi （或KVM等其他虚拟化系统）系统，各路由系统退化一级，作为ESXi的一个虚拟机运行，如此，便可以有更佳灵活的路由组织方式：双软路由 双软路由 概念：由于目前各软路由系统分属不同的社群或公司，各自独立的发展导致目前不存在完美涵盖各种功能的路由系统，各系统各有所长，所以，在使用时可以结合使用，由于采用虚拟机方式安装，对于 主路由 和 旁路由 的物理区分不复存在，有的只是逻辑上的主次，有关于物理网口和虚拟网口的映射，以后单独讲解。需要注意的是，网卡直通对于虚拟机安装软路由系统是非常重要的，直通后基本不存在带宽损失。 搭配：本着取长补短的原则，目前采用的是比较成熟的方案之一，即使用 LEDE 做旁路路由，为的是其留学及禁广告插件；使用 iKuai 做主路由，为的是其 单线多拨 及可能用到的流控功能。虽然双软路由配置繁琐，但配置完成后基本无需改动，且能最大化的利用各系统的优势，提升网络使用体验。 其他虚拟机本质上，ESXi同桌面版的VMware是一个东西，即都可以虚拟出各种设备来安装各镜像文件，所以，在ESXi中支持安装Windows、Linux、Mac等等系统，且对于7200U来说，其本身双路由仍然是性能过剩的，这就允许我们可以安装更多的虚拟机来满足我们的需求，唯一需要注意的就是工控机上的内存要大一些，一般16G比较好。所以，重要的不是能不能安装，而是装了有什么用： Windows7：安装一个纯净版Win7可以满足大部分 挂机需求，例如直接迅雷挂机下载，或者是淘宝助手等需要挂机运行的软件，此外，Win7用作HTPC也不错，搭配 PLEX Server 可以摇身一变变为影音中心，从而充分利用7200U的性能进行编解码。总之，这就相当于一个免费的物理云主机，且硬盘够大、带宽够大、无流量限制，想想阿里云的相同配置的独立云主机，估计要 &gt; 1000 RMB/年 了。 Ubuntu：作为Linux系统的一种，其桌面操作性十分舒适，如果性能捉急，可以安装CentOS等Linux系统。对于Linux系统来说，可做的事情也非常多，比如搭建Gitlab、Web环境、搭建图床、编译软件等等作用，同样地，这也算一个配置超级豪华的独立云主机，如果买阿里云的，那估计要吃土了。 其他：目前只装了上述虚拟机，下一步想探索的是 Xmbcbuntu 系统，作为Kodi的主系统，不知其流媒体处理能力如何，准备安装一试。 附：当前设备清单 安装设备：YANLING-SKUL6, 2 × i5-7200U 2.5GHz, 16GB RAM, 500GB SSD 设备接口：6 × LAN, 4 × USB, COM, HDMI 系统版本：（ESXi）VMware-VMvisor-Installer-6.7.0-8169922.x86_64-DellEMC_Customized-A01 其他设施：普通双头网线一根，优盘一个，显示器+HDMI线+鼠标+键盘（有线即可） 系统镜像： ESXI 网址， 文件 iKuai 网址/选择ISO,64位， 文件 LEDE 网址/选择combined,vmdk， 文件 Image Converter 网址， 文件 关于 ESXi，LEDE + iKuai，以及 Win7 的安装及注意事项，会在今后单独进行详细讲解。","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"组网","slug":"组网","permalink":"https://www.cz5h.com/tags/%E7%BB%84%E7%BD%91/"},{"name":"路由","slug":"路由","permalink":"https://www.cz5h.com/tags/%E8%B7%AF%E7%94%B1/"},{"name":"Synology","slug":"Synology","permalink":"https://www.cz5h.com/tags/Synology/"}]},{"title":"功能强大的NAS云存储","slug":"2019-7-22 功能强大的NAS云存储","date":"2019-07-21T22:00:00.000Z","updated":"2020-02-29T18:43:55.557Z","comments":true,"path":"article/8d84.html","link":"","permalink":"https://www.cz5h.com/article/8d84.html","excerpt":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考","text":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考 初识NASNAS，Network Attached Storage，直译是网络附属存储，但实际上就是一种网络存储器，一般常见的存储设备如3.5寸的机械硬盘（用于台式机）、2.5寸的机械硬盘（用于笔记本或移动硬盘）、固态硬盘、U盘等，各种类型的硬盘接口规格都不相同，有SATA、mSATA、PCI-E、mini-PCIE等等，目前都支持相互转接。 需要强调的是，上述硬盘（存储器）通过硬件插槽或者传输线直接连到电脑主板、然后开机识别出硬盘空间这种流程，是一般存储器的使用方式，其要求硬盘和主机不能存在地理隔离（即便使用传输线也有距离限制），且这种情况下主机向外提供数据只能在上层应用中实现，这限制了设备的可扩展性和灵活性。网络存储器应运而生，其发展速度非常迅速，后期又伴随着硬件虚拟化技术的发展，衍生出了若干种网络传输协议（NFS、iSCSI等），对于家用NAS来说，主要使用NFS（还有SMB）。 为了提供相关服务（协议），NAS通常具备主控（主板）模块，其不再是单纯的硬盘堆叠（硬盘盒或硬盘架），而是对硬盘空间的有效组织，目前市面上存在包括FreeNAS（开源）和Synology的DSM（群晖的闭源系统）在内的多种NAS系统，商业化比较成功的厂家有Synology（群晖）、QNAP（威联通）等，其产品核心价值在于 主板 + 系统，硬件层面没有限制，故而允许通过第三方主板硬件刷入群晖系统来使用，通常这种机器称为“黑群晖”，在某些功能上有许多限制，后续可以通过相关操作进行“洗白”，这样可以用更强的硬件设备搭载群晖的系统从而获得更好的性价比（白群晖价格有相当一部分摊到了系统上），当然这是不正确的、盗版的行为，商用会被追究法律责任。支持正版从我做起。 NAS的特点 有主（控）板，带有内存插槽、网卡网口、CPU、显卡（一般是集显）、USB等等 有若干盘位，一般最少是 2 盘位，插入3.5寸机械硬盘 基于多种网络协议，包括NFS、SMB等等，一根网线即可让任何位置的电脑通过网络访问硬盘空间 提供上层应用，包括多媒体的编码解码，进而可向外提供丰富的流媒体资源 NAS可能的日常使用场景多媒体 视频：NAS端（异地）下载，本地观看。例如身在学校内，每月流量有限，NAS在家中，家中宽带无限制，这时通过远程下载，可以在学校内观看经过编码后的视频流，一是借助家里的大带宽的高网速加速下载过程，二可以通过编解码后的多媒体资源会占用更小的空间（使用更少流量），而实质上的观看体验肉眼基本不可见。上述内容实现需要NAS端的带宽有足够大的上行，一般＞20Mbps即可。 音频：可用作珍贵资源保存，同时也提供完备的音频在线解码服务，可以用类似QQ音乐的方式来访问NAS中的音频文件。 图片：这是NAS的重要功能之一，也是群晖的重要功能之一，为此官方套件有两个是针对图片管理的。从我们使用智能手机、数码相机的第一天开始，我们的手机里、相机里就存满了我们的照片，如何管理是个大问题，一般我们直接存在电脑的某硬盘里就可以，但如果我们想随时随地翻看过去某一天的某张照片、或是在公网环境浏览我们的全部照片，那一般的保存就不够用了，需要专门的照片管理软件，在这里，群晖提供了丰富的照片管理软件，不仅可以很好的对图片进行重新组织；同时，也提供照片的对外发布服务。 电子书：附加功能。Kindle重度用户会比较感兴趣，一般是依靠第三方的docker套件完成。 网盘系统或者叫私有网盘（私有云）。目前，群晖可以快速搭建可道云（KodExplorer）基础版，基础版可道云已完全能够满足个人使用，使用上同115等云盘基本一样，只是所有文件均存储在NAS中，故名“私有云”。上述内容实现需要NAS端下行带宽要大，一般需要50Mbps以上。 上述对应的实现方式需注意：基于群晖的各功能可由以下几种方式提供： 官方随机套件（系统安装后的套件中心内默认列表） 附加套件（同样在套件中心内，但需要手动添加第三方源，之后才能扫描到套件列表） docker应用（强大的功能！可搜素到99%的docker镜像并安装，但需注意要专门为群晖做过适配的镜像才能正常使用：一般必须得配置端口映射+存储空间的映射） Web Station自建应用（Web Station套件实际上是群晖提供的具有php + apache server/nginx + mysql 建站环境的套件，显而易见，其可以支持自定义的网站搭建。WordPress等Web应用均可搭建） Virtual Machine（利用群晖的虚拟机套件可安装Win7、XP等系统，作用等同于VMware或VritualBox） 多媒体 视频： 下载： BT资源下载：Download Station（官方套件）、thunder-xware（docker应用）、Transmission（docker应用）、qBittorrent（docker应用）。（一般用Download Station下载即可，若无下载速度则可能是种子原因） 百度网盘资源下载：baidupcs（docker应用）、CloudSync（官方同步套件，利用指定文件夹的同步进行下载） 人人影视：专职美剧英剧的字幕翻译、压制及发布，群晖上可使用 rrshare（docker应用） 串流：可通过群晖自带的 Video Station 或者第三方PLEX（Server）实现。 客户端：手机应用（DS Video、Plex），电脑使用（DSV只能用Web页面访问，而Plex在Web之外还有独立的桌面应用） 音频： 下载：除了以文件形式的下载外（BT下载或百度网盘下载），还未发现专门的音乐下载应用，目前采用的方式是电脑用QQ音乐、网易云音乐等下载音乐文件，然后上传到群晖NAS中，一般音乐使用场景较少，不过可以当做稀有资源的备份（越来越多的歌曲正在下架） 串流：使用官方套件 Audio Station 或者第三方PLEX实现 客户端：手机应用（DS Audio、PLEX），电脑使用（对应的Web页面） 图片（照片）： 一般由本地上传，系个人拍照、摄影、截图等等 组织管理： Photo Station，官方套件，提供时间线、地图坐标等视图，占用资源较小，功能较单一； Moments，官方套件，已被集成在Drive（官方网盘套件）中，提供主题分析、地点分类等功能，占用资源较多，功能丰富，若使用Drive则推荐使用Moments，否则可只用Photo Station PLEX，主要用于媒体资源管理，同样支持照片的管理 客户端：前两者对应官方套件，后者是PLEX客户端，三者均可通过Web页面访问 电子书（Kindle）： 最近购入了Kindle，有必要说下电子书的下载，采用 my-calibre-webserver（docker应用）可自建一个图书管理系统，其带有电子书推送功能，可以将指定书目推送到Kindle上。 网盘系统 Drive：群晖官方套件，这是同其他官方套件（Moments、CloudSync等）高度耦合的一款网盘服务套件，其可以实现文件的上传、下载、文档的多人协作、版本控制等等功能，功能丰富，但占用资源较大，版本控制也容易产生大量垃圾文件，因此我并没有使用该套件（对应的我也没用Moments）。 KodExplorer：可道云（第三方）套件，该套件为用户搭建了一个基础版的可道云服务，可道云在使用上同一般网盘无异，同时提供简洁的Kod云手机APP应用，本质上是利用Web Station搭建的一个Web应用，故而相对独立，占用资源也较少，因此我使用了该套件搭建网盘系统。 附：我的应用下面是目前我使用的全部套件/应用 → 以及对应的手机端应用 远程管理 DSM → Web WebSSH → Web 文本（简单上传下载） File Station → DS File APP FTP → WinSCP（PC） 音乐视频（解码、在线播放） Video Station → DS Video APP Audio Station → DS Audio APP PLEX Server → PLEX APP 图片照片（管理、重新组织） Photo Station → DS Photo APP Moments → Moments APP（我没用） 上层应用 文件的上传下载/云盘映射 Webdev → 服务 Cloud Sync → DSM内操作 KodExplore → KodExplore APP 电影的下载 迅雷远程 → Web 人人Web版 → Web 百度网盘Web版 → Web Download Station → DS Get APP 电子书的下载 kindle图书管理Web版 → Web 网站环境 Nginx → 服务 Apache Server → 服务 Tomcat → 服务 PHP 5-7 → 服务 扩展容器 第三方套件源 Virtual Machine → DSM内操作 Docker → DSM内操作 承载以上内容的设备的各项基本信息： 产品型号：DS418play CPU：Intel Celeron J3355, 2GHz × 2 内存：6 GB DSM版本：DSM 6.2.1-23824 Update 6 弄清楚上述应用的使用方式后，还有最最最最重要的一点，即外网访问。对于绝大多数家庭用户，其NAS均放在路由器之后，即NAS处于家庭子网（局域网）内，正常是无法在公网环境中直接访问到的。如何做？自然地想到公网环境应该可以直接访问到家中的路由器，但真的访问的到吗？关于家庭网络的若干问题，请参见下一篇文章内容。","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"组网","slug":"组网","permalink":"https://www.cz5h.com/tags/%E7%BB%84%E7%BD%91/"},{"name":"路由","slug":"路由","permalink":"https://www.cz5h.com/tags/%E8%B7%AF%E7%94%B1/"},{"name":"Synology","slug":"Synology","permalink":"https://www.cz5h.com/tags/Synology/"},{"name":"HTPC","slug":"HTPC","permalink":"https://www.cz5h.com/tags/HTPC/"}]},{"title":"前端快速入门之概述","slug":"2019-7-15 前端快速入门之概述","date":"2019-07-14T22:00:00.000Z","updated":"2020-02-29T18:43:55.558Z","comments":true,"path":"article/f8f5.html","link":"","permalink":"https://www.cz5h.com/article/f8f5.html","excerpt":"以下是对（前端）可视化工作的并不系统的总结，新手向，主要是想说一下前端如何组成、功能如何实现、资源如何请求，进而说到数据如何显示，并在最后列举了一些十分重要的参考资料（非常重要）。","text":"以下是对（前端）可视化工作的并不系统的总结，新手向，主要是想说一下前端如何组成、功能如何实现、资源如何请求，进而说到数据如何显示，并在最后列举了一些十分重要的参考资料（非常重要）。 前言：从百度说起案例 点击百度搜索框显示出搜索结果 涉及前端的三大要素，从初级到高级的应用都可以由其完成，三者即： HTML(5)// 页面所有呈现元素（按钮、输入框、图片…）的“附着点”，所有页面可见元素都有对应的html标签，而动态交互的事件（JS完成）实际就是绑定在某些html标签上，例如按钮的点击。 CSS// 页面呈现元素的所有样式均可以由CSS进行控制，即文字的颜色、字号、间距，区块的排列等均由CSS指定，本质上仍然是对html标签的控制，只有该html标签具有某一属性，这时才能通过CSS对其进行控制，例如，table标签具有width属性，所以可以通过CSS对width进行赋值，来达到控制table宽度的目的。 JavaScript// 页面所有的（动态）事件，均由JavaScript绑定到html标签上，并由JavaScript完成整个交互动作的执行，包括鼠标事件、前后端的请求事件等等。 分析上述例子中的词语： 点击：由JavaScript完成，会触发一个request请求 百度搜索框：本质是input标签，通过CSS样式修饰为用户所见的样子 显示搜索结果：先前的请求到达后台后，经过处理返回查询结果（response），注意此刻的结果一般是一些数据（字符串），并非带样式（CSS）的html标签，所以还需要进行转换，这个将数据转换为html代码的过程仍然由JavaScript来完成。 已知的发展（局部） HTML -&gt; HTML5 CSS -&gt; Sass, Less, Stylus JavaScript -&gt; jQuery -&gt; Vue/Node -&gt; 已不单单是前端语言 如何学习（路线图）先易后难；先实践，后理论； 三要素的简单组合（易） 学习HTML的块/div、段落/p、表格/table、按钮/button等基本标签，并熟悉其具有的属性 学习CSS的盒模型，理解边距/padding、间隔/margin、浮动方式/float等的控制命令 学习原生JavaScript的选择器、事件绑定、资源请求等（之后） 增强版本 添加资源请求（数据的读取） form ajax(jQuery) get(jQuery) getjson(jQuery) websocket 结合IDE进行开发 WebStorm/IDEA 数据的展示图形绘制 Canvas//画布标签/容器，显示元素（点线面）的载体，本身有构造点线面的语法规则，嵌入HTML内需要放在Canvas标签内 SVG//本身是一种可视标签，可以直接嵌入在HTML内 绘图库/引擎 D3.js //高自定义图形 Echats.js //图表+地图(baidu地图) Highcharts.js //图表绘制 Tree.js(WebGL,3D) //3d绘制引擎 Mapbox.js(Map) //专注地图 后端操作 Node.js fs //文件操作 child_process //线程管理 body-parser //请求解析 nodejs-websocket //ws协议 express //路由请求拦截 其他细节内容 前后端的跨域资源访问 前端的异步执行顺序控制主要体现在ajax请求方式（如$.ajax()）和同级代码之间的执行先后顺序，一般可将ajax请求方式设为同步执行即可解决，在包含复杂的数据请求时尤其要注意这一隐含问题。 浏览器debug技巧（一般使用） F12/network 看加载的请求 F12/console 看加载出现的异常（info、error、warning） F12/Elements/Style 看样式（盒模型） Chrome调试插件（生产工具） 测试WebAPI/请求 Restlet、Postman 清缓存刷新 Clearcache 其他 网上应用商店 前段环境搭建 Tomacat -&gt; （webapp文件夹内） Apache -&gt; （www文件夹内） IDEA -&gt; （内嵌服务器，一般是tomcat） WebStorm -&gt; （同上） Node -&gt; （http-server等） 请求资源的方式（常见的） servlet方式//前端发送请求url，后端拦截匹配对应后台处理，完成后返回结果（全局、需刷新） ajax方式//流程同上，但返回结果被控制在提交请求的ajax域内（局部结果，无刷新），可在不全局刷新页面的情况下对局部内容进行更新。其有各种实现方式：原生方式、$.ajax()、$.get()、$.getjson()等等。 websocket方式//socket作为后端代码常用的传输手段，其实是一种实现了给定规范（n次握手）的代码接口 优点//通信双方一直保持连接（长连接），在连通情况下双方可以任意的收发消息 实现方式//前端为固定的调用方式，后端有多种实现形式（Java、Node），例如Node的websocket 意义//真正意义上的长连接，在此之前一般只能通过ajax轮询来实现；多次轮询只是多个请求返回各时间点的结果（前端主动发起，后台被动相应），长连接则是建立连接后，后台主动推送（生产出一组数据就发送一组），前端被动接收。符合朴素的设计直觉（前端专职显示/View，不应存在过多请求），优化整体前后端响应性能。 应用场景（刷新率高）//前端仪表盘、实时折线等、图形的连续变化等 待补充…相关资料引导资源 【W3Cschool前端路线图】&gt;&gt;LINK&lt;&lt;非常全面的知识汇总，基础弱的可以详细看下这个 【菜鸟教程的WEB开发路线图】&gt;&gt;LINK&lt;&lt;这里是覆盖前后端的学习路线，思维导图形式，可以了解下有个大致概念 【可视化学习路线图】 &gt;&gt;LINK&lt;&lt; 强力推荐，内容不多，但可以说明白可视化到底在做什么，以及如何做 【JS的DOM操作】&gt;&gt;LINK&lt;&lt; 【JS的选择器】&gt;&gt;LINK&lt;&lt; 【JS的AJAX请求】&gt;&gt;LINK&lt;&lt;看完这几个JavaScript的操作就会一大半了 【Node路线】&gt;&gt;LINK&lt;&lt;Node还是非常强大的，夸张一点可以基本实现现有后台的所有功能 知识点在线查询+在线练习/测试 【W3CSCHOOL】&gt;&gt;LINK&lt;&lt; 【菜鸟教程】&gt;&gt;LINK&lt;&lt; 快速测试 【引用库】&gt;&gt;LINK&lt;&lt;有网络资源就引用网络资源，一些大库可以在上述网站搜索，直接引用网络资源，省去本地保存的多余操作 【构建Node环境】&gt;&gt;LINK&lt;&lt;需要后端支持时可以用node迅速进行搭建，使用上述网站查询相应的包 【官方文档】使用某JS库（例如D3、Echarts）的最好方法没有之一，就是查看官方文档，基本能涵盖八成所需内容 debug常用网站 【csdn、cnblog、github issues】 【V2EX】&gt;&gt;LINK&lt;&lt; 【StackOverflow】&gt;&gt;LINK&lt;&lt; 【OutOfMemory】&gt;&gt;LINK&lt;&lt; 【SegmentFault】&gt;&gt;LINK&lt;&lt;","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"}]},{"title":"数据可视化学习路线","slug":"2019-7-11 数据可视化学习路线","date":"2019-07-10T22:00:00.000Z","updated":"2020-02-29T18:43:55.556Z","comments":true,"path":"article/69dc.html","link":"","permalink":"https://www.cz5h.com/article/69dc.html","excerpt":"写在前面有幸看到了这篇关于数据可视化学习的指导文章，由于原作链接访问异常，只得从百度快照中看到原文，所以这里搬运过来，特此声明本文系【转载】，在此感谢原作者，以下为原文正文（略有删减）。","text":"写在前面有幸看到了这篇关于数据可视化学习的指导文章，由于原作链接访问异常，只得从百度快照中看到原文，所以这里搬运过来，特此声明本文系【转载】，在此感谢原作者，以下为原文正文（略有删减）。 原作者: 张迪 2018年01月03日 于 张迪的blog 发表原文链接 本文将从可视化设计、数据分析、技术实现三个角度来阐述要学习的内容。 （*图中的chinaviz应为chinavis） 可视化设计首先是可视化设计。可视化设计是指数据可视化过程中的表达方式，是我们最终设计的软件的灵魂。然而这个灵魂不可能一开始就得到。需要我们在分析前人工作的基础上，通过不断模仿和摸索才能获得。具体的说，可分为如下几步进行： 收集资料：从chinavis官网上收集所有挑战赛获奖作品、通过百度学术、知网等平台上下载相关论文。 总结：写多篇竞品分析类文章：总结挑战赛获奖作品中所用的可视化设计、分析方法和采用的技术。这些可视化设计、分析方法和技术必须能找到对应的论文。 提炼：以思维导图的形式归纳这几篇文章的核心内容。并得到：哪些可视化设计、分析方法和技术是必须的，哪些是可选的，哪些是可以改进的。 分析探讨：将提炼出的结果，跟其他同学和老师进行探讨。从而形成设计方案。 形成设计方案：将设计方案，用图片+文字的方式加以描述。 数据分析然而可视化设计不能独自进行，在总结、提炼、分析探讨的过程中，必须不断地与数据分析过程结合，从而验证方法的有效性。数据分析与可视分析通常是循环进行，互为补充，具体分为如下几个步骤： 收集资料：从chinavis官网上下载数据集，以及从其他数据源上获取数据。 数据概览：要从数据集的描述信息中了解大致内容（例如，字段的大致内容和数据类型，表之间的关联关系等）然后确保可以通过商业数据可视化软件如tableau、Excel、Gephi、cytoscape将之打开。 经典复现：试着用商业数据可视化软件复现获奖作品和经典论文中的可视化设计。从而确认 哪些可视化设计、分析方法和技术是必须的，哪些是可选的，哪些是可以改进的。 摸索试错：尝试用依赖自己的想法，使用商业数据可视化软件展示数据中的规律。从而帮助形成可视化设计方案。 实践检验：把最后实现的可视分析软件，套用真实数据进行检验，确保其有可用性。 技术实现为了支撑以上的可视化设计与数据分析，还需要学生掌握一些基础技术。这些基础技术包含两大类，其一是商业可视化软件的使用，这个比较简单，要求2周内可以实现示例（例如tableau可以轻松地用它是现实教程示例）。其二是web开发技术。这个比较复杂。每个同学的侧重点各有不同： 编程能力比较好的同学：直接实现web应用的基础框架。通过这个基础框架实现一个API，该API可以从数据库读取数据，然后封装成Json格式发送到前端即可。API的设计风格建议restful。至于具体采用什么技术栈，看自己的特长而定。 对于编程能力比较弱的同学：先学习web前端基础，要求必须学会使用HTML/CSS/JQuery和Bootstrap构建网页。为了保证学习进度和成效，推荐去Github上追这个最火的编程学习项目：freecodecamp。这个项目课程设置得很平滑，既有循序渐进的基础知识，又有自由发挥的小项目，一边实践一边学习，更打游戏闯关一样的体验，比看教学视频、w3cshool之类强的多。 此外，所有同学都要学习前端数据可视化编程的知识，具体的说就是要掌握百度echarts类库的实现一般可视化图例的方法，和使用D3js类库实现自定义可视化图例的方法。对于某些选择了特殊题目的同学来说，webGL的threejs类库也是必须了解的。 考虑可视化整套流程下来要学习的内容较多、较杂，因此定时的知识总结和强力的工作监督必不可少。故而最好在学习过程中每周总结一篇博客，要求有图有真相。内容可以是最近学习的软件或JS库的使用方法总结、技术开发总结、数据分析说明、可视化设计案例总结等等。","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"}]},{"title":"为什么家里要有公网IP","slug":"2019-7-4 为什么家里要有公网IP","date":"2019-07-03T22:00:00.000Z","updated":"2020-02-29T18:43:55.564Z","comments":true,"path":"article/abd.html","link":"","permalink":"https://www.cz5h.com/article/abd.html","excerpt":"最近家里换了宽带，并且在我的强烈要求下选择了能提供公网IP的联通宽带，当然这里是动态公网IP，期间，包括光猫的桥接+路由器拨号，都已经做完，基本上到这里为止家里的网络环境就是外部可访问的了，从此可以抛弃内网穿透，直连站起来重新做人了。为什么要折腾外部访问呢，因为我把群晖放家里了，自然是想随时随地访问一下子的，现在有了公网IP，虽然是动态的，但只需在群晖的DDNS页面配置下，就可以使用域名进行访问了，非常的方便和舒服，速度快的飞起来。欢迎访问ZoneCloud -&gt; http://asus.myds.me:5000/","text":"最近家里换了宽带，并且在我的强烈要求下选择了能提供公网IP的联通宽带，当然这里是动态公网IP，期间，包括光猫的桥接+路由器拨号，都已经做完，基本上到这里为止家里的网络环境就是外部可访问的了，从此可以抛弃内网穿透，直连站起来重新做人了。为什么要折腾外部访问呢，因为我把群晖放家里了，自然是想随时随地访问一下子的，现在有了公网IP，虽然是动态的，但只需在群晖的DDNS页面配置下，就可以使用域名进行访问了，非常的方便和舒服，速度快的飞起来。欢迎访问ZoneCloud -&gt; http://asus.myds.me:5000/ 下面就安装宽带的注意事项做一下记录： 宽带商的选择一般只有三种选择，联通、移动和电信，其中移动手中有的IP资源是最少的，这是事实，但对于宽带是否提供（动态）公网IP，这在不同地域或有差别，但移动肯定是最不靠谱的一家；剩下的两家没太听到过槽点，所以酌情看套餐办理吧。 安装过程的操作一般装宽带是得安装师傅本身到家里来装的，且现在基本是光纤入户，所以“光猫”是必不可少的一个硬件，主要完成的是光信号转换的任务（部分高端路由器也可以完成光信号转换）。一般而言，正常的上网流程是光纤入户后，在光猫处随机完成拨号上网，这种方式的缺点是： 光猫作为家庭网络同外部网络沟通的最后一层，端口转发工作应在最外层即这一层完成，在此处，光猫得完成端口转发的工作，但要命的是光猫并不是路由器，无法替代路由工作，家里的设备总归要通过路由器汇集。这时问题来了，光猫拨号的前提下，路由器变为了二级路由，端口映射会变得非常复杂，DMZ+端口映射或许可以解决问题，但并不完美。 真正方便快捷的方式是在安装时让师傅改光猫为桥接，这样光猫的功能退化为单一的光信号转换，而拨号、端口映射等任务全部交给路由器完成（稍高端的路由器均可支持），此时的网络结构，虽然光猫仍然是最前沿的阵地，但实际对网路信号而言路由器成了排头兵，所有的内外交换数据均通过路由器协调、中转，光猫只是开闸放水不加以任何干涉。 此时，在路由器上设置的端口转发，会直接作用到IP上，即整个环境中只有一级路由，非常清晰和方便，类似群晖这类存储系统的外部访问，只需在路由上配置一连串的端口映射，即可以通过IP+端口的形式在公网环境中进行访问了。 关于动态公网IP的变动固定公网IP的宽带业务一般叫“专线”，属于企业宽带的范畴，动辄月付几百大洋，和普通家用宽带年付几百块相比要贵的多得多，所以，这里讨论的都是基于动态公网IP的各种操作。 顾名思义，动态公网IP的IP地址是会变的，但变前和变后均为公网IP，变动的时机往往是重新拨号上网后，这和路由器的动态分配IP是一样的，每次断开重连都会回收原有IP并分配新的IP，但在分配后，设备会一直独占此IP，因此如果不是经常断电重启，那么动态公网IP也可以看作是短时的固定IP。 但是，可能变化的IP对于外部访问是非常困扰的，例如你跟朋友刚分享了自己看的电影，结果下午你朋友通过IP就访问不到了（可能是经历了路由器重新拨号），杜绝这一现象的手段就是，将多变的IP绑定到单一固定的域名上，这样，在访问该域名时，即会解析到当前的IP（IP同域名之间的绑定关系会一直更新），整个上述过程即叫做DDNS，即动态域名解析。 相比于域名解析，这里的动态体现在域名映射的IP地址不固定，而是变化的，而对于域名同IP的映射的更新，是群晖提供的DDNS功能中的一部分。12即，除了群晖（必须为白群）提空的DDS服务之外，使用阿里云等DNS域名解析服务也能完成此功能，只是可能需要自己来实现“动态绑定”的功能。 写在最后暂时写到这里，换了宽带只是万里长征第一步，只是，原来可能通过FRP内网穿透来访问的东西（非常慢），现在直接访问就行了，现在一步顶原来十步，果然是科技改变生活。关于其他的各种公网IP具备的或者是带来的功能及玩法，在以后有时间在写！ 补充一：使用人人影视Web版来批量下载剧集 这里采用的是在镜像库中评价最好的oldiy/rrshare64（第一个），使用方式很简单，在注册表找到并选中点击下载，在映像中能够找到它，然后双击可以弹出设置页面，初始可以不进行配置，直接点击下一步到最后，之后docker镜像会自动启动，然后在容器中就可以看到它了。 对该镜像的配置需要首先停止其运行，然后点击上方的编辑按钮即可进入配置页，重中之重是对“卷”的配置，这个直接关系到下载文件所在的位置。如上图红圈所示的路径和配置页是一致的，这里要注意： 左侧的文件/文件夹 对应群晖中文件系统的路径（必须是FileStation可见的路径） 右侧的装在路径 对应Web页面的设置中的下载路径（默认就是/opt/work/store） 一定要注意对应关系，在这个地方浪费了好多时间。上述内容需要在配置页的“卷”这页，手动新建一个并配置（默认是空的，下载文件会找不到） 上述配置过程结束后整个配置过程就完成了，然后就是使用人人Web端进行批量下载了，网站非常易用，下载页很给力，经过路径配置后，可以在FileStation中找到对应的文件，可以把上级文件夹绑定到Plex或者DSVideo的视频列表中，这样就可以下载转码一条龙搞定了！","categories":[{"name":"建站组网","slug":"建站组网","permalink":"https://www.cz5h.com/categories/%E5%BB%BA%E7%AB%99%E7%BB%84%E7%BD%91/"}],"tags":[{"name":"组网","slug":"组网","permalink":"https://www.cz5h.com/tags/%E7%BB%84%E7%BD%91/"},{"name":"路由","slug":"路由","permalink":"https://www.cz5h.com/tags/%E8%B7%AF%E7%94%B1/"},{"name":"Synology","slug":"Synology","permalink":"https://www.cz5h.com/tags/Synology/"}]},{"title":"CSC公派读博申请之经验","slug":"2019-6-19 CSC公派读博申请之经验","date":"2019-06-18T22:00:00.000Z","updated":"2020-03-07T17:14:51.098Z","comments":true,"path":"article/bdbd.html","link":"","permalink":"https://www.cz5h.com/article/bdbd.html","excerpt":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考。本文已整理发布到知乎专栏，这里的信息可能时过时的，所以如需阅读请前往我的知乎专栏阅读最新更新的版本，此处仅作备份。","text":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考。本文已整理发布到知乎专栏，这里的信息可能时过时的，所以如需阅读请前往我的知乎专栏阅读最新更新的版本，此处仅作备份。 官网|公派留学基金委 专栏|关于CSC的一切 文章|CSC申请经验 文章|CSC填报指南 工具|填报文字统计 写在前面2019 年的公派留学申请已经结束，长达半年之久的长跑戛然而止，这其中伴随的辛酸、不安、迷茫、无助，或许只有自己能体会，这一路走来自己最大的收获或许就是苦中作乐的能力了，如何调整心态、如何缓解压力，这都是对我今后很长一段时间大有裨益的经验和财富；当然，台面上的能力也得到了提升，和导师沟通、和外导沟通、和辅导员沟通、和国际处老师沟通、和口语陪练沟通、和自己沟通，总结这个总结那个，审视自己确定研究方向等等等等。不善言谈的我（虽然现在依旧）完成这一大圈事情，还是值得小骄傲一下的，但写这些文字时，我是完全没什么激动的感觉了，或许事情经历多了，人就变得麻木了，但也可能这在别人眼里是优点呢 ?.? 话不多说，下面以申请2020年的CSC攻读博士项目为例（申请项目为“高水平大学公派研究生项目”，共计 10000 人，攻读博士名额 2500 人），在申请过程中可能的关键的阶段如下： 准备阶段首先明确必须搞定的材料1. 套磁材料（联系国外导师） 个人简历（CV） 成绩单英文扫描件（本+硕） 第一封套磁信（表明意向） 对自己已有成果的梳理，没有明确的材料要求，主要是为了应对可能的对方导师对这方面的发问！ 2. 语言（可与联系导师一起进行）以下条件满足其一即可，但强烈推荐要考一个托福或雅思成绩，不然CSC这里是通过了，但外校那里还是没有说服力的，尽量不要用最后的语言证明的形式，风险较大。（详见选派办法） 外语专业本科（含）以上毕业（专业语种应与留学目的国使用语种一致）。 近十年内曾在同一语种国家留学一学年（8-12个月）或连续工作一年（含）以上。 参加“全国外语水平考试”（WSK）并达到合格标准。 参加雅思（学术类）、托福、德、法、意、西、日、韩语水平考试，成绩达到以下标准：雅思6.5分，托福（IBT）95分，德、法、意、西语达到欧洲统一语言参考框架（CECRL）的B2级，日语达到二级（N2），韩语达到TOPIK4级。 曾在教育部指定出国留学培训部参加相关语种培训并获得结业证书（英语为高级班，其他语种为中级班）。 通过国外拟留学单位组织的面试、考试等方式达到其语言要求。应在外方入学通知书（正式邀请信）中注明或单独出具证明。 暑假期间（2019.7-9） 学英语 暑假期间时间充足，视自己英语水平来决定要不要报个辅导班（非常贵！），如果是考雅思，我的建议： 剑桥雅思黑皮书，6-14（不知道到第几本了，到最新的一本）要基本浏览完一遍，能做完一遍是最好的，但做不完也没事，尽量看一遍，尤其是作文，看答案也要看完一遍； 最好是专项练习，不建议直接做套题，本身雅思考试就不是连贯的，两次收发卷子，练习专项往往会有更好的做题节奏； 听力的练习做剑雅的就可以，配合 小站雅思APP，吧错误的题目看明白； 阅读也做简雅就OK，同样是使用 小站雅思，可以有详细的解析； 写作是易翻车的部分，这部分可以看 顾北家的作文，但无论看谁的，最后总归是自己写，还是得靠自己的思路，尤其要注意英语行文的逻辑性； 口语是最最不稳定的部分了，因为是人为打分，主观因素非常大，我们能做的就是尽量不要让考官感到异样，不要背素材、不要停顿、不要泛泛而谈，要具象，老外思维只能理解这么多；推荐使用淘宝的 同桌英语，一般是东南亚的留学生陪练，三四十块钱一节课/一小时，具体是练习题目还是对话练习得和陪练沟通，上课就是一对一，用Zoom共享屏幕，总结对话时的问题；考前连续练习个两周，会对语感和语速有非常大的提升，口语不是很强的强烈建议试一下。 检索、确定导师这个阶段不一定非要到发邮件的地步，但要确定下可能的意向导师人选，这是个漫长的过程，可能要通过读论文、查询会议网站等等方法来找到一些相关领域的大佬，然后再筛选出合适自己的人选，然后再确定这些人是否在职（有的是会退休的），然后找到邮箱或联系方式备用，结合所在学校的 四大榜 排名做个排序，最后形成一个意向导师列表（这个列表已经综合了学校实力等等因素）。待到9月份开学，就可以逐个发送套磁信了。 注意：这个导师人数最好不超过 20 人，学校最好是分散开（一人一校最好了），针对性的发送套磁邮件，不要海投，每间隔三天发一封，同一时段内沟通导师数量控制在三人内，如果这个拒了再发下一封。 找导师阶段（2019.9-12）下半学期开始集中套磁（找导师），新学期开始就要正式找导师了，按上述提到的规则，可以一直套磁到11月，时间充裕，连续套两三个月基本就成了，关于如何套磁，怎么写邮件，可以关注小木中的帖子，有非常多经验贴，我的建议： 第一封邮件就附上成绩单，这样拒绝也拒绝的干脆些，避免无用的沟通 有推荐信（英文，和提交的推荐信不是一回事）的附上推荐信 附上自己的成果（论文等） 要说明自己可以申请CSC，关于中国政府奖学金（CSC奖学金的英文名）这些年早就被人熟知了，如果不是对申全奖有执念，那么不妨写上这句话，往往会大大提高老师接收的可能。 尽快确定人选尽量在年底确定导师人选，确定的意思是对方同意发出邀请信即可，不需要年底就拿到，可以年后继续讨论研究计划，年后再发邀请信，但总之是要确定下一个固定的接收导师。元旦后马上是过年，过年回来就是三月了，时间非常紧张！此外，有多个 offer 的申请者最好在年前就确定跟不跟，不要等到年后申请前的这一小段时间再确定，这会让外校导师非常难受。 填报阶段（2020.2-3）这一阶段主要是填报网上报名系统，主要工作量在于四个文本框的填写，原则上突出重点、惜字如金（3000字真的很少，实际2800汉字左右）、不浪费一个字符： 中文计划（3000字）这个根据之前拿到的英文的研究计划来改写，主体就是英文研究计划的翻译，但要重新调整文章的组织结构； 成果概述（3000字）包括自己的已有论文、参加项目、获得奖项等等，不论大小奖励均可详细介绍； 国内导师简历（1000字）和自己导师沟通，挑重点的写； 国外导师简历（1000字）要单独跟外导要这个，并且一定要说明 1-2 页，不要发来冗长的简历还得自己改，拿到PDF的简历后，还是要重新组织下语句，独立成文填到文本框里； 上述内容基本上得全部手动打一遍，这大概七八千字的文字描述，其实是评委能看到的、产生主观印象的重要支持材料，所以对申请时非常非常重要的，一定要认真对待！ 除上述内容外，其余的就是基本信息的填写了，填写的原则是实事求是！奖励不够级别的就最好不要写，科研项目这块据老师交代可以宽松一点，本身研究生也不太会独立承担课题，所以可以写上参与的基金项目，然后负责职位写参与就可以了。 对于一些模棱两可的边角料信息，一定要问清楚在填写，谁负责收材料就问谁，一般是自己学校国际处的负责老师。 最后，有个单独的提交申请材料模块，目的就是上传下面的PDF扫描件，对于攻博的申请来说主要有下面需要提交的材料（详见这里），由于某些表格是最后自动生成的不用准备，所以按准备的先后顺序对原材料进行重排如下（1-9项需全部上传）： 成绩单复印件（自本科阶段起）/PDF 最高学历/学位证书复印件/PDF 有效的《中华人民共和国居民身份证》复印件/PDF 外语水平证明复印件/PDF 邀请信/入学通知书复印件/PDF 学习计划（外文）/PDF 国外导师简历/PDF 国内导师推荐信（攻读博士学位研究生和联合培养博士研究生均需提交）/PDF 两封专家推荐信（攻读博士学位研究生需提交）/PDF （*和上面的推荐信合成一个PDF上传） 在系统最后是《单位推荐意见表》，需下载并由院办盖章，并发电子版到学校国际处审核 全部信息填写完成后，系统会自动生成《国家留学基金管理委员会出国留学申请表》（研究生类），一般要求打印这个文档，并交给学校国际处。 等待阶段（2020.4-5）这段时间基本上可以把申请放一边，该干嘛干嘛了，需要整毕设的整毕设，需要补英语的补英语，出结果的日期一般在五月下旬，从报完名到五月中旬大概有快两个月的中断期，其实比较好熬，但从五月中旬开始的两周，可以说非常难熬了，每天都有人问这问那，也有不少人关注自己页面HTML代码的变更，搞得天天风声鹤唳，非常紧张，加之出结果的时间其实很晚才会确定，这种不确定性会导致一堆人在群里天天分析来分析去，心理承受能力差的人估计要受不了了。 关于代码变化这个事情，我从一开始就是不信的，本身就是写代码的，从前端页面的某个值的变化推测数据库中值的变化，本身我对此持怀疑态度，去年听说是比较准的，但也是因为基金委估计也没想到学生们这些手段，今年已经明确说网站代码改动了，但还是有部分人乐此不疲的每天刷着自己的状态码，对此理解但不加入，这是我的态度，直到查结果前我一次都没有看过，查完结果后看了一下，自己好像是 2，关于 1 和 2 的具体意义，出结果后会发现其实没有什么代表性，所以今年再申请的话，务必不要在这些代码上浪费精力了。 总之，在这段时间的最后，一定要稳住心态，可以水水群聊聊天缓解压力，最终发现，群里的话痨基本上都成功了，这不是巧合，参与的多获得的信息就多，信心也大些，一结合成功率可能就高些。 后续材料办理（更新中） 选择留学生服务中心 可选背景、上海、广州，三选一 主要负责为留学生提供后续服务（订机票等） 公派出国留学协议书 两位担保人 公派无需财产证明和资金证明 交付学校国际处一份 交付国家留学基金委一份 自己拿一份 体检 出入境检疫局 健康旅行证 外校注册 重新发送一些相关文件给国外秘书 国外当地租房 视情况而定，花费时间有长有短 办理签证 具体办什么签证，去对方国家的中国大使馆官网查询 订机票 提前 20 天左右向留服提交申请 会提供联程票（费用全免） 银行卡 填报申请系统时就选择过了，一般是中行或建行； A类地区 一个月往卡里打 1350 欧元； 出国前务必开通激活银行卡，基金委会预付三个月奖学金到卡里，以免去了没钱花； 此卡为一般储蓄卡，非VISA，保险起见尽可能再带一张 VISA 卡，国外可没有蚂蚁呗和京东白条什么的了。。 以上基本涵盖了整个申请过程的全部内容，但真正的申请过程要远比这个复杂，可能会出现各种不确定的情况，还需要各位申请者自己斟酌把握，做出抉择。 整体而言，国家公派留学（攻博）的申请过程过于冗长了，远没有申请全奖来的雷雷风行，但相应的，也没有全奖那样对GPA等的拔高要求。可以说，全奖看成绩、看GPA，CSC看的更多的是发展潜力。 就申请而言，全奖基本是分化到外校每个学校进行竞争，而CSC是对外各找各的，对内评审时全国高校加各省教育厅加海外大使馆的申请人，混在一起，由评审专家评出这 2500 人，和全奖的难易程度机制不同没法比较，但总归CSC有着更多的可能性，评价指标也更加多元，如果不是纯粹的 GPA学霸大佬，那么申请 CSC 或许是一个很棒的途径。","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"CSC","slug":"CSC","permalink":"https://www.cz5h.com/tags/CSC/"},{"name":"攻博","slug":"攻博","permalink":"https://www.cz5h.com/tags/%E6%94%BB%E5%8D%9A/"}]},{"title":"国内外攻读博士之我见","slug":"2019-6-6 国内外攻读博士之我见","date":"2019-06-05T22:00:00.000Z","updated":"2020-02-29T18:43:55.555Z","comments":true,"path":"article/995.html","link":"","permalink":"https://www.cz5h.com/article/995.html","excerpt":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考 由毕业出路说起","text":"注：转载请注明出处，以下内容均为个人总结，不保证百分百正确性和完整性，请酌情参考 由毕业出路说起 目前的形式是，虽然国内顶尖高校发展迅速，但各大排行榜整体上仍被境外学校占据大多数，一定程度上也说明了内陆的高校仍然有很大提升空间，这也是为什么很多人选择外出求学的原因。现阶段国际上评价学校的方式中，各大排名是很具有参考意义的，即便是存在一些倾斜或者对国内高校的歧视，但不妨碍我们对国外一些学校水平的判读，现阶段公认的四大排行榜如下： 泰晤士高等教育排名，THE由英国《泰晤士高等教育》发布，早先与 QS 联合发布排名，现已各自独立发布，由普华永道（PWC）进行独立审计，因存在商业因素和偏向英语国家而经常受到批评。 世界大学学术排名，ARWU前身由上海交通大学高等教育研究院发布，现由“上海软科”（公司）发布，仅针对前500排名进行发布，排名以评价方法的客观、透明和稳定而著称，但也被指责过度偏重理工领域及过多采纳美国的知名期刊与论文发表平台为数据基准。 QS 世界大学排名由英国国际教育市场咨询公司 Quacquarelli Symonds 发布，是参与机构最多、世界影响范围最广的排名之一，因其问卷调查形式的公开透明而获评为世上最受注目的大学排行榜之一，但也因具有过多主观指标和商业化指标而受到批评；很多高校的分项数据缺失，总分出现大幅偏差。 USNEWS 世界大学排名其由《美国新闻与世界报道》发布，榜单排名常被评价为过度钟爱美国本土公立学校。 目前的海外留学归国就业情况：可由各高校对博后（一般应聘这个）的要求看出一点风向，目前大部分高校对博后的要求都有这么几条：年龄限制、学术论文要求、一流高校毕业。其中，对一流高校的衡量比较模糊，国内一般以双一流做标准，国外及港澳台则通过排名确定，下图可见部分高校直接给出定量的指标参考，目前来看世界排名前100的高校是比较有牌面的（*世界排名一般是四大榜之一即可），但这也不是硬性条件，只有少部分学校直接给出这种硬性指标，大多数高效只要求毕业于一流学校。 不过，总体而言随着博士毕业生的增多，整个就业形势会更严峻，就业要求也会水涨船高，所以，在决定攻读博士这件事上更要思考再三，除了自己的不懈努力外，现在看来更优秀的平台也是我们绕不过去的坎，那么如何选择升学去向就显得尤为重要，选择国内还是国外、自费还是全奖，这都需要认真考虑，唯一的原则就是：宁做凤尾，不做….。 这句话可以有不同的理解，凤，要么是好的平台，要么是好的导师，理想状态下是二者皆占，但如若只能二中选一，那么前者的容错性要远大于后者，虽然有种说法是读博士导师最重要，但我认为读博士首先方向是第一位的、其次是平台（研究组、实验室）、再次是导师（大佬级除外），如果有大佬级导师，可以无视一些限制，但何为大佬级导师？院士一定是、其他的就需自己衡量和承但风险了；原则就是看其发展前景，如果退守二线的大boss并无斗志，那么追随其完成学业估计也不会有什么作为，反而，某些“蠢蠢欲动”的boss会最终跳槽到更好的平台，这种情况一般会让其团队整体迁移，所以能否找到这种野草中的百合花，就看自己的本事了。 话说回来，从本到硕再到博，研究领域逐渐固化、精细，如果我们想做出一些成果，必然地需要接触最前沿的知识，而目前，这些资源都很不平衡地被顶尖平台吸引，即在好的平台或许你可以直接去面对面和研究领域内的各种leader对话、探讨，而在稍差一些的地方，或许就只能咀嚼二手信息；知识的滞后性虽然在互联网时代会被冲淡，但毫无疑问对整个领域研究进展的把握并非善用互联网就可以完成。 郑重声明：以上叙述仅在于突出选择的重要性，科研本无三六九等，但今天我们的社会也不是天堂，所以面对社会资源的分配不均，我们只能遵守当下的游戏规则。世界学校千千万，顶尖学府一小半；或许现在不能被他们接收，但这颗向往的心请务必保持。都说高考是一锤定音，可对于真正一心向上的人只是一次小小的转折，从高到本、到硕、到博、到博后，不要再说自己没机会，装睡的人叫不醒，有梦的人不睡觉！不管现在你身处什么位置，要坚信有若干向上的大门为你打开，偏安一隅一时，往往就会偏安一隅一世。是要活的精彩，还是平淡是真，这取决于自己内心的声音，想，就去做，不要考虑太多，这个想法才是最珍贵的东西：有了方向才能打包行囊！ 真正的抉择下面是目前我所知的全部的几种攻读博士的方式及一些注意事项，仅供参考。 大陆境内的选择下述时间花费是指高等教育时间花费； 本硕博连读 确定节点：高考后报志愿时 时间花费：3 + 2 + 3 或其他分配方式，一般 8 年完成 硕博连读 确定节点：考研/保研时 时间花费：一般是 2 + 3 或其他分配方式，一般 9（5+4） 年完成 硕士转博（本校内） 确定节点：正常研二上学期（提前答辩） 时间花费：一般是 2 + 4 或其他分配方式，一般 10（6+4） 年完成 申请考核（外校） 确定节点：硕士毕业前半年/一年 时间花费：正常学制，一般 11（4+3+4） 年完成 统考 确定节点：硕士毕业前半年/一年 时间花费：正常学制，一般 11（4+3+4） 年完成 大陆境外的选择主要分为直博和硕士攻博，两者博士攻读年限并无太大差别，均为 4-5 年，但美国等部分国家会要求本科生学习相关研究生课程，会一定程度上拉长时间，但总体国外提前毕业的案例也非常多，也存在工科 3 年毕业的同学；大部分硕攻博学生属于正常学制，即整体时间花费为 11（4+3+4）年，本科生直博也一般需要 8（4+4）年； 需要明确的是，同大陆国内院校不同，境外攻读博士一般给的“补助”要高的多（自费除外），完全可以将境外攻博看做一份月薪一万rmb左右的工作，事实上欧洲大部分国家就是这样做的。 外校全奖全奖区别于半奖，一般是负担学生的全部学习和生活开支，主要包括学费+生活费，在没有学费的学校就主要包括生活费；半奖的形式比较多样，主要的区别就是Cover的费用不够全面，需要自掏腰包一部分，比如包括学费和生活费的学校，只减免学费生活费自理，这就算一种半奖。 英美加澳新 好处： 学校数量庞大，申请全奖机会多 文化大熔炉，生活更加多元 坏处： 强委员会制度，即学校主导性强，导师话语权弱，能否给offer要看学校/院的意见，而学校/院往往会卡死各种条件，甚至导师都无法argue 普遍需要托福，及一系列考试，难度及时间花费明显高于雅思 欧洲大陆 好处： 名牌学校众多，不过分散于各国，国内名气不如美帝学校来的响亮，但实则排名都非常靠前； 申根国优势，可一国留学，游遍多国，亦方便学术交流 一般无学费，各种职位、全奖其实解决的是生活费 弱学委会制度，导师话语权大，一般是导师拍板、学院辅助的套路 一般要雅思成绩，语言学习成本不高 坏处： 区别于英美派系，欧洲教育普遍将博士视为全职工作人员，即需要按月发放工资，自然是没学费的概念，也就是说全奖机会不多，多的是职位机会，但不幸的是其职位竞争激烈，且多最终发放给了欧盟学生 即需要自己找钱，机会较大的除了职位和小部分全奖，还有导师的项目经费、政府层面的奖学金 港澳 好处： 离家乡不远，离世界很近，足不出户就体验到各种异域文化的交融 高校实力强劲，和国内学校联系密切，发展前景好 雅思托福都承认，导师和学院的权力较平衡，申请可以比较个性化 坏处： 近些年港大等学校竞争非常激烈，导致某种程度上出身很重要 接触不到原本的西方文化及生活方式，有的只是跟深圳类似的高强度快节奏生活 最近各种事件体现出整体社会氛围较急躁，虽对科研没有影响，但毕竟需要在当地生活，还是需要一个稳定向上的社会风气； 政府类奖学金这类奖学金是以国家的名义对部分学生进行学业资助，一般有指定的年限，到时间后需要返回原国家进行工作，资助水平一般不低于当地国家的全奖水平，大多是给本国学生申请，但也面向外籍学生申请，比如日本文部省奖学金，中国学生赴日留学的也可以申请该奖学金。目前我比较熟知的政府奖学金有以下几个： 中国政府奖学金（国家公派留学，CSC） 项目解释：类似项目在我国最早可追溯至清朝洋务运动时期的留美幼童，詹天佑等等一些人的名字我们耳熟能详，今天的公派留学就是做类似当年的事情，只不过现在规模更大（全国外派攻博人数2500人/年），流程更正规（严格申请资料和指标及资格审查），批评更公平（又各高校抽调评议专家评审）。现在，这项工作由教育部牵头，国家留学基金委负责实施。 申报条件： 英语有量化要求，雅思、托福、PET5等等 需提前联系外校导师接收，拿到邀请信 需满国内导师推荐信若干 文章、项目无硬性要求 时间节点： 每年的三月份填写申报系统 一般需要上一年年底前确定接收意向 也就是说下半年九十月份就需开始联系外校老师 先前我就是申请的该项目，项目时间跨度要长达半年多，远多于申请全奖的时间花费，有关于更多我的信息可以访问 关于我 页面，或直接“番茄”后访问 这里 。 德国政府奖学金（DAAD） 和中国国家留学基金委的奖学金项目合作多年，现在一般以 CSC-DAAD 联合奖学金的形式进行申请，即目前对于国内学生来说，申请该奖学金就是申请 CSC奖学金。 日本文部省奖学金 该奖学金同样和 CSC 有合作关系，申请该奖学金需先通过 CSC 的申请，可以看出目前国家留学基金委的合作项目都是这样的套路，除了和政府奖学金的合作项目外，还有一大批CSC和国外院校的合作项目，这部分项目在申请时单列指标，更多奖学金项目请查看国家留学基金委项目检索页面：https://bg.csc.edu.cn/ 以上是我目前粗略的总结，主要的时间节点没有叙述的很明确，详细的内容我会在今后慢慢更新。祝福努力的各位都能有个好的结果，要坚信有付出就会有回报！","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"CSC","slug":"CSC","permalink":"https://www.cz5h.com/tags/CSC/"},{"name":"攻博","slug":"攻博","permalink":"https://www.cz5h.com/tags/%E6%94%BB%E5%8D%9A/"}]},{"title":"图布局算法的发展","slug":"2019-5-2 图布局算法的发展","date":"2019-05-02T22:00:00.000Z","updated":"2020-02-29T18:43:55.552Z","comments":true,"path":"article/f8d9.html","link":"","permalink":"https://www.cz5h.com/article/f8d9.html","excerpt":"图数据的可视化，核心在布局，而布局算法通常是按照一些特定的模型，将抽象数据进行具象展示，这一过程伴随大量的迭代计算，例如朴素的 FR 力导向算法其在计算斥力时的算法时间复杂度达到了 O(n 3 )，这在小规模数据量下可能并不会出现问题，但随着规模的不断增大，采用如此“高昂”计算复杂度的算法变得不能接受，所以，出现了许多针对算法时间复杂度进行改进的方法，需要说明的是，在这一阶段，数据集的规模仍未达到单机处理上限，例如 OpenOrd算法采用多线程并行来加速计算过程。随着数据规模的进一步扩大，图数据节点达到百万级别时，单机并行策略也变得无能为力，这时，分布式并行计算的方式为这种“大规模图数据”的处理提供了可能性。","text":"图数据的可视化，核心在布局，而布局算法通常是按照一些特定的模型，将抽象数据进行具象展示，这一过程伴随大量的迭代计算，例如朴素的 FR 力导向算法其在计算斥力时的算法时间复杂度达到了 O(n 3 )，这在小规模数据量下可能并不会出现问题，但随着规模的不断增大，采用如此“高昂”计算复杂度的算法变得不能接受，所以，出现了许多针对算法时间复杂度进行改进的方法，需要说明的是，在这一阶段，数据集的规模仍未达到单机处理上限，例如 OpenOrd算法采用多线程并行来加速计算过程。随着数据规模的进一步扩大，图数据节点达到百万级别时，单机并行策略也变得无能为力，这时，分布式并行计算的方式为这种“大规模图数据”的处理提供了可能性。 图在数学上通常用 G = (V, E) 来表示，其中 V 是顶点的集合，E 是边的集合，且每条边 e ∈ E 都连接两个顶点 &lt; v i ,v j &gt;，且边 e 通常由 &lt; v i ,v j &gt; 来存储表示，在本文中，我们将图数据的表示限定在二维平面内，因此对于每个顶点 v i来说，布局的最终目的是确定 v i 在画布上的位置 (x i , y i )。对于整个图数据可视化的发展趋势，大致经过了单核串行、单机多核并行和多机分布式并行三个阶段，各阶段存在时间的先后顺序，且由于各阶段的程序运行机制的差异，导致了对可视化处理流程（布局）实现和优化方面的差异。下面就图数据可视化的各个发展阶段进行回顾。 单核串行阶段在这一阶段产生了非常多经典的可视化工具，例如 Gephi [7]，一个能够展现和操作布局的软件，其允许用户进行二次开发加入其自定义算法或实现更复杂的功能，由于它的通用性和开放性，现在已成为最为普遍的网络可视化工具软件。除此之外还有许多类似的可视化分析工具软件，像 Ucinet 是早期针对对网络分析工具 [8] ，不过由于软件本身设计较早，导致整体对数据的处理能力严重不足，在 2011 年被停止维护；Pajek 也是一款诞生很早的网络分析挖掘工具 [9] ，可以对布局进行复杂的分层、置换等操作，其一直发展至今衍生出了多个版本，社区也非常活跃。在 2001 年出现的 NetMiner [10] ，将易用性和扩展性提高到了新的高度，并由此创造了成功的商业模式。在 2008 年，微软研究团队发布了 NodeXL [11] ，通过作为 Microsoft Excel 插件的形式，NodeXL 可以获得良好的易用性，不过由于其采用的.Net 平台开发，也使得其没有跨平台的能力，且 NodeXL Pro 属于付费模块，开源部分有限，总体开发扩展性不高。 表 2.1 针对部分指标对以上软件做了对比，其中，Gephi 是目前适用范围比较广泛的网络可视化工具，对于 Gephi 来说，基于 NetBeans 的模块项目构建、以可视和交互为主的分析模式以及开源的社区支持，都使得 Gephi 成为一款更为纯粹的针对图数据可视化的工具。因此，在本文的实验部分，Gephi 被选做用来完成相关比较。还有一点值得关注的是，上述工具均不支持分布式运行环境，即处理数据的规模严重依赖主机的内存和处理器，虽然在 16G 甚至更大 RAM 的条件下，个别工具依靠多核并行的布局算法可以完成数十万节点规模的粗略布局，但对于更大规模的图布局来说，其高迭代性和高时间复杂度的特性使得上述软件均不能很好的完成大规模图数据的布局操作。 不过在早期的研究阶段中，针对的图数据规模一般较小，并未达到单机处理极限，可视化研究的重点大都集中在布局模型的探索，这一时期出现的力导向模型为图布局的发展起到了重要作用，众多图布局算法均由其改进而来。除此之外，这一阶段也产生了许多基于其他模型的图布局算法。力导向布局算法也称 FDP（Force-Directed Placement）算法是目前在图布局算法上应用最为广泛的算法，其在自然规则模型（弹簧或电荷力）的指导下，能以人类易理解的形式充分展现图的整体结构，通用性强，在图的布局算法中占据主导地位。根据力导向算法得到的布局结果，具有节点间相关的特性，即布局过程取决于节点间的连接而非节点具有的属性，这种方法的缺点是其对初始状态十分敏感，且布局过程可能会陷入局部最优解，同时整个过程具有不确定性，不能确保每次得到相同的结果。尽管该方法存在这些问题，但其仍然是对数据在结构上进行视觉解释的通用方案，并在随后的几十年时间里，有不少的研究都基于这一理论，且同样对图布局的研究产生了很深远的影响。对于力导向布局的改进，主要体现在能量模型和计算方式上。 以图布局为核心的图数据可视化的起点，可以追溯到 1984年，在这一年，Eades提出将图数据的布局模拟为弹簧和铁环的物理系统 Eades[12]，但在实现上其并没有反应真实的胡克定律，且时间复杂度为 O(|V| 2 )。之后，Kamada 和Kawai[13]对其做了改进，引入了非邻居节点间最佳距离的概念（最佳距离l同非邻两节点间的最短路径成正比），并首次将整个布局过程抽象为能量降低（最优化）的问题，通过最小点间斥力和引力的和，来将节点放置在合理的位置，整个系统中的能量模型可用公式2.1表示， 其中 x i 为顶点 v i 对应的坐标位置，k ij 为 x i 与 x j 之间连接弹簧的弹力系数，l ij 为顶点 v i 与 v j 之间的最佳距离。该方法每次只求取一个点的最佳位置，即其内循环（单次算法执行）时间复杂度为 O(|V|)。 之后，Davidson 和 Harel [14] 首次在布局过程中引入模拟退火算法，Fruchter-man 和 Reingold [15] 在此基础上使用了简化的“弹簧-电荷”模型（SEM, Spring-Electrical Model）来指导布局，在其算法中两节点间的斥力 f r 和引力 f a 可由公式2.2表示，并首次在布局时采用画布面积来计算得到最佳的节点间的距离。 其中，K 即为最佳节点距离，C 是调节斥力引力相对强度的参数，∥ x i − x j ∥即为点 x i ,x j 之间的距离。 在力导向布局算法出现后的近十年中，研究者们主要的研究内容集中在模型的改进和减少绘制时的边交叉等问题，主要偏重于理论研究，在数据集的选取上均为规模较小（几十至几百顶点数）的、具有特定结构的图数据。由于这部分算法包含的内容一部分已经不具有时效性，所以这里仅列举了上述对布局算法有很大影响的基础性的工作。 在图布局算法发展的第二个十年里，研究者们不在满足于算法仅适用于小规模数据集的现状，开始向更大规模的数据发起了挑战，这一时期，布局过程中使用多尺度布局算法来加速布局过程逐渐成为研究热点，多尺度算法的最初思路是：对于图 G = (V, E)，G 0 是整体图结构，G 0 在开始生成一系列小尺寸的图G 1 ,G 2 ,…,G k 。然后用经典的力导向算法绘制级别最小的图 G k ，在 G k 的基础上来绘制 G k−1 的布局，递归地执行此过程直至完成 G 0 的绘制。在 1999 年，Hadany和 Harel 首次将多尺度算法 [16] 应用到图布局的过程中，使用简单的梯度下降来最小化计算模型的能量。其算法时间复杂度为 O(|VE|)，但需要的空间复杂度为O(|V| 2 )。不久后，Walshaw 将该思想做了更为细致的阐述，提出了多尺度 FDP 算法 [17] ，并对多尺度算法的时间复杂度做出了详细的分析。2003 年，Chan 等人针对符合幂律分布的网络提出了 ODL 算法 [18] ，首次提出通过图的拓扑结构来分层布局以加速布局过程，这一简单思想对后来的大规模图布局研究产生了很大的影响。第二年，Hachul 和 Jünger 提出了 FM 3 算法 [19] ，其通过一种“势能”系统来完成多尺度的布局思路，且首次将四叉树进入到布局过程来计算节点的近似势能信息。2006 年，HuYifan 从另一个角度出发，采用 Barnes-Hut 技术，通过构造八叉树来计算节点间的近似斥力 [20] ，得到了和 Walshaw 的多尺度布局算法相近的绘制效率。 同期，国内黄竞伟等人开始对“画图”问题（图布局）展开研究 [21]，他们将布局问题抽象为函数优化问题，然后利用遗传算法来求解目标函数的最优解的近似值。随后，孙炜等人又在其基础上引入模拟退火算法来克服遗传算法局部搜索能力较差的缺点 [22] 。在多尺度布局方面，最近几年国内研究者们针对分层布局提出了许多新的解决方案，包括基于图匹配 [23] 及基于改进力导向 [24] 的分层布局方法。 由于数据规模的增大，不可避免的出现了表示混乱的问题，在2009年，Holten等人首次提出边捆绑的概念（Edge bundling）并将其应用到图布局的展现上 [25] ，其提出在边绑定时要遵循一种“位置关联”的规则，即像长短差异很大或是距离间隔很远的边不应该捆绑在一起，这种方法的使用大大提高了图布局结果的展现能力，直到最近边捆绑技术仍然是图可视化研究的热点 [26] 。在这之后的布局算法中，美学设计被提到了和算法本身同等的位置，边捆绑等技术得到了快速的发展。同年，斯坦福大学推出了 SNAP 数据集服务 [27] ，其旨在搜集整理不同种类的网络数据并以公开的方式将这些数据提供给研究者，还是在这一年，Gephi的问世使得图数据的可视化研究变得更为方便，虽然在这之前有包括 Pajek 在内的许多工具软件，但 Gephi 仍然以其通用的开放编程模型和易用的界面使对图数据的可视化研究成为了一款通用可视化工具，同时随着边捆绑等视觉呈现方式的改进，图数据的可视化逐渐进入了一个新阶段：对真实社会网络的描述。 社会网络是复杂网络的一个重要分支，其描述真实的数据关系，同时也具有真实数据集的一些特点：符合幂律分布、易出现小世界网络等等，指虽然之前的FM 3 算法已经能够完成 20 万节点规模的绘制（AT&amp;T 当时规模最大的数据集），但其侧重点仍然放在对算法的原理分析上。视觉展现技术和可视化软件的成熟，使得像社交网络分析等学科通过可视化来辅助验证或直接分析相关问题成为了可能，这也促进了图数据可视化同社交网络等学科的进一步融合。在这一阶段，图数据的可视化进入了第三个十年，将目标转移到描述真实的社会网络之后，如何更好的展现数据的结构似乎成为了首要目的，这一阶段有相当一部分的研究都侧重在美学设计方面，即如何减少混乱、如何呈现小世界网络 [28] 等；同时，在处理数据的规模进一步增大和算法的复杂度不断提高的前提下，传统单机单线程的算法遇到了性能瓶颈，此外，一些边捆绑技术要求更高的计算代价，这也进一步促进了并行算法的出现。 单机并行阶段在这一阶段，传统的图布局算法引入了很多新的技术，例如采用并行计算来加速迭代过程，或是使用 GPU 编程来加速计算渲染过程。2009 年，在 FM 3 算法出现几年后，Godiyal 等人同 NVIDIA 公司的 Garland 对其提出了基于单指令多数据流处理器（SIMD，例如 GPU）的并行计算版本，他们的算法采用了一种基于CPU 和 GPU 结合构建的 kd 树，在布局计算上的时间复杂度保持在 O(NlogN)内。此后，GPU 并行技术正式被引入到图可视化领域，直到目前为止，其仍然图形骨骼绘制（Skeletons）技术的有效手段 [29,30] 。最初，研究者们尝试使用多显示器的方式对图布局进行显示 [31,32] ，这样顶点均匀的分布在不同显示器上，每个显示器关联自己的处理器，各处理器分别计算顶点的位置。该类算法并不具备良好的伸缩性，实验仅限于处理包含数千个顶点的图。之后，Tikhonova 和 Ma 提出了一种并行的力导向算法 [33] ，可以在具有几十万个边的图上运行。其算法运行在 Cray XT3 的 32 个处理器上（类似超级计算机），布局包含 260385 条边的图大约需要 40 分钟，效率仍有提升空间。在 2011 年，美国橡树岭国家实验室提出了 OpenOrd 算法，能够完成百万节点规模的数据布局，这是一个典型的分段布局算法，即对图数据的布局不再是多次布局算法的迭代，而是整个布局算法分为几个阶段，布局时分别执行这几个阶段，执行结束时布局结束，整个布局算法只执行一次，不存在外循环。OpenOrd算法主要采用了四种思想，即多尺度布局、节点聚集、并行计算和启发式的边切割，主要贡献即提出了可行的并行图布局思路和具体实现（在 Gephi 0.9.0 版本被纳入到布局算法中 [34] ），在模型上其沿用了 VxInsight [35] （其前身算法）使用的网络密度模型，其整个系统的能量可由公式2.3表示。 其中，D x i 会向画布中靠近点 x i的点 x 1,…, x n 贡献密度值，上式的求和包括斥力和引力两部分，在计算引力时通过公式中的 w ij （边的权值）来作用使之将关联性强的节点放置在相近的位置，斥力部分通过密度项 D x i 表示，其目的是将节点低密度的填充到画布中，对这两部分的和求取最小值来达到最终的布局状态，即关联性强的节点彼此靠近，但又必须满足低密度放置的要求。这样，在进行布局时，OpenOrd 算法采用多线程模式将带密度信息的画布复制多个副本，每个线程处理不重叠的部分数据，且都对自己的画布密度进行更新，然后在合并步骤将各处理线程的画布密度信息进行统一合并更新并调整节点的位置，循环迭代过程完成布局。该算法依靠多核处理器的并行能力，可以将单机（四核八线程）处理能力提高到百万节点级别。值得注意的是，这一阶段图数据的抽象表示也十分受到研究者的重视，这部分布局算法通常从社交网络分析的角度入手，常常伴随节点聚类和抽象图形表示等方法，例如 Vehlow 等人从重叠社区的展现出发，将不同级别的社区结构通过不同的图形抽象来显示 ；国内研究者也开始关注这一内容，2015年，赵玉聪等人根据分层扩展的思想，提出了一种基于图匹配的分层布局算法 [23] ，递归的对大图进行简化和布局，同时还研究了对简化布局结构的反向扩展，为分层布局算法提供了一种新的思路。此外，结合社区发现等研究领域，出现了以“社区划分/聚类-&gt; 节点抽象-&gt; 分层布局”的布局算法，代表作有朱志良等人提出的方法。 ，先构建社区网络，然后对其进行布局，之后确定社区区域范围，最后在每个社区范围内对社区进行节点填充。在这之后，2016 年汤颖等人提出了一种基于改进 FDP 算法的层级视觉抽象方法 [24] ，通过树结构来构建节点的分层结构，取得了十分明显的分层布局效果。 参考文献[7] BASTIAN M, HEYMANN S, JACOMY M. Gephi: an open source software for exploring and manipulating networks[C]//Third international AAAI conference on weblogs and social media. 2009.[8] BORGATTI S P, EVERETT M G, FREEMAN L C. Ucinet[J]. Encyclopedia of social network analysis and mining, 2014: 2261-2267.[9] MRVAR A, BATAGELJ V. Analysis and visualization of large networks with program package pajek[Z]. 2016.[10] GHIM G H, CHO N, SEO J. Netminer[M]. 2014.[11] SMITH M A, SHNEIDERMAN B, MILIC-FRAYLING N, et al. Analyzing (social media) networks with nodexl[C]//Proceedings of the fourth international conference on Communities and technologies. ACM, 2009: 255-264.[12] EADES P. A heuristic for graph drawing[J]. Congressus numerantium, 1984, 42: 149-160.[13] KAMADA T, KAWAI S, et al. An algorithm for drawing general undirected graphs[J]. Information processing letters, 1989, 31(1): 7-15.[14] DAVIDSON R, HAREL D. Drawing graphs nicely using simulated annealing[J]. ACM Trans- actions on Graphics (TOG), 1996, 15(4): 301-331.[15] FRUCHTERMAN T M, REINGOLD E M. Graph drawing by force-directed placement[J]. Software: Practice and experience, 1991, 21(11): 1129-1164.[16] HADANY R, HAREL D. A multi-scale algorithm for drawing graphs nicely[C]//International Workshop on Graph-Theoretic Concepts in Computer Science. Springer, 1999: 262-277.[17] WALSHAW C. A multilevel algorithm for force-directed graph drawing[C]//International Sym- posium on Graph Drawing. Springer, 2000: 171-182.[18] CHAN D M, CHUA K S, LECKIE C, et al. Visualisation of power-law network topologies[C]// The 11th IEEE International Conference on Networks, 2003. ICON2003. IEEE, 2003: 69-74.[19] HACHULS,JüNGERM. Drawinglargegraphswithapotential-field-basedmultilevelalgorithm [C]//International Conference on Graph Drawing. 2004.[20] HU Y. Efficient, high-quality force-directed graph drawing[J]. Mathematica Journal, 2005.[21] 黄竞伟, 康立山. 基于遗传算法的无向图画图算法[J]. 数学杂志, 1998(s1): 68-72.[22] 孙炜, 吴伟民, 陈志峰. 基于遗传模拟退火算法的图的三维可视化[J]. 广东工业大学学报, 2002, 19(1): 37-41.[23] 赵玉聪, 钟志农, 吴烨, 等. 基于图匹配的分层布局算法[J]. 计算机与现代化, 2015(8): 107-111.[24] 汤颖, 盛风帆, 秦绪佳. 基于改进力导引图布局的层级视觉抽象方法[J]. 计算机辅助设计与图形学学报, 2017, 29(4): 641-650.","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"布局算法","slug":"布局算法","permalink":"https://www.cz5h.com/tags/%E5%B8%83%E5%B1%80%E7%AE%97%E6%B3%95/"}]},{"title":"初识图数据的可视化","slug":"2019-4-29 初识图数据的可视化","date":"2019-04-28T22:00:00.000Z","updated":"2020-02-29T18:43:55.553Z","comments":true,"path":"article/65fe.html","link":"","permalink":"https://www.cz5h.com/article/65fe.html","excerpt":"图数据的产生和发展图是信息科学中最常用的一类抽象数据结构，能够直观的表达现实世界中对象之间的真实关系。许多重要应用都需要用图结构表示，传统应用如最优运输路线的确定、疾病爆发路径的预测、科技文献的引用关系等；新兴应用如社交网络分析、语义 Web 分析、生物信息网络分析等，与图相关的处理和应用几乎无所不在 [1] 。","text":"图数据的产生和发展图是信息科学中最常用的一类抽象数据结构，能够直观的表达现实世界中对象之间的真实关系。许多重要应用都需要用图结构表示，传统应用如最优运输路线的确定、疾病爆发路径的预测、科技文献的引用关系等；新兴应用如社交网络分析、语义 Web 分析、生物信息网络分析等，与图相关的处理和应用几乎无所不在 [1] 。 随着技术的不断发展，网络在我们的生活中比以往任何时候都更加突出，上述一系列网络或图结构中都蕴含了越来越多的隐含信息，对这些网络的进行高效的分析和挖掘是亟待解决的一个问题，可视化技术在这一问题上具有无可比拟的优势，其视觉呈现可以给人们带来直观的对数据的理解和感知，Palmer等人在其文章中证明，图比其他可视化展现形式更适合探索数据的内部关系 [2] 。 面对飞速发展的信息社会，各式各样的数据集均迅速增长，部分原因是它们越来越多地被廉价且众多的信息传感物联网设备收集，如移动设备，航空（遥感），软件日志，照相机，麦克风，射频识别（RFID）阅读器和无线传感器网络。身处大数据时代，其对数据的分析和挖掘显得尤为重要，2016年的大数据的定义在指出大数据代表的数据信息具有 4V 特性之外，还强调了其需要特定的技术和分析方法才能转化为价值。随着技术的不断进步，各大分布式计算框架相继产生，其为大数据处理提供了有力的支撑，作为分布式计算、并行计算和网格计算的发展和延续，其对于单机串行处理性能不足的问题，给出了新的解决方案。 在万物互连的今天，我们已经拥有经典的大规模数据的处理方案，可以借助诸如 Spark 或 Hadoop 等分布式计算框架来实现大规模数据集的各种数据挖掘算法，在此趋势下越来越多的由于数据规模过大而导致的分析难题都被解决，但与此同时，在数据集规模和数据维度的极度膨胀下，我们对分析结果的准确性和可解释性的把握正在逐渐降低，追求可视化的宏观展现与细粒度的数据分析结果的一致性从来都是一个美好的愿景，前者可以对后者提供视觉上的强有力的支持和解释，在信息可视化领域，有越来越多的研究者在为此而努力。 图数据的可视化技术从可视化技术诞生伊始，其目标就是为了帮助人们理解抽象、混乱的数据，至今这一目标依然不曾改变。现在，数据可视化已经发展成为一个广泛的研究领域领域，处于数学、计算机科学、认知和感知科学以及工程学的交叉领域。从信号理论到成像，从计算机图形学到统计学，涵盖所有与可视化原理相关的学科 [3] 。可视化的目的是通过具象的、可交互的图形，让们深入了解我们感兴趣的过程（算法流程、科学模拟或一些真实的过程）的各个方面。可视化本身有许多定义，按照 Williams 等人的观点，可视化是人类在一个空间内构建图像时所进行的认知过程。在计算机和信息科学中，它更具体地说，是使用图形、图像、动画和声音来更详细的表示目标对象的数据、结构和动态行为的空间具象表示 [4] ，这里的目标对象可以指系统、事件、过程、对象和概念的大型复杂数据集。 目前，由于互联网及物联网技术的发展，网络这一名词正在被赋予越来越多的应用场景，随之而来的是我们自身正在越来越多的网络中扮演着重要的角色：大到 Facebook 的全球社交关系网络，小到家庭智能电器组网，都成为我们日常生活中不可或缺的一部分。这些复杂的网络中往往隐含着非常有价值的信息，例如，通过分析特定的社交网络来挖掘犯罪嫌疑人的相关信息 [5] 。经典的社交网络等复杂网络，其本质可用图这一数据结构相对应，自然地，图论中的各种理论也可以被应用到网络分析中，复杂网络分析方法的发展也验证了这一结论，在本文中，将统一称研究对象为“图数据”，将对其可视化呈现称为“图数据的可视化”，这里的图数据除了包含经典的社交网络之外，还包括各种可以抽象为网络的数据，总体上可以将其称为“社会网络”。 根据 IEEE VIS [6] 的分类，可视化研究领域主要分为信息可视化（InfoVIS）、科学可视化（SCIVIS）和可视分析（VAST）三部分，其中以信息可视化最为基础，其核心目的即：将给定的数据集 D 按特定的转换（Transform）规则 T，转换为对应的带空间信息的数据集 V(D) 并显示，以此来帮助人们理解大量、复杂、抽象的原始数据。对于图数据的布局过程，这一转换 T，即数据集 D 在给定二维（或三维）空间内的一组坐标映射 V(D)。 大规模图数据可视化的重要性在对传统的计算技术带来了挑战的同时，大数据技术的发展也促进了数据可视化的研究。作为数据最上层的展示方法，数据可视化使用图形化的手段，可以传达清晰有效的信息，促进人们对信息的理解。目前数据可视化技术可分为这几类：基于几何投影技术的方法、基于图表的方法、基于像素的方法、基于图符的方法、基于层次的方法和组合方法。作为可视化展现形式之一的网络图，是一种简单直观的图数据可视化展现形式；由于高密度数据区域的聚集特征，散点图可以更为直观的发现群体的存在；在探索大规模数据集时，网络拓扑结构的高度重叠是最严重的缺点之一，这常常会导致数据相互的关系被隐藏或很难被发现。 分布式计算框架的飞速发展极大地提高了人们对数据的处理能力，使得人们有机会可以直接研究大规模的数据集，在这方面，图数据占据了重要的位置。随着移动互联网的飞速发展，以经典的“网络”为主的图数据越来越多的出现在我们的日常生活中。在针对图数据的可视化中，提供有效的洞察力非常重要，这具体体现在以下两个方面： 通过布局来展现图数据的具体特征在这方面可以回答用户关于图数据本身的定量问题，例如在数据中拥有邻居最多的节点的信息、数据可以被划分为几个聚簇等等。这类问题的特点，是有明确的“问题驱动”，往往在没有可视化结果的帮助下用户也能得到准确的结果，但对于图数据而言，诸如想聚簇等特征，只有通过具体的布局呈现，才能更直观的得到一些运行结果的评价，像划分的聚簇间有无重叠这一问题，既需要数值上的（重叠度）准确度量，也同样需要布局上的清晰的边界划分。 为产生布局结果的过程提供信息这部分内容并不针对具体问题，但对布局过程的尽可能详细的展现，使用户能够更容易的针对布局结果呈现的状态来反推出一些非常规问题的答案，例如在两方对立的意见网络中，中间（中立）节点对整个布局质量的影响（更紧密或更清晰），对于这个列子，研究者可能在一开始只发现了布局混乱这一现象，而并不知道中间节点对布局的影响，在经过对布局过程的可视化过程研究以后才得到最终结论。这类问题即 Telea 所说的“研究者对一种现象感兴趣，是为了发现数据新的特性并建立意想不到的关联”[3] ，这对于进一步理解数据中的信息起到了至关重要的作用. 分布式大图布局计算的特点和挑战面对大规模图数据的处理需求，分布式图数据布局计算往往有以下特点： 需要进行计算的数据量巨大。节点规模在百万以上，单机甚至无法将全部顶图数据加载到内存中，虽然使用 Spark 等基于内存的分布式框架可以完成对此规模数据的处理，但如此规模的数据量仍然对集群的 IO 性能、网络传输性能提出了很高的要求； 需要完成逻辑上的多次迭代。应用广播等方式可以避免显式的循环使用，但布局计算的完成仍然离不开动辄几百次的迭代步骤，这对于传统 MapReduce计算框架来说是一个灾难，Spark 的 RDD 有效的避免了中间结果的 IO 操作，但上百次的迭代仍需要研究者十分小心的处理 RDD 的各种转换； 必须重写已有布局算法。显而易见，在布局算法的运行机制和环境发生改变后，布局算法也需要重新进行设计以适应不同的计算框架，虽然这和原有布局算法有着相同的算法思想，但在特定分布式计算框架上设计出完全适合且能够高效运行的算法，往往考验着研究人员对特定分布式计算框架的理解程度； 布局结果难以查看。单机布局算法往往能够轻松的对每次迭代的结果进行展示，像可视化布局工具 Gephi [7] 可以通过显示每次布局的结果来呈现动态的布局过程，但在分布式计算框架中这一操作往往很难实现，因为在此环境下数据会分片发往不同的工作节点进行计算，如果每次迭代完成都要汇总一次来展现当前布局结果，这将直接导使用基于内存的计算框架的优点的丧失。 参考文献[1] 于戈, 谷峪, 鲍玉斌, 等. 云计算环境下的大规模图数据处理技术[J]. 计算机学报, 2011,34(10): 1753-1767.[2] PALMER S, ROCK I. Rethinking perceptual organization: The role of uniform connectedness[J]. Psychonomic bulletin &amp; review, 1994, 1(1): 29-55.[3] C T A. Data visualization: principles and practice[M]. AK Peters/CRC Press, 2007.[4] WILLIAMS M. Visualization.[J]. Annual Review of Information Science and Technology(ARIST), 1995(30): 161-207.[5] JOHNSON J, REITZEL J D, NORWOOD B, et al. Social network analysis: A systematic approach for investigating[J]. FBI Law Enforcement Bulletin., 2013, 350.[6] IEEE VIS[EB/OL]. 2019. http://ieeevis.org/year/2019/welcome.[7] BASTIAN M, HEYMANN S, JACOMY M. Gephi: an open source software for exploring and manipulating networks[C]//Third international AAAI conference on weblogs and social media.2009.","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"布局算法","slug":"布局算法","permalink":"https://www.cz5h.com/tags/%E5%B8%83%E5%B1%80%E7%AE%97%E6%B3%95/"}]},{"title":"To Avengers","slug":"2019-4-24 To Avengers","date":"2019-04-23T22:00:00.000Z","updated":"2020-02-29T18:43:55.551Z","comments":true,"path":"article/57e6.html","link":"","permalink":"https://www.cz5h.com/article/57e6.html","excerpt":"前言复仇者联盟又出来打架了，作为粉丝怎么能不围观，所以电影票是少不了地，4DX吹吹风也是必备地…","text":"前言复仇者联盟又出来打架了，作为粉丝怎么能不围观，所以电影票是少不了地，4DX吹吹风也是必备地… 是的，就在刚刚，如约看完了爽片《复联4》，说是特效爽片，但这次看完总觉得电影里还是多了些难等可贵的东西，不吹不黑，在后半场钢铁侠挂掉后我清晰的听到了场内不止一处的哭泣声，由于我平时也不怎么去电影院看感动人的电影，所以这还是让我十分惊讶的。无论如何，看完之后内心有种诉说的冲动，因为这已经不是复联的一次精彩战役，亦或是一次精彩拯救，而是复联的完结，是漫威宇宙这个阶段的完结。写到这里，时间仿佛回到了奥创时代，每每漫威一个阶段的结束，都预示着有些我们印象深刻的角色，可能在将来不会再出现了。让我们和 小罗伯特唐尼饰演的钢铁侠 克里斯埃文斯饰演的美国队长 斯嘉丽约翰逊饰演的黑寡妇 说声再见，See you ~ 注：下文是【回忆文】，是我在看完电影后，在回学校的路上用手机码的，现在用电脑稍作整理，所以记忆或许有偏差，记不清的时间都已模糊处理，记不得的具体名词，比如无限宝石等都用别名代替，然后在叙述中出现穿插已有电影的镜头时会使用【《电影名》】注明。以下内容不打算勘误，本身就是速成，也相当于对自己记忆力的一个考验，原电影本没有分幕和段首句，为了易于理解后加的。 第一幕 愤怒、放逐、希望灭霸毁宝石令众人幻想破灭蚁人谈量子使时间劫持可行 开局承接上文，惊奇女侠帮助斯塔克及飞船回到地球，众人士气低迷斯塔克自暴自弃，后来惊奇女侠携众人通过星云找到灭霸，然而宝石已被灭霸摧毁，再打响指恢复世界的想法破灭，雷神一怒之下砍了灭霸的头。 回到地球后，一只老鼠碰到《蚁人2》最后的面包车里的装置开关，使得被困的蚁人重新变大，虽然在量子空间蚁人才过了五小时，但真实世界已经是灭霸打响指过后的五年。蚁人存活使得众人意识到“时间劫持”的可能性，最终，斯塔克觉悟研发出时间机器，众人分三组回到过去夺去宝石： 注：回到过去取得宝石的部分电影为乱序插叙，这里按回忆分片整理为以下几部分，下面，从未来返回过去的角色均以【角色名+F】的形式出现，过去时间的角色即原名称。 第二幕 万分之一的机会 第一节钢铁侠蚁人意外失手搞丢魔方美队救场携斯塔克返七零年代 斯塔克F、美队F、蚁人F返回201X，时值洛基被众人抓住【《复仇者联盟2》】，蚁人故意破坏过去的钢铁侠的心脏，使返回过去的斯塔克拿到宇宙魔方，但最后被下楼的浩克撞倒，洛基拿到魔方瞬间消失，宇宙魔方抢夺失败；另一边，美队F巧妙利用九头蛇暗示把正在神盾局手中押送的宝石权杖骗了到手，但不料被美队撞到被误以为是伪装后的洛基，美队F和美队撕打过后成功取得宝石权杖。蚁人F拿着宝石权杖返回未来，美队F和斯塔克F则选择赌一把，把用于返回的“红粒子”用在了另一次时间穿梭，穿越到197X年美队诞生的军事基地【《美国队长》】，在基地内美队F和斯塔克F分别顺利取得当时的红粒子和当时的宇宙魔方，然后美队F看到了当时的女友，斯塔克F则和貌似是他老爸的男人攀谈至离别，随后二人返回。 第二节绿巨人苦口婆心未能说服古一奇异博士冥冥之中令师傅妥协 另一边，浩克F其实和斯塔克F一行人一起回去的，但他去另外找奇异博士的师傅光头大师，那时她还没去世【《奇异博士》】，两人见面后开始攀谈，最后光头大师听到浩克F说奇异博士在将来亲手把时间宝石给了灭霸后，便将时间宝石给了浩克F，并强调一定要将宝石物归原主，否则会对原世界线有灾难后果。之后，浩克带时间宝石返回。 第三节胖雷神对话母亲解重负小浣熊背后偷袭得宝石 胖雷神F和浣熊F回到201X年的阿斯加德，时值《雷神》里的人类女主在阿斯加德【《雷神》】，返回当天恰是雷神母亲被谋杀的日子，肥雷神F由于痛苦回忆而未和浣熊F一起行动，最后浣熊F成功潜入人类女主背后将融合在其体内的红色宝石取出，肥雷神F也被其母亲发现，攀谈至离别，二人最终成功取得红色宝石。 第四节黑寡妇牺牲自己换宝石罗德斯设下埋伏锤星爵灭霸巧借星云了解一切坏星云打入复联埋隐患 最复杂的一条线：由星云F、战争机器F、黑寡妇F、鹰眼F四人组成，四人首先在x星泊停，然后黑寡妇F和鹰眼F两人来到沃弥尔星球找到红骷髅【复仇者联盟3】，得知必须牺牲自己心爱之人才可让另一人得到宝石，二人都想牺牲自己但最终黑寡妇F卒，鹰眼F得到黄宝石返回。同时，在停播母船的地方，战争机器F同星云F埋伏了偷圆球宝石的星爵【银河护卫队片段】，顺利拿到宝石并返回，但实际星云F由于原星云的靠近脑袋短路，并没有成功返回，随后被灭霸抓获。在另一边稍早之前，星云F的出现使得原世界线星云脑袋短路，灭霸随即看出端倪，通过观看原星云共享星云F的回忆，灭霸知道了来龙去脉，并将星云F囚禁让原星云冒牌回去抢夺宝石。 小结至此，六颗宝石全部集齐，下面人物不再带F，均发生在同电影开始时时间一致的世界。 获得2颗：斯塔克 + 美队 获得1颗：星云 + 战争机器 (返回的是冒牌星云) 获得1颗：黑寡妇 + 鹰眼 (黑寡妇卒) 获得1颗：浩克 获得1颗：雷神 + 浣熊 随后，斯塔克制作出宝石手套，由于辐射只能浩克穿戴，打完响指后众人回归世界恢复，但与此同时，冒牌星云操作时间机器让灭霸的星舰母船穿越到了当前世界，并就在斯塔克科技大楼的头顶… 第三幕 复联终极之战三打一群殴灭霸却险遭反杀奇异博士携众人怼灭霸军团钢铁侠拼死戴手套打出响指美队听建议度过了完美一生 随即，灭霸导弹齐射把斯塔克大楼扎了个粉碎，众人皆震落摔晕掉到炸出的坑里，手套被鹰眼捡到。同时，在地面，斯塔克、雷神、美队三人爬出来了，灭霸也坐在对面等待冒牌星云给他们找宝石手套，双方随即开锤，三人显然不是灭霸对手，不过美队中途操作了雷神的小锤子打出了一波小高潮，但最终还是打不过，之后，灭霸发动母舰军队向地球正式进攻。同时，空气中出现无数黄色圈圈，奇异博士带众人回归，包括：小蜘蛛侠、绯红女巫、阿斯加德人、黑豹众人、冬兵等。 双方展开大战，另一边，卡魔拉和星云F达成同盟，将宝石手套从冒牌星云手中夺回，之后，由于灭霸仍试图得到手套，众人随即开始保护手套远离战场，手套在小蜘蛛侠手中时被人围殴，这时惊奇队长加入战局，起手就将灭霸母舰穿堂破肚，然后接过小蜘蛛侠手中的手套继续向战场外飞，但被灭霸拦截，双方纠缠中手套还是被灭霸戴上，这时间 奇异博士给斯塔克示意，意思是这是唯一一次机会，看罢斯塔克强行将宝石戴在了自己手上(通过后来斯塔克的遗留视频可以知道斯塔克预先想到了这个操作)，带上之后随即打了响指，灭霸众人全部完结变成灰，但斯塔克也由于辐射原因挂掉。 时间来到斯塔克的葬礼后，美队带着宝石返回过去归还，一来一回虽只有五秒但在时间劫持后的过去，美队可以停留随意时间，他选择了和最心爱的女友过了一辈子，所以，当浩克操作时间机器让美队回来时，他已经变成了垂垂老矣的年纪。老美队亲手将盾牌交到猎鹰手中，完成新老交替，电影在老美队的回忆中落下帷幕。回忆里美队和女友正值年轻模样，在古典乐曲中一起偏偏起舞。 彩蛋彩蛋的作用，即暗示故事接下来的走向，然而对于复仇者联盟来说，或许再无续集…就这样，美队也选择了自己新的接班人…钢铁侠在全体漫威人物的送别中长眠不醒…黑寡妇的灵魂也永远留在了沃弥尔星冰冷的悬崖之下… 复仇者联盟系列就此完结，告别过去放眼未来，复联的终结也意味着漫威演职人员新老交替的完成，同时，这也预示着：漫威电影的宇宙世界观会加速到来，这一点在惊奇队长身上已经开始显现，所以，作为漫威粉丝唯一能做的，除了贡献一张电影票，就是静静的等待新作了！期待漫威能拍出更好的作品！","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"Writing","slug":"Writing","permalink":"https://www.cz5h.com/tags/Writing/"},{"name":"Memory","slug":"Memory","permalink":"https://www.cz5h.com/tags/Memory/"}]},{"title":"分享我的一些对拍照的理解与认识","slug":"2019-1-23 分享我的一些对拍照的理解与认识","date":"2019-01-22T23:00:00.000Z","updated":"2020-02-29T18:43:55.537Z","comments":true,"path":"article/2690.html","link":"","permalink":"https://www.cz5h.com/article/2690.html","excerpt":"自己拿到相机也有两个多月了，这两个多月乱拍也好，认真构图也好，总归是积累了一点点拍照的经验，说是拍照，是因为万万不敢说自己是摄影啊，还有太多的东西自己一知半解，所以，纯粹是摸索着拍、当然，这完全契合我原本的诉求，即拿相机是用来记录的，重在发生的事情而非相片本身，而且我也不对自己做什么专业的要求。每当看到这张照片，能够回忆起当时的记忆，这就足矣！下面就简单介绍下我已有的器材。","text":"自己拿到相机也有两个多月了，这两个多月乱拍也好，认真构图也好，总归是积累了一点点拍照的经验，说是拍照，是因为万万不敢说自己是摄影啊，还有太多的东西自己一知半解，所以，纯粹是摸索着拍、当然，这完全契合我原本的诉求，即拿相机是用来记录的，重在发生的事情而非相片本身，而且我也不对自己做什么专业的要求。每当看到这张照片，能够回忆起当时的记忆，这就足矣！下面就简单介绍下我已有的器材。 注：下述照片如果没有特别说明，即为相机所拍，图片存在压缩，不等同于原画质，机身均为 α6000，ASP-C 画幅，也就是残画幅，即下面介绍的三款镜头也都是残幅镜头。此外，α6000 是款微单相机，非单反相机。 机身α6000+16-50mm套机头这是购入时套机的配置，当初是从某鱼捡的，也不便宜但比全新还是便宜太多，机身性能及参数在这里不做赘述，这里主要说下其套机镜头和其他方面。对于此1650套机头，是索尼原厂镜头且自带机身防抖，即OSS。其光圈3.5并不十分的大，因此提供的景深不高；不过由于其是变焦镜头，且本身焦段涵盖广角到人像的很大一个范围，所以日常使用中，即可以拍拍近景人像，也可以有很好的广角视野。不过在照片质量上，其锐度并不是很好，不过这并不妨碍其成为大众用户的挂机镜头，因为其还有一个特点就是小，我不知道在这算不算饼干头，但其镜头的长度已经算是非常短的了，这样的好处是用此当作挂机头可以十分方便的进行携带，比如我有时会直接塞在羽绒服口袋里。下面是一些机身加1650镜头的实拍图。[ ↓ ]图例：这是1650的实拍样张，11月份在北京的一个下午，可以看出其整体表现还是不错的，这是16mm焦距下的画面，已经比较广了，不足之处也很明显，边缘不锐利，整体偏肉。 [ ↓ ] 适马30mm/1.4光圈定焦头下面是我接触的第一款算是上档次的镜头了，作为微单加残幅相机的用户，这款适马30mm镜头可谓是名声远扬，其以锐度高著称，即‘锐不可挡’。不过由于其是定焦头，即只有一个焦段，对于变焦头用户使用起来可能需要点时间过度，不过适马的黑可以让其可以匹配索尼机身的电子对焦，即如果你很懒，你完全可以不用亲自手动去拧镜头的对焦环，而且配合机身的峰值对焦，其可以让拍照变得非常简单。然后是其具有 1.4 的大光圈，这对于一款平价镜头来说是非常值得肯定的了，大光圈意味着可以提供很好的景深，而且就适马的这款镜头来说，其背景虚化做的也相当不错，当然在部分高端用户的眼里还是不够好，但这款镜头派出的画质及照片质量，已经远胜于套机头了，毕竟价格是其大约六倍，虽然如此，还是可以称得上是一款性价比极高的镜头了。可以看看索尼原厂类似镜头的价格，除去索尼的信仰税，估计也比适马不知道高到哪里去了。回到照片质量上来，总的来看照片质量时非常不错的，锐度和细节刻画的都很不错，算是缺点的地方是其‘紫边问题’，这是这款镜头最大的诟病了，主要体现在照片边缘，虽然对比其他同类镜头其在边缘仍然可以保持很高的锐度，但是同样出现的还有烦人的紫边，当然这在后期是可以通过诸如Lightroom的软件去除的。其他小问题是画面的畸变，我不再赘述，因为在实际使用中我并没有感到十分严重的图像畸变。整体来说，其完全对得起它本身在市面上的地位[ ↓ ]样张：沈阳十一月份的一个下午，在下面的照片中可以清楚的看出，其具有非常好的景色还原度，而且如上所述，其锐度是十分不错的，即使是在边缘也具有非常高的锐度。对于景深，可以参考上图右侧的两张图，其背景虚化的我认为已经非常自然了，光斑显得也很柔和圆润。[ ↓ ] 七工匠15mm/2.8光圈定焦头使用过上述两款镜头以后，我经常在想到底有没有一种接近人眼可视范围的镜头，后来我了解到，这应该是属于广角镜头的范围，在经过一段时间的接触之后，我拿到了下面的这款15mm镜头，跟适马的30mm一样是定焦头，不同于前两款镜头，其是由中国厂家七工匠生产，上面的适马和索尼原装都是日本产。还有各可能比较别扭的地方是，其不能使用机身内的电子对焦，也就是说，如果你想拍照只能手动去拧镜头上的对焦环，不过作为一款广角镜，一般直接对焦到最远处就可以肆意拍照了。其他方面，其具有2.8的光圈，是一个可以接受的光圈大小，当然，其最最吸引人的地方就是价格，基本是一次短程国内机票的价格。然后买这款镜头最大的目的就是使用其大广角，自然地，大广角导致其镜头是所谓的灯泡头，这也就给后续安装UV镜造成了不便。还有个小特点需要讲的是这款机身是全金属机身，即镜头非常的重，他和适马30mm 1.4 的镜头比起来还要重些，虽然其个头比其小了很多很多。[ ↓ ]样例：这是七工匠15mm 2.8拍摄的样张，一月份的下午，可视角度非常的广，而且整体锐度表现的很不错，尤其是中心锐度。值得一提的是其‘畸变’控制的非常好，通俗体现就是呈现的画面该直的地方非常直。几乎看不出有什么畸变，这在‘灯泡头’甚至是‘鱼眼头’上是个很常见的问题，由此可见这款镜头还是很强大的。但美中不足，其光圈在如此广的视角下，在图像的四角会出现暗角，即越到四个角图片就越暗，这在下面体现的很明显。不过追求大广角来拍风景，暗角问题在这价格面前完全不是问题，后期也可以消除。[ ↓ ] 我的图虫从去年十二月份起，我开始加入了图虫大家庭，在一个多月的使用在之后，我认为这是一个非常好的照片分享平台，不止是单反还是手机，都可以将所拍的照片分享在这里。每次分享完照片之后，都会有游客热心的点赞，这是我认为对自己作品的最大鼓励，另外，图虫对版权保护的也比较好，整体来说，其已经是一个非常不错的照片分享平台了，希望图虫越来越好。欢迎访问我的 图虫主页，我会不定期的更新照片，记录并分享每一个值得记忆的时刻！","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"Summary","slug":"Summary","permalink":"https://www.cz5h.com/tags/Summary/"},{"name":"Writing","slug":"Writing","permalink":"https://www.cz5h.com/tags/Writing/"},{"name":"Memory","slug":"Memory","permalink":"https://www.cz5h.com/tags/Memory/"},{"name":"Photos","slug":"Photos","permalink":"https://www.cz5h.com/tags/Photos/"}]},{"title":"Summary and review of 2018, B","slug":"2019-1-20 Summary and review of 2018, B","date":"2019-01-19T23:00:00.000Z","updated":"2020-02-29T18:43:55.536Z","comments":true,"path":"article/aa5f.html","link":"","permalink":"https://www.cz5h.com/article/aa5f.html","excerpt":"This article is a set of memories, belonging to the list of RMBATS. About this test happened on October 20th, I‘ve to say that the nervousness indeed made a negative impact for me, especially when I attended the speaking test. I almost couldn’t fluently speak a complete sentence when I first sat in front of the examiner, which, of course, was the first time for me to talk with the real foreigner. So that it was no surprising that the score of my first speaking test was just 4.5, which was a quiet low score for examinees. Besides, the score of other parts of the test were not good as well, e.g. the listening score was just 5.5, which normally, I need to get 6.5 to reach my target. If I have to find a key reason for this failure, I would say that my preparation was obviously not enough. Also, the failure taught me a lesson that, no pain no gain. [ ↓ ]","text":"This article is a set of memories, belonging to the list of RMBATS. About this test happened on October 20th, I‘ve to say that the nervousness indeed made a negative impact for me, especially when I attended the speaking test. I almost couldn’t fluently speak a complete sentence when I first sat in front of the examiner, which, of course, was the first time for me to talk with the real foreigner. So that it was no surprising that the score of my first speaking test was just 4.5, which was a quiet low score for examinees. Besides, the score of other parts of the test were not good as well, e.g. the listening score was just 5.5, which normally, I need to get 6.5 to reach my target. If I have to find a key reason for this failure, I would say that my preparation was obviously not enough. Also, the failure taught me a lesson that, no pain no gain. [ ↓ ] Time came to the second half of the year, new semester was approaching, and I told my whole plan about PhD to my supervisor, which was a quiet difficult task for me. But, surprisingly, my supervisor showed a fully encouragement to my idea. It seems that people with high-level knowledge always reach the same view. After that, I returned back to the classroom to prepare the IELTS test again. When others prepared their graduated thesis, I spent almost whole time to learn English and contact with foreign supervisor. During this time, I always suffer from the sense of lonely: trying to imagine that everyone was working with a specific target except yourself, worse, you didn’t know what would happen in the future. What a complex feeling! I spent the last two month of last year with this terrible feeling. [ ↓ ] In November, there were several things need to be said, the first one is that I took a speaking lesson form taobao.com, which made my speaking ability up to a new level. My speaking teacher named Daisy, a kind lady, was always talking warmly with me and taught me speak English sentence by sentence with a great patience. We spent exactly 16 class hours together in about half a month. Almost every day at that moment, I spent about one hour to practice my speaking with Daisy. Another important thing is that I bought my first camera, a second-hand ICLE-6000 with the 16-50mm lens. But I didn’t take any formal photos until Dec. What’s more, I attended the yearly PhD Workshop, which made me be able to directly talk with foreign supervisors face to face. The last one was the order of taking IELTS test in Vietnam, and as a consequence, I flied back to my hometown to handle my passport, which is the first time for me to deal with these things. [ ↓ ] In the last month, I experienced too many FIRST things in my life. Mainly speaking, I attended IELTS test twice in one month, which was the first and maybe also the last time for me to do this in my lift. Then the first time to go to Beijing International Airport, the first time to use my passport, the first arrival in Macao, the first experience to travel at first class (as a gift from China International Airline) and the first time to live one week abroad (Hanoi, Vietnam). At that time, too many fresh things came into my life in a very short time (approximately seven days). During this travel or test travel, I had two partners with me, actually they are two enthusiastic little girls, but they both elder than me. We three people spent one week together in Hanoi. The biggest problem for us was to decide what to eat everyday. In these days, they spent much time to prepare test, but I actually walked around in Hanoi with my camera every day time or evening time. Hanoi, the capital of Vietnam, has very crowded streets, and you can see motorbikes anywhere. Moreover, there are too many foreign visitors, of course there are restaurants in every corner of street. This 7-days exprience maybe the most remarkable trip in my life, because I’m not really into traveling as usual. As for the IELTS test, the final result was that I reached the required score (overall = 6.5, each part &gt; 6.0). Generally, I was a little bit excited when I got the final transcript, before that, I sometimes suspected that if it was right for me to walk into this way, but now, the result gave me a big courage. [ ↓ ] Above all, I simply finished writing these important things of 2018. This is a good way, I think, to share my experience to every reader. 2018 has gone in the wind, and we all walk into the new year whether we want or not. In the past twenty days of 2019, which also belong to this semester, there also happened many things, participating the Graduation Ceremony, seeing graduated classmates off or receiving the invited letter from foreign supervisor, each thing left a deep impression on me. [ ↓ ] Totally, by this time, I’ve probably finished all I want to say. Everything above could be expended a long story and all of them impressed me deeply, as I said, maybe, many of them would not happen again in my rest life. So, I told myself I need to write something to record them, since memories could dissipate in my brain, but these words not. In the new year, I also have many things to do, have many problems to solve, have many challenges to overcome, perhaps I would be anxious before to face the situation, but now, I don’t fear them anymore, and I will make full of my afford to beat every monster in the road to the final success! Come on, 2019!!! Note [ ↓ ] I spent about six hours to finish this long article in one time, so perhaps there are some wrong places in sentences, please don’t be too harsh. After it was done, I spent about 5 hours to collect these previous photos and make them into one picture. BTW, what I need to clarify is that I published these three parts in three dates, cause I want them to be arranged in order. Now, the time is as follows, obviously it’s time to sleep, see you tomorrow!","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"Summary","slug":"Summary","permalink":"https://www.cz5h.com/tags/Summary/"},{"name":"Writing","slug":"Writing","permalink":"https://www.cz5h.com/tags/Writing/"},{"name":"Memory","slug":"Memory","permalink":"https://www.cz5h.com/tags/Memory/"},{"name":"Photos","slug":"Photos","permalink":"https://www.cz5h.com/tags/Photos/"}]},{"title":"Summary and review of 2018, A","slug":"2019-1-19 Summary and review of 2018, A","date":"2019-01-18T23:00:00.000Z","updated":"2020-02-29T18:43:55.535Z","comments":true,"path":"article/ab1f.html","link":"","permalink":"https://www.cz5h.com/article/ab1f.html","excerpt":"This article is a set of memories, belonging to the list of RMBATS. It’s time to make a summary for the past whole year. To be honest, I had to say I exprienced a really momerabe year, during which happened a lot of things, all of them are mainly about my further education. Here, I will use some words to write these memories down, for myself and also for all of you. Because of the length of this article, I divided it into three parts, named part A and B. The first part (A) is as follows.","text":"This article is a set of memories, belonging to the list of RMBATS. It’s time to make a summary for the past whole year. To be honest, I had to say I exprienced a really momerabe year, during which happened a lot of things, all of them are mainly about my further education. Here, I will use some words to write these memories down, for myself and also for all of you. Because of the length of this article, I divided it into three parts, named part A and B. The first part (A) is as follows. At the beginning of last year, of course the big point was the Chinese traditional festival, last year, I spent the festival day with my family in our home instead of returning back to the hometown, although we planned to do this for several years, however last year we did that for the first time, so that I didn’t spent the festival with my grandma, I don’t know her thoughts but, for myself, absolutely I want to stay with them as before. The new festival is approaching again, I don’t know the arrangement of my parents, speaking out my own thought is seemly a very tough task for me. [ ↓ ] Well, time went to the new semester, during this whole semester, the biggest problem surrounding me was that if I need to apply for a PhD education, which spent my a lot of time to think about. Another thing I need to mention is that I built this website as my own blog, which I think, is a perfect way to show my thoughts of my daily life, comparing with using the IM applications or the open blogs offered by the third publisher, I think this kind of independence blog, like HEXO, is quiet easy to use, because I don’t care about the end components, e.g., database, of this blog, and there is no trouble about the advertisement, which means the mainly weakness of those public open blogs, such as csdn.net, would not be appeared in my blog. Back to the topic, I spent much time to maintain the blog, which I think, is worth to do. [ ↓ ] In the middle of this semester, I also experienced a very remarkable trip in Wuhan. Originally, I went back there for my driving test, but finally, I met my old friend and tutor of my Alma Mater. Specially, I asked my previous tutor for help about the matter of applying PhD, and he gave me an extremely encouragement. At that time, however I didn’t have any confidence about the application, as a consequence we discuss a lot about the necessity of PhD, the main idea of my tutor was that PhD is an essential step for a high quality life, especially beneficial for the next generation, which I fully understand now. Now, actually, I am working hard for the PhD as he told me. [ ↓ ] Several days later, my cousin’s wedding was coming. In last Labor Day, I came back home and attended his wedding, this was the first time for me to take planes. Such as the airport, check-in, etc. these things are all fresh for me, at that moment I didn’t imagine that in the rest of last year, I went outside by plane for nine times and flying distances exceeded tens of thousands of kilometers. I spent a happy time in my cousin’s wedding, and took a lot of photos of landscape there, which let me start to determine to buy a professional equipment, just like a camera. [ ↓ ] At the end of March, we started to prepare the final check of the project we working on from 2017 to 2018. Later, several students including me were asked to re-build the whole platform in the customer’s machines. The whole process executed in a very slow speed, and after finishing this re-construction, actually the summer vocation was coming. For usual students, this vocation was the time to practice their technical abilities in companies, but for me, as the plan I’ve made, I need to prepare for the PhD application, so those days, I almost used the SWW (small wood worm), a specific application about scientific research, everyday, in which has many advertisement about PhD. At the same time, I found the news about the China Scholarship Council, which had a most important impression for me. After that, I realized that I need to have an English score at first, e.g., IELTS or TOLFE, if I apply for the CSC program. So, an extremely hot and hardworking summer vocation was coming. [ ↓ ] On July 23th, I took my first English lesson in the IELTS training institution. Every morning, I need to spend about one hour to go across several district to arrive the classroom due to the location of my university. What’s more, last summer, the whole Shenyang was in a really high temperature, which gave me a deep impression that I need to change the clothes every day. We eight guys spent about twenty days together to study for the IELTS, during which, there were four teachers teach us reading, listening, speaking and writing respectively. This training let me learn more about IELTS, though the first test after that was failed. [ ↓ ]","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"Summary","slug":"Summary","permalink":"https://www.cz5h.com/tags/Summary/"},{"name":"Writing","slug":"Writing","permalink":"https://www.cz5h.com/tags/Writing/"},{"name":"Memory","slug":"Memory","permalink":"https://www.cz5h.com/tags/Memory/"},{"name":"Photos","slug":"Photos","permalink":"https://www.cz5h.com/tags/Photos/"}]},{"title":"美国版战狼—12勇士","slug":"2019-1-1 美国版战狼—12勇士","date":"2018-12-31T23:00:00.000Z","updated":"2020-02-29T18:43:55.533Z","comments":true,"path":"article/9a85.html","link":"","permalink":"https://www.cz5h.com/article/9a85.html","excerpt":"电影海报Ⅰ","text":"电影海报Ⅰ 电影海报Ⅱ 场景Ⅰ队长失去将军的支持率12人小队孤军奋战在队友附上急需转移但基地组织有火箭炮的情况下队长孤身一人发起冲锋此时赶到的将军的部下也紧随其后一向看不起他的将军也握拳目送他“他要冲了，跟他走！” 场景Ⅱ将军同另一位捷足先登的军阀见面本以为双方要发生冲突但将军却选择了握手言和和平再一次短暂的降临 场景Ⅲ 意味深长的对话：你做了正确的选择正确的选择？这里没那回事这里是阿富汗许许多多帝国的坟场今天是朋友 明天就是敌人就算是你们也一样美国将成为这里的另一个族群如果你们离开就是懦夫，留下来就是我们的敌人只要记住你永远都是我的兄弟等我走进那座城市，我会升起你的旗帜 场景Ⅳ最后的报幕：尽管困难重重第5特战群3营C连第5小队12人仍全数平安归来这支骑兵与友军部队一起攻下马扎里沙里夫为美军最惊人的成就之一军事战略家预计需耗时2年的任务特遣队3周就完成了基地组织视此事为他们所遇到的最惨痛的失利由于是机密任务595小队的队员们低调返家未受到任何英雄式的欢迎或公开致意2014年杜斯塔姆将军出任阿富汗副总统他与米奇·尼尔森至今仍是挚友2012年为了向他们的英勇表现致敬民间团体捐赠在世贸遗址前建立了一座骑兵雕像 尽管本片有极强的主观上的国家主义色彩，但，还是要向保卫世界和平的战士们致敬！","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"观后感","slug":"观后感","permalink":"https://www.cz5h.com/tags/%E8%A7%82%E5%90%8E%E6%84%9F/"},{"name":"胡乱评价","slug":"胡乱评价","permalink":"https://www.cz5h.com/tags/%E8%83%A1%E4%B9%B1%E8%AF%84%E4%BB%B7/"},{"name":"图文","slug":"图文","permalink":"https://www.cz5h.com/tags/%E5%9B%BE%E6%96%87/"},{"name":"剧透","slug":"剧透","permalink":"https://www.cz5h.com/tags/%E5%89%A7%E9%80%8F/"}]},{"title":"应对LeanCloud对于处理性能的限制","slug":"2018-6-26 应对LeanCloud对于处理性能的限制","date":"2018-06-25T22:00:00.000Z","updated":"2020-02-29T18:43:55.531Z","comments":true,"path":"article/502d.html","link":"","permalink":"https://www.cz5h.com/article/502d.html","excerpt":"最近一直想如何才能统计资源分享页面里的资源的下载次数，由于是直接放的资源链接，即点击即可获取，所以没有所谓的拦截页面进行统计，同时作为静态博客也几乎没有带数据存储的动态扩展能力，这时想到了用LeanCloud来实现下载的计数，最后基本实现了这个想法，有兴趣的可以去资源分享里看看效果。","text":"最近一直想如何才能统计资源分享页面里的资源的下载次数，由于是直接放的资源链接，即点击即可获取，所以没有所谓的拦截页面进行统计，同时作为静态博客也几乎没有带数据存储的动态扩展能力，这时想到了用LeanCloud来实现下载的计数，最后基本实现了这个想法，有兴趣的可以去资源分享里看看效果。 更广泛的应用场景其实上面说的对下载次数进行统计实际上就是统计点击的计数，这就可以应用到更多的场景，比如给文章添加喜欢数、顶数、踩数等等，这是可以直接用的，处理逻辑都不变。还有就是类似淘宝五颗星打分数的这种评分机制，也是可以稍作改变就可以。总之，只要是由点击后触发数据更新的场景都可以用此来实现。 这里，对如何实现整个需求暂不细说，主要说一下遇到的主要问题。 LeanCloud查询处理的性能局限 QPS因特网上，经常用每秒查询率来衡量域名系统服务器的机器的性能，其即为QPS。对应fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。计算关系：QPS = 并发量 / 平均响应时间并发量 = QPS * 平均响应时间 这里，LeanCloud对免费实例的QPS做了限制，可以理解，如果同时的查询发送的过多，则会使Lean返回错误代码，如下图所示： 错误码详解 429信息 - Too many requests.含义 - 超过应用的流控限制，即超过每个应用同一时刻最多可使用的工作线程数，或者说同一时刻最多可以同时处理的数据请求。通过 控制台 &gt; 存储 &gt; API 统计 &gt; API 性能 &gt; 总览 可以查看应用产生的请求统计数据，如平均工作线程、平均响应时间等。使用 LeanCloud 商用版或企业版 的用户，如有需要，可以联系我们来调整工作线程数。 原因分析第一遍打开时需要循环发一遍查询来查询每个资源已有的下载数。 这里注意，为什么要循环每次发一次查询呢，因为在LeanCloud中创建的实例场景是广义的计数实例，即我只发一次查询然后处理返回结果，这种方式理论上是可行的，但是在实现上需要附加查询条件，还要考虑在库中的实例不一定只是一个地方的计数统计，总之较为复杂。 更简单的处理方式就是一一对应，一个计数实例（表中的一行记录）就是对应页面某处的一个计数器，只是在这里，由于分享下载的资源有点多，大概200元素，所以如果不加处理的发送查询请求，那几乎算是同时对LeanCloud发起这200次查询请求，由于其访问性能有限，所以需要对请求的发送做一定调整。 123456789101112131415161718$(tar).each(function(index, item) &#123; var url = $(this).attr(\"id\"); var texts = $(this).parent().find(sp).eq(cnt); var url = $(this).attr(\"id\"); query.equalTo(\"url\", url); query.find(&#123; success: function(results) &#123; if (results.length == 0) &#123; texts.text(\"0\"); //初始是 0 return; &#125; for (var i = 0; i &lt; results.length; i++) &#123; var object = results[i]; texts.text(object.get('time')); &#125; &#125; &#125;);&#125;); 我们主要关注QPS的变化，上图中较高的曲线是未经处理时发送查询请求的QPS曲线，这时由于并发的查询数过多，导致LeanCloud达到瞬时的负载上限而出现429错误。官方的错误代码解释为： 429信息 - Too many requests.含义 - 超过应用的流控限制，即超过每个应用同一时刻最多可使用的工作线程数，或者说同一时刻最多可以同时处理的数据请求。通过 控制台 &gt; 存储 &gt; API 统计 &gt; API 性能 &gt; 总览 可以查看应用产生的请求统计数据，如平均工作线程、平均响应时间等。使用 LeanCloud 商用版或企业版 的用户，如有需要，可以联系我们来调整工作线程数。 解决方法解决此问题自然的会想到使用降低同时请求的查询数量，进而想到可以使用延迟执行来实现，这里可以使用setTimeout来针对循环内的每一次查询都进行延时操作。 这里又引出一个问题，那就是如何在each循环中进行延时操作 在each循环中进行延时操作JQuery中的循环each的工作原理，其并不是类似Java那样的顺序循环，即第一次循环代码的执行总是先于第二次循环中代码的执行，这里要特别注意，each中循环的的代码的执行理论上是同时进行的（异步执行），即没有严格的先后执行顺序，对于这一问题，可以统一归类为 JQuery异步执行的代码如何顺序执行 的问题。可以看看这篇文章，JQuery回调、递延对象总结，注意，使用then等对逻辑进行严格控制是正确的，但不是唯一的方法，如果你想完成的按顺序执行仅仅是时间上的先后而没有逻辑上的先后，那么还是用延时来实现比较容易理解。 123$(tar).each(function(index, item) &#123; send.leancloud.query(item); // 伪代码&#125;); 上述执行后几乎是同时发送循环总数的查询请求，这样就容易导致429错误； 12345$(tar).each(function(index, item) &#123; setTimeout(function()&#123; send.leancloud.query(item); // 伪代码 &#125;,1000);&#125;); 上述延时的代码是经典的错误做法，误认为each是同步的顺序的循环，但其实不是，这样添加之后的效果是，全部查询同时在延时1000ms后发出，其结果还是几乎同时发向了LeanCloud。 12345$(tar).each(function(index, item) &#123; setTimeout(function()&#123; send.leancloud.query(item); // 伪代码 &#125;,100*index);&#125;); 上述为正确的写法，注意100*index，这是利用了其异步执行的特点来进行延时，每次循环后的查询请求还是跟之前叙述一样会几乎同时被执行，但是这里执行时的延时时间不一样了，这里变成了0,100,200...，即查询请求会在0ms,100ms,200ms...后被发送给LeanCloud，显然的，达到了控制QPS的要求。 上图是查询发送处理后的QPS曲线，可以看出其值下降了很多，但仍有时很尖锐，可以通过加大查询的发送间隙来降低，当然，图示状态已经可以正常查询且不触发429错误。 最终效果对于时间间隔来说，要综合查询的数量考虑，但总体上不能过大，这样会在前端显示过慢而损失交互性。 其他偶发的断线异常，非本地错误!： 0信息 - (无)含义 - WebSocket 正常关闭，可能发生在服务器重启，或本地网络异常的情况。SDK 会自动重连，无需人工干预。","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://www.cz5h.com/tags/JavaScript/"},{"name":"JQuery","slug":"JQuery","permalink":"https://www.cz5h.com/tags/JQuery/"},{"name":"LeanCloud","slug":"LeanCloud","permalink":"https://www.cz5h.com/tags/LeanCloud/"}]},{"title":"分布式执行代码的认知纠正","slug":"2018-6-10 分布式执行代码的认知纠正","date":"2018-06-09T22:00:00.000Z","updated":"2020-02-29T18:43:55.527Z","comments":true,"path":"article/f5a4.html","link":"","permalink":"https://www.cz5h.com/article/f5a4.html","excerpt":"Spark是一个分布式计算系统/组件/平台，这是都知道的，其用Scala实现Spark任务也是最原生的，但万万不能认为只要是在Spark环境下执行的Scala代码都是分布式执行的，这是大错特错的，一开始一直有错误的认识，但现在想想，如果拿Java和Hadoop的关系来作对比，其就很容易理解了。","text":"Spark是一个分布式计算系统/组件/平台，这是都知道的，其用Scala实现Spark任务也是最原生的，但万万不能认为只要是在Spark环境下执行的Scala代码都是分布式执行的，这是大错特错的，一开始一直有错误的认识，但现在想想，如果拿Java和Hadoop的关系来作对比，其就很容易理解了。 思维纠正 Java&amp;Hadoop的关系 Java是独立的语言，Hadoop本身由Java实现，可以由Java调用； Java编写的一般代码不能够分布式执行，缺少计算模型的支持； Java调用Hadoop实现的具体类方法（如Mapper、Reducer）实现的代码可以在Hadoop之上分布式执行； 同理， Scala&amp;Spark的关系 Scala是独立的语言，Spark本身由Scala实现，可以由Scala调用； Scala编写的一般代码不能够分布式执行，缺少计算模型的支持； Scala调用Spark实现的具体类方法（如Pregel）实现的代码可以在Spark之上分布式执行； 另外值得注意的是，Spark的RDD的Transform和Action操作也都可以分布式执行，这里可以理解为RDD内部的各种算子操作都是基于分布式设计的。除此之外的诸如使用scala基本数据类型实现的代码，都是不能分布式执行的（sacla本身的不可变特性和能不能分布式执行没有关系）。 纠错场景文件的读写如果调用java.util.File来进行文件写入，Local模式自然是没有问题，但是集群分布式运行时，必须先执行collect操作来取回数据到本地，这就造成一个问题，假如在100个节点的集群中执行任务，现在要将文件写入到Linux文件系统，这本身就很搞笑，这样做的后果是，写操作在某个节点上被触发，全部数据都被收集到这个节点，然后此Worker将数据写入到本地，注意，这里的本地就是该Worker所在的节点，如果使用者要查看结果，那么他必须去到该节点的文件系统中查看。 上述就是为什么Spark运行时要将输出写入hdfs的原因，对于hdfs来说，其对于使用者来说就变成了一个存储环境，使用者无需关心数据具体哪部分存在哪个节点上。 所以，对于有写文件操作的代码在提交分布式执行时，切记检查是否调用的java.util.File. 对象的遍历这是最具迷惑性的部分，一开始写Spark代码时可能会在其中充斥着List、Map等等操作对象，更有甚者甚至引用java.util.List，并且希望在循环中对其进行更新，这在本地模式时显然也是正确的，但是其显然也不是分布式执行的代码。 那么，如果我就想维护一个大的K,V结构，并且想分布式执行地更新，那应该怎么做，答案是使用RDD，当然可以使用之前说过的IndexedRDD，这都是RDD做的封装，可能用起来非常别扭，比如由于不可变性，必须每次迭代都重新创建新的RDD等。 正确的分布式执行代码到底什么才是正确的正规的分布式执行代码呢，其实一句话就可以概括，那就是全部逻辑都用RDD操作实现，即如果有个单机串行算法要分布式并行化，如果目标是在Spark上运行，那么最好的方式就是将原算法中的全部逻辑用RDD的操作来实现。 比如，原算法要求度分布，串行版本的程序一般需要遍历一遍数据集并且维护多个集合来完成，那么在这里，只需要句行代码： 12val maxd = rdd.map( x =&gt; &#123; (x._2,1) &#125; ) .reduceByKey( (a,b) =&gt; a+b ).sortBy(x =&gt; x._1) map、reduceByKey、sortBy都是RDD的操作，具体特性以后在细说，这里的实现由于是建立在RDD之上，所以其可以被分布式执行，即原数据量巨大时，其内部实现会令其分发到多个节点的worker进行计算，计算完毕后的结果仍然存储在一个分布式内存数据集RDD中。 后续会继续对RDD的各种操作进行分析。","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"Scala","slug":"Scala","permalink":"https://www.cz5h.com/tags/Scala/"},{"name":"Spark","slug":"Spark","permalink":"https://www.cz5h.com/tags/Spark/"}]},{"title":"IDEA-2017.2-bug-SBT项目初始化的失败","slug":"2018-6-6 IDEA-2017.2-bug-SBT项目初始化的失败","date":"2018-06-05T22:00:00.000Z","updated":"2020-02-29T18:43:55.532Z","comments":true,"path":"article/a61a.html","link":"","permalink":"https://www.cz5h.com/article/a61a.html","excerpt":"这个错误出现过若干次了，每次出现都想记录一下可是都忘了，然后下一次再遇见就又要搞很久才能解决，其实这本身是IntelliJ IDEA 2017.2的一个bug，只要修改一处配置就好了。 注：先前的文章里记得有个地方对这个问题表述的不正确，有时间再改。","text":"这个错误出现过若干次了，每次出现都想记录一下可是都忘了，然后下一次再遇见就又要搞很久才能解决，其实这本身是IntelliJ IDEA 2017.2的一个bug，只要修改一处配置就好了。 注：先前的文章里记得有个地方对这个问题表述的不正确，有时间再改。 原错误信息原错误出现的IDEA版本：2017.2；原错误的出现场景：每次新建SBT项目并添加完SBT依赖之后出现；原错误的表现：build.sbt文件内容解析全部是红的，并且sbt shell报错；原错误报错如下： 12345678910111213141516171819202122[info] Loading settings from idea.sbt ...[info] Loading global plugins from C:\\Users\\msi\\.sbt\\1.0\\plugins[info] Loading settings from plugins.sbt ...[info] Loading project definition from I:\\IDEA_PROJ\\Visualization\\project[info] Loading settings from build.sbt ...[info] Set current project to Visualization (in build file:/I:/IDEA_PROJ/Visualization/)[info] sbt server started at local:sbt-server-a45abbca777ea4158128sbt:Visualization&gt;[info] Defining Global / sbtStructureOptions, Global / sbtStructureOutputFile, shellPrompt[info] The new values will be used by no settings or tasks.[info] Reapplying settings...[info] Set current project to Visualization (in build file:/I:/IDEA_PROJ/Visualization/)[info] Applying State transformations org.jetbrains.sbt.CreateTasks from C:/Users/msi/.IntelliJIdea2017.2/config/plugins/Scala/launcher/sbt-structure-1.1.jar[error] java.lang.ClassNotFoundException: org.jetbrains.sbt.CreateTasks$[error] at java.net.URLClassLoader.findClass(URLClassLoader.java:381)... ... ...[error] at xsbt.boot.Boot$.main(Boot.scala:17)[error] at xsbt.boot.Boot.main(Boot.scala)[error] java.lang.ClassNotFoundException: org.jetbrains.sbt.CreateTasks$[error] Use 'last' for the full log.[info] shutting down serverJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=384M; support was removed in 8.0 Bug解决方法File-&gt;Setting-&gt;Build Tools-&gt;SBT，勾选Use SBT Shell for ... 参考IDEA官方社区的帖子","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"IDEA","slug":"IDEA","permalink":"https://www.cz5h.com/tags/IDEA/"},{"name":"SBT","slug":"SBT","permalink":"https://www.cz5h.com/tags/SBT/"}]},{"title":"音频格式的汇总及压缩比较","slug":"2018-6-5 音频格式的汇总及压缩比较","date":"2018-06-04T22:00:00.000Z","updated":"2020-04-18T21:53:34.655Z","comments":true,"path":"article/2b07.html","link":"","permalink":"https://www.cz5h.com/article/2b07.html","excerpt":"数字音源，也就是数字音频格式，最早指的是CD，CD经过压缩之后，又衍生出多种适于在随身听上播放的格式，这些压缩过的格式，我们可以分为两大类：有损压缩的和无损压缩的。这里所说的压缩，是指把PCM编码的或者是WAV格式的音频流经过特殊的压缩处理，转换成其他格式，从而达到减小文件体积的效果。有损／无损，是指经过压缩过后，新文件所保留的声音信号相对于原来的PCM/WAV格式的信号是否有所削减。","text":"数字音源，也就是数字音频格式，最早指的是CD，CD经过压缩之后，又衍生出多种适于在随身听上播放的格式，这些压缩过的格式，我们可以分为两大类：有损压缩的和无损压缩的。这里所说的压缩，是指把PCM编码的或者是WAV格式的音频流经过特殊的压缩处理，转换成其他格式，从而达到减小文件体积的效果。有损／无损，是指经过压缩过后，新文件所保留的声音信号相对于原来的PCM/WAV格式的信号是否有所削减。 本文转自：果果文本库原文标题：19种音频格式介绍及音质压缩比的比较 音频相关参数速率什么是速率？当然我不能直接给你解释说“速率就是比特率”。大家在用一些软件播放声音文件的时候应该注意到了一个小小的信息。比如“128Kbps”、“1411Kbps”…也有朋友知道了，通常情况下，”Kbps”前面的数字越大，声音效果越好，比如CD就是“1411Kbps”。那么，到底这些数字代表什么呢？简单的说来就是在每秒钟时间内，有多少数据被转换成声音。之所以CD的音质比MP3好，是因为CD在每一秒内的信息比MP3多。比如，128Kbps的MP3文件相比1411Kbps的CD文件，其每秒被转换的数据量，MP3比CD少了近12倍。同样的一首歌曲，CD听来就要细腻得多（当然人群中有这么一群号称“木耳”的人可能觉得效果是一样的）MP3用较少的数据表达相同的内容，其详细程度当然就不如CD了。 采样率采样率也是很常见的一个词语。具体表现形式为“XXHz”，其中“XX”是一个具体数字。比如“44100Hz（44.1KHz）”，“32000Hz（32KHz）”等。之前已经说过了，数码音频文件是由很多个“点”来组成的，那么采样率其实就是采集这些“点”的一个“数量”标准。很显然“44100Hz”比“32000Hz”的采样率要高，所以单位时间内（1秒）搜集到的点就更多。单位时间的点越多声音的信息也就越完善，当然也就更接近于真实。所以，如果在保证速率相同的条件下，“44100Hz”的文件要好于“32000Hz”（当然，这也不是绝对的）。 有损压缩有损压缩的音源，其实我们都比较熟悉，目前流行的有损格式主要有MP3、WMA、OGG、MP3pro、AAC、VQF、ASF等。 WMV格式WMA的全称是WindowsMedia Audio，是微软力推的一种音频格式。WMA格式是以减少数据流量但保持音质的方法来达到更高的压缩率目的，其压缩率一般可以达到1:18，生成的文件大小只有相应MP3文件的一半。 MP3格式MP3的全称是MovingPicture Experts Group Audio Layer Ⅲ。简单的说，MP3就是一种音频压缩技术，由于这种压缩方式的全称叫MPEGAudio Layer 3，所以人们把它简称为MP3。它诞生于1993年，其“父母”是德国夫朗和费研究院（FaunhofeIIS）和法国汤姆生（Thomson）公司。 MP3是利用MPEGAudio Layer 3的技术，将音乐以1:10甚至1:12的压缩率，压缩成容量较小的文件，换句话说，能够在音质丢失很小的情况下把文件压缩到更小的程度。而且还非常好的保持了原来的音质。正是因为MP3体积小，音质高的特点使得MP3格式几乎成为网上音乐的代名词。每分钟音乐的MP3格式只有1MB左右大小，这样每首歌的大小只有3-4兆字节。使用MP3播放器对MP3文件进行实时的解压缩（解码），这样，高品质的MP3音乐就播放出来了。 MP3编码质量分为：固定码率（CBR），平均码率（ABR）和动态码率（VBR）。早期的MP3编码技术并不完善，很长的一段时间以来，大多数人都使用128Kbps的CBR（固定编码率）格式来对MP3文件编码，直到最近，VBR（可变编码率）和ABR（平均编码率）的压缩方式出现，编码的比特率最高可达320Kbps，MP3文件在音质上才开始有所进步，而LAME的出现，则为这一进步带来了质的飞跃。 补充：最高比特率320K，高频部分一刀切是它的缺点。音质不高！ WMA格式WMA的全称是WindowsMedia Audio，是微软力推的一种音频格式。WMA格式是以减少数据流量但保持音质的方法来达到更高的压缩率目的，其压缩率一般可以达到1:18，生成的文件大小只有相应MP3文件的一半。 WMA与MP3音质和体积上的对比特点，可以总结为：低比特率（小于128Kbps）时，WMA体积比MP3小，音质比MP3好；而在高比特率（大于128K）时，MP3的音质则比WMA好。 WMA相对于MP3的最大特点就是有极强的可保护性，可以说，WMA的推出，就是针对MP3没有版权保护的缺点来的。WMA可以通过DRM（Digital Rights Management）方案加入防止拷贝，或者加入限制播放时间和播放次数，甚至是播放机器的限制，可有力地防止盗版。 补充：128kbps为wma最优压缩比，128kbpswma=192kbps mp3 OGG格式Ogg全称应该是OGGVobis（oggVorbis）是一种新的音频压缩格式，类似于MP3等现有的音乐格式。但有一点不同的是，它是完全免费、开放和没有专利限制的。OGGVobis有一个很出众的特点，就是支持多声道，随着它的流行，以后用随身听来听DTS编码的多声道作品将不会是梦想。 Vorbis 是这种音频压缩机制的名字，而Ogg则是一个计划的名字，该计划意图设计一个完全开放性的多媒体系统。目前该计划只实现了OggVorbis这一部分。 Ogg Vorbis文件的扩展名是。OGG。这种文件的设计格式是非常先进的。现在创建的OGG文件可以在未来的任何播放器上播放，因此，这种文件格式可以不断地进行大小和音质的改良，而不影响旧有的编码器或播放器。 补充：目前最好的有损格式之一，MP3部分支持，智能手机装软件部分可以支持，最高比特率500kbps。 Mp3Pro格式在WMA刚开始流行的时候，还没有高品质的MP3，所以当时MP3的地位真的有点动摇了，于是Thomson公司在2001年6月，携手FaunhofeIIS，发布了一种新的格式MP3pro，这是对MP3格式的改良，编码算法比MP3要复杂得多，简单的说，就是分两层编码，在MP3的基础上，再与另外一种技术（SB频段复制技术）混合编码。 Mp3Pro是Mp3编码格式的升级版本。MP3Pro是由瑞典Coding科技公司开发的，在保持相同的音质下同样可以把声音文件的文件量压缩到原有MP3格式的一半大小。而且可以在基本不改变文件大小的情况下改善原先的MP3音乐音质。它能够在用较低的比特率压缩音频文件的情况下，最大程度地保持压缩前的音质。这种格式在低位率的时候，压缩效率非常高，所以在一般音质情况下，同位率的MP3pro的体积要比MP3甚至WMA都小得多，而音质却是三者中最好的。 MP3pro可以实现完全的兼容性。经过mp3Pro压缩的文件，扩展名仍旧是。mp3。可以在老的mp3播放器上播放。老的mp3文件可以在新的mp3pro播放器上进行播放。实现了该公司所谓的“向前向后兼容”。 从技术上讲，MP3pro是一种非常优秀的编码方式，但是它高昂的专利费，使它没有真正流行起来。 RA系列RA、RAM和RM都是Real公司成熟的网络音频格式，采用了“音频流”技术，所以非常适合网络广播。在制作时可以加入版权、演唱者、制作者、Mail 和歌曲的Title等信息。 RA可以称为互联网上多媒体传播的霸主，适合于网络上进行实时播放，是目前在线收听网络音乐最好的一种格式。 MOD格式MOD是一种类似波表的音乐格式，但它的结构却类似MIDI，使用真实采样，体积很小，音质好，在以前的DOS年代，MOD经常被作为游戏的背景音乐。现在的MOD可以包含很多音轨，而且格式众多，如S3M、NST、669、MTM、XM、IT、XT和RT等。 MD格式MD（即MiniDisc）是SONY 公司于1992年推出的一种完整的便携音乐格式，它所采用的压缩算法就是ATRAC技术（压缩比是1∶5）。MD又分为可录型MD（Recordable，有磁头和激光头两个头）和单放型MD（Pre-recorded，只有激光头）。 强大的编辑功能是MD的强项，可以快速选曲、曲目移动、合并、分割、删除和曲名编辑等多项功能，比CD更具个性化，随时可以拥有一张属于自己的MD专辑。MD的产品包括MD随身听、MD床头音响、MD汽车音响、MD录音卡座、MD摄像枪和MD驱动器等。 ASF格式ASF的全称是AdvancedStreaming Format，是微软所制订的一种媒体播放格式，适合在网络上播放。而WindowsMedia On-Demand Producer则是制作ASF档案的免费软件，让即使是初学者也能很轻易的利用现成的WAV或AVI档案制作ASF文件。 AAC格式AAC实际上是高级音频编码的缩写。AAC是由Fraunhofer IIS-A、杜比和AT&amp;T共同开发的一种音频格式，它是MPEG-2规范的一部分。AAC所采用的运算法则与MP3的运算法则有所不同，AAC通过结合其他的功能来提高编码效率。AAC的音频算法在压缩能力上远远超过了以前的一些压缩算法（比如MP3等）。它还同时支持多达48个音轨、15个低频音轨、更多种采样率和比特率、多种语言的兼容能力、更高的解码效率。总之，AAC可以在比MP3文件缩小30%的前提下提供更好的音质。 补充：目前最好的有损格式之一。有多种编码，faac，nero为常见，比特率最高448kbps。硬件支持方面，高级mp3和现在手机普遍支持。 MID格式MID是midi的简称，是它的扩展名。MIDI是英语MusicInstrument Digital Interface 的缩写，翻译过来就是“数字化乐器接口”，也就是说它的真正涵义是一个供不同设备进行信号传输的接口的名称。我们如今的MIDI音乐制作全都要靠这个接口，在这个接口之间传送的信息也就叫MIDI信息。MIDI最早是应用在电子合成器一种用键盘演奏的电子乐器上，由于早期的电子合成器的技术规范不统一，不同的合成器的链接很困难，在1983年8月，YAMAHA、ROLAND、KAWAI等著名的电子乐器制造厂商联合指定了统一的数字化乐器接口规范，这就是MIDI1.0技术规范。此后，各种电子合成器已经电子琴等电子乐器都采用了这个统一的规范，这样，各种电子乐器就可以互相链接起来，传达MIDI信息，形成一个真正的合成音乐演奏系统。 由于多媒体计算机技术的迅速发展，计算机对数字信号的强大的处理能力，使得计算机处理MIDI信息成为顺理成章的事情了，所以，现在不少人把MIDI音乐称之为电脑音乐。事实上，利用多媒体计算机不但可以播放、创作和实时地演奏MIDI音乐。甚至可以把MIDI音乐转变成看的见的乐谱（五线谱或简谱）打印出来，反之，也可以把乐谱变成美妙的音乐。利用MIDI的这个性质，可以用于音乐教学（尤其是识谱），让学生利用计算机学习音乐知识和创作音乐。 M4A格式M4A是MPEG4音频标准的文件的扩展名。在MPEG4标准中提到，普通的MPEG4文件扩展名是。mp4。自从Apple开始在它的iTunes以及iPod中使用。m4a以区别MPEG4的视频和音频文件以来，。m4a这个扩展名变得流行了。目前，几乎所有支持MPEG4音频的软件都支持。m4a。最常用的。m4a文件是使用AAC格式的（文件），不过其他的格式，比如AppleLossless甚至mp3也可以被放在。m4a容器里（TC注：这个container的概念类似于。mkv文件）。可以安全的把只包含音频的。mp4文件的扩展名改成。m4a，以便让它能在你喜欢的播放器里播放，反之亦然。 VQF格式VQF格式是由YAMAHA和NTT共同开发的一种音频压缩技术，它的核心是减少数据流量但保持音质的方法来达到更高的压缩比，它的压缩率能够达到1:18，因此相同情况下压缩后VQF的文件体积比MP3小30%～50%，更便利于网上传播，同时音质极佳，接近CD音质（16位44.1kHz立体声）。可以说VQF技术上也是很先进的，但VQF未公开技术标准，由于宣传不力，这种格式难有用武之地，至今未能流行开来。 AAC+格式AAC+也称之为HE-AAC。 HE意思是”highefficiency”（高效性）。HE-AAC混合了AAC与SBR技术。SBR代表的是SpectralBand Replication（频段复制）。SBR的关键是在低码流下提供全带宽的编码而不会产生多余的信号。传统认为音频编码在低码流下意味着减少带宽和降低采样率（见MP3FAQ #7）或产生令人不快的噪音信号。SBR解决问题的方法是让核心编码去编码低频信号，而SBR解码器通过分析低频信号产生高频信号和一些保留在比特流中的指导信号（通常码流极低，~2kbps）。这就是采用无SBR解码器的原因，这样你的带宽（frequencyresponse）（频率响应）会被严重浪费。这也是为什么被叫做SpectralBand Replication的原因，它只是增加音频的带宽，而非重建。 AIFF与AU格式这里顺便提一下由苹果公司开发的AIFF（Audio Interchange FileFormat） 格式和为UNIX系统开发的AU格式，它们都和WAV非常相像，在大多数的音频编辑软件中也都支持它们这几种常见的音乐格式。 无损压缩对于我们最常说的“无损音频”来说，一般都是指传统CD格式中的16bit/44.1kHz采样率的文件格式，而知所以称为无损压缩，也是因为其包含了20Hz-22.05kHz这个完全覆盖人耳可闻范围的频响频率而得名。 音频的无损压缩，大家可能还比较陌生，但这并不意味着无损压缩技术发展得不好，相反，在无损压缩领域，早就有许多很出色的作品，比如APE、FLAC、WavPack、LPAC、WMALossless、AppleLossless、La、OptimFOG、Shoten等。 CD格式即CD唱片，一张CD可以播放74分钟左右的声音文件，Windows系统中自带了一个CD播放机，另外多数声卡所附带的软件都提供了CD播放功能，甚至有一些光驱脱离电脑，只要接通电源就可以作为一个独立的CD播放机使用。 标准CD格式为44.1K的采样，速率88K/秒，16位量化位数，因为CD 可以说是近似无损的，因此它的声音基本上是忠于原声的，因此如果你如果是一个音响发烧友的话，CD是你的首选。它会让你感受到天籁之音。CD光盘可以在CD唱机中播放，也能用电脑里的各种播放软件来重放。但注意：不能直接的复制CD格式的文件到电脑硬盘上播放，需要使用像EAC这样的抓音轨软件把CD格式的文件转换成WAV，这个转换过程基本上是无损的。推荐大家使用这种方法。 WAV格式WAV格式是微软公司开发的一种声音文件格式，也叫波形声音文件，是最早的数字音频格式，被Windows平台及其应用程序广泛支持。WAV格式是以RIFF格式为标准的。RIFF是英文ResourceInterchange File Format的缩写，每个WAVE文件的头四个字节便是“RIFF”。WAVE文件由文件头和数据体两大部分组成。 WAV格式支持许多压缩算法，支持多种音频位数、采样频率和声道，采用44.1kHz的采样频率，16位量化位数，因此WAV的音质与CD相差无几，但WAV格式对存储空间需求太大不便于交流和传播。 补充：WAV属于无损音乐格式，缺点：体积十分大！ FLAC格式FLAC即是FreeLossless Audio Codec的缩写，全称应该叫OGGFLAC，中文可解为无损音频压缩编码。它是OGG计划的一部分，当然也就是开源、免费的了，这也难怪它这么快就得到了多家MP3厂商的支持。 FLAC是一套著名的自由音频压缩编码，其特点是无损压缩。FLAC压缩比可以达到2：1，对于无损压缩来说，这已经是相当高的比例了；而且它解码速度快，只需进行整数运算即可完成整个解码过程，对CPU的运算能力要求很低，所以普通的随身听，都可以轻松实现实时解码。 不同于其他有损压缩编码如MP3 及AAC，它不会破坏任何原有的音频资讯，所以可以还原音乐光盘音质。现在它已被很多软件及硬件音频产品所支持。简而言之，FLAC与MP3相仿，但是是无损压缩的，也就是说音频以FLAC方式压缩不会丢失任何信息。这种压缩与Zip的方式类似，但是FLAC将给你更大的压缩比率，因为 FLAC是专门针对音频的特点设计的压缩方式，并且你可以使用播放器播放FLAC压缩的文件，就象通常播放你的MP3文件一样。 补充：为无损格式，较ape而言，他体积大点，但是兼容性好，编码速度快，播放器支持更广 APE格式APE是目前流行的、由Monkey’sAudio出品的一种数字音乐文件格式，它出现得比FLAC早，而且名气也比FLAC大。与MP3、OGG这类有损压缩方式不同，APE是目前世界上惟一得到公认的音频无损压缩格式，也就是说当您将从音频CD上读取的音频数据文件压缩成APE格式后，您还可以再将APE格式的文件还原，而还原后的音乐文件与压缩前一模一样，没有任何损失。而现在越来越多的人将它在网络传播，因为被压缩后的APE文件容量要比WAV源文件小一半多，可以节约传输所用的时间，也更方便传播！由于APE的采样率高达800kbps～1400kbps，接近于音乐CD的1411.2kbps，远远高于MP3的128kbps，因此它在压缩后的音质和源文件音质几乎毫无差异，其音质之佳已经过了严格的盲听测试，得到了全世界发烧友的公认。APE的这些特点，都是其他无损压缩格式所争相效仿的。 在APE出现之前，音乐迷们都认为以CD或者WAV来保存自己喜欢的音乐素材是最好的方法了，但APE的出现，足以使他们改变这种看法，因为APE既可以保持音乐信号的无损，又可以以比WAV高得多的压缩率（接近2：1）压缩WAV文件，而且可以无须解压而直接播放。由于压缩后的APE文件只有原文件一半左右大小，APE格式受到了许多音乐爱好者的喜爱，特别是对于希望通过网络传输音频CD的朋友来说，APE可以帮助他们节约大量的资源。APE如此流行，在网上也比较容易能下载到APE格式的文件。 补充：为无损压缩格式，较flac而言，他体积较小。编码速度偏慢。 压缩比较压缩比aac &gt; ogg &gt; mp3（wma） &gt; ape &gt; flac &gt; wav（同一音源条件下） mp3和wma以192kbps为分界线，192kbps以上mp3好，192kbps以下wma好。 音质wav = flac = ape &gt; aac &gt; ogg &gt; mp3 &gt; wma 综合音质体积编码率aac &gt; ogg &gt; flac &gt; ape &gt; mp3 &gt; wav、wma","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"音频","slug":"音频","permalink":"https://www.cz5h.com/tags/%E9%9F%B3%E9%A2%91/"},{"name":"压缩","slug":"压缩","permalink":"https://www.cz5h.com/tags/%E5%8E%8B%E7%BC%A9/"}]},{"title":"使用D3.JS进行坐标轴绘制和图绘制","slug":"2018-6-2 使用D3.JS进行坐标轴绘制和图绘制","date":"2018-06-01T22:00:00.000Z","updated":"2020-02-29T18:43:55.529Z","comments":true,"path":"article/30b8.html","link":"","permalink":"https://www.cz5h.com/article/30b8.html","excerpt":"前面已经说过D3的功能十分强大，但是往往实际使用时只需要用到一部分内容，在这里，就只用到了 *比例尺 *和 *布局 *两部分，外加 *核心 *的请求部分（请求数据），分别用来绘制Graph的显示坐标轴和图的顶点及边；","text":"前面已经说过D3的功能十分强大，但是往往实际使用时只需要用到一部分内容，在这里，就只用到了 *比例尺 *和 *布局 *两部分，外加 *核心 *的请求部分（请求数据），分别用来绘制Graph的显示坐标轴和图的顶点及边； 绘制坐标轴传统坐标轴这里指的是 *第一象限 *的坐标轴，即两轴的坐标均为正数，坐标原点为(0,0) 具体可以看 这里，说的比较详细。 十字坐标轴这里指的是 *全象限 *坐标轴，即两轴的坐标均从-∞开始，坐标原点为(0,0) 本质上，仍然是一般坐标轴的变形，主要原理有两点： 一是利用 *比例尺 *对源数据做符合中心坐标轴的变换； 二是创建坐标轴时利用attr(&quot;transform&quot;,&quot;translate(0,&quot;+0.5*svgHight+&quot;)&quot;)来对坐标轴进行平移，从而达到原点在画布中心的十字坐标轴的效果。 创建比例尺123456// 创建比例尺 var xScale = d3.scale.linear() .domain([-50,50]).range([0,1000]); //意思为生成数据是[-50,50]，现在要映射到[0,1000]var yScale = d3.scale.linear() .domain([-50,50]).range([1000,0]); 添加坐标轴123456789101112131415161718192021222324252627var formatPrecision = d3.format(''); // 定义X轴 var xAxis = d3.svg.axis() .scale(xScale) .ticks(11) // 粗略的设置刻度线的数量，包括原点 .orient('bottom') .tickFormat(formatPrecision); // 设置刻度格式// 定义Y轴 var yAxis = d3.svg.axis() .scale(yScale) .orient('left') .ticks(11) .tickFormat(formatPrecision);// 创建X轴, svg中： g元素是一个分组元素 svg.append('g') .attr('class', 'axis') .attr(\"transform\",\"translate(0,\"+0.5*svgHight+\")\") // 平移到水平中间 .call(xAxis); // 创建Y轴 svg.append('g') .attr('class', 'axis') .attr(\"transform\",\"translate(\"+0.5*svgWidth+\",0)\") // 平移到竖直中间 .call(yAxis); 绘制图（circle+line）关于图的绘制，本质上就是圆点和线的绘制，所以这也解释了为什么输入文件中的边数据也需要包含坐标的原因，因为在d3中绘制顶点和绘制边是互不相关的。 另外需要注意的是，这里不要直接返回源数据坐标，要带入到上述定义的 *比例尺 *中。 话不多说直接上代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 创建SVG var svg = d3.select('#div'+index) .append('svg') .attr('width', svgWidth) .attr('height', svgHight); // 设置标题 svg.append('text') .attr('x', svgWidth / 2 - 120) .attr('y', 30) .attr('class', 'title') .text('这是一个用d3画的简略坐标轴'); // 画点，即绘制图的顶点svg.selectAll('circle') .data(data.nodes) // json对象 .enter() .append('circle') .attr('cx', function(d) &#123; return xScale(d.cx); // 使用比例尺返回合适的变换 &#125;) .attr('cy', function(d) &#123; return yScale(d.cy); // 同上 &#125;) .attr(\"fill\",\"#6495ed\") // 填充颜色 .attr(\"origin\", function(d) &#123; return d.cx+\",\"+d.cy; &#125;) .attr('r', function(d) &#123; // 圆点直径（大小） if(d.value === 0||d.value/2===0)return 2; else return d.value/2; &#125;); // 画线，即绘制图的边svg.selectAll('line') .data(data.links) // json对象 .enter() .append('line') .attr('x1',function(d)&#123; return xScale(d.x1); &#125;) .attr('y1',function(d)&#123; return yScale(d.y1); &#125;) .attr('x2',function(d)&#123; return xScale(d.x2); &#125;) .attr('y2',function(d)&#123; return yScale(d.y2); &#125;) .attr(\"stroke\",\"gray\") // 边的颜色 .attr('stroke-width', function() &#123; return 0.2; // 边的宽度（粗细） &#125;); 数据读入在数据读取方面，d3.js本身提供了一套请求操作，属于 *核心 *部分。具体的操作如下： 可以直接使用上述的API进行文件读取，非常方便 1234d3.csv(\"data.csv\",function(error,data)&#123; if(error)&#123;...&#125; esle&#123;...&#125;&#125;); 当然可以完美的结合JQuery进行数据操作： 123$.getJSON(\"data.json, \"\", function(data) &#123; // 前面各部分内容，对data进行解析即可&#125; 最终效果数据格式，见文章 D3+Node快速实现图数据的可视化","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"图数据","slug":"图数据","permalink":"https://www.cz5h.com/tags/%E5%9B%BE%E6%95%B0%E6%8D%AE/"},{"name":"D3","slug":"D3","permalink":"https://www.cz5h.com/tags/D3/"}]},{"title":"D3+Node快速实现图数据的可视化","slug":"2018-5-27 D3+Node快速实现图数据的可视化","date":"2018-05-26T22:00:00.000Z","updated":"2020-02-29T18:43:55.524Z","comments":true,"path":"article/45f4.html","link":"","permalink":"https://www.cz5h.com/article/45f4.html","excerpt":"这里的图数据特指布局后的图数据，主要包括顶点信息（ID和坐标等）以及边信息，先前已经写过如何使用Gephi来进行数据的可视化，具体文章见：","text":"这里的图数据特指布局后的图数据，主要包括顶点信息（ID和坐标等）以及边信息，先前已经写过如何使用Gephi来进行数据的可视化，具体文章见： Gephi-Toolkit的引入与使用 Gexf Gexf是Gephi的输入数据格式，其本质上是XML文件格式，标注了顶点信息和边信息。 如果我们想让自己的布局代码生成的数据直接拿到Gephi中展示，那就还需要有一步将数据构造成上图的格式，说道使用Gephi进行布局的可视化，虽然可以使用Gephi-Toolkit进行，已经是比较轻量的嵌入到原有项目中，但还是耦合较高，需要多处硬编码联动，并且在二次利用时针对XML的解析往往是不够高效的。 JSONJSON格式大家都十分耳熟能详了，针对上述的Gexf的种种局限，使用前端可视化工具可以作为一个解决方案，布局程序只需生成指定格式的JSON数据，然后由D3.js进行解析绘制即可。 由上图可以看出，其整体结构比Gexf要更简单，但是有部分坐标冗余，不过影响不大，使用JSON的好处是可以方便的对数据进行操作（无论是在前端还是后端） D3.JS关于D3的详细叙述，请移步 这里，注意现在已经有 D3.V4 版本了（其实V5也有了）。 Node的作用这里为什么要用Node，其实主要是为了起一个Server，由上面的叙述可以知道，这里d3需要读取json文件，那么问题来了，直接静态打开是会报错的，必须放到一个Server内以请求的方式进行才可以，这个Server从何而来，当然可以是Tomcat，但是，人总是要接受新事物，Node的强大已经在各方面都慢慢体现出来了，服务容器当然也有他的身影，这里我只用了其中一种方式（http-server），如果时间允许自己实现一个也是可以的。 http-server除了可以快速起Server外，还具有实时更新的功能，即，我只管往目录内写（更新）文件，然后用d3进行绘制，更新的部分会自动更新到Server，即重写覆写文件后我不需要重开Server，只需要刷新一下页面即可。 http-server的使用方式123456789101112131415hadoop@msi-PC MINGW64 ~/Desktop/CNPM$ node --version // 随便新建个目录并初始化v8.9.0hadoop@msi-PC MINGW64 ~/Desktop/CNPM$ cnpm install http-server -g // 安装http-serverhadoop@msi-PC MINGW64 ~/Desktop/CNPM$ http-server -p 8888 // 开启ServerStarting up http-server, serving ./Available on: http://192.168.230.1:8888 http://192.168.146.1:8888 http://127.0.0.1:8888Hit CTRL-C to stop the server 完成后的目录如下所示，这里的整个目录就如同Tomcat的Webapp目录一样； 注意：这里的文件是可以动态增删改的 最后的检验这里使用d3直接尝试读取生成的csv文件，目的是验证d3是否能够取到生成的数据文件。 1234567891011121314151617&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width\"&gt; &lt;script src=\"https://cdn.bootcss.com/d3/4.13.0/d3.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; d3.csv(\"nodes.csv\",function(error,csvdata)&#123; if(error)&#123; console.log(error); &#125; console.log(csvdata); &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt;&lt;/body&gt;&lt;/html&gt; 打开浏览器输入localhost:8888/view.html在Console中会发现csv内的数据已经被读取到。 注意，必须是以请求的方式进行读取，否则会发生如下错误（如果直接以静态资源的方式打开view.html） 坐标轴绘制、图绘制详见 使用D3.JS进行坐标轴绘制和图绘制","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"D3","slug":"D3","permalink":"https://www.cz5h.com/tags/D3/"},{"name":"Node","slug":"Node","permalink":"https://www.cz5h.com/tags/Node/"}]},{"title":"IDEA自动生成Javadoc代码注释","slug":"2018-5-26 IDEA自动生成Javadoc代码注释","date":"2018-05-25T22:00:00.000Z","updated":"2020-02-29T18:43:55.523Z","comments":true,"path":"article/f986.html","link":"","permalink":"https://www.cz5h.com/article/f986.html","excerpt":"在日常写代码时往往不会注重注释的格式、规范等问题，可能注释都不会写，但是一旦代码完成后要交付他人，就需要考虑注释的问题了，因为重要函数、方法的注释往往对整个代码的阅读起着十分重要的作用，在eclipse中，我们可以自动生成注释的模板，在IDEA中显然也是可以的，下面就介绍两种生成注释的方式。","text":"在日常写代码时往往不会注重注释的格式、规范等问题，可能注释都不会写，但是一旦代码完成后要交付他人，就需要考虑注释的问题了，因为重要函数、方法的注释往往对整个代码的阅读起着十分重要的作用，在eclipse中，我们可以自动生成注释的模板，在IDEA中显然也是可以的，下面就介绍两种生成注释的方式。 JindentJindent是一个十分强大的代码格式化工具，它不局限于IDE插件，其本身就是进行代码格式化的，官网如下：http://www.newforms-tech.com/products/jindent/about 安装教程 点击此处 注意：注释添加不成功，弹出需要License许可，看来不是拿来即用的插件；可以对代码进行格式化，但只能对Java和C/C++代码进行代码格式化，其他语言执行格式化不会有反应； Live Templates这是IDEA的自带功能，主要目的是使用快捷键快速生成固定模式的代码： 123def main(args: Array[String]): Unit &#x3D; &#123; $END$&#125; 比如上述代码只需在编辑器内输入main四个字符，然后敲击tab键即可出现整段代码，利用这一特性，当然的可以进行函数注释的生成。 首先，在File-&gt;Setting-&gt;搜索Live转到Live Templates，然后新建Template并输入名称 选中建好的Template再点击上图中的Live Template，新建一个模板； 对于注释的模板，可以按类和函数分为class和def两个关键词，例如def的注释 可以发现，其关键就是变量参数指定的Expression，正是这些指定使得模板可以将参数、返回类型等统统取到并替换进模板中。 最终的效果是，只需编辑def+tab键，即可在指定位置生成注释，然后将description添加上之后，对这个函数的注释就算完成了。 按同样的方式，可以对比如Class等结构定义模板，进行注释的快速生成","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"IDEA","slug":"IDEA","permalink":"https://www.cz5h.com/tags/IDEA/"},{"name":"扩展","slug":"扩展","permalink":"https://www.cz5h.com/tags/%E6%89%A9%E5%B1%95/"}]},{"title":"Spark集群从搭建到任务提交-第N次记录","slug":"2018-5-24 Spark集群从搭建到任务提交-第N次记录","date":"2018-05-23T22:00:00.000Z","updated":"2020-02-29T18:43:55.522Z","comments":true,"path":"article/1289.html","link":"","permalink":"https://www.cz5h.com/article/1289.html","excerpt":"作为一名合格的计算机人士，百折不挠的瞎折腾精神是必备的。今天本想使用一下尘封已久的VMware虚拟机搭的集群，结果发现 Spark 有各种问题，应该是之前潦草搭集群时挖下的坑（前几天也用过，但并不是cluster mode，我现在才知道..），面对这些坑，果断的选择重装啊，所以叒叒叒开始愉快的搭环境了，，","text":"作为一名合格的计算机人士，百折不挠的瞎折腾精神是必备的。今天本想使用一下尘封已久的VMware虚拟机搭的集群，结果发现 Spark 有各种问题，应该是之前潦草搭集群时挖下的坑（前几天也用过，但并不是cluster mode，我现在才知道..），面对这些坑，果断的选择重装啊，所以叒叒叒开始愉快的搭环境了，， 不过这次格外注重了各处细节，力图条理清晰的记录一次搭建过程，除了 Scala 和 Spark 的搭建过程，当然还有运行调试（这才是关键）部分，包括用IDEA打包 jar 上传执行 和IDEA远程提交执行，这里也都分别作了记录。 关于IDEA提交Spark任务的几种方式，可以参见我 另一篇文章 . 集群环境 得亏了我16G的内存，四个虚拟机全开还可以娱乐的玩耍，这四台虚拟机已经装过Hadoop了，Hadoop集群用起来也没什么问题，就保留了。 各版本如下： 配置项 版本 备注 Hadoop 2.7.3 Java 1.8.0 Scala 2.11.8 待安装 Spark 2.2.0 待安装 主节点安装Scala环境 下载、解压、改名、放到自定义路径 123$ wget http://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz$ tar -zxvf scala-2.11.8.tgz$ mv scala-2.11.8.tgz scala 更新 /etc/profile 1234567$ sudo vi /etc/profile //在文件的最后插入 export SCALA_HOME=/usr/local/scala export PATH=$PATH:$SCALA_HOME/bin$ source /etc/profile 检测是否安装成功 1$ scala -version 主节点配置Spark 下载、解压、改名、放到自定义目录 123$ wget http://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz$ tar -zxvf spark-2.2.0-bin-hadoop2.7.tgz$ mv spark-2.2.0-bin-hadoop2.7 spark 更新 /etc/profile 1234567$ vi /etc/profile //在最后加入 export SPARK_HOME=/usr/local/spark export PATH=$PATH:$SPARK_HOME/bin$ source /etc/profile 修改Spark配置文件 12345678910111213141516171819202122232425$ cd spark/conf//先改名，把template去掉$ mv spark-env.sh.template spark-env.sh$ mv slaves.sh.template slaves.sh$ vi conf/spark-env.sh //在最后添加各项变量值 export JAVA_HOME=/usr/local/java/jdk1.8.0_112 export HADOOP_HOME=/usr/local/hadoop export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop export SCALA_HOME=/usr/local/scala export SPARK_MASTER_IP=192.168.146.130(hadoop01) export SPARK_WORKER_MEMORY=1g export SPARK_WORKER_CORES=1 export SPARK_WORKER_INSTANCES=1$ vi conf/slaves //在最后添加各从节点映射（主机名或IP） hadoop02 hadoop03 hadoop04//还有spark-defaults.conf，一开始没改，结果导致出错 $ # spark-defaults.conf 的修改在后面 拷贝分发调试集群 分发拷贝到各 Slave 节点（其实可以脚本化，偷懒..） 123456789101112//scala$ scp -r scala hadoop02:/usr/local/$ scp -r scala hadoop03:/usr/local/$ scp -r scala hadoop04:/usr/local///spark$ scp -r spark hadoop02:/usr/local/$ scp -r spark hadoop03:/usr/local/$ scp -r spark hadoop04:/usr/local///profile$ sudo scp /etc/profile hadoop02:/etc/profile$ sudo scp /etc/profile hadoop03:/etc/profile$ sudo scp /etc/profile hadoop04:/etc/profile 调试集群 因为我们只需要使用hadoop的HDFS文件系统，所以我们并不用把hadoop全部功能都启动。 1$ start-dfs.sh 因为 hadoop/sbin 以及 spark/sbin 均配置到了系统的环境中，它们同一个文件夹下存在同样的 start-all.sh 文件。最好是打开spark-2.2.0 ，在文件夹下面打开该文件。 12$ cd /usr/local/spark/sbin$ ./start-all.sh 各节点的正常状态 123456789101112131415161718192021[hadoop@hadoop01 ~]$ jps18822 SecondaryNameNode18521 NameNode18634 DataNode18990 Master19055 Jps[hadoop@hadoop02 ~]$ jps33380 DataNode33589 Jps33519 Worker[hadoop@hadoop03 ~]$ jps25876 Jps25656 DataNode25806 Worker[hadoop@hadoop04 ~]$ jps32162 Worker32025 DataNode32234 Jps 注意这里是 3 个 worker 因为 master 节点没有 配置启动 Worker，当然可以配置（比如 hdfs 就是四个 datanode）但是这里 spark 要执行计算任务，所以主节点最好不要有worker以免出现计算任务争夺主节点资源 Spark UI 正常视图 IDEA 项目打包 项目示例 这里的实例程序 读取 hdfs 文件 Vote-demo.txt，并且使用 GraphX 读取文件生成图，并打印图的边数。 示例代码 RemoteDemo.scala 12345678910111213141516171819202122232425262728293031323334package Remoteimport org.apache.spark.graphx.&#123;Edge, Graph&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object RemoteDemo &#123; def main(args: Array[String]) &#123; val conf = new SparkConf() .setAppName(\"SimpleDemo\") .setMaster(\"spark://hadoop01:7077\") .setJars(List(\"I:\\\\IDEA_PROJ\\\\VISNWK\\\\out\\\\artifacts\\\\visnwk_jar\\\\visnwk.jar\")) val sc = new SparkContext(conf) def loadEdges(fn: String): Graph[Any, String] = &#123; val edges: RDD[Edge[String]] = sc.textFile(fn).filter(l =&gt; !(l.startsWith(\"#\"))).map &#123; //无放回 line =&gt; val fields = line.split(\"\\t\") Edge(fields(0).toLong, fields(1).toLong, \"1.0\") &#125; val graph: Graph[Any, String] = Graph.fromEdges(edges, \"defaultProperty\") graph &#125; val graph = loadEdges(\"hdfs://hadoop01:9000/TVCG/SNAP/DATASET/Vote-demo.txt\") println(s\"graph.edges.count() = $&#123;graph.edges.count()&#125;\") sc.stop // 一开始没加，报错了 &#125;&#125; 打包项目，注意指定 Main Class Build 打包 运行配置 Run Configure 错误，IDEA远程连接失败 错误详情 错误排查一 Spark://Hadoop01:7077 spark://host_name:7077 改为 spark://master_ip:7077 修改配置 spark-env.sh 重启 saprk 集群 12$ ./sbin/stop-all.sh$ ./sbin/start-all.sh Spark UI Master_IP:8080 显示正常 错误排查二 修改 Spark spark-defaults.conf 错误排查三 排除集群本身问题，尝试spark-submit 提交 采用不打包依赖的方式打包（注意打包后只有 300kb） 集群 打印了下述错误 123456789101112131415 [hadoop@hadoop01 bin]$ ./spark-submit --class \"Remote.RemoteDemo\" ~/visnwk-build.jar17/07/03 19:48:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable17/07/03 19:49:03 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.org.apache.spark.SparkException: Could not find AppClient. at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:154) at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:134) at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:644) at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:178) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107) at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) 解决： 这里 示例代码最后添加： 1sc.stop 集群提交 运行正常 1http://192.168.146.130:4040/jobs/ 4040 UI界面只有在job运行时才可见，运行完后就不可访问 集群输出正常 回到IDEA提交问题 比较这个错误（和一个错误IP相比） 1234567&#x2F;&#x2F;直接用xxxx这个错误IP报错如下18&#x2F;05&#x2F;25 19:06:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http:&#x2F;&#x2F;118.202.40.210:404218&#x2F;05&#x2F;25 19:06:20 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark:&#x2F;&#x2F;xxxxx:7077...18&#x2F;05&#x2F;25 19:06:23 WARN TransportClientFactory: DNS resolution for xxxxx:7077 took 2551 ms18&#x2F;05&#x2F;25 19:06:23 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master xxxxx:7077org.apache.spark.SparkException: Exception thrown in awaitResult at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77) 123456789&#x2F;&#x2F;用master_ip报错如下18&#x2F;05&#x2F;25 19:07:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http:&#x2F;&#x2F;118.202.40.210:404018&#x2F;05&#x2F;25 19:07:27 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark:&#x2F;&#x2F;192.168.146.130:7077...18&#x2F;05&#x2F;25 19:07:27 INFO TransportClientFactory: Successfully created connection to &#x2F;192.168.146.130:7077 after 23 ms (0 ms spent in bootstraps)18&#x2F;05&#x2F;25 19:07:27 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 192.168.146.130:7077org.apache.spark.SparkException: Exception thrown in awaitResult at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77) &#x2F;&#x2F;比较上述代码，会发现虽然最后的错误一样，但是中间日志并不一样，所以并不是简单的连接失败 怀疑是 7077 端口的问题，但发现绑定一切正常 怀疑是版本的问题了，集群是 scala-2.11.8 + Spark-2.2.0 解决： 这里 修改 sbt 中 spark 的版本，原来的是 2.1.0 我擦！ 12libraryDependencies +&#x3D; &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.2.0&quot;libraryDependencies +&#x3D; &quot;org.apache.spark&quot; %% &quot;spark-graphx&quot; % &quot;2.2.0&quot; 再运行，已越过那个错误~（原来是setJar注释掉了），补全之： 1234567val conf &#x3D; new SparkConf() .setAppName(&quot;SimpleDemo&quot;) .setMaster(&quot;spark:&#x2F;&#x2F;192.168.146.130:7077&quot;) &#x2F;&#x2F;.setIfMissing(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;) &#x2F;&#x2F; 不设置会默认使用本机的物理IP .setJars(List(&quot;I:\\\\IDEA_PROJ\\\\VISNWK\\\\out\\\\artifacts\\\\visnwk_jar\\\\visnwk.jar&quot;))val sc &#x3D; new SparkContext(conf) 完美的收官： 12345678910111213141518&#x2F;05&#x2F;25 19:25:36 INFO DAGScheduler: Job 0 finished: reduce at EdgeRDDImpl.scala:90, took 60.614562 sgraph.edges.count() &#x3D; 103689 &#x2F;&#x2F; 终于等到你！！18&#x2F;05&#x2F;25 19:25:36 INFO SparkUI: Stopped Spark web UI at http:&#x2F;&#x2F;118.202.40.210:404018&#x2F;05&#x2F;25 19:25:36 INFO StandaloneSchedulerBackend: Shutting down all executors18&#x2F;05&#x2F;25 19:25:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down18&#x2F;05&#x2F;25 19:25:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!18&#x2F;05&#x2F;25 19:25:36 INFO MemoryStore: MemoryStore cleared18&#x2F;05&#x2F;25 19:25:36 INFO BlockManager: BlockManager stopped18&#x2F;05&#x2F;25 19:25:36 INFO BlockManagerMaster: BlockManagerMaster stopped18&#x2F;05&#x2F;25 19:25:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!18&#x2F;05&#x2F;25 19:25:36 INFO SparkContext: Successfully stopped SparkContext18&#x2F;05&#x2F;25 19:25:36 INFO ShutdownHookManager: Shutdown hook called18&#x2F;05&#x2F;25 19:25:36 INFO ShutdownHookManager: Deleting directory C:\\Users\\msi\\AppData\\Local\\Temp\\spark-fae200dd-12cc-4b8a-b2ec-751d641d3689Process finished with exit code 0 任务运行时 http://118.202.40.210:4040/environment/ 显示的状态 注：本机 windows ip 为 118.202.40.210 其他各种问题 Spark常见问题解决办法 Spark各种问题的解决 Spark集群配置","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"集群","slug":"集群","permalink":"https://www.cz5h.com/tags/%E9%9B%86%E7%BE%A4/"},{"name":"Spark","slug":"Spark","permalink":"https://www.cz5h.com/tags/Spark/"},{"name":"IDEA","slug":"IDEA","permalink":"https://www.cz5h.com/tags/IDEA/"}]},{"title":"使用SBT正确构建IndexedRDD环境","slug":"2018-5-20 使用SBT正确构建IndexedRDD环境","date":"2018-05-19T22:00:00.000Z","updated":"2020-02-29T18:43:55.520Z","comments":true,"path":"article/970d.html","link":"","permalink":"https://www.cz5h.com/article/970d.html","excerpt":"IndexedRDD由AMPLab的Ankur Dave提出，它是Immutability和Fine-Grained updates的精妙结合。IndexedRDD是一个基于RDD的Key-Value Store，扩展自RDD[(K, V)]，可以在IndexRDD上进行高效的查找、更新以及删除。由于其并没有合并到 Spark 的主项目分支，所以在使用时需要引入特别的对其的支持。","text":"IndexedRDD由AMPLab的Ankur Dave提出，它是Immutability和Fine-Grained updates的精妙结合。IndexedRDD是一个基于RDD的Key-Value Store，扩展自RDD[(K, V)]，可以在IndexRDD上进行高效的查找、更新以及删除。由于其并没有合并到 Spark 的主项目分支，所以在使用时需要引入特别的对其的支持。 IndexedRDD的详细分析 这里主要是记录引进 IndexedRDD 之后项目出现的各种错误及解决过程，目前关于 IndexedRDD 的文章不多，百度出来的与搭环境有关系的也就十几篇左右，出现错误更是无解，所以特此记录一下填坑之路。 开始引入 IndexedRDD参见 Github 的说明，在 build.sbt 中添加： 123456&#x2F;&#x2F;这句很关键resolvers +&#x3D; &quot;Spark Packages Repo&quot; at &quot;http:&#x2F;&#x2F;dl.bintray.com&#x2F;spark-packages&#x2F;maven&quot;libraryDependencies +&#x3D; &quot;amplab&quot; % &quot;spark-indexedrdd&quot; % &quot;0.3&quot;&#x2F;&#x2F;顺带引入GraphXlibraryDependencies +&#x3D; &quot;org.apache.spark&quot; %% &quot;spark-graphx&quot; % &quot;2.2.0&quot; 编译错误注意：这里出现了一个天坑，总是编译（包含IndexedRDD时）出错的问题 历经解决过程： 解决措施一明确 scala 和 spark 版本的对照关系，版本确定为：scala-2.11.8spark-core-2.1.0（graphx同2.1.0） 上述版本是 spark-rdd 代码库中 build.sbt 的版本，详见 Github-spark-indexedrdd 明确 spark-indexedrdd 版本注意，maven源 的版本只有 0.1 0.2 0.3 0.4.0 这四个，Github代码库中的实例程序推荐的是 0.3 但是编译时会出现如下错误： 123456Run:18&#x2F;05&#x2F;22 01:29:47 WARN ClosureCleaner: Expected a closure; got edu.berkeley.cs.amplab.spark.indexedrdd.IndexedRDD$MultiputZipperException in thread &quot;main&quot; java.lang.NoSuchMethodError: org.apache.spark.SparkContext.runJobSbt shell:could not find implicit value for evidence parameter of type edu.berkeley.cs.amplab.spark.indexedrdd.KeySerializer[Long] 解决措施二这时看到了 源库 的这个 Issue于是将 spark-indexedrdd 改为 0.4.0 版本，注意是三位数字 然后继续编译仍然出同样的错（没效果） 解决措施三这时，又看见了 这个问题 其错误跟咱们的不一样，但是格式太像了，然后看他的解决方案： 报错分析：这种异常的发生通常是因为程序需要一个隐式参数 (implicit parameter)，方法的定义中有个 [R: TypeInformation] ，但程序并没有指定任何有关隐式参数的定义，编译代码无法创建 TypeInformation ，所以出现上面提到的异常信息。解决方案：1） 我们可以直接在代码里面加上以下的代码：&emsp;&emsp;implicit val typeInfo = TypeInformation.of(classOf[Int])然后再去编译代码就不会出现上面的异常。2） 但是这并不是Flink推荐我们去做的，推荐的做法是在代码中引入一下包：&emsp;&emsp;import org.apache.flink.streaming.api.scala._如果数据是有限的（静态数据集），我们可以引入以下包：&emsp;&emsp;import org.apache.flink.api.scala._然后即可解决上面的异常信息。 同样的思路，翻过头来看，自己项目里的 import 确实少了一个！ 1234import edu.berkeley.cs.amplab.spark.indexedrdd.IndexedRDD&#x2F;&#x2F; 下面这个不引入也不会报错，但是会编译出错&#x2F;&#x2F; 还要注意顺序，上下颠倒IDEA会自动省略import edu.berkeley.cs.amplab.spark.indexedrdd.IndexedRDD._ 同时还要注意，之前改为 0.4.0 版本是对的，如果换做 0.3 ，此时还是会编译出错 结论综上，IndexedRDD 环境（示例运行正常）应该如下： scala-2.11.8 spark-core-2.1.0 graphx-2.1.0（非必须） spark-indexedrdd-0.4.0build.sbt 文件： 1234567name :&#x3D; &quot;VISNWK&quot;version :&#x3D; &quot;0.1&quot;scalaVersion :&#x3D; &quot;2.11.8&quot;libraryDependencies +&#x3D; &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.1.0&quot;libraryDependencies +&#x3D; &quot;org.apache.spark&quot; %% &quot;spark-graphx&quot; % &quot;2.1.0&quot;resolvers +&#x3D; &quot;Spark Packages Repo&quot; at &quot;http:&#x2F;&#x2F;dl.bintray.com&#x2F;spark-packages&#x2F;maven&quot;libraryDependencies +&#x3D; &quot;amplab&quot; % &quot;spark-indexedrdd&quot; % &quot;0.4.0&quot; IndexedRDD demo（IDEA环境下）： 1234567891011121314151617181920212223242526272829303132333435363738394041import edu.berkeley.cs.amplab.spark.indexedrdd.IndexedRDD //缺一不可import edu.berkeley.cs.amplab.spark.indexedrdd.IndexedRDD._ //缺一不可import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.util.Randomobject graphxDemo &#123; def main(args: Array[String]) &#123; //设置运行环境 val conf = new SparkConf().setAppName(\"SimpleGraphX\").setMaster(\"local\") val sc = new SparkContext(conf) // Create an RDD of key-value pairs with Long keys. val rdd = sc.parallelize((1 to 1000000).map(x =&gt; (x.toLong, 0))) // Construct an IndexedRDD from the pairs, hash-partitioning and indexing // the entries. val indexed = IndexedRDD(rdd).cache() // Perform a point update. val indexed2 = indexed.put(1234L, 10873).cache() // Perform a point lookup. Note that the original IndexedRDD remains // unmodified. indexed2.get(1234L) // =&gt; Some(10873) indexed.get(1234L) // =&gt; Some(0) // Efficiently join derived IndexedRDD with original. val indexed3 = indexed.innerJoin(indexed2) &#123; (id, a, b) =&gt; b &#125;.filter(_._2 != 0) indexed3.collect // =&gt; Array((1234L, 10873)) // Perform insertions and deletions. val indexed4 = indexed2.put(-100L, 111).delete(Array(998L, 999L)).cache() indexed2.get(-100L) // =&gt; None indexed4.get(-100L) // =&gt; Some(111) indexed2.get(999L) // =&gt; Some(0) indexed4.get(999L) // =&gt; None sc.stop() &#125;&#125; 其他错误注意，之前还出现过 Apache Spark: Java.Lang.NoSuchMethodError .RddToPairRDDFunctions 这个错误，但是今天明确版本后就没有复现，所以该错误八成是因为版本不兼容的缘故，总之还是版本不兼容引起的编译错误。 还有这个错误 unresolved dependency: com.ankurdave#part_2.10;0.1，之前是使用 Sbt 和 Maven 混用，然后用 Maven 添加的 spark-indexedrdd 才出现的这个错误，在改用 Sbt 单一管理依赖后该错误也没有复现。","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://www.cz5h.com/tags/Spark/"},{"name":"SBT","slug":"SBT","permalink":"https://www.cz5h.com/tags/SBT/"},{"name":"IndexedRDD","slug":"IndexedRDD","permalink":"https://www.cz5h.com/tags/IndexedRDD/"}]},{"title":"添加轻量的RSS订阅内容阅读展示支持","slug":"2018-5-19 添加轻量的RSS订阅内容阅读展示支持","date":"2018-05-18T22:00:00.000Z","updated":"2020-02-29T18:43:55.521Z","comments":true,"path":"article/fab.html","link":"","permalink":"https://www.cz5h.com/article/fab.html","excerpt":"大部分资讯类的网站都提供RSS订阅功能，其一般是是XML格式的网页信息，目的是为了能够方便第三方站点轻松的获取本站的最新内容，在Hexo博客中一般也会包含seed这一插件，即可以直接将你的网站内容生成RSS订阅，这对于一些独立站点，尤其是一些SEO欠佳（搜索引擎排名靠后）但内容却十分高质量的博客来说，就可以利用RSS订阅跟进其内容的更新。","text":"大部分资讯类的网站都提供RSS订阅功能，其一般是是XML格式的网页信息，目的是为了能够方便第三方站点轻松的获取本站的最新内容，在Hexo博客中一般也会包含seed这一插件，即可以直接将你的网站内容生成RSS订阅，这对于一些独立站点，尤其是一些SEO欠佳（搜索引擎排名靠后）但内容却十分高质量的博客来说，就可以利用RSS订阅跟进其内容的更新。 曾经一度认为RSS订阅是一种落后的阅读方式，那时还没有真正体会到其正确应用场景，直到随着接触技术网站、个人博客越来越多，慢慢发现了许许多多的高质量站点，先前感叹完之后能做的也只能是将其收藏到浏览器的书签中，但没过几天就会发现早就忘记了，而且，重要的是，你无法跟踪其站点内容的更新，这部分站点又大多都不具有推送功能，只是高冷的存在在那里等待别人的访问，那么真的就无法跟踪到最新的内容了吗，RSS订阅的出现就完美的解决了这一问题，如果把文章推送内容站的主动推送，那么RSS订阅其实更像一种用户端向内容站发起的主动拉取。 RSS阅读器现在，我们只需要一个RSS阅读器和各站点的rss订阅路径，就可以随时获取到各个站点的最新内容（通常是文章），回到一开始的问题，想象一个场景，如果你收藏了几十个干货技术博客，并想看看最近他们都更新了哪些新文章，那么如果不适用RSS阅读器，难道要每一个网站都打开看一遍吗，这是难以想象的。 RSS阅读器极大地方便了我们获取各站点的最新资源，但是这是否就是最精简的操作方式？是否就是最终的RSS使用场景？答案是否定的，其实真正的应用场景更多偏向于对各站点资源的‘整合’，比如新闻聚合网站等。对于我们自己而言，当然也可以对我们收藏的一些订阅进行整合，并且内嵌到我们自己的站点中来分享我们的订阅。上述场景完全可以自己实现，但其实已经存在很多类似的开源项目，不过很多是需要服务端解析的，所以有些许局限。这里介绍的FeedEk特点就是基于JQuery的轻量实现，完全可以胜任上述场景。 注意：RSS、ATOM 本质差别不大，都是具有相似的基于XML的格式，基本结构是相同的，只在节点的表达式上有点区别，所以对于 ATOM 处理基本没有差别。 FeedEk的使用Github地址：点击这里 对于日期格式参数，如果需要使用，则必须添加Moment.js，如果想本地化日期格式（日期名称，月份名称），则必须将Moment.js与langs一起包含。Moment.js地址 注意FeedEK.js的引入，源代码非常简单，所以后期可以十分方便的改写，查看源代码可知，其实现原理其实是利用了 Yahoo 的API，将解析路径封装进字符串，然后调用 Yahoo 的API对其解析并返回json串，然后就是对其解析和拼接，整体上逻辑很简单。 核心代码： 123456789101112131415161718var YQLstr = 'SELECT channel.item FROM feednormalizer WHERE output=\"rss_2.0\" AND url =\"' + def.FeedUrl + '\" LIMIT ' + def.MaxCount;$.ajax(&#123; url: \"https://query.yahooapis.com/v1/public/yql?q=\" + encodeURIComponent(YQLstr) + \"&amp;format=json&amp;diagnostics=false&amp;callback=?\", dataType: \"json\", success: function (data) &#123; $(\"#\" + id).empty(); if (!(data.query.results.rss instanceof Array)) &#123; data.query.results.rss = [data.query.results.rss]; &#125; $.each(data.query.results.rss, function (e, itm) &#123; //... ... &#125;); //$(\"#\" + id).append('&lt;ul class=\"feedEkList\"&gt;' + s + '&lt;/ul&gt;'); //注意:源码中上述语句少了一个&lt;/li&gt;标签 $(\"#\" + id).append('&lt;ul class=\"feedEkList\"&gt;' + s + '&lt;/li&gt;&lt;/ul&gt;'); &#125;&#125;); 这段代码的引入注意两个地方，第一个是源代码中有个错误，缺少一个标签，第二个是源代码采用的是(function(){})(jQuery);这种形式的加载方式，测试时没有执行，偷懒给改为$(document).ready(function(){});这种方式，其区别详见 这里。 嵌入到页面中的效果 这里由于罗列了多个订阅信息，所以必然的需要对解析进行触发而不是直接全部解析（这样加载时间会非常慢），点击展开时就调用一次API，并且解析内容生成html代码更新到指定div即可。 具体效果可移步 此页面 查看。","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://www.cz5h.com/tags/JavaScript/"},{"name":"Hexo","slug":"Hexo","permalink":"https://www.cz5h.com/tags/Hexo/"},{"name":"Rss","slug":"Rss","permalink":"https://www.cz5h.com/tags/Rss/"}]},{"title":"提交Spark任务的三种方式","slug":"2018-5-14 提交Spark任务的三种方式","date":"2018-05-13T22:00:00.000Z","updated":"2020-02-29T18:43:55.519Z","comments":true,"path":"article/4900.html","link":"","permalink":"https://www.cz5h.com/article/4900.html","excerpt":"在使用Spark的过程中，一般都会经历调试，提交任务等等环节，如果每个环节都可以确认程序的输入结果，那么无疑对加快代码的调试起了很大的作用，现在，借助IDEA可以非常快捷方便的对Spark代码进行调试，在借助IDEA来完成Spark时，可以大致通过以下几个步骤来完成：","text":"在使用Spark的过程中，一般都会经历调试，提交任务等等环节，如果每个环节都可以确认程序的输入结果，那么无疑对加快代码的调试起了很大的作用，现在，借助IDEA可以非常快捷方便的对Spark代码进行调试，在借助IDEA来完成Spark时，可以大致通过以下几个步骤来完成： 初始构建项目阶段，使用Local模式本地运行 项目大致完成阶段，使用IDEA连接集群自动提交任务运行 最终部署运行阶段，手动将源码包上传到集群并使用 spark-submit 提交任务运行 下面，针对三种方式分别举例说明每种方式需要注意的地方。 使用IDEA本地运行（Local模式）本地运行，本地计算，本地输出，与集群无关12345678910111213141516171819import org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;object demo &#123; def main(args: Array[String]) &#123; Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN) Logger.getLogger(\"org.eclipse.jetty.server\").setLevel(Level.OFF) val conf = new SparkConf().setAppName(\"DemoApp\").setMaster(\"local\") val sc = new SparkContext(conf) val data = sc.makeRDD(Seq(1,2,3,4,5,6,7,8,9)) data.map&#123; x =&gt;&#123; (x,s\"My number is $x\") &#125; &#125;.foreach(println) &#125;&#125; 使用IDEA本地连接集群运行运行在集群，计算在集群，输出可以在本地（从远程取回）注意： 1. 此处打包时需要将环境依赖包含在内 2. 注意勾选 Include in build，然后 Rebuild Module 即可打包 3. 代码内需要指定jar包的具体路径（setJar）和主节点（setMaster） 4. 注意setMaster地址就是webUI中置顶的地址 5. 注意这种方式的代码输出 这种方式 rdd.foreach(println) 或者是一般的 println() 都不能在 Console 打印出结果，如果希望在控制台打印出特定输出必须使用 collect() 将数据取回本地（这时可以将本地想象为集群中的一个节点），对于文件也是同理，其操作相当于对远程hdfs的操作，这里不展开. 手动上传Jar包到集群运行运行在集群，计算在集群，输出在集群注意： 1. 此时打包时只打包源码文件，即无需添加环境依赖 2. 此Jar文件内只有源码，一般很小 3. 代码内 Sparkconf 的获取不用具体指定 123456789101112131415161718import org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;object demo &#123; def main(args: Array[String]) &#123; Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN) Logger.getLogger(\"org.eclipse.jetty.server\").setLevel(Level.OFF) val conf = new SparkConf().setAppName(\"DemoApp\") val sc = new SparkContext(conf) val data = sc.makeRDD(Seq(1,2,3,4,5,6,7,8,9)) data.map&#123; x =&gt;&#123; (x,s\"My number is $x\") &#125; &#125;.foreach(println) &#125;&#125; 4. 需要使用 spark-submit 命令提交任务 留意这种形式1234567891011import org.apache.spark.&#123;SparkConf, SparkContext&#125;object demo &#123; def main(args: Array[String]) &#123; val conf &#x3D; new SparkConf().setAppName(&quot;DemoApp&quot;)setMaster(&quot;local&quot;) val sc &#x3D; new SparkContext(conf) val text &#x3D; sc.textFile(&quot;hdfs:&#x2F;&#x2F;192.168.146.130:9000&#x2F;spark&#x2F;look.sh&quot;) println(&quot;remote clusters&#39; file output:&quot;) text.foreach(println) sc.stop() &#125;&#125; 上述代码中，Spark仍然是Local模式，但资源文件却在远程集群的HDFS上，这也是可以运行的！这时访问的资源确实是远程的资源，但是计算仍然在本地，仍然算做第一种方式（Local模式）。 结束语在提交任务的过程中可能会遇到各种各样的问题，一般分为task本身的配置项问题和Spark集群本身的问题两部分，task本身的配置问题一般可以通过:&emsp;- SparkContext().set() 来设置，第二种方式即IDEA连接集群&emsp;- spark-submit添加参数–executor-memory 来设置，即手动提交方式&emsp;- 具体配置项参见[ Spark配置参数 ]集群本身的问题涉及Worker、Master的启动等等，关联的地方较多，在此不进行展开。 最后，整个提交过程的前提是IDEA项目配置和Spark集群环境的正确，以及两者正确的匹配（比如打包的1.x版本的Saprk任务大概率是不能运行在Spark2.x的集群上的）。","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"Spark","slug":"Spark","permalink":"https://www.cz5h.com/tags/Spark/"},{"name":"IDEA","slug":"IDEA","permalink":"https://www.cz5h.com/tags/IDEA/"}]},{"title":"GraphX具体功能的代码使用实例-Scala实现","slug":"2018-5-11 GraphX具体功能的代码使用实例-Scala实现","date":"2018-05-10T22:00:00.000Z","updated":"2020-02-29T18:43:55.517Z","comments":true,"path":"article/4ba7.html","link":"","permalink":"https://www.cz5h.com/article/4ba7.html","excerpt":"GraphX 为整个图计算流程提供了强大的支持，先前已经有若干篇文章先后介绍了GraphX的强大功能，在GraphX官方编程指南中，提供了部分简单易懂的示例代码，其为GraphX的使用提供了一个初步的认识，作为需要用GraphX来编码实现需求的读者来说是十分宝贵的资源。","text":"GraphX 为整个图计算流程提供了强大的支持，先前已经有若干篇文章先后介绍了GraphX的强大功能，在GraphX官方编程指南中，提供了部分简单易懂的示例代码，其为GraphX的使用提供了一个初步的认识，作为需要用GraphX来编码实现需求的读者来说是十分宝贵的资源。 本文利用一个初始示例代码，结合部分官方文档中的说明，对GraphX的部分功能方法进行了实践，在全部亲自运行通过后，对大部分代码添加了自己的理解和认识，并且在Pregel模型编程部分结合运行结果对其运行流程做了一定梳理，来意图理解其执行机制。 下面，是ben程序代码中使用到的主要程序部分，即定义出一个简单的图结构，并构造一个图Graph[VD,ED]，对具体功能的实现均放置在代码的后半部分，主要包括一下几部分： Property Operators Structural Operators Computing Degree Collecting Neighbors Join Operators mapReduceTriplets aggregateMessages Pregel API Functions 主程序代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.graphx._import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object RDD_println &#123; def main(args: Array[String]) &#123; //屏蔽日志 Logger.getLogger(\"org.apache.spark\").setLevel(Level.ERROR) Logger.getLogger(\"org.eclipse.jetty.server\").setLevel(Level.OFF) //设置运行环境 val conf = new SparkConf().setAppName(\"XXXAppName\").setMaster(\"local\") val sc = new SparkContext(conf) //设置顶点和边，注意顶点和边都是用元组定义的Array //顶点的数据类型 val vertexArray = Array( (1L, (\"Alice\", 28)),(2L, (\"Bob\", 27)), (3L, (\"Charlie\", 65)),(4L, (\"David\", 42)), (5L, (\"Ed\", 55)),(6L, (\"Fran\", 50)) ) //边的数据类型 val edgeArray = Array( Edge(2L, 1L, 7),Edge(2L, 4L, 2),Edge(3L, 2L, 4), Edge(3L, 6L, 3),Edge(4L, 1L, 1),Edge(5L, 2L, 2), Edge(5L, 3L, 8),Edge(5L, 6L, 3) ) //构造vertexRDD和edgeRDD val vertexRDD: RDD[(Long, (String, Int))] = sc.parallelize(vertexArray) val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray) //构造图Graph[VD,ED] val graph: Graph[(String, Int), Int] = Graph(vertexRDD, edgeRDD) //----------------- Property Operators ----------------- //----------------- Structural Operators ----------------- //----------------- Computing Degree ----------------- //----------------- Collecting Neighbors ----------------- //----------------- Join Operators ----------------- //----------------- mapReduceTriplets ----------------- //----------------- aggregateMessages ----------------- //----------------- Pregel API Functions ----------------- &#125;&#125; Property Operators12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758graph.edges.foreach(println)printlngraph.vertices.foreach(println)printlngraph.triplets.foreach(println)printlngraph.triplets.foreach(e =&gt; prin(s\"edge($&#123;e.srcId&#125;,$&#123;e.dstId&#125;)\\tage($&#123;e.srcAttr._2&#125;,$&#123;e.dstAttr._2&#125;)\"))//---- vertexRDD.foreach ----原始的vertexRDD保持顺序//1 name=Alice age=28//2 name=Bob age=27//3 name=Charlie age=65//4 name=David age=42//5 name=Ed age=55//6 name=Fran age=50//---- edgeRDD.foreach ----// 2 to 1 w=7//2 to 4 w=2//3 to 2 w=4//3 to 6 w=3//4 to 1 w=1//5 to 2 w=2//5 to 3 w=8//5 to 6 w=3//---- graph.vertices.foreach ----生成graph之后顺序被打乱//4 name=David age=42//1 name=Alice age=28//6 name=Fran age=50//3 name=Charlie age=65//5 name=Ed age=55//2 name=Bob age=27//---- graph.edges.foreach ----// 2 to 1 w=7//2 to 4 w=2//3 to 2 w=4//3 to 6 w=3//4 to 1 w=1//5 to 2 w=2//5 to 3 w=8//5 to 6 w=3//take(n) 按与按顺序！取出前 n 个graph.edges.take(3).foreach(println)// mapVertices 对Vertices进行map操作// 对整个顶点集的 某一部分 进行批量操作graph.vertices.foreach( x =&gt; println(x))// 不改变顺序的进行map操作graph.mapVertices&#123; case (id, (name, age)) =&gt; (id, (name, age+10))&#125;.vertices.collect.foreach(v =&gt; println(s\"$&#123;v._2._1&#125; is $&#123;v._2._2&#125;\"))//collect可有可无//mapVertices&#123;case()=&gt; ()&#125; 这种必须用&#123;&#125;//但是对于 mapVertices&#123; () =&gt; () &#125; 这种也可以用 mapVertices( () =&gt; () )graph.mapEdges&#123; e=&gt;e.attr*2&#125;.edges.collect.foreach(e =&gt; println(s\"$&#123;e.srcId&#125; to $&#123;e.dstId&#125; att $&#123;e.attr&#125;\")) Structural Operators12345678910111213141516171819202122232425262728293031323334353637383940414243// 原始图结构graph.edges.foreach(println)println// 满足要求的子图结构 这里只用到了 参数epredval sub = graph.subgraph( epred = e =&gt; e.srcId &gt; e.dstId )sub.edges.foreach(println)//println(\"原图顶点数：\"+graph.vertices.count()+\"\\t子图顶点数：\"+sub.vertices.count())//println(\"原图边数：\"+graph.edges.count()+\"\\t子图边：\"+sub.edges.count())//原图顶点数：6 子图顶点数：6//原图边数：8 子图边：5//分析：过滤掉了3条边，但是仍然包含全部顶点// 错误的写法： subgraph( epred = e =&gt; e.srcId &gt; e.dstId, vpred = (id,(_,_)) =&gt; id &gt; 4 )val sub2 = graph.subgraph( epred = e =&gt; e.srcId &gt; e.dstId, vpred = (id,_) =&gt; id &gt; 4 )//println(\"原图顶点数：\"+graph.vertices.count()+\"\\t子图顶点数：\"+sub2.vertices.count())//println(\"原图边数：\"+graph.edges.count()+\"\\t子图边：\"+sub2.edges.count())//原图顶点数：6 子图顶点数：2//原图边数：8 子图边：0//分析：同时有epred和vpred两个条件，最终只剩两点，且无边连接，即边数为0graph.edges.foreach(println)printlngraph.edges.reverse.foreach(println)//Edge(2,1,7)//Edge(2,4,2)//Edge(3,2,4)//Edge(3,6,3)//Edge(4,1,1)//Edge(5,2,2)//Edge(5,3,8)//Edge(5,6,3)// reverse之后，是边的入点出点 相互交换，即边进行翻转//Edge(1,2,7)//Edge(1,4,1)//Edge(2,3,4)//Edge(2,5,2)//Edge(3,5,8)//Edge(4,2,2)//Edge(6,3,3)//Edge(6,5,3)// 注意：graph.vertices 没有reverse这个方法，即不能对顶点进行翻转 Computing Degree1234567891011121314151617//输出全部 ( 度数 , 该度的个数 )graph.outDegrees.foreach( x =&gt; println(x))graph.degrees.foreach( x =&gt; println(x))println// 为什么这样设计// 因为：跟的是graph.degrees，即格式为 (VertexId, Int)，// 所以对其的处理的参数 max(参数a:参数b):(结果)，均为此格式// 得到最大度的节点：首先需要一个比较两点度大小的函数 max()def max(a: (VertexId, Int), b: (VertexId, Int)): (VertexId, Int) = &#123; if (a._2 &gt; b._2) a else b&#125;println(graph.degrees.reduce(max))//foreach( x =&gt; println(x))println(graph.inDegrees.reduce(max))//foreach( x =&gt; println(x))println(graph.outDegrees.reduce(max))//foreach( x =&gt; println(x)) Collecting Neighbors12345678910111213141516171819202122232425262728293031323334353637383940// GraphOps实现的，即是属于 graph 的方法graph.collectNeighborIds(EdgeDirection.Out).foreach( x =&gt; &#123; print(x._1+\" cnt=\") x._2.foreach(print) println&#125;)// 按出度，找到的点的邻居节点//4 cnt=1//1 cnt=//6 cnt=//3 cnt=26//5 cnt=236//2 cnt=14printlngraph.collectNeighborIds(EdgeDirection.Either).foreach( x =&gt; &#123; print(x._1+\" cnt=\") x._2.foreach(print) println&#125;)// 按出入度，不能用Both，要用Either//4 cnt=21//1 cnt=24//6 cnt=35//3 cnt=265//5 cnt=236//2 cnt=1435printlngraph.collectNeighbors(EdgeDirection.Either).foreach( x =&gt; &#123; print(x._1+\" cnt=\") x._2.foreach(print) //collectNeighbors 找到的邻居是全属性，而不仅是ID println&#125;)//4 cnt=(2,(Bob,27))(1,(Alice,28))//1 cnt=(2,(Bob,27))(4,(David,42))//6 cnt=(3,(Charlie,65))(5,(Ed,55))//3 cnt=(2,(Bob,27))(6,(Fran,50))(5,(Ed,55))//5 cnt=(2,(Bob,27))(3,(Charlie,65))(6,(Fran,50))//2 cnt=(1,(Alice,28))(4,(David,42))(3,(Charlie,65))(5,(Ed,55)) Join Operators123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 属性重置为0val rawGraph = graph.mapVertices((id,_) =&gt; 0)rawGraph.vertices.foreach(println)printlnrawGraph.joinVertices[Int](rawGraph.degrees)((_,_,outDeg) =&gt; outDeg).vertices.foreach(println)//(4,0)//(1,0)//(6,0)//(3,0)//(5,0)//(2,0)// joinVertices 按degree使用join操作 加入到 rawGraph 中//(4,2)//(1,2)//(6,2)//(3,3)//(5,3)//(2,4)rawGraph.outerJoinVertices(rawGraph.degrees)((_,_,outDeg) =&gt; outDeg).vertices.foreach(println)//(4,Some(2))//(1,Some(2))//(6,Some(2))//(3,Some(3))//(5,Some(3))//(2,Some(4)//进阶用法 //顶点用 类对象来代替，利用到 outerJoinVerticesval inDegrees: VertexRDD[Int] = graph.inDegreescase class User(name: String, age: Int, inDeg: Int, outDeg: Int)//创建一个新图，顶点VD的数据类型为User，并从graph做类型转换// Graph[User, Int] 即 Graph[VD，ED]，意思是顶点由User类代替，ED是Int类型val ugraph: Graph[User, Int] = graph.mapVertices &#123; case (id, (name, age)) =&gt; User(name, age, 0, &#125;//initialUserGraph与inDegrees、outDegrees（RDD）进行连接，并修改initialUserGraph中inDeg值、outD值//outerJoinVertices 这个方法属于 Graph 类型的方法val userGraph = ugraph.outerJoinVertices(ugraph.inDegrees) &#123; //required: (graphx.VertexId, User, Option[Int]) case (id, u, in) =&gt; User(u.name, u.age, in.getOrElse(0), u.outDeg)&#125;.outerJoinVertices(ugraph.outDegrees) &#123; case (id, u, out) =&gt; User(u.name, u.age, u.inDeg,out.getOrElse(0))&#125;println(\"连接图顶点的属性：\")userGraph.vertices.collect.foreach(v =&gt; println(s\"$&#123;v._2.name&#125; inDeg: $&#123;v._2.inDeg&#125; outDeg: v._2.outDeg&#125;\"))println(\"出度和入读相同的：\")userGraph.vertices.filter &#123; //此处应该格式为 (graphx.VertexId, User) case (id, u) =&gt; u.inDeg == u.outDeg&#125;.collect.foreach &#123; case (id, property) =&gt; println(property.name)&#125; mapReduceTriplets12345678910111213141516171819202122232425//在早的GraphX版本中我们计算邻居聚合使用mapReduceTriplets操作;//注意：当前版本 Graph已经不存在MapReduceTriplets这个方法// libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.2.0\"// libraryDependencies += \"org.apache.spark\" %% \"spark-graphx\" % \"2.2.0\"////mapReduceTriplets// 操作应用用户定义的map函数到每一个triplet ，使用用户定义的reduce函数聚合产生 messages。。// 然而，我们发现用户返回迭代器是昂贵的，它抑制了我们应用额外优化(例如，本地顶点的重新编号)的能// 在 aggregateMessages 中我们引进了EdgeContext，其暴露triplet属性，也明确了函数发送信息的源和顶点//。// 除此之外，我们移除了字节码检测，取而代之的是要求用户指明哪个triplet属性被需要。val graph: Graph[Int, Float] = ...def msgFun(triplet: Triplet[Int, Float]): Iterator[(Int, String)] = &#123; Iterator((triplet.dstId, \"Hi\"))&#125;def reduceFun(a: Int, b: Int): Int = a + bval result = graph.mapReduceTriplets[String](msgFun, reduceFun)def msgFun(t: EdgeContext[(String,Int), Int, Int]) &#123; t.sendToSrc(100)&#125;def reduceFun(a: Int, b: Int): Int = a + bval result = graph.aggregateMessages[Int](msgFun, reduceFun)result.foreach(println) aggregateMessages123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102//对每个节点的邻接点的属性进行聚合统计//老版本使用 mapReduceTripletsval oldFlowers: VertexRDD[(Int,Double)] = graph.mapReduceTriplets[(Int,Double)]&#123; triplet =&gt; &#123; if (triplet.srcAttr &gt; triplet.dstAttr) &#123; Iterator((triplet.dstId, (1, triplet.srcAttr ))) &#125; else &#123; Iterator.empty &#125; &#125;, (a,b) =&gt; (a._1+b._1, a._2+b._2) &#125;&#125;// 现在使用 aggregateMessages 代替// 首先生成一个随机图val g: Graph[Double,Int] = GraphGenerators.logNormalGraph(sc, numVertices = 100).mapVertices( d,_) =&gt; id.toDouble )//g.edges.foreach(println)//printlndef msgFun(triplet: EdgeContext[Double, Int, (Int,Double)]) &#123; if (triplet.srcAttr &gt; triplet.dstAttr) &#123; triplet.sendToDst((1, triplet.srcAttr )) //注意：原先的 Iterator((triplet.dstId, (1, triplet.srcAttr ))) 上述代码替换，作用完全一样 //sendToDst 意思为向 目标点 发送消息 //println(\"Iterator((\"+triplet.dstId+\", (1, \"+triplet.srcAttr+\" )))\"); &#125;else&#123; Iterator.empty &#125;&#125;// 有个问题：写成函数定义的形式 下面代码总是出错// def reduceFun(a:Int,b:Double): (Int,Double) = (a._1+b._1, a._2+b._2)// aggregateMessages[OOO] OOO处的类型，会附加到图g类型之后，// 即：g为 Double,Int --&gt; result为 Double,Int,OOOval result = g.aggregateMessages[(Int,Double)](msgFun, (a,b) =&gt; (a._1+b._1,a._2+b._2))//result.collect.foreach(println)//上述代码执行完后，形如(45,2913.0)，已经是reduce完成的状态，如果下面继续计算平均值，直接后项除项即可//(19,(45,2913.0))//(39,(45,2873.0))//(34,(32,2102.0))//(4,(62,3466.0))//(71,(12,1028.0))//result现在是(Int,Double)形式，现在对Double元素即value//这里的的match-case类似于switch-case//对于avg:VertexRDD[Double]，后项变量类型可以省去不写val avg:VertexRDD[Double] = result.mapValues( (_,value) =&gt; //这句话意思是前项保持不动，对后项value即形如(12,1028.0)的部分进行加工 value match &#123; case (count,total) =&gt; total/count &#125;) //整体返回值avg类型为(Int,Double)//avg.collect.foreach(println)//下面不使用随机图，使用开头自定义的图结构时：graph.triplets.foreach(e =&gt; intln(s\"edge($&#123;e.srcId&#125;,$&#123;e.dstId&#125;)\\tage($&#123;e.srcAttr._2&#125;,$&#123;e.dstAttr._2&#125;)\"))def msgFun(triplet: EdgeContext[(String,Int), Int, (Int,Double)]) &#123; if (triplet.srcAttr._2 &gt; triplet.dstAttr._2) &#123; triplet.sendToDst((1, triplet.srcAttr._2 )) &#125;else&#123; Iterator.empty &#125;&#125;val result = graph.aggregateMessages[(Int,Double)](msgFun, (a,b) =&gt; (a._1+b._1,a._2+b._2))printlnresult.collect.foreach(println)//分析：原始图结构//((2,(Bob,27)),(1,(Alice,28)),7)//((2,(Bob,27)),(4,(David,42)),2)//((3,(Charlie,65)),(2,(Bob,27)),4) 年龄条件符合：顶点2 邻居的Age为 65//((3,(Charlie,65)),(6,(Fran,50)),3) 年龄条件符合：顶点6 邻居的Age为 65//((4,(David,42)),(1,(Alice,28)),1) 年龄条件符合：顶点1 邻居的Age为 42 = 42 1个邻居//((5,(Ed,55)),(2,(Bob,27)),2) 年龄条件符合：顶点2 邻居的Age为 65+55 = 120 2个邻居//((5,(Ed,55)),(3,(Charlie,65)),8)//((5,(Ed,55)),(6,(Fran,50)),3) 年龄条件符合：顶点6 邻居的Age为 65+55 = 120 2个邻居//上述计算的意义是：找到每个顶点用户的比自身年龄大的邻居节点用户的年龄之和，以及邻居数；//所以，result为：//(1,(1,42.0))//(6,(2,120.0))//(2,(2,120.0))//继续求平均值val avg:VertexRDD[Double] = result.mapValues( (_,value) =&gt; value match &#123; case (count,total) =&gt; total/count &#125;)printlnavg.collect.foreach(println)//上述计算的意义是：找到每个顶点用户的比自身年龄大的邻居节点用户的平均年龄，即原本的计算目的//结果为：// (1,42.0)// (6,60.0)// (2,60.0) Pregel API Functions Shortest path 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109// ShortestPath 没有被封装成方法，需要自己实现val sourceId: VertexId = 5L // 定义源点// 用一个新图initialGraph 来初始化 顶点5 同其他顶点间的距离：本身距离为0，其他距离为MAXval initialGraph = graph.mapVertices((id,_) =&gt; if (id == sourceId) 0.0 else uble.PositiveInfinity)//initialGraph.vertices.foreach(println)//(4,Infinity)//(1,Infinity)//(6,Infinity)//(3,Infinity)//(5,0.0)//(2,Infinity)//initialGraph.triplets.foreach(println)//((2,Infinity),(1,Infinity),7)//((2,Infinity),(4,Infinity),2)//((3,Infinity),(2,Infinity),4)//((3,Infinity),(6,Infinity),3)//((4,Infinity),(1,Infinity),1)//((5,0.0),(2,Infinity),2)//((5,0.0),(3,Infinity),8)//((5,0.0),(6,Infinity),3)//Unspecified value parameters: vprog:// 第一部分：(graphx.VertexId, Double, Double) =&gt; Double,// 第二部分：sendMsg: EdgeTriplet[Double, Int] =&gt; Iterator[(graphx.VertexId, Double)],// 第三部分：mergeMsg: (Double, Double) =&gt; Doubleval sssp = initialGraph.pregel(Double.PositiveInfinity)( (id, dist, newDist) =&gt; &#123;math.min(dist, newDist)&#125;, triplet =&gt; &#123; // 计算权重 if (triplet.srcAttr + triplet.attr &lt; triplet.dstAttr) &#123; println(\"Iterator((\"+triplet.dstId+\", \"+triplet.srcAttr+\" + \"+triplet.attr+\"))\") //triplet.attr即边的权重，不断加入triplet.attr，最后就能找到最短路径 Iterator((triplet.dstId, triplet.srcAttr + triplet.attr)) &#125; else &#123; println(\"Iterator.empty\") Iterator.empty &#125; &#125;, (a,b) =&gt; math.min(a,b) // 更新点⑤到该点的距离)//解释下过程，拿Triplets结构来说//((2,Infinity),(1,Infinity),7)//((2,Infinity),(4,Infinity),2)//((3,Infinity),(2,Infinity),4)//((3,Infinity),(6,Infinity),3)//((4,Infinity),(1,Infinity),1)//((5,0.0),(2,Infinity),2)//((5,0.0),(3,Infinity),8)//((5,0.0),(6,Infinity),3)//首先由于前五行，顶点的attr均为Infinity，所以sendMsg中的Iterator均是empty空迭代//第六行，0+2&lt;Infinity，满足条件，传递（0+2=2）到入点，即变为((5,0.0),(2,2),2)，意为5到点2距2//第七行，0+8&lt;Infinity，满足条件，传递（0+8=8）到入点，即变为((5,0.0),(3,8),8)，意为5到点3距8//第八行，0+2&lt;Infinity，满足条件，传递（0+2=2）到入点，即变为((5,0.0),(6,3),3)，意为5到点6距3//==进行mergeMsg步骤== 注意现在更新为：//((2,2),(1,Infinity),7)//((2,2),(4,Infinity),2)//((3,8),(2,2),4)//((3,8),(6,3),3)//((4,Infinity),(1,Infinity),1)//((5,0.0),(2,2),2)//((5,0.0),(3,8),8)//((5,0.0),(6,3),3)//第一行，2+7&lt;Infinity，满足条件，传递（2+7=9）到入点，即变为((2,2),(1,9),7)，意为5到点1距离为2=9//第二行，2+2&lt;Infinity，满足条件，传递（2+2=4）到入点，即变为((2,2),(4,4),2)，意为5到点4距离为2=4//Iterator.empty 因为 8+4&gt;2//Iterator.empty 因为 8+3&gt;3//Iterator.empty 因为 Infinity无法比较//Iterator.empty 因为 0+2=2//Iterator.empty 因为 0+8=8//Iterator.empty 因为 0+3=3//==进行mergeMsg步骤== 注意现在更新为：//((2,2),(1,9),7)//((2,2),(4,4),2)//((3,8),(2,2),4)//((3,8),(6,3),3)//((4,4),(1,9),1)//((5,0.0),(2,2),2)//((5,0.0),(3,8),8)//((5,0.0),(6,3),3)//第一行：Iterator.empty 因为 2+7=9//第二行：Iterator.empty 因为 2+2=4//第三行：Iterator.empty 因为 8+4&gt;2//第四行：Iterator.empty 因为 8+3&gt;3//第五行：4+1&lt;9，满足条件，传递（4+1=5）到入点，即变为((4,4),(1,5),1)，意为5到点1距离由9更新为5//第六行：Iterator.empty 因为 0+2=2//第七行：Iterator.empty 因为 0+8=8//第八行：Iterator.empty 因为 0+3=3//==进行mergeMsg步骤== 注意现在更新为：//((2,2),(1,5),7)//((2,2),(4,4),2)//((3,8),(2,2),4)//((3,8),(6,3),3)//((4,4),(1,5),1)//((5,0.0),(2,2),2)//((5,0.0),(3,8),8)//((5,0.0),(6,3),3)//此时，每个顶点的格式即（顶点id，顶点5到该顶点的最短距离），即最短路径算法完成sssp.vertices.foreach(println)//最终输出的结果://(4,4.0)//(1,5.0)//(6,3.0)//(3,8.0)//(5,0.0)//(2,2.0) PageRank 12345678910111213141516171819202122232425262728293031// PageRank 有封装好的方法//注意PageRank的参数，其实可以用精度来理解，这个值越小rank计算越精确，但计算时间也越长graph.pageRank(0.01).vertices.foreach(println)//(4,0.9727164143364966)//(1,1.7757164399923602)//(6,1.0009207604397985)//(3,0.7024005336419639)//(5,0.5473250911495823)//(2,1.0009207604397985)graph.pageRank(0.05).vertices.foreach(println)//(4,0.9993165004824702)//(1,1.700587005467996)//(6,0.9890640077195239)//(3,0.7430041814088131)//(5,0.5789642972016725)//(2,0.9890640077195239)graph.pageRank(0.1).vertices.foreach(println)//(4,1.035409289731306)//(1,1.5453030618621122)//(6,1.0247865028119143)//(3,0.7698396167465112)//(5,0.5998750260362425)//(2,1.0247865028119143)graph.pageRank(0.5).vertices.foreach(println)//(4,0.9999999999999999)//(1,0.9999999999999999)//(6,0.9999999999999999)//(3,0.9999999999999999)//(5,0.9999999999999999)//(2,0.9999999999999999) TrangleCount 1234567891011// TrangleCount 有封装好的方法// 注意，老版本GraphX需要 srcID&lt;dstID，有些教程还是这样说的，但是2.x已经没有这个要求graph.triangleCount().vertices.foreach(println)//输出结果：//(4,1) 意思为顶点4 外接一个三角形//(1,1) ..//(6,1) ..//(3,2) 意思为顶点2 外接两个三角形//(5,2) ..//(2,2) ..","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://www.cz5h.com/tags/Scala/"},{"name":"整合","slug":"整合","permalink":"https://www.cz5h.com/tags/%E6%95%B4%E5%90%88/"},{"name":"实践","slug":"实践","permalink":"https://www.cz5h.com/tags/%E5%AE%9E%E8%B7%B5/"},{"name":"GraphX","slug":"GraphX","permalink":"https://www.cz5h.com/tags/GraphX/"}]},{"title":"Java向Oracle数据库表中插入CLOB、BLOB字段","slug":"2018-5-10 Java向Oracle数据库表中插入CLOB、BLOB字段","date":"2018-05-09T22:00:00.000Z","updated":"2020-02-29T18:43:55.515Z","comments":true,"path":"article/cd06.html","link":"","permalink":"https://www.cz5h.com/article/cd06.html","excerpt":"在需要存储较长字符串到数据库中时往往需要使用一些特殊类型的字段，在Oracle中即blob和clob字段，一般而言：Clob字段存储字符信息，比如较长的文字、评论，Blob字段存储字节信息，比如图像的base64编码。 注意，上述字段的使用均可以用其他方式替代，比如用MongoDB或者图片直接存储为文件等等，这里不纠结场景的合适与否，只是针对Blob和Clob类型的使用来举例。","text":"在需要存储较长字符串到数据库中时往往需要使用一些特殊类型的字段，在Oracle中即blob和clob字段，一般而言：Clob字段存储字符信息，比如较长的文字、评论，Blob字段存储字节信息，比如图像的base64编码。 注意，上述字段的使用均可以用其他方式替代，比如用MongoDB或者图片直接存储为文件等等，这里不纠结场景的合适与否，只是针对Blob和Clob类型的使用来举例。 操作场景主要有三种场景： 仅对已知表中的某一字段写入Blob和Clob字段的值 更新已知表中全部字段的值（均为Blob和Clob字段） 插入数据中带有部分需要插入Blob和Clob字段的数据 总结来看，后两种均以第一种场景为基础，即我们必须明确如何向Blob和Clob字段写入数据。第二种场景实际上是第一种的重复操作，那么对于第三种，需要十分注意，这里意味着需要向表中插入一行记录，操作有部分差异，在此我们就用第三种场景为例来给出示例。 插入时带Blob和Clob字段情景再现： 从数据源接收数据，解析完成后产生SQL语句并批量插入数据表，注意，原记录中含有若干个Blob字段（图片编码）和若干个Clob字段（记录信息），其余字段均为一般类型（String，Integer） 在给出代码前，注意几点： Blob和Clob需要单独处理，即一个SQL语句无法完成上述需求 整个过程分为三部分：组装SQL语句、第一遍插入、第二次插入Blob和Clob类型 组装SQL语句时：Blob需要人为empty_blob()，置空为Clob需要人为置空为empty_clob() 每次插入都需要对特殊字段进行处理，故无法使用batch操作 特殊字段处理（第二次插入），必须在第一遍插入之后进行，此时已初始化为empty_blob()或empty_clob() 下面就以带特定场景需求的代码来展示写入示例。 代码背景数据源每次发送一个XML字符串非常长，代码端每次解析这个串，解析后会成为 N 条记录，其中每条记录要解析为 M 个字段，其中含有 m 个Blob字段和 n 个Clob字段，现在需要把这 N 条记录插入到数据表中。 上述的 N，M，n，m 大小均不定且动态变化（已知某些字段是，但这些字段不一定出现），即大小未知。 大致代码流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135// ... ... 整个过程围绕xml节点的迭代来完成while(iter1.hasNext())&#123; Element e = iter1.next(); Iterator&lt;Element&gt; iter2 = e.elementIterator(); // 每一条SQL while(iter2.hasNext())&#123; boolean flag1 = false; // 标志是否含有Clob字段 boolean flag2 = false; // 标志是否含有Blob字段 String blobId = \"\"; // 储存所在SQL语句的主键值 // ... ... // 开始组装每一条SQL语句 Iterator&lt;Element&gt; iter3 = f.elementIterator(); while(iter3.hasNext())&#123; // ... ... switch(colname)&#123; case \"CLOB字段名1\" : case \"CLOB字段名2\" : // ... case \"CLOB字段名N\" : &#123; //暂存CLOB数据 cList.add(colname); cList.add(h.getStringValue()); flag1 = true; break; // break switch &#125; case \"BLOB字段名1\" : case \"BLOB字段名2\" : // ... case \"BLOB字段名N\" :&#123; //暂存BLOB数据 bList.add(colname); bList.add(h.getStringValue()); flag2 = true; break; &#125; default:&#123; if( this value is the primary key )&#123; blobId = this value &#125; strVALUE.append( this valu , ); // 字段值 strNAMES.append( his value , ); // 字段名 &#125; &#125; &#125; // 去掉最后一个多余的逗号 strVALUE.deleteCharAt( strVALUE.length() - 1); strNAMES.deleteCharAt( strNAMES.length() - 1); // 然后追加处理 empty_clob()和empty_blob() for(int i = 0;i &lt; cList.size(); i=i+2)&#123; strNAMES.append(\",\\\"\"+cList.get(i)+\"\\\"\"); strVALUE.append(\",empty_clob()\"); &#125; for(int i = 0;i &lt; bList.size(); i=i+2)&#123; strNAMES.append(\",\\\"\"+bList.get(i)+\"\\\"\"); strVALUE.append(\",empty_blob()\"); &#125; // 最终形态（第一次插入的语句） sqlStr.append(\"INSERT INTO 表名 ( \"+strNAMES+\" ) VALUES ( \"+strVALUE+\" )\"); pstmt = con.prepareStatement(sqlStr.toString()); pstmt.executeUpdate(); // first insert done if(pstmt != null)&#123; pstmt.close(); &#125; // 上述第一次插入完成后，开始单独处理特殊类型（第二次插入） // 根据 flag1 判断是否有Clob类型的数据 if(flag1)&#123; for(int i = 0;i &lt; cList.size(); i=i+2)&#123; pstmt = con.prepareStatement( \"SELECT \"+cList.get(i)+\" FROM 表名 WHERE 表主键 = \"+blobId+\" for update\"); ResultSet rs = pstmt.executeQuery(); Writer outStream = null; if (rs.next()) &#123; //得到java.sql.Clob对象后强制转换为oracle.sql.CLOB oracle.sql.CLOB clob = (oracle.sql.CLOB) rs.getClob(cList.get(i)); outStream = clob.getCharacterOutputStream(); //传入字符串 char[] c = cList.get(i+1).toCharArray(); outStream.write(c, 0, c.length); &#125; outStream.flush(); outStream.close(); con.commit(); if(pstmt != null)&#123; pstmt.close(); &#125; &#125; &#125; // 根据 flag1 判断是否有Blob类型的数据 if(flag2)&#123; for(int i = 0;i &lt; bList.size(); i=i+2)&#123; pstmt = con.prepareStatement( \"SELECT \"+bList.get(i)+\" FROM 表名 WHERE 表主键 = \"+blobId+\" for update\" ); ResultSet rs = pstmt.executeQuery(); OutputStream os = null; if (rs.next()) &#123; // 得到java.sql.Blob对象后强制转换为oracle.sql.BLOB oracle.sql.BLOB blob = (oracle.sql.BLOB) rs.getBlob(bList.get(i)); // 通过getBinaryOutputStream()方法获得向数据库中插入图片的流 os = blob.getBinaryOutputStream(); // 读取想要存储的图片文件（或串值） InputStream is = new ByteArrayInputStream(bList.get(i+1).getBytes()); // 依次读取流字节,并输出到已定义好的数据库字段中. int b = 0; while ((b = is.read()) != -1) &#123; os.write(b); &#125; &#125; os.flush(); os.close(); con.commit(); if(pstmt != null)&#123; pstmt.close(); &#125; &#125; &#125; &#125; // end while &#125; // end while 上述代码段的环境非常特殊，前面已经说了，是一个比较复杂的处理逻辑，代码中有些变量定义没写出来，有些地方也去掉了特定变量换成了文字叙述，所以，上述代码仅仅是为了提供思路，并且包含了一些处理技巧： 如何结合XML对象解析构造SQL 如何拼接SQL字符串 如何暂存特殊类型字段 如何在第一次插入时设置empty_blob() 如何通过主键值来进行第二次插入 如何插入Blob和Clob字段 如果你有更好的方法或者是对该文章有任何的疑问或想法，请在下方留言，我会第一时间回复的！","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://www.cz5h.com/tags/Oracle/"},{"name":"CLOB","slug":"CLOB","permalink":"https://www.cz5h.com/tags/CLOB/"},{"name":"BLOB","slug":"BLOB","permalink":"https://www.cz5h.com/tags/BLOB/"},{"name":"SQL插入","slug":"SQL插入","permalink":"https://www.cz5h.com/tags/SQL%E6%8F%92%E5%85%A5/"}]},{"title":"GraphX编程指南-官方文档-整理","slug":"2018-5-7 GraphX编程指南-官方文档-整理","date":"2018-05-06T22:00:00.000Z","updated":"2020-02-29T18:43:55.528Z","comments":true,"path":"article/2d41.html","link":"","permalink":"https://www.cz5h.com/article/2d41.html","excerpt":"GraphX 是新的(alpha)的图形和图像并行计算的Spark API。从整理上看，GraphX 通过引入 弹性分布式属性图(Resilient Distributed Property Graph)继承了Spark RDD：一个将有效信息放在顶点和边的有向多重图。为了支持图形计算，GraphX 公开了一组基本的运算（例如，subgraph，joinVertices和mapReduceTriplets），以及在一个优化后的PregelAPI的变形。此外，GraphX 包括越来越多的图算法和 builder 构造器，以简化图形分析任务。","text":"GraphX 是新的(alpha)的图形和图像并行计算的Spark API。从整理上看，GraphX 通过引入 弹性分布式属性图(Resilient Distributed Property Graph)继承了Spark RDD：一个将有效信息放在顶点和边的有向多重图。为了支持图形计算，GraphX 公开了一组基本的运算（例如，subgraph，joinVertices和mapReduceTriplets），以及在一个优化后的PregelAPI的变形。此外，GraphX 包括越来越多的图算法和 builder 构造器，以简化图形分析任务。 GraphX 目前是一个 alpha 组件。虽然我们会尽量减少 API 的变化，但是一些 API 可能会在将来的版本中改变。 图并行计算的背景从社交网络到语言建模，日益扩大的规模和图形数据的重要性已带动许多新的图像并行系统（例如，Giraph和 GraphLab）。通过限制可表示计算的类型以及引入新的技术来划分和分布图，这些系统比一般的数据并行系统在执行复杂图形算法方面有大幅度地提高。 然而，这些限制在获得重大性能提升的同时，也使其难以表达一个典型的图表分析流程中的许多重要阶段：构造图，修改它的结构或表达计算跨越多重图的计算。此外，如何看待数据取决于我们的目标，相同的原始数据，可能有许多不同的表(table)和图表视图(graph views)。 因此，能够在同一组物理数据的表和图表视图之间切换是很有必要的，并利用各视图的属性，以方便地和有效地表达计算。但是，现有的图形分析管道必须由图并行和数据并行系统组成，从而导致大量的数据移动和重复以及复杂的编程模型。 该 GraphX 项目的目标是建立一个系统，建立一个统一的图和数据并行计算的 API。该GraphX API 使用户能够将数据既可以当作一个图，也可以当作集合（即RDDS）而不用进行数据移动或数据复制。通过引入在图并行系统中的最新进展，GraphX能够优化图形操作的执行。 GraphX 替换 Spark Bagel 的 API在GraphX 的发布之前，Spark的图计算是通过Bagel实现的，后者是Pregel的一个具体实现。GraphX提供了更丰富的图属性API，从而增强了Bagel。从而达到一个更加精简的Pregel抽象，系统优化，性能提升以及减少内存开销。虽然我们计划最终弃用Bagel，我们将继续支持Bagel的API和Bagel编程指南。不过，我们鼓励Bagel用户，探索新的GraphXAPI，并就从Bagel升级中遇到的障碍反馈给我们。 从 Spark 0.9.1 迁移GraphX 在Spark 1.1.0 包含Spark-0.9.1一个用户面向接口的改变。EdgeRDD现在可以存储相邻顶点属性来构建triplets，因此它获得了一个类型参数。一个Graph[VD，ED]的边的类型是EdgeRDD[ED，VD]而不是 EdgeRDD[ED]。 入门首先，你要导入 Spark 和 GraphX 到你的项目，如下所示： 1234import org.apache.spark._import org.apache.spark.graphx._// To make some of the examples work we will also need RDDimport org.apache.spark.rdd.RDD 如果你不使用Spark shell，你还需要一个 SparkContext。要了解更多有关如何开始使用Spark参考 Spark快速入门指南。 属性图该 属性图是一个用户定义的顶点和边的有向多重图。有向多重图是一个有向图，它可能有多个平行边共享相同的源和目的顶点。多重图支持并行边的能力简化了有多重关系（例如，同事和朋友）的建模场景。每个顶点是 唯一 的 64位长的标识符（VertexID）作为主键。GraphX并没有对顶点添加任何顺序的约束。同样，每条边具有相应的源和目的顶点的标识符。 该属性表的参数由顶点（VD）和边缘（ED）的类型来决定。这些是分别与每个顶点和边相关联的对象的类型。 GraphX 优化顶点和边的类型的表示方法，当他们是普通的旧的数据类型（例如，整数，双精度等）通过将它们存储在专门的阵列减小了在内存占用量。 在某些情况下，可能希望顶点在同一个图中有不同的属性类型。这可以通过继承来实现。例如，以用户和产品型号为二分图我们可以做到以下几点： 123456class VertexProperty()case class UserProperty( val name: String) extends VertexPropertycase class ProductProperty( val name: String, val price: Double) extendsVertexProperty// The graph might then have the type:var graph: Graph[VertexProperty, String] = null 和 RDDS 一样，属性图是不可变的，分布式的和容错的。对图中的值或结构的改变是通过生成具有所需更改的新图来完成的。注意原始图的该主要部分（即不受影响的结构，属性和索引）被重用，从而减少这个数据结构的成本。该图是通过启发式执行顶点分区，在不同的执行器(executor)中进行顶点的划分。与 RDDS 一样，在发生故障的情况下，图中的每个分区都可以重建。 逻辑上讲，属性图对应于一对类型集合（RDDS），这个组合记录顶点和边的属性。因此，该图表类包含成员访问该图的顶点和边： 1234class Graph[VD, ED] &#123; val vertices: VertexRDD[VD] val edges: EdgeRDD[ED, VD]&#125; 类 VertexRDD [VD]和 EdgeRDD[ED，VD]继承和并且分别是一个优化的版本的RDD[(VertexID,VD)]和RDD[Edge[ED]]。这两个VertexRDD[VD]和EdgeRDD[ED,VD]提供各地图的计算内置附加功能，并充分利用内部优化。我们在上一节顶点和边RDDS中详细讨论了VertexRDD和EdgeRDD的API，但现在，他们可以简单地看成是RDDS形式的： RDD[(VertexID,VD)]和 RDD [EDGE[ED]] 。 属性图的例子假设我们要建立一个 GraphX项目各合作者的属性图。顶点属性可能会包含用户名和职业。我们可以使用一组字符注释来描述代表合作者关系的边： 由此产生的图形将有类型签名： 1val userGraph: Graph[(String, String), String] 有许多方法可以从原始数据文件，RDDS，甚至合成生成器来生成图，我们会在 graph builders 更详细的讨论。可能是最通用的方法是使用 Graph ojbect。例如，下面的代码从一系列的RDDS的集合中构建图： 123456789101112// Assume the SparkContext has already been constructedval sc: SparkContext// Create an RDD for the verticesval users: RDD[(VertexId, (String, String))] = sc.parallelize( Array((3L, (\"rxin\", \"student\")), (7L, (\"jgonzal\",\"postdoc\")),(5L, (\"franklin\", \"prof\")), (2L, (\"istoica\", \"prof\"))))// Create an RDD for edgesval relationships: RDD[Edge[String]] = sc.parallelize( Array( Edge(3L, 7L, \"collab\"), Edge(5L, 3L, \"advisor\"),Edge(2L, 5L, \"colleague\"), Edge(5L, 7L, \"pi\")))// Define a default user in case there are relationship with missing userval defaultUser = (\"John Doe\", \"Missing\")// Build the initial Graphval graph = Graph(users, relationships, defaultUser) 在上面的例子中，我们利用了Edge的case类。Edge具有srcId和dstId，它们分别对应于源和目的地顶点的标识符。此外，Edge 类具有 attr属性，并存储的边的特性。 我们可以通过 graph.vertices 和 graph.edges属性，得到图到各自的顶点和边的视图。 12345val graph: Graph[(String, String), String] // Constructed from above// Count all users which are postdocsgraph.vertices.filter &#123; case (id, (name, pos)) =&gt; pos == \"postdoc\" &#125;.count// Count all the edges where src &gt; dstgraph.edges.filter(e =&gt; e.srcId &gt; e.dstId).count 需要注意的是graph.vertices返回VertexRDD[(String,String)]延伸RDD[(VertexID，(String，String))]，所以我们使用Scala的case表达来解构元组。在另一方面，graph.edges返回EdgeRDD包含Edge[String]对象。我们可以也使用的如下的类型的构造器： 1graph.edges.filter &#123; case Edge(src, dst, prop) =&gt; src &gt; dst &#125;.count 除了 图的顶点和边的意见，GraphX 也提供了三重视图。三重视图逻辑连接点和边的属性产生的 RDD[EdgeTriplet [VD，ED]包含的实例 EdgeTriplet类。此 连接 可以表示如下的SQL表达式： 123SELECT src.id, dst.id, src.attr, e.attr, dst.attrFROM edges AS e LEFT JOIN vertices AS src, vertices AS dstON e.srcId = src.Id AND e.dstId = dst.Id 或图形方式： 该 EdgeTriplet类继承了 Edge并加入了类属性:srcAttr和dstAttr,用于包含了源和目标属性。我们可以用一个图的三元组视图渲染描述用户之间的关系字符串的集合。 123456val graph: Graph[(String, String), String] // Constructed from above// Use the triplets view to create an RDD of facts.val facts: RDD[String] = graph.triplets.map(triplet =&gt; triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1)facts.collect.foreach(println(_ _)) Graph 操作正如RDDs有这样基本的操作mpa，filter，和reduceByKey，属性图也有一系列基本的运算,采用用户定义的函数，并产生新的图形与变换的性质和结构。定义核心运算已优化的实现方式中定义的Graph，并且被表示为核心操作的组合定义在GraphOps。然而，由于Scala的implicits特性,GraphOps中的操作会自动作为Graph的成员。例如，我们可以计算各顶点的入度（定义在的 GraphOps）： 123val graph: Graph[(String, String), String]// Use the implicit GraphOps.inDegrees operatorval inDegrees: VertexRDD[Int] = graph.inDegrees 将核心图操作和 GraphOps区分开来的原因是为了将来能够支持不同的图表示。每个图的表示必须实现核心操作并且复用 GraphOps中很多有用的操作。 运算列表总结以下列出了Graph图 和 GraphOps中同时定义的操作.为了简单起见,我们都定义为Graph的成员函数。请注意，某些函数签名已被简化（例如，默认参数和类型的限制被删除了）,还有一些更高级的功能已被删除，完整的列表,请参考API文档。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** Summary of the functionality in the property graph */class Graph[VD, ED] &#123; // Information about the Graph val numEdges: Long val numVertices: Long val inDegrees: VertexRDD[Int] val outDegrees: VertexRDD[Int] val degrees: VertexRDD[Int] // Views of the graph as collections val vertices: VertexRDD[VD] val edges: EdgeRDD[ED, VD] val triplets: RDD[EdgeTriplet[VD, ED]] // Functions for caching graphs def persist(newLevel: StorageLevel = StorageLevel. MEMORY_ONLY): Graph[VD, ED] def cache(): Graph[VD, ED] def unpersistVertices(blocking: Boolean = true): Graph[VD, ED] // Change the partitioning heuristic def partitionBy(partitionStrategy: PartitionStrategy): Graph[VD, ED] // Transform vertex and edge attributes def mapVertices[VD2](map: (VertexID, VD) =&gt; VD2): Graph[VD2, ED] def mapEdges[ED2](map: Edge[ED] =&gt; ED2): Graph[VD, ED2] def mapEdges[ED2](map: (PartitionID, Iterator[Edge[ED]]) =&gt; Iterator[ED2]):Graph[VD, ED2] def mapTriplets[ED2](map: EdgeTriplet[VD, ED] =&gt; ED2): Graph[VD, ED2] def mapTriplets[ED2](map: (PartitionID, Iterator[EdgeTriplet[VD, ED]]) =&gt; Iterator[ED2]): Graph[VD, ED2] // Modify the graph structure def reverse: Graph[VD, ED] def subgraph( epred: EdgeTriplet[VD,ED] =&gt; Boolean = (x =&gt; true), vpred: (VertexID, VD) =&gt; Boolean = ((v, d) =&gt; true) ): Graph[VD, ED] def mask[VD2, ED2](other: Graph[VD2, ED2]): Graph[VD, ED] def groupEdges(merge: (ED, ED) =&gt; ED): Graph[VD, ED] // Join RDDs with the graph def joinVertices[U](table: RDD[(VertexID, U)])(mapFunc: (VertexID, VD, U) =&gt; VD): Graph[VD, ED] def outerJoinVertices[U, VD2](other: RDD[(VertexID, U)]) (mapFunc: (VertexID, VD, Option[U]) =&gt; VD2) : Graph[VD2, ED] // Aggregate information about adjacent triplets def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexID]] def collectNeighbors(edgeDirection: EdgeDirection):VertexRDD[Array[(VertexID, VD)]] def mapReduceTriplets[A: ClassTag]( mapFunc: EdgeTriplet[VD, ED] =&gt; Iterator[(VertexID, A)], reduceFunc: (A, A) =&gt; A, activeSetOpt: Option[(VertexRDD[_ _], EdgeDirection)] = None ): VertexRDD[A] // Iterative graph-parallel computation def pregel[A](initialMsg: A, maxIterations: Int, activeDirection: EdgeDirection)( vprog: (VertexID, VD, A) =&gt; VD, sendMsg: EdgeTriplet[VD, ED] =&gt; Iterator[(VertexID,A)], mergeMsg: (A, A) =&gt; A ): Graph[VD, ED] // Basic graph algorithms def pageRank(tol: Double, resetProb: Double = 0.15): Graph[Double, Double] def connectedComponents(): Graph[VertexID, ED] def triangleCount(): Graph[Int, ED] def stronglyConnectedComponents(numIter: Int): Graph[VertexID, ED]&#125; 属性操作和RDD的 map操作类似，属性图包含以下内容： 12345class Graph[VD, ED] &#123; def mapVertices[VD2](map: (VertexId, VD) =&gt; VD2): Graph[VD2, ED] def mapEdges[ED2](map: Edge[ED] =&gt; ED2): Graph[VD, ED2] def mapTriplets[ED2](map: EdgeTriplet[VD, ED] =&gt; ED2): Graph[VD, ED2]&#125; 每个运算产生一个新的图,这个图的顶点和边属性通过 map方法修改。 请注意，在所有情况下的图的机构不受影响。这是这些运算符的关键所在，它允许新得到图可以复用初始图的结构索引。下面的代码段在逻辑上是等效的，但第一个不保留结构索引，所以不会从 GraphX 系统优化中受益： 123val newVertices = graph.vertices.map &#123; case (id, attr) =&gt; (id, mapUdf(id,attr)) &#125;val newGraph = Graph(newVertices, graph.edges) 相反，使用 mapVertices保存索引： 1val newGraph = graph.mapVertices((id, attr) =&gt; mapUdf(id, attr)) 这些操作经常被用来初始化图的特定计算或者去除不必要的属性。例如，给定一个将出度作为顶点的属性图（我们之后将介绍如何构建这样的图），我们初始化它作为 PageRank： 1234567// Given a graph where the vertex property is the out-degreeval inputGraph: Graph[Int, String] = graph.outerJoinVertices(graph.outDegrees)((vid, _ _, degOpt) =&gt; degOpt.getOrElse(0))// Construct a graph where each edge contains the weight// and each vertex is the initial PageRankval outputGraph: Graph[Double, Double] = inputGraph.mapTriplets(triplet =&gt; 1.0 / triplet.srcAttr).mapVertices((id, _ _) =&gt; 1.0) 结构操作当前 GraphX 只支持一组简单的常用结构化操作，我们希望将来增加更多的操作。以下是基本的结构运算符的列表。 123456class Graph[VD, ED] &#123; def reverse: Graph[VD, ED] def subgraph(epred: EdgeTriplet[VD,ED] =&gt; Boolean, vpred: (VertexId, VD) =&gt; Boolean): Graph[VD, ED] def mask[VD2, ED2](other: Graph[VD2, ED2]): Graph[VD, ED] def groupEdges(merge: (ED, ED) =&gt; ED): Graph[VD,ED]&#125; 该reverse操作符返回一个新图,新图的边的方向都反转了。这是非常实用的，例如，试图计算逆向PageRank。因为反向操作不修改顶点或边属性或改变的边的数目，它的实现不需要数据移动或复制。 该子图subgraph将顶点和边的预测作为参数，并返回一个图，它只包含满足了顶点条件的顶点图（值为true），以及满足边条件 并连接顶点的边。subgraph子运算符可应用于很多场景，以限制图表的顶点和边是我们感兴趣的，或消除断开的链接。例如，在下面的代码中，我们删除已损坏的链接： 12345678910111213141516171819202122// Create an RDD for the verticesval users: RDD[(VertexId, (String, String))] = sc.parallelize( Array((3L, (\"rxin\", \"student\")), (7L, (\"jgonzal\",\"postdoc\")),(5L, (\"franklin\", \"prof\")), (2L, (\"istoica\", \"prof\")),(4L, (\"peter\", \"student\"))))// Create an RDD for edgesval relationships: RDD[Edge[String]] = sc.parallelize( Array( Edge(3L, 7L, \"collab\"), Edge(5L, 3L, \"advisor\"),Edge(2L, 5L, \"colleague\"), Edge(5L, 7L, \"pi\"),Edge(4L, 0L, \"student\"), Edge(5L, 0L, \"colleague\")))// Define a default user in case there are relationship with missing userval defaultUser = (\"John Doe\", \"Missing\")// Build the initial Graphval graph = Graph(users, relationships, defaultUser)// Notice that there is a user 0 (for which we have no information) connected to users// 4 (peter) and 5 (franklin).graph.triplets.map( triplet =&gt; triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1).collect.foreach(println(_ _))// Remove missing vertices as well as the edges to connected to themval validGraph = graph.subgraph(vpred = (id, attr) =&gt; attr._2 != \"Missing\")// The valid subgraph will disconnect users 4 and 5 by removing user 0validGraph.vertices.collect.foreach(println(_ _))validGraph.triplets.map( triplet =&gt; triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1).collect.foreach(println(_ _)) 注意，在上面的例子中，仅提供了顶点条件。如果不提供顶点或边的条件，在subgraph 操作中默认为 真 。 mask操作返回一个包含输入图中所有的顶点和边的图。这可以用来和subgraph一起使用，以限制基于属性的另一个相关图。例如，我们用去掉顶点的图来运行联通分量，并且限制输出为合法的子图。 123456// Run Connected Componentsval ccGraph = graph.connectedComponents() // No longer contains missing field// Remove missing vertices as well as the edges to connected to themval validGraph = graph.subgraph(vpred = (id, attr) =&gt; attr._2 != \"Missing\")// Restrict the answer to the valid subgraphval validCCGraph = ccGraph.mask(validGraph) 该 groupEdges操作合并在多重图中的平行边（即重复顶点对之间的边）。在许多数值计算的应用中，平行的边缘可以加入 （他们的权重的会被汇总）为单条边从而降低了图形的大小。 Join 操作在许多情况下，有必要从外部集合（RDDS）中加入图形数据。例如，我们可能有额外的用户属性，想要与现有的图形合并，或者我们可能需要从一个图选取一些顶点属性到另一个图。这些任务都可以使用来 join 经操作完成。下面我们列出的关键联接运算符： 1234class Graph[VD, ED] &#123; def joinVertices[U](table: RDD[(VertexId, U)])(map: (VertexId, VD, U) =&gt; VD): Graph[VD, ED] def outerJoinVertices[U, VD2](table: RDD[(VertexId, U)])(map: (VertexId, VD, Option[U]) =&gt; VD2): Graph[VD2, ED]&#125; 该 joinVertices运算符连接与输入RDD的顶点，并返回一个新的图，新图的顶点属性是通过用户自定义的 map功能作用在被连接的顶点上。没有匹配的RDD保留其原始值。 需要注意的是，如果RDD顶点包含多于一个的值，其中只有一个将会被使用。因此，建议在输入的RDD在初始为唯一的时候，使用下面的 pre-index 所得到的值以加快后续join。 12345val nonUniqueCosts: RDD[(VertexID, Double)]val uniqueCosts: VertexRDD[Double] = graph.vertices.aggregateUsingIndex(nonUnique, (a,b) =&gt; a + b)val joinedGraph = graph.joinVertices(uniqueCosts)( (id, oldCost, extraCost) =&gt; oldCost + extraCost) 更一般 outerJoinVertices操作类似于joinVertices，除了将用户定义的map函数应用到所有的顶点，并且可以改变顶点的属性类型。因为不是所有的顶点可能会在输入匹配值RDD的mpa函数接受一个Optin类型。例如，我们可以通过 用 outDegree 初始化顶点属性来设置一个图的 PageRank。 12345678val outDegrees: VertexRDD[Int] = graph.outDegreesval degreeGraph = graph.outerJoinVertices(outDegrees) &#123; (id, oldAttr, outDegOpt) =&gt; outDegOpt match &#123; case Some(outDeg) =&gt; outDeg case None =&gt; 0 // No outDegree means zero outDegree &#125;&#125; 您可能已经注意到，在上面的例子中采用了多个参数列表的curried函数模式（例如，f(a)(b)）。虽然我们可以有同样写f(a)(b)为f(a,b)，这将意味着该类型推断b不依赖于a。其结果是，用户将需要提供类型标注给用户自定义的函数： 12val joinedGraph = graph.joinVertices(uniqueCosts, (id: VertexID, oldCost: Double, extraCost: Double) =&gt; oldCost + extraCost) 邻居聚集图形计算的一个关键部分是聚集每个顶点的邻域信息。例如，我们可能想要知道每个用户追随者的数量或每个用户的追随者的平均年龄。许多图迭代算法（如PageRank，最短路径，连通分量等）反复聚集邻居节点的属性， （例如，当前的 PageRank 值，到源节点的最短路径，最小可达顶点 ID）。 mapReduceTripletsGraphX中核心（大量优化）聚集操作是 mapReduceTriplets操作： 123456class Graph[VD, ED] &#123; def mapReduceTriplets[A]( map: EdgeTriplet[VD, ED] =&gt; Iterator[(VertexId, A)], reduce: (A, A) =&gt; A ): VertexRDD[A]&#125; 该 mapReduceTriplets运算符将用户定义的map函数作为输入，并且将map作用到每个triplet，并可以得到triplet上所有的顶点（或者两个，或者空）的信息。为了便于优化预聚合，我们目前仅支持发往triplet的源或目的地的顶点信息。用户定义的reduce功能将合并所有目标顶点相同的信息。该mapReduceTriplets操作返回 VertexRDD [A] ，包含所有以每个顶点作为目标节点集合消息（类型 A）。没有收到消息的顶点不包含在返回 VertexRDD。 需要注意的是 mapReduceTriplets需要一个附加的可选activeSet（上面没有显示,请参见API文档的详细信息），这限制了 VertexRDD地图提供的邻接边的map阶段： 1activeSetOpt: Option[(VertexRDD[_], EdgeDirection)] = None 该EdgeDirection指定了哪些和顶点相邻的边包含在map阶段。如果该方向是in，则用户定义的 mpa函数 将仅仅作用目标顶点在与活跃集中。如果方向是out，则该map函数将仅仅作用在那些源顶点在活跃集中的边。如果方向是 either，则map函数将仅在任一顶点在活动集中的边。如果方向是both，则map函数将仅作用在两个顶点都在活跃集中。活跃集合必须来自图的顶点中。限制计算到相邻顶点的一个子集三胞胎是增量迭代计算中非常必要，而且是GraphX 实现Pregel中的关键。 在下面的例子中我们使用 mapReduceTriplets算子来计算高级用户追随者的平均年龄。 1234567891011121314151617181920212223242526// Import random graph generation libraryimport org.apache.spark.graphx.util.GraphGenerators// Create a graph with \"age\" as the vertex property. Here we use a random graph for simplicity.val graph: Graph[Double, Int] = GraphGenerators.logNormalGraph(sc, numVertices = 100).mapVertices( (id, _ _) =&gt; id.toDouble )// Compute the number of older followers and their total ageval olderFollowers: VertexRDD[(Int, Double)] = graph.mapReduceTriplets[(Int,Double)]( triplet =&gt; &#123; // Map Function if (triplet.srcAttr &gt; triplet.dstAttr) &#123; // Send message to destination vertex containing counter and age Iterator((triplet.dstId, (1, triplet.srcAttr))) &#125; else &#123; // Don't send a message for this triplet Iterator.empty &#125; &#125;, // Add counter and age (a, b) =&gt; (a._1 + b._1, a._2 + b._2) // Reduce Function)// Divide total age by number of older followers to get average age of older followersval avgAgeOfOlderFollowers: VertexRDD[Double] = olderFollowers.mapValues( (id, value) =&gt; value match &#123; case (count, totalAge) =&gt; totalAge / count &#125; )// Display the resultsavgAgeOfOlderFollowers.collect.foreach(println(_ _)) 注意，当消息（和消息的总和）是固定尺寸的时候（例如，浮点运算和加法而不是列表和连接）时，mapReduceTriplets 操作执行。更精确地说，结果 mapReduceTriplets 最好是每个顶点度的次线性函数。 计算度信息一个常见的聚合任务是计算每个顶点的度：每个顶点相邻边的数目。在有向图的情况下，往往需要知道入度，出度，以及总度。该GraphOps类包含一系列的运算来计算每个顶点的度的集合。例如，在下面我们计算最大的入度，出度，总度： 12345678// Define a reduce operation to compute the highest degree vertexdef max(a: (VertexId, Int), b: (VertexId, Int)): (VertexId, Int) = &#123; if (a._2 &gt; b._2) a else b&#125;// Compute the max degreesval maxInDegree: (VertexId, Int) = graph.inDegrees.reduce(max)val maxOutDegree: (VertexId, Int) = graph.outDegrees.reduce(max)val maxDegrees: (VertexId, Int) = graph.degrees.reduce(max) 收集邻居在某些情况下可能更容易通过收集相邻顶点和它们的属性来表达在每个顶点表示的计算。这可以通过使用容易地实现 collectNeighborIds和 collectNeighbors运算。 1234class GraphOps[VD, ED] &#123; def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]] def collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[ Array[(VertexId, VD)] ]&#125; 需要注意的是，这些运算计算代价非常高，因为他们包含重复信息，并且需要大量的通信。如果可能的话尽量直接使用 mapReduceTriplets。 缓存和清空缓存在Spark中，RDDS默认并不保存在内存中。为了避免重复计算，当他们需要多次使用时，必须明确地使用缓存（见 Spark编程指南）。在GraphX中Graphs行为方式相同。当需要多次使用图形时，一定要首先调用Graph.cache。 在迭代计算，为了最佳性能，也可能需要清空缓存。默认情况下，缓存的RDDS和图表将保留在内存中，直到内存压力迫使他们按照LRU顺序被删除。对于迭代计算，之前的迭代的中间结果将填补缓存。虽然他们最终将被删除，内存中的不必要的数据会使垃圾收集机制变慢。一旦它们不再需要缓存，就立即清空中间结果的缓存，这将会更加有效。这涉及物化（缓存和强迫）图形或RDD每次迭代，清空所有其他数据集，并且只使用物化数据集在未来的迭代中。然而，由于图形是由多个RDDS的组成的，正确地持续化他们将非常困难。对于迭代计算，我们推荐使用 Pregel API，它正确地 unpersists 中间结果。 Pregel 的 API图本质上是递归的数据结构，因为顶点的性质取决于它们的邻居，这反过来又依赖于邻居的属性。其结果是许多重要的图形算法迭代重新计算每个顶点的属性，直到定点条件满足为止。一系列图像并行方法已经被提出来表达这些迭代算法。GraphX 提供了类似与Pregel 的操作，这是 Pregel 和 GraphLab 方法的融合。 从总体来看，Graphx 中的 Pregel 是一个批量同步并行消息传递抽象 约束到该图的拓扑结构。Pregel 运算符在一系列超步骤中，其中顶点收到从之前的步骤中流入消息的总和，计算出顶点属性的新值，然后在下一步中将消息发送到相邻的顶点。不同于Pregel，而是更像GraphLab消息被并行计算，并且作为edge-triplet，该消息的计算可以访问的源和目的地的顶点属性。没有收到消息的顶点在一个超级步跳过。当没有消息是，Pregel 停止迭代，并返回最终图形。 请注意，不像更标准的 Pregel的实现，在GraphX中顶点只能将消息发送到邻近的顶点，并且信息构建是通过使用用户定义的消息函数并行执行。这些限制使得在 GraphX 有额外的优化。 以下是类型签名 Pregel，以及一个 初始的实现 （注调用graph.cache已被删除）中： 12345678910111213141516171819202122232425262728293031323334class GraphOps[VD, ED] &#123; def pregel[A]( initialMsg: A, maxIter: Int = Int. MaxValue, activeDir: EdgeDirection = EdgeDirection. Out)( vprog: (VertexId, VD, A) =&gt; VD, sendMsg: EdgeTriplet[VD, ED] =&gt; Iterator[(VertexId, A)], mergeMsg: (A, A) =&gt; A ): Graph[VD, ED] = &#123; // Receive the initial message at each vertex var g = mapVertices( (vid, vdata) =&gt; vprog(vid, vdata, initialMsg) ).cache() // compute the messages var messages = g.mapReduceTriplets(sendMsg, mergeMsg) var activeMessages = messages.count() // Loop until no messages remain or maxIterations is achieved var i = 0 while (activeMessages &gt; 0 &amp;&amp; i &lt; maxIterations) &#123; // Receive the messages:------ // Run the vertex program on all vertices that receive messages val newVerts = g.vertices.innerJoin(messages)(vprog).cache() // Merge the new vertex values back into the graph g = g.outerJoinVertices(newVerts) &#123; (vid, old, newOpt) =&gt; newOpt.getOrElse(old) &#125;.cache() // Send Messages:----- -- // Vertices that didn't receive a message above don't appear in newVerts and therefore don't // get to send messages. More precisely the map phase of mapReduceTriplets is only invoked // on edges in the activeDir of vertices in newVerts messages = g.mapReduceTriplets(sendMsg, mergeMsg, Some((newVerts, activeDir))).cache() activeMessages = messages.count() i += 1 &#125; g &#125;&#125; 请注意，Pregel 需要两个参数列表（即graph.pregel(list1)(list2)）。第一个参数列表中包含的配置参数包括初始信息，迭代的最大次数，以及发送消息（默认出边）的方向。第二个参数列表包含用于用户定义的接收消息（顶点程序 vprog），计算消息（sendMsg），并结合信息 mergeMsg。 我们可以使用 Pregel 运算符来表达计算，如在下面的例子中的单源最短路径。 123456789101112131415161718192021import org.apache.spark.graphx._// Import random graph generation libraryimport org.apache.spark.graphx.util.GraphGenerators// A graph with edge attributes containing distancesval graph: Graph[Int, Double] = GraphGenerators.logNormalGraph(sc, numVertices = 100).mapEdges(e =&gt; e.attr.toDouble)val sourceId: VertexId = 42 // The ultimate source// Initialize the graph such that all vertices except the root have distance infinity.val initialGraph = graph.mapVertices((id, _ _) =&gt; if (id == sourceId) 0.0 else Double.PositiveInfinity)val sssp = initialGraph.pregel( Double. PositiveInfinity)( (id, dist, newDist) =&gt; math.min(dist, newDist), // Vertex Program triplet =&gt; &#123; // Send Message if (triplet.srcAttr + triplet.attr &lt; triplet.dstAttr) &#123; Iterator((triplet.dstId, triplet.srcAttr + triplet.attr)) &#125; else &#123; Iterator.empty &#125; &#125;, (a,b) =&gt; math.min(a,b) // Merge Message)println(sssp.vertices.collect.mkString(\"\\n\")) Graph BuilderGraphX 提供多种从RDD或者硬盘中的节点和边中构建图。默认情况下，没有哪种Graph Builder会重新划分图的边;相反，边会留在它们的默认分区（如原来的HDFS块）。Graph.groupEdges需要的图形进行重新分区，因为它假设相同的边将被放在同一个分区同一位置，所以你必须在调用Graph.partitionBy之前调用groupEdges。 12345678object GraphLoader &#123; def edgeListFile( sc: SparkContext, path: String, canonicalOrientation: Boolean = false, minEdgePartitions: Int = 1 ): Graph[Int, Int]&#125; GraphLoader.edgeListFile提供了一种从磁盘上边的列表载入图的方式。它解析了一个以下形式的邻接列表（源顶点ID，目的地顶点ID）对，忽略以#开头的注释行： 1234# This is a comment2 14 11 2 它从指定的边创建了一个图表，自动边中提到的任何顶点。所有顶点和边的属性默认为1。canonicalOrientation参数允许重新定向边的正方向（srcId &lt; dstId），这是必需的connected-component算法。该minEdgePartitions参数指定边缘分区生成的最小数目;例如，在HDFS文件具有多个块, 那么就有多个边的分割. 1234567891011121314151617object Graph &#123; def apply[VD, ED]( vertices: RDD[(VertexId, VD)], edges: RDD[Edge[ED]], defaultVertexAttr: VD = null ): Graph[VD, ED] def fromEdges[VD, ED]( edges: RDD[Edge[ED]], defaultValue: VD): Graph[VD, ED] def fromEdgeTuples[VD]( rawEdges: RDD[(VertexId, VertexId)], defaultValue: VD, uniqueEdges: Option[PartitionStrategy] = None ): Graph[VD, Int]&#125; Graph.apply允许从顶点和边的RDDS中创建的图。重复的顶点会任意选择，并在边RDD中存在的顶点， 但不是顶点RDD会被赋值为默认属性。 Graph.fromEdges允许从只有边的元组RDD创建的图，自动生成由边中存在的顶点，并且给这些顶点赋值为缺省值。 Graph.fromEdgeTuples允许从只有边的元组的RDD图中创建图，并将的边的值赋为1，并自动创建边中所存在的顶点，并设置为缺省值。它也支持删除重边; 进行删除重边时，传入 PartitionStrategy的Some 作为uniqueEdges参数（例如，uniqueEdges=Some（PartitionStrategy.RandomVertexCut））。分区策略是必要的，因为定位在同一分区相同的边，才能使他们能够进行重复删除。 顶点和边 RDDsGraphX 公开了图中 RDD 顶点和边的视图。然而，因为GraphX将顶点和边保存在优化的数据结构，并且为这些数据结构提供额外的功能，顶点和边分别作为VertexRDD和EdgeRDD返回。在本节中，我们回顾一些这些类型的其他有用的功能。 VertexRDDs该VertexRDD [A]继承RDD [(VertexID, A)]，并增加了一些额外的限制 ，每个VertexID只出现 一次 。此外，VertexRDD[A]表示一个顶点集合，其中每个顶点与类型的属性为A。在内部，这是通过将顶点属性中存储在一个可重复使用的哈希表。因此，如果两个VertexRDDs继承自相同的基类VertexRDD（例如，通过filter或mapValues ），他们可以参加在常数时间内实现合并，而不需要重新计算hash值。要充分利用这个索引数据结构，VertexRDD提供了以下附加功能： 1234567891011121314class VertexRDD[VD] extends RDD[(VertexID, VD)] &#123; // Filter the vertex set but preserves the internal index def filter(pred: Tuple2[VertexId, VD] =&gt; Boolean): VertexRDD[VD] // Transform the values without changing the ids (preserves the internal index) def mapValues[VD2](map: VD =&gt; VD2): VertexRDD[VD2] def mapValues[VD2](map: (VertexId, VD) =&gt; VD2): VertexRDD[VD2] // Remove vertices from this set that appear in the other set def diff(other: VertexRDD[VD]): VertexRDD[VD] // Join operators that take advantage of the internal indexing to accelerate joins (substantially) def leftJoin[VD2, VD3](other: RDD[(VertexId, VD2)])(f: (VertexId, VD, Option[VD2]) =&gt; VD3): VertexRDD[VD3] def innerJoin[U, VD2](other: RDD[(VertexId, U)])(f: (VertexId, VD, U) =&gt; VD2): VertexRDD[VD2] // Use the index on this RDD to accelerate a `reduceByKey` operation on the input RDD. def aggregateUsingIndex[VD2](other: RDD[(VertexId, VD2)], reduceFunc: (VD2, VD2) =&gt; VD2): VertexRDD[VD2]&#125; 请注意，例如，如何filter操作符返回一个VertexRDD。过滤器使用的是实际通过BitSet实现的，从而复用索引和保持能快速与其他 VertexRDD 实现连接功能。 类似地，mapValues 操作不允许mapha函数改变 VertexID，从而可以复用统一HashMap中的数据结构。当两个VertexRDD派生自同一HashMap，并且是通过线性少买而非代价昂贵的逐点查询时，无论是 leftJoin 和 innerJoin 连接时能够识别 VertexRDD 。 该aggregateUsingIndex操作是一种新的有效的从RDD[(VertexID,A)]构建新的VertexRDD的方式。从概念上讲，如果我在一组顶点上构建了一个VertexRDD[B]，这是一个在某些顶点RDD[(VertexID,A)]的超集，然后我可以重用该索引既聚集，随后为RDD[(VertexID, A)]建立索引。例如： 123456789val setA: VertexRDD[Int] = VertexRDD(sc.parallelize(0L until 100L).map(id =&gt; (id, 1)))val rddB: RDD[(VertexId, Double)] = sc.parallelize(0L until 100L).flatMap(id =&gt; List((id, 1.0), (id, 2.0)))// There should be 200 entries in rddBrddB.countval setB: VertexRDD[Double] = setA.aggregateUsingIndex(rddB, _ _ + _ _)// There should be 100 entries in setBsetB.count// Joining A and B should now be fast!val setC: VertexRDD[Double] = setA.innerJoin(setB)((id, a, b) =&gt; a + b) EdgeRDDs该EdgeRDD [ED，VD] ，它继承RDD[Edge[ED],以各种分区策略PartitionStrategy将边划分成不同的块。在每个分区中，边属性和邻接结构，分别存储，这使得更改属性值时，能够最大限度的复用。 EdgeRDD 是提供的三个额外的函数： 123456// Transform the edge attributes while preserving the structuredef mapValues[ED2](f: Edge[ED] =&gt; ED2): EdgeRDD[ED2, VD]// Revere the edges reusing both attributes and structuredef reverse: EdgeRDD[ED, VD]// Join two `EdgeRDD`s partitioned using the same partitioning strategy.def innerJoin[ED2, ED3](other: EdgeRDD[ED2, VD])(f: (VertexId, VertexId, ED, ED2) =&gt; ED3): EdgeRDD[ED3, VD] 在大多数应用中，我们发现，在 EdgeRDD 中的操作是通过图形运算符来实现，或依靠在基类定义的 RDD 类操作。 优化图的表示关于 GraphX 中如何表示分布式图结构的详细描述，这个话题超出了本指南的范围，一些高层次的理解可能有助于设计可扩展的算法设计以及 API 的最佳利用。GraphX 采用顶点切的方法来分发图划分： 不通过边划分图，GraphX 沿顶点来划分图，这样可以减少顶点之间的通信和存储开销。逻辑上，这对应于将边分配到不同的机器，并允许顶点跨越多个机器。分配边的确切方法取决于PartitionStrategy并有多个权衡各种试探法。用户可以通过重新分区图与不同的策略之间进行选择Graph.partitionBy操作。默认分区策略是按照图的构造，使用图中初始的边。但是，用户可以方便地切换到二维-分区或GraphX中其他启发式分区方法。 一旦边被划分，并行图计算的关键挑战在于有效的将每个顶点属性和边的属性连接起来。由于在现实世界中，边的数量多于顶点的数量，我们把顶点属性放在边中。因为不是所有的分区将包含所有顶点相邻的边的信息，我们在内部维护一个路由表，这个表确定在哪里广播顶点信息，执行 triplet 和 mapReduceTriplets 的连接操作。 图算法GraphX 包括一组图形算法来简化分析任务。该算法被包含于org.apache.spark.graphx.lib包中，并可直接通过 GraphOps而被Graph中的方法调用。本节介绍这些算法以及如何使用它们。 PageRankPageRank记录了图中每个顶点的重要性，假设一条边从u到v，代表从u传递给v的重要性。例如，如果一个Twitter用户有很多粉丝，用户排名将很高。 GraphX 自带的PageRank的静态和动态的实现，放在PageRank对象中。静态的PageRank运行的固定数量的迭代，而动态的PageRank运行，直到排名收敛（即当每个迭代和上一迭代的差值，在某个范围之内时停止迭代）。GraphOps允许Graph中的方法直接调用这些算法。 GraphX 还包括，我们可以将PageRank运行在社交网络数据集中。一组用户给出graphx/data/users.txt，以及一组用户之间的关系，给出了graphx/data/followers.txt。我们可以按照如下方法来计算每个用户的网页级别： 123456789101112131415// Load the edges as a graphval graph = GraphLoader.edgeListFile(sc, \"graphx/data/followers.txt\")// Run PageRankval ranks = graph.pageRank(0.0001).vertices// Join the ranks with the usernamesval users = sc.textFile(\"graphx/data/users.txt\").map &#123; line =&gt; val fields = line.split(\",\")( fields(0).toLong, fields(1) )&#125;val ranksByUsername = users.join(ranks).map &#123; case (id, (username, rank)) =&gt; (username, rank)&#125;// Print the resultprintln(ranksByUsername.collect().mkString(\"\\n\")) 联通分量连接分量算法标出了图中编号最低的顶点所联通的子集。例如，在社交网络中，连接分量类似集群。GraphX 包含在ConnectedComponents对象的算法，并且我们从该社交网络数据集中计算出连接组件的PageRank部分，如下所示： 123456789101112131415// Load the graph as in the PageRank exampleval graph = GraphLoader.edgeListFile(sc, \"graphx/data/followers.txt\")// Find the connected componentsval cc = graph.connectedComponents().vertices// Join the connected components with the usernamesval users = sc.textFile(\"graphx/data/users.txt\").map &#123; line =&gt; val fields = line.split(\",\")( fields(0).toLong, fields(1) )&#125;val ccByUsername = users.join(cc).map &#123; case (id, (username, cc)) =&gt; (username, cc)&#125;// Print the resultprintln(ccByUsername.collect().mkString(\"\\n\")) 三角计数当顶点周围与有一个其他两个顶点有连线时，这个顶点是三角形的一部分。GraphX在TriangleCount对象实现了一个三角形计数算法，这个算法计算通过各顶点的三角形数目，从而提供集群的度。我们从PageRank部分计算社交网络数据集的三角形数量。注意TriangleCount要求边是规范的指向（srcId &lt; dstId），并使用 Graph.partitionBy来分割图形。 123456789101112131415// Load the edges in canonical order and partition the graph for triangle countval graph = GraphLoader.edgeListFile(sc, \"graphx/data/followers.txt\", true).partitionBy( PartitionStrategy. RandomVertexCut)// Find the triangle count for each vertexval triCounts = graph.triangleCount().vertices// Join the triangle counts with the usernamesval users = sc.textFile(\"graphx/data/users.txt\").map &#123; line =&gt; val fields = line.split(\",\")( fields(0).toLong, fields(1) ) &#125;val triCountByUsername = users.join(triCounts).map &#123; case (id, (username, tc)) =&gt; (username, tc) &#125;// Print the resultprintln(triCountByUsername.collect().mkString(\"\\n\")) 示例 假设我想从一些文本文件中构建图，只考虑图中重要关系和用户，在子图中运行的页面排名算法，然后终于返回与顶级用户相关的属性。我们可以在短短的几行 GraphX 代码中实现这一功能： 1234567891011121314151617181920212223242526272829// Connect to the Spark clusterval sc = new SparkContext(\"spark://master.amplab.org\", \"research\")// Load my user data and parse into tuples of user id and attribute listval users = (sc.textFile(\"graphx/data/users.txt\").map(line =&gt; line.split(\",\")).map( parts =&gt; (parts.head.toLong,parts.tail) ) )// Parse the edge data which is already in userId -&gt; userId formatval followerGraph = GraphLoader.edgeListFile(sc,\"graphx/data/followers.txt\")// Attach the user attributesval graph = followerGraph.outerJoinVertices(users) &#123; case (uid, deg, Some(attrList)) =&gt; attrList // Some users may not have attributes so we set them as empty case (uid, deg, None) =&gt; Array.empty[String]&#125;// Restrict the graph to users with usernames and namesval subgraph = graph.subgraph(vpred = (vid, attr) =&gt; attr.size == 2)// Compute the PageRankval pagerankGraph = subgraph.pageRank(0.001)// Get the attributes of the top pagerank usersval userInfoWithPageRank = subgraph.outerJoinVertices(pagerankGraph.vertices) &#123; case (uid, attrList, Some(pr)) =&gt; (pr, attrList.toList) case (uid, attrList, None) =&gt; (0.0, attrList.toList) &#125;println(userInfoWithPageRank.vertices.top(5)( Ordering.by(_ _._2._1)).mkString(\"\\n\")) 翻译者：吴卓华整理自：《Spark GraphX 图计算和图挖掘》","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://www.cz5h.com/tags/Scala/"},{"name":"Spark","slug":"Spark","permalink":"https://www.cz5h.com/tags/Spark/"},{"name":"GraphX","slug":"GraphX","permalink":"https://www.cz5h.com/tags/GraphX/"}]},{"title":"如何使用IDEA加载已有Spark项目","slug":"2018-5-6 如何使用IDEA加载已有Spark项目","date":"2018-05-05T22:00:00.000Z","updated":"2020-02-29T18:43:55.525Z","comments":true,"path":"article/cc29.html","link":"","permalink":"https://www.cz5h.com/article/cc29.html","excerpt":"背景是这样的：手上有一个学长之前实现的Spark项目，使用到了GraphX，并且用的Scala编写，现在需要再次运行这个项目，但如果直接在IDEA中打开项目，则由于各种错误会导致运行失败，这里就记录一下该如何使用IDEA来加载老旧的Spark项目。注意：默认你的机器已有Scala环境，项目使用IDEA打开，对Sbt不做要求，因为这里采用的是NoSbt方式添加依赖的。","text":"背景是这样的：手上有一个学长之前实现的Spark项目，使用到了GraphX，并且用的Scala编写，现在需要再次运行这个项目，但如果直接在IDEA中打开项目，则由于各种错误会导致运行失败，这里就记录一下该如何使用IDEA来加载老旧的Spark项目。注意：默认你的机器已有Scala环境，项目使用IDEA打开，对Sbt不做要求，因为这里采用的是NoSbt方式添加依赖的。 确定项目的版本环境这一步是非常重要的，很多情况下就是由于版本的不匹配导致代码解析出现错误，主要的环境版本包括： Java Version 1.8 必须 scala-sdk-x.xx.x spark-assembly-x.x.x-hadoop.x.x.jar //注意这是在No-sbt模式下必须的，这个包很大，大概170M，导入后不用再添加其他依赖即可对Spark程序进行本地(Local)运行，其已包括GraphX模块。 Java的版本这里由于要是用Scala所以必须使用 Version 1.8+，关于如何修改版本这里不赘述。 Scala的版本这里可以通过右键项目名称，进入项目设置页面具体查看原项目使用的版本： 之后可以添加相应的Scala版本支持，比如假设这里需要 2.10.4 那么直接勾选即可，但是如果本机没有对应的版本，那么可以点击下方的New Liberay选择Scala SDK，进入如下页面： 在上述页面中你可以选择更多版本的Scala环境，如果还是没有你需要的版本，那么点击下方的Download按钮，可以进一步选择你需要的版本（涵盖所有版本），这是在线下载的操作，所以可能时间会非常慢，非常慢！ Spark-assembly的版本关于这个地方要特别注意版本的对应，老项目里有代码用到了 GraphX中 图的 mapReduceTriplets ，这应该在Spark-2.x.x以后被取消了，所以如果下次再在网上看到使用mapReduceTriplets的代码，复制到本地却无法识别时，不要慌张，那是他们使用了老版本的Spark-GraphX。在这里，原项目使用的是 spark-assembly-1.4.1-hadoop2.6.0.jar 但是这个jar包早就不在项目文件中了，然后在网上也没有搜到完全匹配的Jar包，但上文已说到，找个spark-1.x 版本的即可，所以在网上找了一个 spark-assembly-1.5.1-hadoop2.6.0.jar，同样在 上图 中的右侧点击加号后选择JARS or direct..添加到项目依赖中即可。 确定项目代码的运行环境在上一部分中对原项目的项目的所需依赖的版本进行了更正对应之后，可以发现原先满屏飘红的代码已经没有错误了，即这时IDEA已经具有了对于代码的完全的解析能力，这时我们写代码调方法都可以自动补全等等。 虽然代码无措，但是直接运行仍然是出不来结果的，因为原项目的代码有原来的运行环境，可能是集群环境或其他，另外，源代码的执行也有可能需要传入若干参数，贸然运行当然就不会得到预期结果。 这部分的修改要具体情况具体分析，但大致都有以下几步： 查看Main函数的传入参数，如果带参数的，要明确参数的具体意义，一个是参数类型，一个是参数意义。比如迭代次数，或是文件路径。如果偷懒，可以去掉传入参数，直接对相应变量赋值，这样就可以在IDE中直接运行调试了。 1234567891011&#x2F;&#x2F;诸如下面赋值内容要搞清楚具体意义if(args.length!&#x3D;5)&#123; printf(&quot;Please input right parameters &lt;vertex path&gt; &lt;edges path&gt; &lt;output path&gt; &lt;deep&gt; &lt;time&gt; &quot;) return &#125; &#x2F;&#x2F;可以直接对变量赋值，而不用输入参数args(x)val time&#x3D;args(4).toInt val vertexPath&#x3D;args(0) val edgesPath&#x3D;args(1) val outputPath&#x3D;args(2) val deep:Int&#x3D;args(3).toInt 集群运行还是本地运行，这里比较好改： 123val conf &#x3D; new SparkConf().setAppName(&quot;Proj_SIG&quot;) &#x2F;&#x2F;如果是简单调试，直接改为本地运行即可val conf &#x3D; new SparkConf().setAppName(&quot;Proj_SIG&quot;).setMaster(&quot;local&quot;) 可能需要的Hadoop支持如果出现错误：Failed to locate the winutils binary in the hadoop binary path 那么说明当前IDEA环境缺失hadoop的支持。 解决方案： 首先我们需要明白，hadoop只能运行在linux环境下，如果我们在windows下用idea开发spark的时候底层比方说文件系统这些方面调用hadoop的时候是没法调用的，这也就是为什么会提示这样的错误。当我们有这样的错误的时候，其实还是可以使用spark计算框架的，不过当我们使用saveAsTextFile的时候会提示错误，这是因为spark使用了hadoop上hdfs那一段的程序，而我们windows环境下没有hadoop，怎么办？第一步： 官网下载相应版本的hadoop。第二步：解压到你想要安装的任何路径，解压过程会提示出现错误，不去管他，这是因为linux文件不支持windows。第三步：设置环境变量，在系统变量中添加HADOOP_HOME，指向你解压的文件路径。然后再path中添加 %HADOOP_HOME%bin和%HADOOP_HOME%sbin第四步：找一找可以使用的重新编译的winutils兼容工具插件包，这个可以在这里下载：第五步：下载完以后在我们hadoop文件夹中替换下载包中的两个目录。 回到idea会发现bug完美解决。 上述几步修改完成后，原先的代码基本就可以跑起来了，再次强调这里使用了NoSBT的模式，手动添加了一个assembly包，再就是对应Scala-SDK的版本，最后对代码内容上进行部分改动，使其可以在本地单机进行调试运行。","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://www.cz5h.com/tags/Scala/"},{"name":"Spark","slug":"Spark","permalink":"https://www.cz5h.com/tags/Spark/"},{"name":"IDEA","slug":"IDEA","permalink":"https://www.cz5h.com/tags/IDEA/"}]},{"title":"Gephi-Toolkit的引入与使用","slug":"2018-5-3 Gephi-Toolkit的引入与使用","date":"2018-05-02T22:00:00.000Z","updated":"2020-02-29T18:43:55.526Z","comments":true,"path":"article/66d5.html","link":"","permalink":"https://www.cz5h.com/article/66d5.html","excerpt":"Gephi-Toolkit是一个工具包，可以不依赖NetBeans平台来对输入数据进行可视化，输入数据一般是gexf等格式的文件，大多已经完成了坐标计算过程，用此Toolkit的目的就是使用Gephi强大的绘图功能（还有独立的其他功能，这里暂不展开）。详细项目地址点击 这里 。","text":"Gephi-Toolkit是一个工具包，可以不依赖NetBeans平台来对输入数据进行可视化，输入数据一般是gexf等格式的文件，大多已经完成了坐标计算过程，用此Toolkit的目的就是使用Gephi强大的绘图功能（还有独立的其他功能，这里暂不展开）。详细项目地址点击 这里 。 在上述Github的项目地址中，有详细的代码demo和使用的部分示例数据。所以具体使用直接参见GitHub即可。 项目建立首先，这里使用 IntelliJ IDEA 来构建测试我们的项目，这里我们先新建一个 Maven 项目，然后在 pom.xml 中引入 Toolkit 的依赖： 123456&lt;!-- https://mvnrepository.com/artifact/org.gephi/gephi-toolkit --&gt;&lt;dependency&gt; &lt;groupId&gt;org.gephi&lt;/groupId&gt; &lt;artifactId&gt;gephi-toolkit&lt;/artifactId&gt; &lt;version&gt;0.9.1&lt;/version&gt;&lt;/dependency&gt; 然后，去GitHub中的项目里的 demo 中，选择自己所需要的部分，拿出来放到新建的项目里即可。注意：对于数据 LesMiserables.gexf 中的这句代码，如下，会显示一个 未注册 的错误，修改方法是找到 IDEA 的 File -&gt; Settings -&gt; Schemas and DTDs -&gt; 点击设置页下方的绿色加号，将下面代码中的两个网址添加进去，然后未注册错误就没有了！ 1&lt;gexf xmlns:viz=\"http:///www.gexf.net/1.1draft/viz\" version=\"1.1\" xmlns=\"http://www.gexf.net/1.1draft\"&gt; HeadlessSimple.java首先关注 HeadlessSimple 这一部分，关于什么是 Headless ? 参见Chrome的 Headless Mode 来理解：Headless Chrome指在headless模式下运行谷歌浏览器。本质就是不用谷歌运行谷歌！它将由Chromium和Blink渲染引擎提供的所有现代网页平台的特征都转化成了命令行它有什么用？Headless浏览器是一种很好的工具，用于自动化测试和不需要可视化用户界面的服务器。例如，你想在一个网页上运行一些测试，从网页创建一个PDF，或者只是检查浏览器怎样递交URL。更多关于Chrome的 headless 模式请参见 这里； 简单来说，就是通过一系列操作，在不借助GUI的条件下，完成原来Gephi客户端能完成的功能，下面代码展示了几个使用toolkit完成的操作。 HeadlessSimple 包括一个完整的从数据导入到结果输出的这样一个处理流程，具体为： 创建一个 project 和 workspace，这是必须的操作 使用 import container 导入polblog.gml这个图数据 将此 container 追加到主体图结构中 对图结构进行 Filter，这里使用 DegreeFilter Run layout manually. 计算图的矩阵 根据节点度值分配节点颜色 分配节点大小 配置预览显示不同的标签和边 将布局图导出为PDF 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150import org.gephi.appearance.api.AppearanceController;import org.gephi.appearance.api.AppearanceModel;import org.gephi.appearance.api.Function;import org.gephi.appearance.plugin.RankingElementColorTransformer;import org.gephi.appearance.plugin.RankingNodeSizeTransformer;import org.gephi.filters.api.FilterController;import org.gephi.filters.api.Query;import org.gephi.filters.api.Range;import org.gephi.filters.plugin.graph.DegreeRangeBuilder.DegreeRangeFilter;import org.gephi.graph.api.*;import org.gephi.io.exporter.api.ExportController;import org.gephi.io.importer.api.Container;import org.gephi.io.importer.api.EdgeDirectionDefault;import org.gephi.io.importer.api.ImportController;import org.gephi.io.processor.plugin.DefaultProcessor;import org.gephi.layout.plugin.force.StepDisplacement;import org.gephi.layout.plugin.force.yifanHu.YifanHuLayout;import org.gephi.preview.api.PreviewController;import org.gephi.preview.api.PreviewModel;import org.gephi.preview.api.PreviewProperty;import org.gephi.preview.types.EdgeColor;import org.gephi.project.api.ProjectController;import org.gephi.project.api.Workspace;import org.gephi.statistics.plugin.GraphDistance;import org.openide.util.Lookup;import java.awt.*;import java.io.File;import java.io.IOException; public class HeadlessSimple &#123; public void script() &#123; //A. 此部分是初始部分，必须的，初始Project、Workspace //Init a project - and therefore a workspace ProjectController pc = Lookup.getDefault().lookup(ProjectController.class); pc.newProject(); Workspace workspace = pc.getCurrentWorkspace(); //B. GraphModel是全局都需要的 //Get models and controllers for this new workspace - will be useful later GraphModel graphModel = Lookup.getDefault().lookup(GraphController.class).getGraphModel(); //C. 数据读入部分 ImportController importController = Lookup.getDefault().lookup(ImportController.class); //Import file Container container; try &#123; File file = new File(getClass().getResource(\"/org/gephi/toolkit/demos/polblogs.gml\").toURI()); container = importController.importFile(file); container.getLoader().setEdgeDefault(EdgeDirectionDefault.DIRECTED); //Force DIRECTED &#125; catch (Exception ex) &#123; ex.printStackTrace(); return; &#125; //Append imported data to GraphAPI importController.process(container, new DefaultProcessor(), workspace); //See if graph is well imported DirectedGraph graph = graphModel.getDirectedGraph(); System.out.println(\"Nodes: \" + graph.getNodeCount()); System.out.println(\"Edges: \" + graph.getEdgeCount()); //D. 过滤部分 FilterController filterController = Lookup.getDefault().lookup(FilterController.class); //Filter DegreeRangeFilter degreeFilter = new DegreeRangeFilter(); degreeFilter.init(graph); degreeFilter.setRange(new Range(3, Integer.MAX_VALUE)); //Remove nodes with degree &lt; 30 Query query = filterController.createQuery(degreeFilter); GraphView view = filterController.filter(query); graphModel.setVisibleView(view); //Set the filter result as the visible view //See visible graph stats UndirectedGraph graphVisible = graphModel.getUndirectedGraphVisible(); System.out.println(\"Nodes: \" + graphVisible.getNodeCount()); System.out.println(\"Edges: \" + graphVisible.getEdgeCount()); //Run YifanHuLayout for 100 passes - The layout always takes the current visible view YifanHuLayout layout = new YifanHuLayout(null, new StepDisplacement(1f)); layout.setGraphModel(graphModel); layout.resetPropertiesValues(); layout.setOptimalDistance(200f); layout.initAlgo(); for (int i = 0; i &lt; 100 &amp;&amp; layout.canAlgo(); i++) &#123; layout.goAlgo(); &#125; layout.endAlgo(); //Get Centrality GraphDistance distance = new GraphDistance(); distance.setDirected(true); distance.execute(graphModel); //E. 此部分Rank对节点大小和颜色进行设置 AppearanceController appearanceController = Lookup.getDefault().lookup(AppearanceController.class); AppearanceModel appearanceModel = appearanceController.getModel(); //Rank color by Degree Function degreeRanking = appearanceModel.getNodeFunction(graph, AppearanceModel.GraphFunction.NODE_DEGREE, RankingElementColorTransformer.class); RankingElementColorTransformer degreeTransformer = (RankingElementColorTransformer) degreeRanking.getTransformer(); degreeTransformer.setColors(new Color[]&#123;new Color(0xFEF0D9), new Color(0xB30000)&#125;); degreeTransformer.setColorPositions(new float[]&#123;0f, 1f&#125;); appearanceController.transform(degreeRanking); //Rank size by centrality Column centralityColumn = graphModel.getNodeTable().getColumn(GraphDistance.BETWEENNESS); Function centralityRanking = appearanceModel.getNodeFunction(graph, centralityColumn, RankingNodeSizeTransformer.class); RankingNodeSizeTransformer centralityTransformer = (RankingNodeSizeTransformer) centralityRanking.getTransformer(); centralityTransformer.setMinSize(3); centralityTransformer.setMaxSize(10); appearanceController.transform(centralityRanking); //F. 最后展现时图的模式：点、边的样式 PreviewModel model = Lookup.getDefault().lookup(PreviewController.class).getModel(); //Preview model.getProperties().putValue(PreviewProperty.SHOW_NODE_LABELS, Boolean.TRUE); model.getProperties().putValue(PreviewProperty.EDGE_COLOR, new EdgeColor(Color.GRAY)); model.getProperties().putValue(PreviewProperty.EDGE_THICKNESS, new Float(0.1f)); model.getProperties().putValue(PreviewProperty.NODE_LABEL_FONT, model.getProperties().getFontValue(PreviewProperty.NODE_LABEL_FONT).deriveFont(8)); //G. 输出部分 ExportController ec = Lookup.getDefault().lookup(ExportController.class); try &#123; ec.exportFile(new File(\"headless_simple.pdf\")); &#125; catch (IOException ex) &#123; ex.printStackTrace(); return; &#125; &#125; public static void main(String[] args)&#123; HeadlessSimple s = new HeadlessSimple(); s.script(); &#125;&#125; 注意上述代码中的A.B.C.D.E.F.G 部分，都是XxxxController的实例化，并且每一部分都是相对独立的。上述代码执行完成后，可以得到如下PDF的图像（注释掉Filter阶段让其保留全部布局）： PreviewJFrame.java注意在使用PreviewJFrame时，源代码中有个细节要注意，下图结构中的 plugin 文件夹下有需要使用的依赖代码，必须添加到项目中，否则会出错。 此部分的官方说明：这个文件夹包含一个基本预览插件的结构，它可以做一些简单的渲染，也可以对鼠标事件做出反应。只要有@ Service提供商注释，插件就可以在任何预览窗口中自动包含。可以运行 PreviewJFrame.java 来尝试这个插件。 PreviewJFrame 代码如下（出现PreviewSketch找不到的问题就是没引入plugins文件夹）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import org.gephi.io.importer.api.Container;import org.gephi.io.importer.api.ImportController;import org.gephi.io.processor.plugin.DefaultProcessor;import org.gephi.preview.api.*;import org.gephi.preview.types.DependantOriginalColor;import org.gephi.project.api.ProjectController;import org.gephi.project.api.Workspace;import org.openide.util.Lookup;import plugins.PreviewSketch;import javax.swing.*;import java.awt.*;import java.awt.event.ComponentAdapter;import java.awt.event.ComponentEvent;import java.io.File;/** * * @author Mathieu Bastian */public class PreviewJFrame &#123; public void script() &#123; //Init a project - and therefore a workspace ProjectController pc = Lookup.getDefault().lookup(ProjectController.class); pc.newProject(); Workspace workspace = pc.getCurrentWorkspace(); //Import file ImportController importController = Lookup.getDefault().lookup(ImportController.class); Container container; try &#123; File file = new File(getClass().getResource(\"/Java.gexf\").toURI()); container = importController.importFile(file); &#125; catch (Exception ex) &#123; ex.printStackTrace(); return; &#125; //Append imported data to GraphAPI importController.process(container, new DefaultProcessor(), workspace); //Preview configuration PreviewController previewController = Lookup.getDefault().lookup(PreviewController.class); PreviewModel previewModel = previewController.getModel(); previewModel.getProperties().putValue(PreviewProperty.SHOW_NODE_LABELS, Boolean.TRUE); previewModel.getProperties().putValue(PreviewProperty.NODE_LABEL_COLOR, new DependantOriginalColor(Color.WHITE)); previewModel.getProperties().putValue(PreviewProperty.EDGE_CURVED, Boolean.FALSE); previewModel.getProperties().putValue(PreviewProperty.EDGE_OPACITY, 50); previewModel.getProperties().putValue(PreviewProperty.BACKGROUND_COLOR, Color.BLACK); //New Processing target, get the PApplet G2DTarget target = (G2DTarget) previewController.getRenderTarget(RenderTarget.G2D_TARGET); final PreviewSketch previewSketch = new PreviewSketch(target); previewController.refreshPreview(); //Add the applet to a JFrame and display JFrame frame = new JFrame(\"Test Preview\"); frame.setLayout(new BorderLayout()); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); frame.add(previewSketch, BorderLayout.CENTER); frame.setSize(1024, 768); //Wait for the frame to be visible before painting, or the result drawing will be strange frame.addComponentListener(new ComponentAdapter() &#123; @Override public void componentShown(ComponentEvent e) &#123; previewSketch.resetZoom(); &#125; &#125;); frame.setVisible(true); &#125; public static void main(String[] args) &#123; PreviewJFrame previewJFrame = new PreviewJFrame(); previewJFrame.script(); &#125;&#125; 此时的布局结果是窗体性质的，且内容可缩放，可平移，结果如下： 其他，待续","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"Maven","slug":"Maven","permalink":"https://www.cz5h.com/tags/Maven/"},{"name":"GitHub","slug":"GitHub","permalink":"https://www.cz5h.com/tags/GitHub/"}]},{"title":"Spark图计算及GraphX简单入门","slug":"2018-4-28 Spark图计算及GraphX简单入门","date":"2018-04-27T22:00:00.000Z","updated":"2020-02-29T18:43:55.513Z","comments":true,"path":"article/a486.html","link":"","permalink":"https://www.cz5h.com/article/a486.html","excerpt":"GraphX介绍GraphX应用背景Spark GraphX是一个分布式图处理框架，它是基于Spark平台提供对图计算和图挖掘简洁易用的而丰富的接口，极大的方便了对分布式图处理的需求。","text":"GraphX介绍GraphX应用背景Spark GraphX是一个分布式图处理框架，它是基于Spark平台提供对图计算和图挖掘简洁易用的而丰富的接口，极大的方便了对分布式图处理的需求。 众所周知·，社交网络中人与人之间有很多关系链，例如Twitter、Facebook、微博和微信等，这些都是大数据产生的地方都需要图计算，现在的图处理基本都是分布式的图处理，而并非单机处理。Spark GraphX由于底层是基于Spark来处理的，所以天然就是一个分布式的图处理系统。 图的分布式或者并行处理其实是把图拆分成很多的子图，然后分别对这些子图进行计算，计算的时候可以分别迭代进行分阶段的计算，即对图进行并行计算。下面我们看一下图计算的简单示例： 从图中我们可以看出：拿到Wikipedia的文档以后，可以变成Link Table形式的视图，然后基于Link Table形式的视图可以分析成Hyperlinks超链接，最后我们可以使用PageRank去分析得出Top Communities。在下面路径中的Editor Graph到Community，这个过程可以称之为Triangle Computation，这是计算三角形的一个算法，基于此会发现一个社区。从上面的分析中我们可以发现图计算有很多的做法和算法，同时也发现图和表格可以做互相的转换。 GraphX的框架设计GraphX时，点分割和GAS都已成熟，在设计和编码中针对它们进行了优化，并在功能和性能之间寻找最佳的平衡点。如同Spark本身，每个子模块都有一个核心抽象。GraphX的核心抽象是Resilient Distributed Property Graph，一种点和边都带属性的有向多重图。它扩展了Spark RDD的抽象，有Table和Graph两种视图，而只需要一份物理存储。两种视图都有自己独有的操作符，从而获得了灵活操作和执行效率。 如同Spark，GraphX的代码非常简洁。GraphX的核心代码只有3千多行，而在此之上实现的Pregel模式，只要短短的20多行。GraphX的代码结构整体下图所示，其中大部分的实现，都是围绕Partition的优化进行的。这在某种程度上说明了点分割的存储和相应的计算优化，的确是图计算框架的重点和难点。 发展历程l早在0.5版本，Spark就带了一个小型的Bagel模块，提供了类似Pregel的功能。当然，这个版本还非常原始，性能和功能都比较弱，属于实验型产品。 l到0.8版本时，鉴于业界对分布式图计算的需求日益见涨，Spark开始独立一个分支Graphx-Branch，作为独立的图计算模块，借鉴GraphLab，开始设计开发GraphX。 l在0.9版本中，这个模块被正式集成到主干，虽然是Alpha版本，但已可以试用，小面包圈Bagel告别舞台。1.0版本，GraphX正式投入生产使用。 值得注意的是，GraphX目前依然处于快速发展中，从0.8的分支到0.9和1.0，每个版本代码都有不少的改进和重构。根据观察，在没有改任何代码逻辑和运行环境，只是升级版本、切换接口和重新编译的情况下，每个版本有10%~20%的性能提升。虽然和GraphLab的性能还有一定差距，但凭借Spark整体上的一体化流水线处理，社区热烈的活跃度及快速改进速度，GraphX具有强大的竞争力。 GraphX实现分析如同Spark本身，每个子模块都有一个核心抽象。GraphX的核心抽象是Resilient Distributed Property Graph，一种点和边都带属性的有向多重图。它扩展了Spark RDD的抽象，有Table和Graph两种视图，而只需要一份物理存储。两种视图都有自己独有的操作符，从而获得了灵活操作和执行效率。 GraphX的底层设计有以下几个关键点。 对Graph视图的所有操作，最终都会转换成其关联的Table视图的RDD操作来完成。这样对一个图的计算，最终在逻辑上，等价于一系列RDD的转换过程。因此，Graph最终具备了RDD的3个关键特性：Immutable、Distributed和Fault-Tolerant，其中最关键的是Immutable（不变性）。逻辑上，所有图的转换和操作都产生了一个新图；物理上，GraphX会有一定程度的不变顶点和边的复用优化，对用户透明。 两种视图底层共用的物理数据，由RDD[Vertex-Partition]和RDD[EdgePartition]这两个RDD组成。点和边实际都不是以表Collection[tuple]的形式存储的，而是由VertexPartition/EdgePartition在内部存储一个带索引结构的分片数据块，以加速不同视图下的遍历速度。不变的索引结构在RDD转换过程中是共用的，降低了计算和存储开销。 图的分布式存储采用点分割模式，而且使用partitionBy方法，由用户指定不同的划分策略（PartitionStrategy）。划分策略会将边分配到各个EdgePartition，顶点Master分配到各个VertexPartition，EdgePartition也会缓存本地边关联点的Ghost副本。划分策略的不同会影响到所需要缓存的Ghost副本数量，以及每个EdgePartition分配的边的均衡程度，需要根据图的结构特征选取最佳策略。目前有EdgePartition2d、EdgePartition1d、RandomVertexCut和CanonicalRandomVertexCut这四种策略。 存储模式图存储模式巨型图的存储总体上有边分割和点分割两种存储方式。2013年，GraphLab2.0将其存储方式由边分割变为点分割，在性能上取得重大提升，目前基本上被业界广泛接受并使用。 边分割（Edge-Cut）：每个顶点都存储一次，但有的边会被打断分到两台机器上。这样做的好处是节省存储空间；坏处是对图进行基于边的计算时，对于一条两个顶点被分到不同机器上的边来说，要跨机器通信传输数据，内网通信流量大。 点分割（Vertex-Cut）：每条边只存储一次，都只会出现在一台机器上。邻居多的点会被复制到多台机器上，增加了存储开销，同时会引发数据同步问题。好处是可以大幅减少内网通信量。 虽然两种方法互有利弊，但现在是点分割占上风，各种分布式图计算框架都将自己底层的存储形式变成了点分割。主要原因有以下两个。 磁盘价格下降，存储空间不再是问题，而内网的通信资源没有突破性进展，集群计算时内网带宽是宝贵的，时间比磁盘更珍贵。这点就类似于常见的空间换时间的策略。 在当前的应用场景中，绝大多数网络都是“无尺度网络”，遵循幂律分布，不同点的邻居数量相差非常悬殊。而边分割会使那些多邻居的点所相连的边大多数被分到不同的机器上，这样的数据分布会使得内网带宽更加捉襟见肘，于是边分割存储方式被渐渐抛弃了。 GraphX存储模式Graphx借鉴PowerGraph，使用的是Vertex-Cut(点分割)方式存储图，用三个RDD存储图数据信息： lVertexTable(id, data)：id为Vertex id，data为Edge data lEdgeTable(pid, src, dst, data)：pid为Partion id，src为原定点id，dst为目的顶点id lRoutingTable(id, pid)：id为Vertex id，pid为Partion id 点分割存储实现如下图所示： 计算模式图计算模式目前基于图的并行计算框架已经有很多，比如来自Google的Pregel、来自Apache开源的图计算框架Giraph/HAMA以及最为著名的GraphLab，其中Pregel、HAMA和Giraph都是非常类似的，都是基于BSP（Bulk Synchronous Parallell）模式。 Bulk Synchronous Parallell，即整体同步并行，它将计算分成一系列的超步（superstep）的迭代（iteration）。从纵向上看，它是一个串行模式，而从横向上看，它是一个并行的模式，每两个superstep之间设置一个栅栏（barrier），即整体同步点，确定所有并行的计算都完成后再启动下一轮superstep。 每一个超步（superstep）包含三部分内容： 计算compute：每一个processor利用上一个superstep传过来的消息和本地的数据进行本地计算； 消息传递：每一个processor计算完毕后，将消息传递个与之关联的其它processors 整体同步点：用于整体同步，确定所有的计算和消息传递都进行完毕后，进入下一个superstep。 GraphX计算模式如同Spark一样，GraphX的Graph类提供了丰富的图运算符，大致结构如下图所示。可以在官方GraphX Programming Guide中找到每个函数的详细说明，本文仅讲述几个需要注意的方法。 图的缓存每个图是由3个RDD组成，所以会占用更多的内存。相应图的cache、unpersist和checkpoint，更需要注意使用技巧。出于最大限度复用边的理念，GraphX的默认接口只提供了unpersistVertices方法。如果要释放边，调用g.edges.unpersist()方法才行，这给用户带来了一定的不便，但为GraphX的优化提供了便利和空间。参考GraphX的Pregel代码，对一个大图，目前最佳的实践是： 大体之意是根据GraphX中Graph的不变性，对g做操作并赋回给g之后，g已不是原来的g了，而且会在下一轮迭代使用，所以必须cache。另外，必须先用prevG保留住对原来图的引用，并在新图产生后，快速将旧图彻底释放掉。否则，十几轮迭代后，会有内存泄漏问题，很快耗光作业缓存空间。 邻边聚合mrTriplets（mapReduceTriplets）是GraphX中最核心的一个接口。Pregel也基于它而来，所以对它的优化能很大程度上影响整个GraphX的性能。mrTriplets运算符的简化定义是： 它的计算过程为：map，应用于每一个Triplet上，生成一个或者多个消息，消息以Triplet关联的两个顶点中的任意一个或两个为目标顶点；reduce，应用于每一个Vertex上，将发送给每一个顶点的消息合并起来。 mrTriplets最后返回的是一个VertexRDD[A]，包含每一个顶点聚合之后的消息（类型为A），没有接收到消息的顶点不会包含在返回的VertexRDD中。 在最近的版本中，GraphX针对它进行了一些优化，对于Pregel以及所有上层算法工具包的性能都有重大影响。主要包括以下几点。 Caching for Iterative mrTriplets &amp; Incremental Updates for Iterative mrTriplets：在很多图分析算法中，不同点的收敛速度变化很大。在迭代后期，只有很少的点会有更新。因此，对于没有更新的点，下一次mrTriplets计算时EdgeRDD无需更新相应点值的本地缓存，大幅降低了通信开销。 Indexing Active Edges：没有更新的顶点在下一轮迭代时不需要向邻居重新发送消息。因此，mrTriplets遍历边时，如果一条边的邻居点值在上一轮迭代时没有更新，则直接跳过，避免了大量无用的计算和通信。 Join Elimination：Triplet是由一条边和其两个邻居点组成的三元组，操作Triplet的map函数常常只需访问其两个邻居点值中的一个。例如，在PageRank计算中，一个点值的更新只与其源顶点的值有关，而与其所指向的目的顶点的值无关。那么在mrTriplets计算中，就不需要VertexRDD和EdgeRDD的3-way join，而只需要2-way join。 所有这些优化使GraphX的性能逐渐逼近GraphLab。虽然还有一定差距，但一体化的流水线服务和丰富的编程接口，可以弥补性能的微小差距。 进化的Pregel模式GraphX中的Pregel接口，并不严格遵循Pregel模式，它是一个参考GAS改进的Pregel模式。定义如下： 这种基于mrTrilets方法的Pregel模式，与标准Pregel的最大区别是，它的第2段参数体接收的是3个函数参数，而不接收messageList。它不会在单个顶点上进行消息遍历，而是将顶点的多个Ghost副本收到的消息聚合后，发送给Master副本，再使用vprog函数来更新点值。消息的接收和发送都被自动并行化处理，无需担心超级节点的问题。 常见的代码模板如下所示： 可以看到，GraphX设计这个模式的用意。它综合了Pregel和GAS两者的优点，即接口相对简单，又保证性能，可以应对点分割的图存储模式，胜任符合幂律分布的自然图的大型计算。另外，值得注意的是，官方的Pregel版本是最简单的一个版本。对于复杂的业务场景，根据这个版本扩展一个定制的Pregel是很常见的做法。 图算法工具包GraphX也提供了一套图算法工具包，方便用户对图进行分析。目前最新版本已支持PageRank、数三角形、最大连通图和最短路径等6种经典的图算法。这些算法的代码实现，目的和重点在于通用性。如果要获得最佳性能，可以参考其实现进行修改和扩展满足业务需求。另外，研读这些代码，也是理解GraphX编程最佳实践的好方法。 作者：石山园出处：http://www.cnblogs.com/shishanyuan/","categories":[{"name":"Spark学习笔记","slug":"Spark学习笔记","permalink":"https://www.cz5h.com/categories/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://www.cz5h.com/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"GraphX","slug":"GraphX","permalink":"https://www.cz5h.com/tags/GraphX/"},{"name":"Demo","slug":"Demo","permalink":"https://www.cz5h.com/tags/Demo/"}]},{"title":"两个水壶相互倒水—水壶问题","slug":"2018-4-27 两个水壶相互倒水—水壶问题","date":"2018-04-26T22:00:00.000Z","updated":"2020-02-29T18:43:55.511Z","comments":true,"path":"article/113e.html","link":"","permalink":"https://www.cz5h.com/article/113e.html","excerpt":"","text":"点击此处快速跳到程序部分 水壶问题有两个容量分别为 x升 和 y升 的水壶以及无限多的水。请判断能否通过使用这两个水壶，从而可以得到恰好 z升 的水？如果可以，最后请用以上水壶中的一或两个来盛放取得的 z升 水。 你允许：装满任意一个水壶清空任意一个水壶从一个水壶向另外一个水壶倒水，直到装满或者倒空 示例1: (From the famous &quot;Die Hard&quot; example) 输入: x = 3, y = 5, z = 4 输出: True 示例2: 输入: x = 2, y = 6, z = 5 输出: False第一印象乍一看这道题目和题目的示例，以为直接用条件判断就能搞定，于是立刻写了如下非常错误的代码： 123456789public boolean canMeasureWater(int x, int y, int z) &#123; if( (0.5*x + 0.5*y == z)||(x + y == z)|| (0.5*x + y == z)||(x + 0.5*y == z)|| (x + x == z)||(y + y == z) )&#123; return true; &#125;else&#123; return false; &#125; &#125; 当然执行的结果是错误的，这里主要烦了两个重要的错误： 杯子只能倒空或倒满，不能倒一半（像脑筋急转弯那样） 两个杯子，当然可以让水从一个倒入另一个 从上述一开始遇到的错误，可以引出此题的关键： 需要不断将水倒来倒去，从而利用容量差得到更多样的容量的水 为了减少实现复杂程度，输入的x，y应该按指定顺序，如前小后大 对特定情况，能提前排除就提前排除，比如（0,1,2） 进入正题首先，是问题的抽象，该问题可以转化为问一个三元组r(x,y,z)，按一定规则对x，y进行运算，规则就是： x，y只能被赋值不比自身大的数， x，y运算过程中，只要有一个状态，或x，或y，或x+y，等于z，则问题得解 问题无解的条件？ 然后，由特殊推普通，实际计算几个例子，看看有什么规律，杯子x，y每次的状态有： 注意：下面的每一步都是注定的，即‘只有这样操作才对目标的获得有意义’， 目标是什么？目标就是每次操作都要‘利用容量差产生更多样的容量的水’。12345678910111213141516171819x=4 y=54 5 //初始状态0 5 //小的倒空，大的倒满【问题I】4 5-(4-0)=1 //大的将小的倒满，大的会剩余，这里剩余 10 1 //小的倒空1 0 //大的中的剩余倒入小的1 5 //大的倒满4 5-(4-1)=2 //大的继续将小的倒满，大的还会剩余，这里剩余 20 2 //往复上述操作2 02 54 5-(4-2)=30 33 03 54 5-(4-3)=4 //x==y，后续会再重新回到开始状态，故这是终止状态【问题II】0 44 04 5 问题I，为什么小的倒空大的倒满？ 因为如果相反，小的满，大的空，水只能从小杯子倒入大杯子， 且小杯子无剩余，故没有利用到容量差，所以这种顺序的操作时无意义的。 问题II，如果x和y不相等，且y&gt;x即剩余的水多于小杯子的容量怎么办？ 答：此时归结为【第二种情况】后续会讲到，所以当 y&lt;=x 时可当做【第一种情况】 初步的编码实现由上述思路作指导，很自然的想到了使用递归这种方式，于是有了以下代码： 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; public int zx; public int sm; public int lg; public boolean MK = false; public boolean canMeasureWater(int x, int y, int z) &#123; if(z == x||z == y||z==x+y||z==0) &#123; return true; &#125; if(z &gt; x &amp;&amp; z&gt; y &amp;&amp; z&gt;(x+y))&#123; return false; &#125; zx = z; sm = x; lg = y; if(x&gt;y)&#123; int tp = sm;sm = lg;lg = tp; &#125;else&#123; recursion(sm,lg-(sm-0)); &#125; return MK; &#125; public void recursion(int x, int y)&#123; if(zx == x+y || zx == x || zx==y) MK = true; if(x &lt;= y||y&gt;zx)&#123; if(!MK) MK = false; return; &#125; x = 0; if(zx == x+y || zx == x || zx==y) MK = true; int t = x;x = y;y = t; x = x; y = lg; if(zx == x+y || zx == x || zx==y) MK = true; recursion(sm,lg-(sm-x)); &#125;&#125; 补全思路得到新的实现上述代码不出意外的还是没过，解答出错，为什么呢，原来是原先的思路有问题，思考不全面，如下： 1234567891011x=4 y=74 70 74 7-(4-0)=30 33 03 74 7-(4-3)=6 //上述步骤同先前同理0 6 //但此时出现 大的里的剩余 比小的容量还大【问题I】? ? ... 问题I 如果大杯子剩余的比小杯子容量大，怎么处理 答：此时应不同于一般情况，一般情况是大的里的剩余是比小的容量小 &apos;所以剩余水可以倒入小的，但此时无法将大杯子的剩余水倒入小杯子， 因为会溢出，那么应该如何操作呢，本着利用‘容量差’的原则，所以： 需要将小杯子用大杯子的剩余水倒满，此时大杯子的水仍有剩余， 如果此时剩余的水还比小杯子的大，那么先倒空小杯子， 然后继续用剩余的水装满小杯子，直到剩余的水 小于小杯子容量， 此时停止，因为此时剩余的水可以倒入小杯子了，所以又回到了第一种情况。下面，我们按刚才说的思路将先前的例子列完： 1234567891011121314151617181920x=4 y=74 70 74 7-(4-0)=30 33 03 74 7-(4-3)=6 //此时，按刚才的思路【第二种情况】继续0 6 //小杯子倒空4 6-4=2 //用大杯子剩余的水倒满小杯子，大杯子还剩余 20 2 //此时回归【第一种情况】2 02 74 7-(4-2)=5 //此时，又回到【第二种情况】0 54 5-4=1 //此时又回归【第一种情况】0 11 01 74 7-(4-1)=4 //此时xy相等，如果继续，则状态回归初始，故此时终止 上述例子即完整的展现了正确的迭代步骤，即需要分为【两种情况】，所以在先前递归形式的基础上，对每次的迭代中的 x y的大小分情况进入不同的迭代入口即可。然而这一新的实现仍然出错误了：Stack Overflow！ 如何避免递归的栈溢出对于溢出时的测试用例：22003,31237,137，在我本机跑时递归了九千次左右就溢出停止了，但对于一般的小的测试用例，答案已经都是正确的了，所以此时的思路应是正确的，只是实现形式有问题，需要优化！ 重新对上一章节中说到的例子进行考究，发现真正有用的（跟Z比较判断的）状态其实只有如下： 1234567x=4 y=74 7-(4-0)=3 //【情况一】4 7-(4-3)=6 4 6-4=2 //【情况二】 4 7-(4-2)=5 4 5-4=1 4 7-(4-1)=4 优化情况二：按先前的思路可知，情况二其实不需要迭代操作，其就是在大杯子剩余水多于小杯子容量时，就拿小杯子每次都去装大杯子的剩余水，直到大杯子中剩余的水小于小杯子的容量，抽象为数学运算： 取余 y % x == c 实现上，由于需要中间的每个状态（和z比较），所以可以用while循环来完成，优化后的代码：【情况一】：大水杯里的剩水不断的增加，直到增加到剩水大于小水杯容量；【情况二】：大水杯剩水不断的减少，直到剩水小于小水杯的容量；再次明确：情况一时小水杯的容量 &gt;= 大水杯的剩水； 情况二时小水杯的容量 &lt; 大水杯的剩水；注意：对于每次迭代，都对两种情况分别对待，然后进行下一次迭代，此时迭代是靠递归驱动的。 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123; public static int zx; public static int sm; public static int lgt; public static boolean MK = false; public static boolean canMeasureWater(int x, int y, int z) &#123; sm = x; lgt = y; if(x &gt; y)&#123; sm = y; lgt = x; &#125; zx = z; recursion(lgt); return MK; &#125; //static int cnt = 0; public static void recursion(int rs) &#123; if(sm + rs == zx|| sm + lgt == zx|| sm + rs == zx|| sm == zx|| rs == zx|| zx == 0)&#123; MK = true; return; &#125; if(rs &lt;= 0)&#123; if(!MK) MK = false; return; &#125; if(sm &lt;= rs)&#123; int tmp = rs; for(int i = 1; tmp &gt;= sm;i ++)&#123; tmp = tmp - sm; &#125; recursion(tmp); //情况一 &#125;else&#123; int tmp = rs; for(int i = 1; tmp &lt; sm;i ++)&#123; tmp = lgt - (sm - tmp); &#125; recursion(tmp); //情况一 &#125; &#125;&#125; 终极优化，递归的转化上述代码仍然存在栈溢出的错误，所以还是递归的锅，显然，不是题目的测试样例刁钻，而是有些情况就是需要迭代几万次，即用递归是错误的实现方式。故转念一想，如何将递归转化为循环？ 1234567x=4 y=74 7-(4-0)=3 //【情况一】4 7-(4-3)=6 4 6-4=2 //【情况二】 4 7-(4-2)=5 4 5-4=1 4 7-(4-1)=4 在此回顾上述的内容，发现其实代码中的递归已经没有复杂的处理逻辑，且退化成了驱动迭代的一种结构，所以显然可以改成循环。由于原递归可以连续执行，所以转为循环理所应当的 最外层是 while(true)来制造连续迭代，然后循环的退出可以用return 或者break都可以，对于两种情况的处理还是要分别采用不同的实现，综上，去掉递归改用循环的代码： 123456789101112131415161718192021222324252627282930313233public static boolean canMeasureWater(int x, int y, int z) &#123; if(z == 0) return true; if((x == 0 || y == 0)) return (x + y == z); if(x &gt; y) int t = x; x = y; y = t; int rs = y; while(true)&#123; if(x + rs == z|| x + y == z|| x + rs == z|| x == z|| rs == z|| z == 0) return true; if(rs &lt;= 0) return false; int tmp = rs; if(x &lt;= rs)&#123; while(tmp &gt;= x)&#123; tmp = tmp - x; //System.out.println((++cnt)+\"\\tb\\tr(\"+x+\",\"+tmp+\")\"+\"\\t\"+z); if(x + tmp == z|| x + y == z|| x + tmp == z|| x == z|| tmp == z|| z == 0)&#123; return true; &#125; &#125; rs = tmp; &#125;else&#123; while(tmp &lt; x)&#123; tmp = y - (x - tmp); if(x + tmp == z|| x + y == z|| x + tmp == z|| x == z|| tmp == z|| z == 0)&#123; return true; &#125; &#125; rs = tmp; &#125; &#125;&#125; 精益求精 上述代码终于是得到了绿绿的通过，然而其执行速度却是最慢的，查看代码其实发现在判断停止的条件的语句上过于冗余了，然后有些变量还做了无畏的赋值，于是继续精简，最后得到如下代码： 123456789101112131415161718192021222324252627public static boolean canMeasureWater(int x, int y, int z) &#123; if(z == 0|| x + y == z) return true; if((x == 0 || y == 0)) return (x + y == z); if(x &gt; y)&#123; int t = x; x = y; y = t; &#125; int rs = y; while(true)&#123; if(rs &lt;= 0) return false; if(x &lt;= rs)&#123; while(rs &gt;= x)&#123; rs = rs - x; if( x + rs == z|| rs == z) return true; &#125; &#125;else&#123; while(rs &lt; x)&#123; rs = y - (x - rs); if( x + rs == z|| rs == z) return true; &#125; &#125; &#125;&#125; 上述代码的执行速度得到了不少的提升，由原先的21ms提升到了 8ms，如下图所示，现在下图提交的所有代码中，我的代码还成了第二个和第五个柱状图即8ms和21ms的样例程序，想想还有点小激动。 附第一梯队代码当然，对于第一梯队的代码，使用到了 gcd() 函数对最大公约数进行求解，技巧性比较强，速度当然也快。相比之下，我这里其实相当于实现了一下gcd函数。但一般的小白（比如我）是很难将这个问题抽象成求解最大公约数的，所以我的思路其实更笨一点，但也更符合思考的逻辑。下面是第一梯队的样例代码和解读。 12345678910111213class Solution &#123; public boolean canMeasureWater(int x, int y, int z) &#123; if (z==x+y || z==x || z==y || z==0) return true; if (x==0 || y==0 || z&gt;x+y || z%gcd(x, y)!= 0) return false; return true; &#125; private int gcd(int a, int b)&#123; return b == 0? a : gcd(b, a%b); &#125;&#125; 此种题解的解题思路，转自网络 这道问题其实可以转换为有一个很大的容器，我们有两个杯子，容量分别为x和y，问我们通过用两个杯子往里倒水，和往出舀水，问能不能使容器中的水刚好为z升。那么我们可以用一个公式来表达：z = m * x + n * y其中m，n为舀水和倒水的次数，正数表示往里舀水，负数表示往外倒水，那么题目中的例子可以写成: 4 = (-2) * 3 + 2 * 5，即3升的水罐往外倒了两次水，5升水罐往里舀了两次水。那么问题就变成了对于任意给定的x,y,z，存不存在m和n使得上面的等式成立。根据裴蜀定理，ax + by = d的解为 d = gcd(x, y)，那么我们只要只要z % d == 0，上面的等式就有解，所以问题就迎刃而解了，我们只要看z是不是x和y的最大公约数的倍数就行了，别忘了还有个限制条件x + y &gt;= z，因为x和y不可能称出比它们之和还多的水。 由于这道题前前后后想了两天多，且期间不断地修正自己的想法，修改对应的实现，最后AC。虽然只是一道普通的题目，但以前还真没真真正正的不借助参考的独立思考过这样难度的题目，所以就在这里啰嗦了一大堆，看来以后还是得多独立思考，正应那句话：“不是不会做，而是不去想” 啊！","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://www.cz5h.com/tags/Java/"},{"name":"recursion","slug":"recursion","permalink":"https://www.cz5h.com/tags/recursion/"},{"name":"stackoverflow","slug":"stackoverflow","permalink":"https://www.cz5h.com/tags/stackoverflow/"}]},{"title":"在站点中添加Valine评论系统并修改评论样式","slug":"2018-4-20 在站点中添加Valine评论系统并修改评论样式","date":"2018-04-19T22:00:00.000Z","updated":"2020-02-29T18:43:55.512Z","comments":true,"path":"article/c4f9.html","link":"","permalink":"https://www.cz5h.com/article/c4f9.html","excerpt":"最近在浏览indigo主题原作者的网站时，发现其评论系统已经更新为了Valine，看这个评论插件的第一眼就觉着一股浓浓的极简风，而且，十分重要的是，其无后端的设定！或许有些地方跟原先的友言或者多说的插件相似，但由于历史原因今天都无法使用了，现在比较不错的评论系统有disqus还有基于Github Issues的gitalk以及gitment，disqus需要翻墙速度才可以接受，然后一开始我是使用的基于Github的gitment，但是用户必须登录Github账号后才可以留言，所以有一丝丝的局限。","text":"最近在浏览indigo主题原作者的网站时，发现其评论系统已经更新为了Valine，看这个评论插件的第一眼就觉着一股浓浓的极简风，而且，十分重要的是，其无后端的设定！或许有些地方跟原先的友言或者多说的插件相似，但由于历史原因今天都无法使用了，现在比较不错的评论系统有disqus还有基于Github Issues的gitalk以及gitment，disqus需要翻墙速度才可以接受，然后一开始我是使用的基于Github的gitment，但是用户必须登录Github账号后才可以留言，所以有一丝丝的局限。 出于其非常吸引人的无需登陆和评论易管理的特性，当然需要尝试一波了。 Valine 的特点：无后端实现高速，使用国内后端云服务提供商 LeanCloud 提供的存储服务开源，自定义程度高支持邮件通知支持验证码支持 Markdown 获取Valine需要引入的AppId和AppKeyValine的存储是基于LeanCloud的，所以首先需要注册一个LeanCloud账号，然后登陆创建一个应用： 点击设置按钮，进入应用的设置页面 找到应用的APPID和APPKEY，复制备用 整合Valine到Hexo的主题中，此处为indigoValine对Hexo的支持可以再此页面中查看，对于Hexo一般大同小异，由于之前添加过gitment，所以官方扩展方式都没看，下面是我自己的添加方式，应该和官方的差不多。 在indigo\\layout_partial\\plugins目录下添加valine.ejs 1234567891011121314151617181920212223242526272829&lt;% if (theme.valine )&#123; %&gt; &lt;!-- Valine Comments --&gt; &lt;div class=\"comments vcomment\" id=\"comments\"&gt;&lt;/div&gt; &lt;script src=\"//cdn1.lncld.net/static/js/3.0.4/av-min.js\"&gt;&lt;/script&gt; &lt;script src=\"//unpkg.com/valine@latest/dist/Valine.min.js\"&gt;&lt;/script&gt; &lt;!-- Valine Comments script --&gt; &lt;script&gt; var GUEST_INFO = ['nick','mail','link']; var guest_info = '&lt;%= theme.valine.guest_info %&gt;'.split(',').filter(function(item)&#123; return GUEST_INFO.indexOf(item) &gt; -1 &#125;); new Valine(&#123; el: '#comments', notify: '&lt;%= theme.valine.notify %&gt;' == 'true', verify: '&lt;%= theme.valine.verify %&gt;' == 'true', appId: \"&lt;%= theme.valine.appId %&gt;\", appKey: \"&lt;%= theme.valine.appKey %&gt;\", avatar: \"&lt;%= theme.valine.avatar %&gt;\", placeholder: \"&lt;%= theme.valine.placeholder %&gt;\", guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info, pageSize: \"&lt;%= theme.valine.pageSize %&gt;\" &#125;) console.log(\"Valine done!\") &lt;/script&gt; &lt;!-- Valine Comments end --&gt;&lt;% &#125; %&gt;注意：上述的 new Valine&#123;&#125; 中的变量参见 https://valine.js.org/configuration/ 修改indigo\\layout_partial\\post\\comment.ejs，添加上述文件 12345&lt;% if(post.comments)&#123; %&gt; &lt;%- partial(&#39;..&#x2F;plugins&#x2F;valine&#39;) %&gt; &lt;%&#125; %&gt; 在indigo_config.yaml中添加变量的赋值 1234567891011121314注意：valine.ejs中的变量部分，theme.xxx凡是使用theme.xxx的变量都需要在_config.yaml中被赋值，所以，按照我这里第一步中valine.ejs的写法，在_config中的初始配置代码应该如下：valine: notify: true verify: true appId: xxxxxxxxxxxxxxxxx 第一步中得到的appId appKey: xxxxxxxxxxxxxxx 第一步中得到的appKey avatar: monsterid placeholder: 请在这里书写你想说的话~ guest_info: nick,mail,link pageSize: 10 注意：上述代码段有严格的格式要求，冒号后边有个空格，还有第二级变量需要一个2字符的缩进 修改valine评论空间的风格样式 上述是默认的主题样式，比较白开水风格，个人感觉太单调，而且背景是透明的，如果你的站点有背景图案，可能或导致视觉混乱，所以在这里尝试查找修改风格的方法。 头像部分 官网的说明：https://valine.js.org/avatar/，有多种样式，作用于变量 avatar 邮件提醒 这部分不过多陈述，请移步官方介绍：https://valine.js.org/notify/ 添加底层容器块，保持评论与全站的风格一致 12345678主要是修改valine.ejs中的代码，在初始化valine插件之前添加下面的&lt;section&gt;内容&lt;% if (theme.valine )&#123; %&gt;&lt;section class=\"comments\" id=\"comments\" style=\"margin-top:40px;padding:1px 15px 1px 10px;background-color:rgba(255,255,255,0.7);box-shadow: 0px 0px 20px #bbbbbb;border-radius: 5px;\"&gt;... 原来的内容 ...&lt;/section&gt; 最终的效果 管理已有的评论","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Valine","slug":"Valine","permalink":"https://www.cz5h.com/tags/Valine/"},{"name":"第三方评论插件","slug":"第三方评论插件","permalink":"https://www.cz5h.com/tags/%E7%AC%AC%E4%B8%89%E6%96%B9%E8%AF%84%E8%AE%BA%E6%8F%92%E4%BB%B6/"}]},{"title":"OpenOrd-面向大规模图布局的开源算法-研读","slug":"2018-4-18 OpenOrd-面向大规模图布局的开源算法-研读","date":"2018-04-17T22:00:00.000Z","updated":"2020-02-29T18:43:55.510Z","comments":true,"path":"article/3cd5.html","link":"","permalink":"https://www.cz5h.com/article/3cd5.html","excerpt":"原文名：OpenOrd: An Open-Source Toolbox for Large Graph Layout 中译名：OpenOrd-面向大规模图布局的开源算法 刊载源：出版源 Visualization &amp; Data Analysis , 2011 , 7868 (3) :- 作者们：Shawn Martin a , W. Michael Brown b 机构名： - Sandia National Laboratories [a] 桑迪亚国家实验室，US - Oak Ridge National Laboratories [b] 橡树岭国家实验室，US","text":"原文名：OpenOrd: An Open-Source Toolbox for Large Graph Layout 中译名：OpenOrd-面向大规模图布局的开源算法 刊载源：出版源 Visualization &amp; Data Analysis , 2011 , 7868 (3) :- 作者们：Shawn Martin a , W. Michael Brown b 机构名： - Sandia National Laboratories [a] 桑迪亚国家实验室，US - Oak Ridge National Laboratories [b] 橡树岭国家实验室，US ABSTRACT 我们创作了一个用于绘制大型无向图的开源工具箱。这个工具箱是基于一个以前实现的闭源算法，即VxOrd。我们的工具箱，我们称之为OpenOrd，通过合并切割incorporating edge-cutting、多级方法multi-level approach、平均链接聚类average-link clustering和并行实现parallel implementation，将VxOrd的功能扩展到大型图形布局。在每个层次上，顶点都使用力导向布局和平均链接聚类来分组。分组的顶点会被重新绘制，上述过程不断重复。When a suitable drawing of the coarsened graph is obtained, the algorithm is reversed to obtain a drawing of the original graph.在得到粗化图coarsened graph的一幅合适的图时，该算法得到了相反的结果，得到了原始图的图像。这种方法导致了包含本地和全局结构的大图形的布局。本文给出了该算法的详细描述。给出了使用超过600 K个节点的数据集的例子。代码可在www.cs.sandia.gov/smartin上获得。 关键词：多级Multilevel、力导向、并行、大规模的图形布局1。 INTRODUCTION用于可视化关系数据的图绘制，通常在二维空间中1,2。图绘制的一些应用场景包括：社会网络分析3、科学文献分析4,5、地图制作6和生物信息学7,8。有各种各样的算法可供图绘制，每一种算法都具有不同优化的美学标准aesthetic criteria。一些美学标准的例子包括：尽量减少边的数量，最小化总边长度，并最大化顶点之间的分离。对于用直线straight line边缘绘制的无向图，最常用的算法之一是力导向布局9-13。 在本文中，我们记录了一种用于绘制大型真实世界real-world图的图绘制算法。我们的算法采用了边切edge-cutting、平均链接average-link聚类、多级图粗化multilevel graph coarsening，以及基于模拟退火的力-导向方法的并行实现。已存在的力-导向布局的相关算法，包括采用多级方法的算法13-16;节点聚类的算法16-18;使用并行GPU架构实现的算法19,20。然而，我们的算法是唯一一个包含所有这三个想法的算法：多级的、节点聚类的以及并行的实现（注意，我们的并行性是基于聚类的，而不是基于GPU的）。is cluster based, instead of GPU based此外，我们还引入了一种用于边缘切割的启发式heuristic方法，它的设计目的是为了使图结构可视化，而这些图结构可能没有理想的度分布（通常在真实数据的图中找到）。 我们的算法是基于一种叫做VxOrd的力-导向算法。此新版本，OpenOrd，将在接下来的几页中进行描述。在第2部分中，我们给出了修改VxOrd的动机。在第3部分中，我们描述了算法的各个部分，包括力-导向的布局;并行布局(3.2);递归地recursive图粗化(3.3);平均链接聚类and average-link clustering(3.4)。在第4部分中，我们演示了我们的算法的一些属性，使用应用程序到applications to几个真实的数据集，包括一个659 K顶点Wikipedia文章的数据集。最后，在第5部分中，我们提供了我们的结论。OpenOrd的代码可在http://www.cs.sandia.gov/smartin中获得。 MOTIVATION 动机OpenOrd是一种力-导向的布局算法，专门用于处理非常大的图结构，如sci-科学文献引用分析4,5中遇到的。它是基于一个被称为VxOrd22的实现了“Frutcherman-Reingold思想”11的算法，该算法在科学引文分析中被使用23-25，并在生物信息数据的分析中被使用26-28。OpenOrd算法展示了我们将VxOrd扩展到非常大的图结构（大于100 K个顶点）的努力。 图1.全局纠缠Tangled global的结构。左边（a）显示的是瑞士卷Swiss roll数据集，由瑞士卷副本manifold随机抽取的2万个点组成19,30。中间（b）显示的是一个由使用VxOrd的20个最近的邻居 这种算法从瑞士卷中获得的力-导向布局。只有节点被绘制了出来，图被着色以说明全局结构的纠缠tangling。右边（c）中显示了正确绘制的图结构，使用OpenOrd的多级版本生成。 我们已经确定了将VxOrd力-导向布局算法扩展到大图形的三个问题，在这一节中将其描述为我们在第3部分中对后续算法的动机。首先，算法能够正确地揭示布局的全局结构，这种正确性随着图结构的增大而减小。这种正确性无疑也会随着节点的度分布而变化，但即使是相对简单的网格状图也很明显。为了说明这一现象，我们使用了最初在非线性维度nonlinear dimension中引入的瑞士卷数据集29,30。这个数据集的设计目的是为了证明线性方法的不足之处（因为线性方法不能毫无错误地投射出瑞士卷）。在我们的例子中，我们只使用这个数据集作为一个例子，我们的算法是用来绘图，而不是降维的。We imposed a graph on 20,000 points sampled from the Swiss roll manifold using 20 nearest neighbors and drew the graph using VxOrd.我们用’邻居数=20’的配置从瑞士卷数据中抽取了2万个点生成图结构，并使用VxOrd进行了绘制。结果如图1（a，b）所示。在以前的工作中13,31也注意到，力-导向的布局无法正确地揭示全局结构，在这里显示了可以使用多级方法来减轻这种影响。因此，我们在OpenOrd中包含了一个多级策略。 无标度scale-free network网络具有严重的异质性，其各节点之间的连接状况（度数）具有严重的不均匀分布性：网络中少数称之为Hub点的节点拥有极其多的连接，而大多数节点只有很少量的连接。少数Hub点对无标度网络的运行起着主导的作用。从广义上说，无标度网络的无标度性是描述大量复杂系统整体上严重不均匀分布的一种内在性质。现实世界的网络大部分都不是随机网络，少数的节点往往拥有大量的连接，而大部分节点却很少，一般而言他们符合zipf定律，(也就是80/20马太定律)。将度分布符合幂律分布的复杂网络称为无标度网络。 其次，我们发现使用力-导向布局对大规模真实图结构进行处理，通常会导致视觉上不吸引人的布局。通常这些图都是稀疏的，但仍然有良好的关连（不是无标度的），因此得到的图看起来是完全fully连接的。图2（a）中显示了一个使用6147个节点和61646边的酵母微阵列数据的例子（在第4节中详细描述）尽管这张图只有61646条边（可能只是全连接边数的0.3%），但它看起来是完全连通的。为了鼓励更具有视觉吸引的布局，我们使用了一个边缘切割edge-cutting策略来鼓励节点的聚集。这可以看作是在力-导向布局中两个相互竞争的力量之间的权衡，在第3.1节中描述。 最后，力-布局的Frutcherman-Reingold方法有一个较高的运行时间O(n2)，这当然是力-导向布局在大规模图结构中应用的一个主要约束。使用基于网格的密度计算，并采用多级方法，可以改进运行时间。我们实现了上述两个想法，还包括在OpenOrd中使用并行计算的想法。 ALGORITHM 算法描述第2节中提到的算法，OpenOrd是基于Frutcherman-Reingold算法的力-导向布局算法11，以前这种算法是以VxOrd来实现的22。为了提供算法背景，我们在这里描述了这个VxOrd算法，并在下面的小节中描述了对其的改进。假设我们有一个无向加权图G=(V,E)，其中顶点由V={v1…vn}给出,边由E=E{eij}给出W=(wij)是与图G相对应的邻接矩阵adjacency matrix，所以边eij有权重wij。由于这个图是无定向的，我们知道wij=wji，所以矩阵W是对称的。 OpenOrd的目标是在二维空间中绘制图G。Xi=(xi1，xi2)表示vi在平面上的位置。OpenOrd通过尝试求解下列算式的最小值： Dxi表示点xi附近x1…xn的密度。1式的算式和包含了一个引力和斥力的项。引力部分Σj(wij ·d(xi,xj)2) 试图通过wij将具有强关联的顶点组合在一起。排斥项Dxi试图把顶点推到平面上密度较小的地方。 式（1）的最小化是一个困难的非线性问题。出于这个原因，我们使用一个基于模拟退火的贪婪的优化过程。我们的过程是贪婪的，我们通过优化内部和inner-sumΣj(wij·d(xi,xj)2)+Dxi来更新每个顶点的位置，同时修正其他顶点的位置。所有的顶点最初都放在原点处，并且图中的每个顶点都进行重复更新从而完成优化的一个迭代。迭代是通过一个模拟退火思想的流程来控制的，它由五个不同的阶段组成：液化liquid，膨胀expansion，冷却cool-down，紧缩crunch，和慢煮simmer。 在退火流程的每一个阶段，我们都改变了几个优化参数：温度、吸引力和阻尼damping。这些参数控制了顶点被允许移动的距离。在算法的每一步中，我们都计算两个可能的顶点移动。第一个可能的移动是一个随机的跳跃，它的距离是由温度决定的。第二种可能的移动是经过分析计算的（称为“障碍跳跃22”）。barrier jump这个移动被作为顶点邻居的加权质心weighted centroid来计算。阻尼乘数damping multiplier决定了顶点相对于上述中心被允许移动的距离，引力参数的作用是进一步加强上述导致移动的作用。the attraction factor weights the resulting energy to determine the desirability of such a move.在这两种可能的移动中，我们选择的移动结果是最低的内和能量 Σj(wij·d(xi,xj)2)+Dxi。 退火流程的各阶段分配是由每个阶段花费的时间决定的，并且通过调整各种参数来确定优化的行为。默认的流程下，液体liquid阶段花费了大约25%的时间，膨胀阶段25%，冷却阶段25%，收缩crunch阶段10%，慢煮simmer阶段15%。liquid、expansion和cooldown的阶段都使用相同的温度，但使用不同的吸引参数和阻尼乘数。慢炖simmer阶段使用较低的温度（大约是liquid、expansion和cooldown中所使用的温度的1/4），以及较低的吸引力和阻尼。退火流程各阶段的分配是通过对各种数据集（包括第4节中描述的酵母数据集）的广泛实验来确定的。这些实验是通过一个交互式的可视化环境进行的，这样就可以在退火过程的每个阶段观察到参数的影响。从定性Qualitatively上讲，膨胀阶段在吸引力和阻尼上允许顶点最大的移动，液体阶段允许下一个最大的运动the liquid stage allows the next largest movements，冷却、收缩和慢炖的阶段都限制定点的移动。 最后，我们使用基于网格的方法来计算密度项Dxi。通常情况下，密度计算应该是O(n2)，但是通过使用网格来进行密度计算，我们可以将成本降低到O(n)，这是得益于我们计算中使用的许多网格框grid boxes，不可避免的内存使用量会增加。然而，这种粗化coarsening可能是导致布局不准确的原因，因为在网格线上，密度会随着时间的变化而变化。由于这个原因，在最后的慢炖阶段中不使用密度网格。 Edge-Cutting 边切割为了产生视觉上吸引人的布局，我们开发了一种启发式的heuristic，允许用户控制布局中节点聚类和空白区域的数量。为了控制节点聚类，我们的启发式方法在方程（1）中影响了斥力和引力部分的相对重要性，为了控制空白，我们允许在目标函数的优化过程中忽略某些长边long edges。事实上，节点聚类和空白都可以通过忽略或切割长边来控制。 正如前面所提到的，方程（1）的目标函数中的Σj(wij·d(xi,xj)2)是为了吸引具有较大边权重nodes with large weight connections的节点，而Dxi是排斥的，并且不鼓励高平均节点密度，和顶点聚集。这两个部分的相对重要性决定了布局中顶点聚集的程度。如果引力项占主导地位，那么就会减少聚类的数目;如果排斥项占主导地位，就会出现更多的聚集现象。在优化过程中切割长边会降低引力项的重要性，并增加了排斥项的重要性，这种作用在布局中可以使得边切割对节点的聚集进行控制。 空白的空间可以由我们在计算中使用的长边数控制。Edges that are long but have large weight can exert undue influence on distant clusters, causing two groups to be placed on top of each other when a more appealing layout might be obtained if they were allowed to separate.边很长但有较大权重的边会对远处的集群施加不适当undue的影响，在一个更吸引人的允许两个聚簇分开的布局中，两个聚簇会被重合的放置在一起。在优化过程中切割长边允许聚簇分离。同时还改进了最终的绘图，因为这些边扩展了整个布局，因此模糊obscure了绘图中更精细finer的结构。（这种效果可以通过在最终的渲染中使用边的透明性transparency来增强，但是必须在计算中出现，以避免聚簇会放置于彼此之上。） OpenOrd的边切割参数从0到1指定。一个边切割参数值为0对应于标准的Frutcherman-Reingold布局算法（没有切割），而边切值为1对应于侵略性aggressive的切割。在退火流程的expansion和cooldown阶段允许进行边切割。在这两个阶段的开始阶段，一个阈值被指定为图中两个节点之间可能的最大距离的百分比。这个百分比是由边切参数控制的，值为0是最大距离的100%，1是最大距离的0%。边切割受制于每个节点必须至少有一个边的约束条件。在expansion和cooldown阶段，如果两点间的距离大于阈值，就会被切断。 边切割参数与将要切割的边的比例没有对应关系。根据输入图的不同，边切割参数可能没有什么效果，因为在优化过程中，图可能会有很长的边（通常是网状结构meshes的情况）。然而，对于真实世界的图结构，边切割参数可以对最终图的布局产生重大影响，并将在第4部分中进行探讨。 Parallel Force-Directed Layout 并行力-导向布局OpenOrd可以以串行和并行的方式在计算机上运行。并行版本类似于串行版本。两者都使用相同的贪婪更新，并遵循相同的退火流程。然而，在并行版本中，更新是并行执行的，而不是顺序的。 在OpenOrd中，并行强制布局算法首先为每个处理器分配一个随机的非重叠non-overlapping子集的图结构。处理器一直跟踪(tracking)它所分配的顶点以及顶点的全部邻居节点。所有的处理器都能跟踪每个顶点的位置，这样每个处理器就可以维护一个相同的网格密度副本。 每个处理器负责将其指定顶点按优化目标函数方程(1)进行优化。因为每个处理器知道指定顶点的位置,以及顶点的邻居节点的位置,所以可以使用相同的用于串行版本的贪婪的更新顶点位置的算法。在每个顶点更新之后，位置信息在处理器之间交换，然后各进程继续进行计算任务。 除了增加计算速度之外，OpenOrd的并行版本还有一个优势，即它可以在许多处理器上散布一个非常大的图形，从而使用具有大量有效内存的计算机。这是可行的，因为任何给定的图形都有比顶点多得多的边。此外，通过保持相同的贪婪更新程序和模拟退火流程，OpenOrd的串行和并行版本的结果是相似的。这两种算法的性能以及输出的差异在第4.2节中讨论。 Multilevel Graph Layout 多级图布局使用OpenOrd的并行策略允许非常大的图形进行布局。边切割增强了布局的视觉吸引力。然而，在使用第2节中描述的瑞士卷数据集进行实验时，产生不正确的全局结构的可能性仍然存在。为了解决这个问题，我们采用了Walshaw13的多级方法来实现OpenOrd。Walshaw的程序如下:首先，一组图G0=G,G1..G是用一个随机的粗化过程产生的32。在粗化过程中，相邻的顶点是随机合并的，如果两个相邻的相邻点有一个共同的邻居，那么他们的边权值就会被加到这两节点间的边权值中来。这个过程会重复，直到得到一个足够小的图GL。图GL是用力-导向算法绘制的。GL的绘制中顶点的放置位置被用作绘制图GL-1的起始点。例如，如果GL-1中的顶点u和v在GL中被合并成w，那么u和v就会被放置在以前的GL图中的w的位置。这个里-导向算法再次应用于获得的图GL-1，并重复该过程。 沃尔肖Walshaw的方法非常快，并且被证明在大图上很好地工作13。我们使用Walshaw的方法进行了额外的改进。我们使用的是基于聚集的粗化过程（下面描述），而不是随机的粗化过程。当然，我们也使用OpenOrd作为力导向的布局。但是我们将使用前一段中概述的Walshaw的过程。 使用我们的力导向布局算法需要额外的调整来适应模拟退火流程，并允许在多级方法中使用边切割。在获得了一系列粗化图G0…GL之后，我们用标准退火流程，生产出带有default的边切割的GL的布局。在细化过程中，我们按Walshaw的叙述进行顶点放置，再次使用default的边切割，但是修改我们的退火流程的分配以减少avoid liquid阶段和最小化expansion阶段。在细化阶段，我们也去掉eliminate了simmer的阶段。最后的布局是使用更激进的边切割，并且包含simmer阶段。令人惊讶的是，我们的实验表明，我们的多级布局方法适用于从6K到850K个顶点的数据集，甚至对这些不同数据集使用了相同的退火流程/边切割参数。在第4节中描述了所执行的实验和数据集。 Average-Link Clustering 平均链接聚类在我们的多级布局算法中，我们使用平均链接Average-Link聚类来提供粗化图G0=G,G1…GL.我们的聚类算法是基于一个平均链接聚类agglomerative模型，在此模型中，我们使用两点之间的边权值和两点之间的距离来产生顶点的聚簇。距离是由我们的力-导向布局算法从图的布局中获取的。一旦确定了聚簇，我们就合并该聚簇中的所有顶点，从而在新的粗图中得到一个顶点。边根据前面描述的方法合并。 在描述我们的方法时，让我们假设我们对G0进行粗化以获得G1。获取其余的粗化图是使用相同的过程完成的。我们的聚类算法的第一步是用我们的力导向布局算法加之最大化的边切割来绘制G0。这个步骤为我们提供了一个代理proxy表示，它可以用来得到顶点之间的距离，除了定义G 0的边权值之外。in addition to the edge weights defining G0 .最大的边切割鼓励布局算法生成一个自然的聚簇表示（参见第4.3节）。 接下来，我们将生成一个新的无向加权图G0，它的边是根据G0布局中绘制的距离计算的。在此布局中的边包括没有被布局算法裁剪的边，以及G0中每个节点的最大权边。我们令布局包含最大的权边，以确保图的连通。G0中的边权值不再是G0中提供的原始边权值;它们现在是G0~中互相连接的顶点之间的距离。 我们的导出derived图G0~可以在聚类算法中使用。我们使用的算法是一种平均链接层次hierarchical聚类的形式33,34。在这个算法中，每个顶点最初被分配给一个特定unique的聚簇。聚簇与相邻的聚簇会在稍后进行合并，合并与否用聚簇的几何中心centroids之间的距离来确定。习惯上，这个过程是重复的，直到所有顶点都被合并到一个聚簇中。然而，在我们的算法中，我们提供了一个距离阈值，之后我们停止形成新的聚簇。在平均链接聚类中，这个距离阈值由用户提供。 或者，阈值可以由OpenOrd自动选择。The automatic selection is done by locating the point on the plot of normalized rank vs. normalized distance in e G 0 at which the slope is 45 .自动选择是通过在G0中定位顶点来实现的，比较顶点在G0中规约化的rank值和规约化的距离。此时斜率为45度.Rank是通过对G0中的边值edge values进行排序来计算的;规约化的rank是将rank值规约到01之间。距离是由G0~的实际边值actual edge value给出的;规约化的距离是将距离规约到从0到1。对于真实世界的数据集，我们已经发现了规约化的rank和距离曲线curve，当斜率为45度时，通常可以为平均链接聚类提供一个良好的截止点（这个阈值在规约化后通常在0.9和0.95之间）。 EXAMPLES 示例我们在几个数据集上对OpenOrd进行了基准测试。如第2节所述，我们使用了在酵母细胞周期研究中产生的微阵列基因表达数据集进行实验35。这个数据集之前已经用作为OpenOrd的前身的VxOrd算法进行了测试22,27。数据包括对18个时间点的6 147个基因的同步simultaneous测量。A graph structure was imposed on the data by taking for each gene the top 10 genes most highly correlated over time.对每一个基因的数据构造imposed一个图结构，随着时间的推移，这10个基因高度相关。这产生了一个有6,147个节点和61646个边的图结构。我们使用酵母数据集来演示OpenOrd的边切割的效果和使用并行计算的效果。 我们还使用了瑞士卷数据集的一个替代品incarnation``29,30，替代品拥有2000个点，并使用每个点的20个最接近的邻居来构造图结构。a graph imposed using the 20 nearest neighbors of each point我们使用瑞士卷数据集来检查OpenOrd的多级multilevel功能。此外，我们还使用了来自维基媒体基金会（http://www.wikimedia.org）的一个大数据集来进一步测试多级功能。这个数据集是由B.Herr等人36在2007年使用维基百科的文章收集和处理的（参见http://scimaps.org/maps/wikipedia）。该数据集由659,388维基百科文章组成，这些文章连接了16,582,426个超链接。 最后，我们在OpenOrd上执行了许多附加数据集来对参数进行测试。These datasets include 8,712 journals from the year 2003 taken from Thomson Scientific (http://scientific.thomson.com/isi) Institute of Scientific In- formation (ISI) Journal Citation Reports and linked with 98,705 edges;这些数据集包括来自2003年Thomson-Scientific（http://scientific.thomson.com/isi）科学信息研究所（ISI）期刊引文报告中获得的8 712份期刊，并连接出了98,705条边37;集中于过去25年里从三军情报局获取的有关固态照明solid-state-lighting的32,776个文件数据集，并将其与222,626个共同引用连接起来38;从2003年第一季度开始的，在ISI数据库中收集的218716个元素数据集，并使用了1821,976的共同引用相似性co-citation similarities;最后是2004年在ISI数据库中使用的849,888篇文章数据集，并使用了5,843,729的联合引用相似性。我们使用这些数据集来研究各种参数对不同大小和类型的数据集的影响。 Edge-Cutting 边切割边切割在OpenOrd中，使用一个从0到1的值来指定。一个边切值为0对应于标准的Frutcherman-Reingold布局算法（没有切割），而边切值为1对应于侵略性的切割。侵略性Aggressive的切割可以促进顶点聚集，但不会把每一条边都砍掉。在OpenOrd中，边切割的默认值是0.8。在图2中，我们演示了边切割对Spellman’s酵母数据的布局的影响。 图2，酵母数据集上的边切割。从左到右显示，我们有Spellman’s酵母微阵列数据集35的布局：(a)没有边切割(参数值0)，(b)默认切割(参数值0.8)，和(c)最大切割(参数值1)。边切割的影响参数对目标函数（方程1）的吸引力和排斥力影响如图(d)所示。边切割参数在x轴上从0到1变化，在y轴上显示归一化的能量值（斥力引力两项）。正如在第3.1节中所描述的，当边被切割时，吸引力项就会减少，从而增加排斥项，从而鼓励顶点聚集。 为了在图2(D)中生成图，我们对酵母数据的11个布局计算了总吸引项Σj(wij·d(xi,xj)2)和总排斥项Dxi，使用边切割参数值0, 0.1, …, 0.9, 1。这些工作比较了规约在0和1之间的吸引项和排斥项的影响。正如在第3.1节中所讨论的，当边被切割时，吸引力项就会减少，从而允许增加排斥项，从而鼓励聚集。同样如在第3.1节中所讨论的，切割长边在图中提供了额外的空白区域。这些效果在图2（a-c）中很明显。 Serial vs. Parallel 串行和并行OpenOrd可以在串行或并行模式下运行。两种情况下的算法都是相同的，但是并行的结果不能保证与串行中的结果相同。因此，我们对OpenOrd的第一次测试是为了评估这两种模式之间的潜在差异。在这个测试中，我们再次使用了Spellman’s的酵母数据。我们在酵母数据上使用了1、2、4、8、16和32个处理器。1处理器的情况是串行版本。 We compared the outputs of each run by computing a similarity metric s ε (U,V) [0,1], whereU,V are two layouts of the same m-node dataset {x 1 ,…我们通过计算一个相似度量similarity-metricSε(U,V)∈[0,1]来比较每个运行的输出，U,V是m个顶点的相同数据集{X1,…Xm}的两个不同的布局。度量metric是通过首次构造邻域的关联矩阵NUε和NVε来计算的，其中N[·]ε是一个m×m矩阵，N[·]ε=(Nij)： NUε·NVε是二者的点积，两个矩阵都被认为是长度为m2的向量这个度量metric介于0和1之间，较大的值表示更大的相似性。这是对先前提出的相似度度量39的修改。 在我们的计算中，我们把U和V的每一个布局都缩放到[0,1]×[0,1]范围内，然后使用ε=0.1。我们得到了串行布局与并行布局的平均相似度为0.72。邻居节点数的平均大小是24。除了这个度量之外，我们还对每个运行的输出进行了定性的比较，如图3所示（a-f）。我们的度量计算表明，在并行计算过程中，布局的局部结构得到了保留，而定性qualitative结果表明，全局结构也得到了保护。除了提供类似的结果之外，OpenOrd的并行版本还提供了计算速度的测试，如图3（g）所示。 图3，OpenOrd的并行执行。在这里，我们提供了使用1、2、4、8、16和32个处理器的OpenOrd结果的定性qualitative比较。在（a）中，我们展示了串行（1处理器）布局。为了表达清晰我们只画出顶点，我们用蓝色、黑色、绿色和红色来给布局上色。在（b-f）中，我们分别显示了2、4、8、16和32个处理器布局的结果。这些布局中的顶点使用与（a）中的单个处理器布局中顶点相同的颜色进行着色，在（g）中，我们展现了不同数目处理器的计算速度变化。 Multilevel Layout 多级布局如前所述，Frutcherman-Reingold算法不能很好地扩展到非常大的图形。除了高运行时间之外，该算法还常常会混淆输入的全局结构，如图1（a，b）所示。在瑞士卷数据中，这幅图在局部规模上是正确的，但在全局范围内却很混乱。为了取得更好的结果，OpenOrd使用多级图粗化13，在第3节中描述了各种修改。我们首先通过重访revisiting瑞士卷数据集来演示我们的结果。我们使用了9个级别的粗化和没有边切割的场景，我们得到了如图1（c）所示的瑞士卷的全局正确布局。 接下来，我们将使用从2007年开始的生成了659388维基百科文章的网络的布局来展示多级方法的结果。这个布局是使用6个递归级别计算的，如图4所示。该方法在粗化过程中使用了侵略性的边切割和聚类，并在细化过程中反复应用了标准的OpenOrd布局算法。在没有粗化的情况下，OpenOrd将绘制出相同的图形，它是一个密度均匀uniformly、高度连接和视觉上不吸引人的球状结构。 图4，OpenOrd递归recursion。在这里，我们展示了一个从2007年开始的659388维基百科文章数据集的布局。最终的布局显示在图的中心。在递归过程中产生的各种布局在最终布局的外部显示。从11点到6点方向，我们按逆时针方向进行，我们展示了使用图形粗化coarsen过程产生的布局。从5点到1点方向，我们展示了在细化refine过程中产生的布局。 Parameter Testing 参数测试在OpenOrd中使用的布局算法有大量的参数，包括开始时随机的种子、模拟退火流程的分配和边切割参数等。当在多级模式中使用布局算法时，我们必须根据粗化或细化的当前阶段调整这些参数，以便在我们执行的过程中保持布局的连续性。在本节中，我们使用大量的数据集来对参数的选择进行基准测试。 我们的基准测试的结果如表1所示。尽管数据集具有多样性，使用多级模式时我们发现了一组通用的参数提供了的良好布局。这些参数在www.cs.sandia.gov/smartin的代码中作为默认值提供。数据集、它们的大小和使用的递归级别the level of recursion都显示在表1中。还提供了运行时间，以向用户提供给定数据集大小所需的工作。运行时间是使用4GB内存的英特尔Xeon工作站获得的。 表1，参数测试。在第一列中列出了用于为OpenOrd的多级版本设定基准的默认参数的不同数据集。第二列和第三列包含数据集大小，第四列包含所使用的级别，第五列显示在工作站上运行所需的时间（小时:分钟:秒）。 CONCLUSIONS我们已经创造了一组collection用于绘制大图形的算法（OpenOrd）。这些算法的重点是处理现实世界的数据集，例如在科学文献和生物应用中遇到的数据集。我们的方法是基于Frutcherman-Reingold力导向布局方法11。这种方法已经采用模拟退火和网格计算方法进行了改进，得到了一种实用的算法，在实际数据集上得到了良好的应用23-28。然而，该算法对大型数据集产生了视觉上不吸引人的和全局不准确的布局。我们已经使用边切割解决了视觉吸引力问题，以及使用一种具有平均链接聚类的多级方法来捕获真实数据集的全局结构的不准确的问题。 我们已经在各种数据集上演示了OpenOrd的行为，包括酵母微阵列数据35、科学期刊数据37、科学文献数据38、维基百科文章36。我们已经确定了适当的默认参数，可以在不同大小的数据集上使用，并在我们的开放源代码中提供了这些默认值，可以在www.cs.sandia.gov/smartin获得。最后，该算法已经适配于并行计算的计算机上来处理超大数据集。 ACKNOWLEDGMENTS感谢作者感谢B.Herr提供了维基百科的文章数据。Sandia公司是由洛克希德马丁公司的Sandia公司运营的一个多项目实验室，为美国能源部提供合同，该公司的合同是deac04-94AL85000。这项工作得到了Sandia的计算机科学研究基金的支持。 REFERENCES[ 1 ] Battista, G. D., Eades, P., Tamassia, R., and Tollis, I. G., [Graph Drawing Algorithms for the Visualization of Graphs], Prentice Hall (1999).[ 2 ] Jünger, M. and Mutzel, P., [Graph Drawing Software], Springer-Verlag (2004).[ 3 ] Freeman, L. C., Visualizing social networks, Journal of Social Structure 1(1) (2000).[ 4 ] Borner, K., Chen, C., and Boyack, K., Visualizing knowledge domains, in [Annual Review of Information Science and Technology], Cronin, B., ed., 37, ch. 5, 179 255, American Society for Information Science and Technology, Medford, NJ (2003).[ 5 ] Shiffrin, R. and Borner, K., Mapping knowledge domains, Proc. Natl. Acad. Sci. 101 suppl. 1, 5183 5185 (2004).[ 6 ] Wolff, A., Drawing subway maps: A survey, Informatik - Forschung und Entwicklung 22, 23 44 (2007).[ 7 ] Wiese, K. C. and Eicher, C., Graph drawing tools for bioinformatics research: An overview, in [CBMS 06: Pro- ceedings of the 19th IEEE Symposium on Computer-Based Medical Systems], 653 658, IEEE Computer Society, Washington, DC, USA (2006).[ 8 ] Adai, A. T., Date, S. V., Wieland, S., and Marcotte, E. M., LGL: Creating a map of protein function with an algorithm for visualizing very large biological networks, Journal of Molecular Biology 340, 179 190 (2004).[ 9 ] Eades, P., A heuristic for graph drawing, Congressus Numerantium 42, 149 160 (1984).[ 10 ] Kamada, T. and Kawai, S., An algorithm for drawing general undirected graphs, Information Processing Letters 31, 7 15 (1989).[ 11 ] Frutcherman, T. and Reingold, E., Graph drawing by force-directed placement, Software-Practice and Experi- ence 21, 1129 1164 (1991).[ 12 ] Davidson, R. and Harel, D., Drawing graphs nicely using simulated annealing, ACM Trans. on Graphics 15, 301 331 (1996).[ 13 ] Walshaw, C., A multilevel algorithm for force-directed graph-drawing, Journal of Graph Algorithms and Applica- tions 7(3), 253 285 (2003).[ 14 ] Harel, D. and Koren, Y., A fast multi-scale method for drawing large graphs, Journal of Graph Algorithms and Applications 6, 179 202 (2002).[ 15 ] Hachul, S. and Junger, M., Drawing large graphs with a potential-field-based multilevel algorithm, in [Graph Drawing], Pach, J., ed., 285 295, Springer Berlin (2005).[ 16 ] Hu, Y. F., Efficient and high quality force-directed graph drawing, The Mathematica Journal 10, 37 71 (2005).[ 17 ] Quigley, A. and Eades, P., Fade: Graph drawing, clustering, and visual abstraction, in [Graph Drawing], Marks, J., ed., 197 210, Springer Berlin (2001).[ 18 ] Noack, A., An energy model for visual graph clustering, in [Graph Drawing], Liotta, G., ed., 425 436, Springer Berlin (2004).[ 19 ] Frishman, Y. and Tal, A., Multi-level graph layout on the gpu, IEEE Transactions on Visualization and Computer Graphics 13, 1310 1319 (2007).[ 20 ] Godiyal, A., Hoberock, J., Garland, M., and Hart, J. C., Rapid multipole graph drawing on the gpu, in [Graph Drawing], Tollis, I. G. and Patrignani, M., eds., 90 101, Springer Berlin (2009).[ 21 ] Davidson, G., Hendrickson, B., Johnson, D., Meyers, C., and Wylie, B., Knowledge mining with VxInsight: Dis- covery through interaction, Journal of Intelligent Information Systems 11, 259 285 (1998).[ 22 ] Davidson, G., Wylie, B., and Boyack, K., Cluster stability and the use of noise in interpretation of clustering, in [IEEE Symposium on Information Visualization (INFOVIS)], 23 30 (2001).[ 23 ] Boyack, K., Wylie, B., and Davidson, G., Domain visualization using VxInsight for science and technology man- agement, Journal of the American Society for Information Science and Technology 53(9), 74 774 (2002).[ 24 ] Boyack, K., Mapping knowledge domains: Characterizing PNAS, Proc. Natl. Acad. Sci. 101 suppl. 1, 5192 5199 (2004).[ 25 ] Boyack, K., Klavans, R., and Borner, K., Mapping the backbone of science, Scientometrics 64(3), 351 374 (2005).[ 26 ] Kim, S., Lund, J., Kiraly, M., Duke, K., Jiang, M., Stuart, J., Eizinger, A., Wylie, B., and Davidson, G., A gene expression map for Caenorhabditis elegans, Science 293(5537), 2087 2092 (2001).[ 27 ] Werner-Washburne, M., Wylie, B., Boyack, K., Fuge, E., Galbraith, J., Weber, J., and Davidson, G., Comparative analysis of multiple genome-scale data sets, Genome Research 12(10), 1564 1573 (2002).[ 28 ] Wilson, C., Davidson, G., Martin, S., Andries, E., Potter, J., Harvey, R., Ar, K., Xu, Y., Kopecky, K., Ankerst, D., Gundacker, H., Slovak, M., Mosquera-Caro, M., Chen, I.-M., Stirewalt, D., Murphy, M., Schultz, F., Kang, H., Wang, X., Radich, J., Appelbaum, F., Atlas, S., Godwin, J., and Willman, C., Gene expression profiling of adult acute myeloid leukemia identifies novel biologic clusters for risk classification and outcome prediction, Blood 108, 685 696 (2006).[ 29 ] Tenenbuam, J. B., de Silva, V., and Langford, J. C., A global geometric framework for nonlinear dimensionality reduction, Science 290, 2319 2323 (2000).[ 30 ] Roweis, S. and Saul, L., Nonlinear dimensionality reduction by locally linear embedding, Science 290, 2323 2326 (2000).[ 31 ] Davidson, G., Martin, S., Boyack, K., Wylie, B., Martinez, J., Aragon, A., Werner-Washburne, M., Mosquera- Caro, M., and Willman, C., Robust methods in microarray analysis, in [Genomics and Proteomics Engineering in Medicine and Biology], Akay, M., ed., 99 130, Wiley IEEE (2007).[ 32 ] Hendrickson, B. and Leland, R., A multilevel algorithm for partitioning graphs, in [Proc. Supercomputing 95, San Diego], ACM Press (1995).[ 33 ] King, B., Step-wise clustering procedures, J. Am. Stat. Assoc. 69, 86 101 (1967).[ 34 ] Jain, A. K., Murty, M. N., and Flynn, P. J., Data clustering: A review, ACM Computing Surveys 31(3), 264 323 (1999).[ 35 ] Spellman, P. T., Sherlock, G., Zhang, M. Q., Iyer, V. R., Anders, K., Eisen, M. B., Brown, P. O., Botstein, D., and Futcher, B., Comprehensive identification of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization, Molecular Biology of the Cell 9, 3273 3297 (1998).[ 36 ] Herr, B., Holloway, T., and Borner, K., An emergent mosaic of Wikipedian activity. International Workshop and Conference on Network Science (2007).[ 37 ] Boyack, K., Using detailed maps of science to identify potential collaborations, Scientometrics 79, 27 44 (2009).[ 38 ] Boyack, K., Tsao, J., Miksovic, A., and Huey, M., A recursive process for mapping and clustering literatures: International trends in solid state lighting, International Journal of Technology Transfer and Commercialization 8, 51 87 (2009).[ 39 ] Ben-Hur, A., Elisseeff, A., and Guyon, I., A stability based method for discovering structure in clustered data, in [Pacific Symposium on Biocomputing], 6 17 (2002).","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"并行计算","slug":"并行计算","permalink":"https://www.cz5h.com/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"},{"name":"层次数据","slug":"层次数据","permalink":"https://www.cz5h.com/tags/%E5%B1%82%E6%AC%A1%E6%95%B0%E6%8D%AE/"},{"name":"力导向","slug":"力导向","permalink":"https://www.cz5h.com/tags/%E5%8A%9B%E5%AF%BC%E5%90%91/"},{"name":"图简化","slug":"图简化","permalink":"https://www.cz5h.com/tags/%E5%9B%BE%E7%AE%80%E5%8C%96/"}]},{"title":"可视化算法VxOrd论文研读","slug":"2018-4-15 可视化算法VxOrd论文研读","date":"2018-04-14T22:00:00.000Z","updated":"2020-02-29T18:43:55.508Z","comments":true,"path":"article/428e.html","link":"","permalink":"https://www.cz5h.com/article/428e.html","excerpt":"原文名：Cluster Stability and the Use of Noise in Interpretation of Clustering 中文译：聚类的稳定性和在聚类解释中添加噪声 源刊载：IEEE Symposium on Information Visualization , 2001 :23 机构名：Sandia National Laboratories 桑迪亚国家实验室，US 研究点： - Clustering algorithms - Data visualization - Stability analysis - Algorithm design and analysis - Best practices","text":"原文名：Cluster Stability and the Use of Noise in Interpretation of Clustering 中文译：聚类的稳定性和在聚类解释中添加噪声 源刊载：IEEE Symposium on Information Visualization , 2001 :23 机构名：Sandia National Laboratories 桑迪亚国家实验室，US 研究点： - Clustering algorithms - Data visualization - Stability analysis - Algorithm design and analysis - Best practices 摘要本文介绍了一种适合挖掘超大型数据库的聚类和排序ordination算法，包括微阵列表达式研究microarray expression studies产生的数据库，并对其稳定性进行了分析。在实际条件下，利用一个酵母细胞周期实验，对6000个基因进行实验，并对每个基因进行18个实验测量。将数据库对象分配X、Y坐标及顺序的过程，在随机启动条件下，以及在开始相似度估计中对小扰动的处理是稳定的。对聚类通常共同定位的方式进行了仔细的分析，而在不同的初始条件下偶尔出现的大位移则被证明在解释数据时非常有用。当只报告一个聚类时，就会丢失这种额外的稳定性信息，这是目前已被接受的实践。然而，在分析大型数据收集的计算机聚类时，人们认为这里提出的方法应该成为最佳实践的标准部分。 Introduction我们感兴趣的是在大量的实验数据中发现意想不到的关系。不幸的是，我们很容易看到虚幻的模式illusory patterns我们的思想是建立在寻找模式的基础上的，即使我们知道感知到的模式仅仅是随机的工件，我们也会这样做。因此，我们感兴趣的模式必须进行一些测试，这些测试表明，如果我们从另一个类似的数据集开始，或者，可能是由于添加了少量的噪音而随机干扰的数据，它们肯定会再次发生。我们使用计算机来查看这些大型数据集，因此我们有必要相信我们的计算工具是可靠的，特别是在我们的例子中，它们使用随机数。我们想知道的结果是对工具内部随机过程的细节不敏感;也就是说，我们想知道我们使用的工具是稳定的，在每次的使用中。当然，我们希望这些工具是实用的，对实践科学家有用，他们想知道这些模式是真实的，并且它们可能指向世界上的一些物理事实或重要的过程。 几个世纪的实践证明，最后一个问题只能通过仔细控制的实验来回答。在这里，我们必须把我们的工具所发现的关系的测试和解释留给实验家。相反，我们的目的是解决发现潜在重要模式的过程，以及对过程本身的可靠性的调查，以及过程对初始数据的细微变化的敏感性。我们将报告对一个特定数据挖掘工具的可靠性和灵敏度的各种计算调查，VxInsight1998 使用的是中等大小的微阵列数据集。 VxInsight使用一个地形隐喻来描述大量的数据，通过在地形上相互靠近，来总结相似元素的簇。二维的星团被想象成由山谷和开阔空间分隔的山脉和山丘。山脉的高度表明了每座山下聚集的元素数量。山脉之间的局部分组和分离也包含了关于集群间相似性的信息。在被广泛隔离的山脉中，数据元素的相似性要小于邻近山脉的数据元素。 图一。在VxInsight中关系的持续变化 不像自组织的maps4、k-means5、或约克的快速聚类6.7,我们的方法不需要预先猜测应该创建多少个聚类。而且，像地形一样的聚类可以表示 比 仅仅列出聚类元素 更多的信息。在山下的局部结构会显示出更细、更细的关系，当将其放大到地形的表示时（图1），数据对象在特定的聚类或层次结构中并不是显式的成员。物体的位置是通过在二维空间中的连通图的能量最小化来确定的。对每个数据元素的X、Y坐标的赋值是一个我们称之为序化的过程。 本文的主要主题是我们的序化过程的稳定性。简要地介绍了我们的力导向序化算法的随机元素的稳定性特征。这些特征在一系列的实验中被研究，这些实验用不同的随机起始条件重新排列相同的数据集，并比较（从视觉上和统计上）结果。正如在结果和讨论部分中所描述的，序化算法展示了可预测的、可理解的行为。 在确定该工具可接受的稳定之后，我们研究了在相似关系中添加噪声的影响，在序化过程的输入阶段。正如预期的那样，在真实的数据集下，某些聚类比其他聚类更稳定。重要的是，一些聚类不仅保留了相同的成员，而且在地形图中保持了物理上的接近。其他聚类往往会伴随较大的位移，这可以通过在地形图中聚类间强相似性连接来理解。 In the Results and Discussion section, we describe how this analysis suggests important strategies for testing the robustness of clustering algorithms.在结果和讨论部分中，我们描述了该分析如何提出测试聚类算法的健壮性的重要策略。 方法如何生成一个VxInsight图图2显示了数据必须通过的一般流程，以生成VxInsight图。一个典型的数据库，在图中表示为表格，它由几千个元素（行）组成，其中一个或多个属性为（列）。必须对这些数据进行处理，以计算每一对数据元素的相似性，然后用它们来构造一个抽象图。在这个节点和弧的图中，节点表示单个的数据元素，而弧是元素之间的相似性。序化过程将每个数据元素分配给抽象可视化表面的X、Y位置。最后，这些坐标被用来生成山地地形。 图二。将数据处理成VxInsight图 选择一个数据集在我们的实验中，我们选择了一个现成的数据集http://genomewww.stanford。edu/cellcycle/data/rawdata，这是一个包含大约6000个数据元素的表格（酵母中的基因）。我们选择了这个数据集的一个子集，在细胞生长分裂的过程中，对这些基因的活动进行了18次测量。这些数据足够大，可以提供数据挖掘技术发现的机会，而且远远超出了测试聚类方法的玩具toy问题。此外，酵母数据集已经被很好地研究过，某些基因也被认为是一起工作的，应该把它们聚集在一起，可以作为对我们算法的简单测试。最后，研究这一数据集使人们有可能对未被研究的基因的功能进行重要的预测，这些基因聚集在这些具有已知功能的基因附近。重要的是，这些预测可以通过检验这些数据最初发布以来发表的文献来验证;其中很多都可以在网上找到，用基因名索引，例如，可以看到http://www.proteome.com或斯坦福网站http://基因组-www.stanford。edu/cgi-bin/sg/search。 计算基因的相似性电子表格中的每一列都记录了单个显微镜载片上6000个点的相对亮度。除了受控制的变量之外，各种各样的条件将会系统地改变这些测量值。例如，一张幻灯片的整体亮度可能因位置不同的材料而不同，处理条件略有不同，或者在扫描光强度方面存在差异。为了弥补这些影响，每一张幻灯片的测量值（在扩展表中的一个列）通过减去这张幻灯片的中值来进行标准化，然后除以四分位范围（75百分位数和25%亮度值之间的差值）。这种健壮的标准化对异常值的敏感度要低于标准化，通过减去平均值并除以标准偏差8。皮尔逊的相关系数9被用来计算每一对基因之间的相似性。 没有相似之处的基因将会有接近于0的值，而与之相似的基因将有接近于1的值。Using the raw correlations unduly weights the low similarities and does not adequately represent the information content contained in a strong similarity.使用原始的相关性对弱相关进行了过度的加权，并且不能充分地表示强相关的信息内容。The non-linearity of this information, or rareness, is extreme and can change the total range of observed similarity weights by orders of magnitude.该信息的非线性，或罕见性，是极端的并且可以按数量级改变观察到的相似度值的总范围。We created all of the clusters reported here using gene pair similarities based on the t-statistic of the correlation coefficient, not on the correlation coefficient itself我们使用基于相关系数的t统计量的基因对的相似度 来创建所有的聚类，而不是相关系数本身。 这种转换具有逻辑支持，在实践中运行良好，并且易于计算。我们认为，至少对于微阵列（DNA微阵列）实验来说，在所有的聚类分析中，都应该替换直接基于相关性的相似性。 在这个实验中，每6000个基因中都记录了20个最强烈的正相关。最后，对于6000个基因中的每一个，基因的名称，它相关的基因的名称和相关的t统计量都被写入到一个文件中，供序化程序使用。 重要的是要使用大量的相似点来确保有序的精细结构被捕获。然而，我们发现，对具有最强烈相似性的基因的放置进行的视觉检查，为评估这些序列的质量和理解它们的结构提供了一种方式。例如，在图3中，强链接表明红色聚类可以很好地放置在山脊的两侧（由黄色、粉色和浅蓝色定义）。 图3：两个随机运行（左、中）显示红色集群交换位置。强链接（蓝线）表明，顺序是可以接受的。第三幅图像更清晰地显示了山脊内的强连接密度。 确定一个合适的阈值来确定高度相关的基因是有问题的。1963年，为了报告相关性的统计意义，奥斯特尔[Ostel, 1963] 的常见做法是测试下一种情况 Ho : The observed n-sample correlation is consistent with observing two processes with a true correlation 0 = ρ ,观察到的n-样本相关性与观察两个具有真相关的过程是一致的， using a t-test with 2 n degrees of freedom and reject the hypothesis with some level of confidence α .使用n-2自由度的t-test测试，并以一定程度的置信度α拒绝这个假设。然而，有6000个基因我们有1800万对相关性。即使使用了0.001的置信水平，假设真正的相关性为0时，我们就会期望有36000对相关性超过临界值。 确定一组高度相关的基因(特别是当n很大),更好的方法是进行动力分析a power analysis,这需要一些假设的选择实际相关,ρ0,和一些可接受的机会不是检测对真正有关联的基因ρ0由于观测值的变化。例如，我们选择了ρ0=0.9 作为实际相关度，并且假设我们需要保留在二十次测量中有此相关度大于一次的基因对（这时β=0.05）。 对于测试Ho的上述公式 仅仅在ρ0=0时有效。当ρ0≠0，可以使用基于费雪10的近似值，which把r转化为标准的Z统计分量（Zρ0和方差σ2） 对于Z，有如下： 因此，接收pair的关键是他们是强相关的，根据我们的规范，我们会错误地拒绝一种相关性在20%的情况下，当真正的潜在相关性是ρ0=0.9且n=18： 这个临界值对应于 α &lt; 0.0005。匹配该规范的一对基因被保存下来，以便稍后在VxInsight中显示。 VxInsight序化程序序化程序通过考虑整个集合中的对象之间的所有相似性来确定数据对象的空间位置，图4显示了具有许多相似链接（边）的对象在地图上聚集在一起;而那些几乎没有或没有相似链接的对象是分开的。 2000个顶点图（顶部）的布局，以及已知的K 5和双K 5（底部）的解决方案。 一个抽象的，边缘加权的图，G=（V，E），是用一个节点列表和它们的相似点生成的，其中顶点V对应于数据对象，相似点对应于加权边，E，有大量的文献用于图形绘制和布局算法11-19Fruchterman和Reingold 12的工作与我们的方法特别相关。 在开发和实现我们的算法时，我们遵循了四个重要原则： 由边连接的顶点应该相互靠近。 非连接的顶点应该相互远离。 结果应该对随机的初始条件不敏感。 计算的复杂性应该降低到最小值。 这些原则非常重要，我们将详细讨论每一个原则。 原则1和2Fruchterman等人计算了吸引力和排斥力。然后，这些术语用于为图形顶点生成新的位置。我们的算法将吸引力和排斥项结合成一个势能方程（方程式3），第一个部分，在括号中，是由于连接顶点之间的吸引;第部分是排斥项。 Ki(x,y) = 一个顶点在一个特定的x，y位置的能量 ni = 连接到顶点i的边数 wi,j = 顶点i与顶点j连接的顶点之间的边权值。 l2i,j = 顶点i的平方距离和边j的另一端的顶点之间的距离。 Dx,y = 一个力项与xy附近顶点的密度成比例。在我们的顺序中，方程3在三个阶段中以迭代的方式逐渐被最小化。第一阶段通过将顶点扩展到它们最终所属的一般区域，从而减少系统中的自由能量。下一阶段类似于模拟退火算法中出现的淬火步骤，节点会进行更小的随机跳跃，以最小化它们的能量方程式。最后一个是酝酿中的阶段，详细的地方修正。 所有的运动都是随机的;每个顶点都可以从当前位置跳转到一个新的、随机的位置。如果移动减少了顶点的势能，那么顶点就被允许停留在新的位置。否则，顶点将保留旧位置到下一个迭代的位置。其他更复杂的技术，包括梯度下降法和动量项法，在理论上很具有吸引力。然而，成千上万个顶点的能量表面是如此的混乱（在空间上和时间上），在实践中，我们发现更简单的方法表现更好。请注意，对于每个顶点，只有它自己的能量被考虑，这是贪婪算法的一个特征，它只会间接地导致整个系统的全局最小化。然而，系统的总能量，见方程4，仍然可以作为算法终止的标准。 文献11,16,17讨论了许多其他的终止标准，其中一些没有明确地遵循总能量。例如，Eades11建议简单地运行固定数量的迭代，在它们的情况下是100。我们发现，对于更复杂的图形，800个迭代很好地工作。我们通常处理的图形有上万个顶点。本文讨论的图中有6000个顶点，需要90秒才能完成600 MHz的奔腾III的800次迭代。 很明显，尽量减少潜在的能量应该会导致与我们的前两个原则相符合的ordinations。吸引项激励点的移动，将使(带权的)强相关顶点之间的边的长度的最小化。第二项，Dx,y，是基于附近顶点的局部密度的力，当顶点移动到不那么拥挤的区域时，它被最小化。为了减少这两部分的计算，一个顶点必须接近它的连接顶点，并且与非连接的顶点保持一定的距离。 原则3一个序化过程可以很容易地以防止平稳过渡到正确的答案的方式开始。也就是说，算法会被困在局部极小值中，并且很可能在计算的早期被迫趋向局部极小值。问题是，初始的配置可能会导致一些应该彼此靠近的顶点，被一个大的障碍分隔开。各种随机技术被用来避免这个问题。例如模拟退火，它采用一定概率来决定，是否采取增加节点相关能量的动作。这种技术允许顶点克服与局部极小值相关的障碍，以寻找更低的能量状态。在对我们的能量方程式进行检查后，我们可以清楚地看到，通过直接求解单个顶点的能量的位置，可以使其迅速地通过一个能量屏障。在我们的算法中，我们已经成功地使用了这种分析方法来避免局部极小值。在过程的早期实现一个良好的配置，独立于初始配置，对于有效的序化是十分重要的，这也契合原则3。 模拟退火算法的原理概述爬山法是一种贪婪的方法，其目标是要找到函数的最大值，若初始化时，初始点的位置在C处，则会寻找到附近的局部最大值A点处，由于A点出是一个局部最大值点，故对于爬山法来讲，该算法无法跳出局部最大值点。若初始点选择在D处，根据爬山法，则会找到全部最大值点B。这一点也说明了这样基于贪婪的爬山法是否能够取得全局最优解与初始值的选取由很大的关系。对于这个优化问题，其大致图像如下图所示：其目标是要找到函数的最大值，若初始化时，初始点的位置在C处，则会寻找到附近的局部最大值A点处，由于A点出是一个局部最大值点，故对于爬山法来讲，该算法无法跳出局部最大值点。若初始点选择在D处，根据爬山法，则会找到全部最大值点B。这一点也说明了这样基于贪婪的爬山法是否能够取得全局最优解与初始值的选取由很大的关系。模拟退火算法(Simulated Annealing, SA)的思想借鉴于固体的退火原理，当固体的温度很高的时候，内能比较大，固体的内部粒子处于快速无序运动，当温度慢慢降低的过程中，固体的内能减小，粒子的慢慢趋于有序，最终，当固体处于常温时，内能达到最小，此时，粒子最为稳定。模拟退火算法便是基于这样的原理设计而成。模拟退火算法从某一较高的温度出发，这个温度称为初始温度，伴随着温度参数的不断下降，算法中的解趋于稳定，但是，可能这样的稳定解是一个局部最优解，此时，模拟退火算法中会以一定的概率跳出这样的局部最优解，以寻找目标函数的全局最优解。如上图中所示，若此时寻找到了A点处的解，模拟退火算法会以一定的概率跳出这个解，如跳到了D点重新寻找，这样在一定程度上增加了寻找到全局最优解的可能性。 我们通过将顶点移动到方程3中指定的方向来达到这个结果。然而，为了跳过能量屏障，一小部分顶点忽略了排斥项，并在计算上将吸引项最小化。这是通过在所有连接的顶点上计算加权的中心来实现的。然后顶点跳到那个计算的质心，不管是否有任何可能的能量增加，如图5所示。 图五，通过忽略密度项来跳过障碍。 在冷却过程中，障碍跳跃是与冷却时间相联系的，在淬火期间，障碍跳跃的设置频率从25%下降到10%，在炖煮(simmer)阶段根本不使用。在开始时，为了达到稳定性而在随机的初始阶段使用较高的频率。糟糕的初始位置或初始的错误的跳跃，将不可逆转地改变一个纯粹随机算法的结果。由于这一过程的修正性质，极大地减轻了上述的影响。图6显示了两个随机运行的图像。 第一行的使用障碍跳跃，第二行没有。我们可以看到使用障壁跳技术取得的优异的重复性。第二行显示，6000个顶点被绝望地困在一个局部极小值的网络中。图7中的柱状图提供了进一步的支持，即障碍跳跃提高了随机迭代求解器的可重复性。 对于本文的直方图，我们想通过计算在地图的一小部分（1%）内的相同邻居的数量来测量有序算法的稳定性。这些图包含了6000个基因，所以对于每一个基因，我们测量了60个最近的基因中有多少在迭代过程保持不变。（如图所示，图中柱状图的和应为6000，上面的表示大部分节点在布局结束后都能发现45个左右的相同的邻居节点，下面的表示大部分节点在每次布局后都没有/0个相同的邻居节点） 原则4计算Dxy的蛮力方法肯定不符合我们的第四个原则。因为每个顶点都必须检查它对所有其他顶点的位置，这个简单的方法对于每一个Dxy的确定都要进行|V|次比较。当每个节点都必须计算出Dxy在特定位置xy中确定它的能量时，这个算法需要总运行时间O（|v|2）。 对于现实世界中的问题O（|v|2）算法是非常耗时的。我们已经开发了一种基于网格的计算Dxy的方法，它允许每个顶点在恒定时间,O(1),内确Dxy的近似值，从而将总运行时间减少到令人满意的O（V）。 Fruchterman8所讨论的网格变量算法使用一种binning技术来考虑特定区域内的那些顶点。一种方法是，通过对顶点的均匀分布把计算减少到O（V）。然而，如果边的数量很小，那么图形就只有一个均匀的分布。高度连接的图形在小区域中会有密集的顶点集中，并且运行时间不再是线性的，而是取决于顶点数量。为了对所有的图形有效，我们的排斥项利用了非特定密度度量。顶点不会被其他特定的顶点排斥，而是被普遍的过度拥挤所排斥。 这种对排斥力标准的微小修改使得计算复杂度大大降低。这种密度场density field算法是通过让每个节点将能量足迹footprint放置在二维（密度场）阵列上实现的。能量足迹可以是空间中的任何函数。我们的实现：使用一个半径为r的圆和一个在圆的中心能量达到峰值的函数，同时从圆的中心向外的距离能量下降。总密度场是该区域每个顶点的贡献之和。考虑到密度字段，每个节点可以使用一个常数时间表来查找并确定近似的Dxy值。这种方法将斥力项的计算复杂度从O(|V|2)降低到O(|V|)，并且与我们的第四个原则相一致，这是使文中算法可以应用到实际场景的重要内容。 计算实验为了测试算法的稳定性，我们用不同的种子进行了100次重新排列。在一个布局中(序列)，视觉地标记了每个聚类的元素，并观察它们是否在另一个布局中视觉上仍然聚集在一起。然后我们计算了如下所述的社区统计数据。 为了确定微小的变化或噪音是否会对分类结果产生改变，我们执行了八十组添加了符合一定规则的噪声的re-ordinations，规则是噪音符合高斯分布、均值为零、标准偏差分别为0.001,0.010,0.050,和0.100，这些噪音的相关性被切断以保持相关系数的有效范围是(-1.0 + 1.0)。这些不同的序列的最后布局均在视觉和统计上进行了比较。 评估方法我们用一种邻域分析法neighborhoods analysis来比较不同的序列ordinations。当两个序列非常相似时，我们有理由相信，对于每一个基因，它的最接近的邻居，比如临近的60个基因在这两个序列中几乎是相同的。事实上，我们对全部序列都有上述的期待。另一方面，如果这些序列几乎没有什么共同之处，那么观察一个在两序列中都有相同的邻居的基因是很罕见的。我们计算了这两种序列中每个基因的邻居统计数据。对于两个序列中的每一个基因，我们分别确定60个最近的基因，然后计算出共同的基因数量。这个数字被用来增加表中的值,所以最后,我们有一个柱状图展示了在这两个序列中的没有共同邻居的基因数量,有多少有一个共同邻居的基因数量,等等,一直到在两个序列中都有相同的60个邻居的基因数量。由上述可以表示为直方图，如图8所示。 图8.随机起始条件下序列的邻居分布，20副本 We then visually compared the results of the two ordinations by coloring all of the genes in a cluster found in the first ordination and seeing where those colored genes were placed in the other ordination (so that a similar ordination would not break up the group of colored genes, but would still have them co-located;然后我们对 在第一个序列中发现的某个聚类中所有的基因进行着色并观察这些彩色基因被放置在另一个序列中的布局结果，然后将这两个序列的结果进行比较（所以类似的序列不会破坏有色基因的位置，参见图9和图10）。 图9.有不同的随机初始条件的序列的布局结果 图10.演示了增加边噪声对聚类的稳定性 图11.添加了噪音(std.001)的邻域稳定性的直方图图12.添加了噪音(std.010)的邻域稳定性的直方图图13.添加了噪音(std.050)的邻域稳定性的直方图图14.添加了噪音(std.100)的邻域稳定性的直方图 结果和讨论计算实验揭示了两种类型的信息。首先，我们发现大型结构通常非常健壮，可以从不同的初始条件开始。其次，在有差异的地方，关于为什么聚类位置改变的见解和它们确实改变的事实一样有趣。我们提出了两种衡量这些结构稳定性的方法：视觉解释，以及我们邻域分析的结果。视觉解释在清晰度上是惊人的，但同时也得到了柱状图所示的数值结果的支持。 直方图的数字可以被解释为从二项分布中提取的数据。例如，如果这两个序列是完全随机的，那么第二个序列中的一个基因的邻居就会从所有其他基因中随机抽取。考虑到我们有6000个基因，并且使用了60个左右的邻域，大约是整个基因的1%，在交叉点intersection恰好有k个邻居的概率是 当邻域的大小是基因总数的1%时观察0个邻居的预期频率大约是0.547;观察一个邻居的期望频率是0.332;两个邻居的频率大约是0.099，这使得观察三个或更多的邻居的期望频率只有0.022。对于6000个基因来说，随机状态下，只有132个基因在两个随机序列之间有两个以上的共同之处，而实际上有几千个基因被观察到。因此，柱状图和视觉比较表明，我们的排列组合之间的差异与随机的差别非常大。 图9显示了来自不同初始条件的6个典型的序列。第一个序列的布局是用手和颜色勾勒出来的。在其他的序列布局中，这些相同的基因被跟踪观察它们的相对位置是如何变化的。两个引人注目的模式出现了。在一个案例中，尽管有不同的随机种子，但聚类与第一个的聚类几乎完全相同。在第二种情况下，产生的聚类是初始聚类的镜像。这种镜像是非常合理的，因为只要保持相对距离，就没有理由期望任何首选的自然位置，所以旋转和镜像应该是很正常且可以被观察到的。柱状图显示了镜像布局的良好的邻域特性（符合先前的预期）。 对结构的更密切关注确实揭示了一些大的变化，例如在图9中，我们注意到红色聚类从内部翻转到外部的位置。这个红色的聚类有一些非常相似的连接，将它与山脊连接起来，如图3所示。因此，它可以很容易地出现在山脊的相反方向/镜像位置。请注意，邻域分析只会在两个的边界frontiers上发现一些差异。正如预期的那样，柱状图显示的两个序列在邻里分析上几乎没有差别。最令人鼓舞的事实是，大多数群体不仅在不同的初始条件下维持他们的相对位置，而且他们还保持着相似的聚类形状，这表示良好的内部一致性，这又一次得到了直方图的支持。这些结果表明，当呈现同一数据集时，序化工具具有健壮的稳定性。有了这些信息，我们就开始了对相似数据的微小变化如何影响聚类位置的研究。 理想情况下，人们会想要一种序化算法，它能对相似点的细微变化做出反应，即在布局中产生微小的变化。在某种程度上，其会影响布局从有序的状态到完全无序的、高熵的状态，因为相似点与越来越多的噪音混合在一起。图10显示了基于实际相似性的初始聚类，以及在相关性中添加了越来越多的噪声的4个情况。图11-14是相应的直方图，反映了与不断增加的噪声相关的变化。值得注意的是，随着噪音的增加，一些大的结构仍然完好无损，但是有些，例如紫色和棕色的簇会变得更加无序。它们本质上随着噪音的增加而融化。另外，注意红色和绿色的点簇显然更能抵抗噪音。这个熔化的比喻特别合适，因为它反映了在聚类群开始分裂之前必须进行内部顺序的随机化。 将越来越多的噪声与相似点similarities混合在一起，可以快速地看到哪些聚簇更有可能是工件artifact;这些是在最小的噪音中融化的聚类。这些信息很容易获得，我们相信它应该是基于聚类的每个分析的一部分。 结论理解和使用稳定性是这里提出的重要主题。尤其重要的是: 确保聚类工具对随机起始条件的稳定性 确保可能的聚类的范围是充分覆盖(通过系统地搜索一个大范围的选择,或使用一个工具,不需要先验判断聚类的数量) 使用聚类工具应对逐渐添加噪声的反映来深入了解聚簇的实际强度。 The two important analysis strategies presented here are: (1) use a probability weighted transformation of the correlation coefficients for the similarities, and (2) compute a small series of clusters with similarities mixed with increasing noise.这里提出的两个重要的分析策略是： 使用相似度的概率加权变换 计算一个具有相似性和不断增加的噪声的小系列的聚类。 第一种策略可以更好地分离集群，而第二种策略则能观察单个聚类的强度。我们还展示了一种有用的视觉方法，通过在一个碱基序列中对基因进行着色，并遵循这些彩色基因在其他序列中的相对运动，来跟踪另一个聚类的效果。最后，我们提出了一个基于不同聚类下基因的本地邻居的交集的统计指标。 致谢我们要感谢查克迈耶斯的开始和资助在这个富有成果的领域的工作;玛格丽特沃纳-沃什伯恩和斯图亚特金帮助使这成为微阵列分析的有用工具;其余的开发团队包括布鲁斯亨德里克森、大卫约翰逊和海伦科勒。我们也要感谢审查人员的有益评论和更正。 References[ 1 ] Davidson, G.S., Hendrickson, B., Johnson, D.K., Meyers, C.E. &amp; Wylie, B.N. Knowledge mining with VxInsight: discovery through interaction . Journal of Intelligent Information Systems 11, 1998, 259-285.[ 2 ] Spellman, P.T., Sherlock, G., Zhang, M.Q., Iyer, V.R., Anders, K., Eisen, M.B., Brown, P.O., Botstein, D. and Futcher, B., Comprehensive identification of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization , Mol. Biology of the Cell, 1998, 9:3273-3297.[ 3 ] Wise, J.A., Thomas, J.J., Pennock, K., Lantrip, D., Pottier, M., Schur, A., &amp; Crow, V. Visualizing the Non-Visual: Spatial Analysis and Interaction with Information from Text Documents , Proceedings of InfoVis ‘95, IEEE, 1995, 51-58.[ 4 ] Kohonen, T. Self-organized formation of topologically correct feature maps . Biological Cybernetics, 1982, 43:59-69.[ 5 ] MacQueen, J. Some methods for classification and analysis of multivariate observations . Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability. Volume I: Statistics,. University of California Press, Berkeley and Los Angeles, CA, 1967, pages 281-297[ 6 ] York, J., Bohn, S., Pennock, K., &amp; Lantrip, D. Clustering and Dimensionality Reduction in SPIRE . Symp. on Advanced Intelligence Processing and Analysis, (1995), 73[ 7 ] Wise, J.A. The ecological approach to text visualization . Journal of the American Society for Information Science 50(13), (1999), 1224-1233.[ 8 ] Wilcox, R.R., Introduction to Robust Estimation and Hypothesis Testing , Academic Press, 1997, ISBN 0-12-751545-3[ 9 ] Ostel, B., Statistics In Research Basic Concepts and Techniques for Research Workers , Iowa State University Press, Ames, Iowa, USA, 1963[ 10 ] Fisher, R.A., On the probable error of a coefficient of correlation deduced from a small sample . Metron., 1921, 1 (No.4):3.[ 11 ] Eades, P., A heuristic for graph drawing , Congressus Numerantium, 42, 1984, 149-160[ 12 ] Fruchtermann, T. and Rheingold, E. Graph drawing by force-directed placement . Technical Report UIUCDCS-R-90- 1609, Computer Science, Univ. Illinois, Urbana-Champagne, Il., 1990[ 13 ] Quinn, N. and Breur M., A force directed component placement procedure for printed circuit boards , IEEE Trans on Circuits and Systems, 1979, CAS-26, (6), 377-388[ 14 ] Otten, R. and van Ginneken, L., The Annealing Algorithm , Kluwer Academic Publishers, Boston MA., 1989[ 15 ] Kamada, T. and Kawai, S., Automatic display of network structures for human understanding , Technical Report 88-007, Department of Information Science, Tokyo University, 1988[ 16 ] Davidson, R. and Harel, D., Drawing graphs nicely using simulated annealing , Technical Report CS89-13, Department of Applied Mathematics and Computer Science, The Weizmann Institute, Rehovot, Israel., 1989[ 17 ] Kamada, T. and Kawai, S., An algorithm for drawing general undirected graphs , Information Processing Letters, 1989, 31, (1), 7-15[ 18 ] Kamada, T. and Kawai, S., A simple method for computing general position is displaying three-dimensional objects , Computer Vision, Graphics, and Image Processing, 1988, 41, 43-56[ 19 ] Kirkpatrick, S. Gelatt, C.D. and Vecchi, M.P., Optimization by simulated annealing , Science, 1983, 220, (4598), 671-680","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"布局算法","slug":"布局算法","permalink":"https://www.cz5h.com/tags/%E5%B8%83%E5%B1%80%E7%AE%97%E6%B3%95/"}]},{"title":"Gephi源码的调试及Git同步","slug":"2018-4-9 Gephi源码的调试及Git同步","date":"2018-04-08T22:00:00.000Z","updated":"2020-02-29T18:43:55.514Z","comments":true,"path":"article/3f8f.html","link":"","permalink":"https://www.cz5h.com/article/3f8f.html","excerpt":"Fork原始Gephi项目 进入Gephi的github地址：https://github.com/gephi/gephi 点击右上角的fork按钮将其fork到自己的github中：","text":"Fork原始Gephi项目 进入Gephi的github地址：https://github.com/gephi/gephi 点击右上角的fork按钮将其fork到自己的github中： fork完毕后我们就在自己的github中有了完整的备份： 然后点击上图中绿色的Clone按钮，注意必须在自己的备份中进行Clone以便后期提交自己的修改；（如何确认本地或源只需看左上角的根目录名称和有无fork标记） 将上图中的https地址复制待用；关于如何将代码Clone到本地，有多种方法，可以选用NetBeans自带的Clone功能： 不过我这里连接github时总是出错（如下），所以使用了git for windows来进行克隆。 Git for Windows 的安装 **在Windows上使用Git，可以从Git官网直接下载安装程序(Setup)，（网速慢的同学请移步国内镜像），然后按默认选项安装即可。安装完成后，在开始菜单里找到“Git”-&gt;“GitBash”，蹦出一个类似命令行窗口的东西，就说明Git安装成功！install-git-on-windows安装完成后，还需要最后一步设置，在命令行输入：$ git config --global user.name &quot;Your Name&quot;$ git config --global user.email &quot;email@example.com&quot;因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。这里的名字和地址对应你最常用的git仓库的用户名和密码（比如这里就是用Github的）注意git config命令的–global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。** 克隆项目到本地 继续上述过程，首先进入到NetBeans的项目空间目录（不是必须的）。然后单机鼠标右键，这时应该有Git Bush Here的选项，点击后在弹出的黑框中输入如下命令： git clone是克隆项目的指令https://github.com/TianZonglin/gephi对应上文中复制备用的https的地址YourProjectName对应你想要给你的项目起的名字，如果不带这个参数，则会默认一个文件名； 克隆完成后文件夹如下所示： NetBeans关联Maven现在开始使用NetBeans进行操作，由于Gephi的源代码使用的Maven进行构建的，所以首先需要在本机安装Maven（与Netbeans无关），Windows安装Maven的教程如下： Windows下的Maven下载与安装 前往https://maven.apache.org/download.cgi下载最新版的Maven程序： 将文件解压到D:\\Program Files\\Apache\\maven目录下: 新建环境变量MAVEN_HOME，赋值D:\\Program Files\\Apache\\maven 编辑环境变量Path，追加%MAVEN_HOME%\\bin; 至此，maven已经完成了安装，我们可以通过DOS命令mvn -v检查一下我们是否安装成功：配置Maven本地仓库 在D:\\Program Files\\Apache\\目录下新建maven-repository文件夹，该目录用作maven的本地库。 打开D:\\Program Files\\Apache\\maven\\conf\\settings.xml文件，查找下面这行代码：/path/to/local/repolocalRepository节点默认是被注释掉的，需要把它移到注释之外，然后将localRepository节点的值改为我们在之前创建的目录D:\\Program Files\\Apache\\maven-repository。 localRepository节点用于配置本地仓库，本地仓库其实起到了一个缓存的作用，它的默认地址是 C:\\Users\\用户名.m2。当我们从maven中获取jar包的时候，maven首先会在本地仓库中查找，如果本地仓库有则返回；如果没有则从远程仓库中获取包，并在本地库中保存。此外，我们在maven项目中运行mvn install，项目将会自动打包并安装到本地仓库中。 运行一下DOS命令mvn help:system如果前面的配置成功，那么D:\\Program Files\\Apache\\maven-repository会出现一些文件。** 需要将NetBeans和本地的Maven进行关联，点击 工具-&gt;选项-&gt;Java-&gt;Maven，在页面中修改Maven主目录，浏览本地的Maven目录并选定，如果正常，则会显示如下： NetBeans打开Gephi源码 Maven配置成功后，依次点击 文件-&gt;打开项目-&gt;找到之前克隆的项目的文件，然后会发现有特殊的[ma]图标，这是Maven项目的标识。双击打开即可： 打开之后会发现项目名称为gephi，后跟[master]说明是从主分支上克隆的，并且已经被关联git。初次打开时由于需要加载gephi的依赖文件（Maven），所以会有一段时间处于读条状态（右下角）。第一次运行需要进行构建，这时会下载一些本地Maven库中不存在的jar包，所以可能构建过程很慢。 NetBeans修改、执行、同步Gephi源代码 执行：由于NetBeans的模块化构建，使得gephi整个项目没有一个传统的Main函数入口，这里的入口，在位于gephi源代码项目目录下的模块内的gephi-app模块，双击后即可独立的打开该模块，然后右键选项中点击运行即可打开Gephi主界面。 上述打开的Gephi就是具有完整功能的客户端。 修改布局源代码：类似的，找到模块中的LayoutPlugin，双击打开，然后在源包中就是全部的布局算法的源码了，针对具体的代码文件进行修改即可。一般的调试源码的步骤：修改源码-&gt;运行gephi-app，如果修改无效，请在修改源码后，对LayoutPlugin模块先进行一次构建 同步：回到开头的内容，使用Git的主要目的是进行版本的控制，这对于对源码的修修补补来说显得尤为重要，现在已经对源码进行了修改，可以按下面的操作将修改同步到自己fork的github中： 3.1. 右键所修改的模块-&gt;点击Git-&gt;点击提交-&gt;添加修改的备注-&gt;点击提交 3.2. 右键所有该的模块-&gt;点击Git-&gt;点击远程-&gt;点击推入-&gt;选择配置的Git资源库位置，如果没有默认资源库则需要配置-&gt;全部下一步结束","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"Maven","slug":"Maven","permalink":"https://www.cz5h.com/tags/Maven/"},{"name":"GitHub","slug":"GitHub","permalink":"https://www.cz5h.com/tags/GitHub/"},{"name":"NetBeans","slug":"NetBeans","permalink":"https://www.cz5h.com/tags/NetBeans/"}]},{"title":"IDEA如何在项目中快速添加Maven依赖","slug":"2018-3-19 IDEA如何在项目中快速添加Maven依赖","date":"2018-03-18T23:00:00.000Z","updated":"2020-02-29T18:43:55.507Z","comments":true,"path":"article/663c.html","link":"","permalink":"https://www.cz5h.com/article/663c.html","excerpt":"前言在日常项目开发中，组件的引入是很平常的事情，一般来说，我们的项目由Maven构建，然后在需要新引入一个依赖时，只需在pom.xml中添加依赖描述即可，但是，有时我们的项目未必采用Maven构建，比如Spark项目就多采用sbt，或者直接添加jar包的方式，这时，如果需要添加某个外部依赖如果采用添加jar包的方式就会非常繁琐，不过IDEA为我们提供了方便的添加方式。","text":"前言在日常项目开发中，组件的引入是很平常的事情，一般来说，我们的项目由Maven构建，然后在需要新引入一个依赖时，只需在pom.xml中添加依赖描述即可，但是，有时我们的项目未必采用Maven构建，比如Spark项目就多采用sbt，或者直接添加jar包的方式，这时，如果需要添加某个外部依赖如果采用添加jar包的方式就会非常繁琐，不过IDEA为我们提供了方便的添加方式。 现在，以Spark项目中添加gephi-toolkit为例，来说明整个添加流程，具体流程如下： 流程 右键项目名称，点击Open Module Settings，转到Dependencies 点击右侧绿色的加号，并选择 Library 在弹出页面选择下面的New Library中的Frome Maven 在弹出页面输入gephi-tookit，点击搜索，然后选择对应的版本 之后，注意选择该Library的级别为Project即可 选完一定要点Add Selected 最后，查看dependencies，我们发现toolkit已经被添加到项目","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"IDEA","slug":"IDEA","permalink":"https://www.cz5h.com/tags/IDEA/"},{"name":"Maven","slug":"Maven","permalink":"https://www.cz5h.com/tags/Maven/"}]},{"title":"NameNode HA：如何防止集群脑裂现象","slug":"2018-3-11 NameNode HA：如何防止集群脑裂现象","date":"2018-03-10T23:00:00.000Z","updated":"2020-02-29T18:43:55.506Z","comments":true,"path":"article/aec3.html","link":"","permalink":"https://www.cz5h.com/article/aec3.html","excerpt":"转自：http://www.cnblogs.com/shenh062326/p/3870219.html作者：南国故人 社区hadoop2.2.0 release版本开始支持NameNode的HA，本文将详细描述NameNode HA内部的设计与实现。","text":"转自：http://www.cnblogs.com/shenh062326/p/3870219.html作者：南国故人 社区hadoop2.2.0 release版本开始支持NameNode的HA，本文将详细描述NameNode HA内部的设计与实现。 为什么要Namenode HA？ NameNode High Availability即高可用。 NameNode 很重要，挂掉会导致存储停止服务，无法进行数据的读写，基于此NameNode的计算（MR，Hive等）也无法完成。 Namenode HA 的实现、技术难题 如何保持主和备NameNode的状态同步，并让Standby在Active挂掉后迅速提供服务，namenode启动比较耗时，包括加载fsimage和editlog（获取file to block信息），处理所有datanode第一次blockreport（获取block to datanode信息），保持NN的状态同步，需要这两部分信息同步。 脑裂（split-brain），指在一个高可用（HA）系统中，当联系着的两个节点断开联系时，本来为一个整体的系统，分裂为两个独立节点，这时两个节点开始争抢共享资源，结果会导致系统混乱，数据损坏。 NameNode切换对外透明，主Namenode切换到另外一台机器时，不应该导致正在连接的客户端失败，主要包括Client，Datanode与NameNode的链接。 社区NN的HA架构、原理、机制 非HA的Namenode架构，一个HDFS集群只存在一个NN，DN只向一个NN汇报，NN的editlog存储在本地目录。 社区的NN HA包括两个NN，主（active）与备（standby），ZKFC，ZK，share editlog。流程：集群启动后一个NN处于active状态，并提供服务，处理客户端和datanode的请求，并把editlog写到本地和share editlog（可以是NFS，QJM等）中。另外一个NN处于Standby状态，它启动的时候加载fsimage，然后周期性的从share editlog中获取editlog，保持与active的状态同步。为了实现standby在sctive挂掉后迅速提供服务，需要DN同时向两个NN汇报，使得Stadnby保存block to datanode信息，因为NN启动中最费时的工作是处理所有datanode的blockreport。为了实现热备，增加FailoverController和ZK，FailoverController与ZK通信，通过ZK选主，FailoverController通过RPC让NN转换为active或standby。 关键问题： 保持NN的状态同步，通过standby周期性获取editlog，DN同时想standby发送blockreport。 防止脑裂共享存储的fencing，确保只有一个NN能写成功。使用QJM实现fencing，下文叙述原理。datanode的fencing。确保只有一个NN能命令DN。HDFS-1972中详细描述了DN如何实现fencing 每个NN改变状态的时候，向DN发送自己的状态和一个序列号。 DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接收到这个返回是认为该NN为新的active。 如果这时原来的active（比如GC）恢复，返回给DN的心跳信息包含active状态和原来的序列号，这时DN就会拒绝这个NN的命令。 特别需要注意的一点是，上述实现还不够完善，HDFS-1972中还解决了一些有可能导致误删除block的隐患，在failover后，active在DN汇报所有删除报告前不应该删除任何block。 客户端fencing，确保只有一个NN能响应客户端请求。让访问standby nn的客户端直接失败。在RPC层封装了一层，通过FailoverProxyProvider以重试的方式连接NN。通过若干次连接一个NN失败后尝试连接新的NN，对客户端的影响是重试的时候增加一定的延迟。客户端可以设置重试此时和时间。 ZKFC的设计 FailoverController实现下述几个功能 监控NN的健康状态 向ZK定期发送心跳，使自己可以被选举。 当自己被ZK选为主时，active FailoverController通过RPC调用使相应的NN转换为active。 为什么要作为一个deamon进程从NN分离出来 防止因为NN的GC失败导致心跳受影响。 FailoverController功能的代码应该和应用的分离，提高的容错性。 使得主备选举成为可插拔式的插件。 FailoverController主要包括三个组件， HealthMonitor 监控NameNode是否处于unavailable或unhealthy状态。当前通过RPC调用NN相应的方法完成。 ActiveStandbyElector 管理和监控自己在ZK中的状态。 ZKFailoverController 它订阅HealthMonitor 和ActiveStandbyElector 的事件，并管理NameNode的状态。 QJM的设计 Namenode记录了HDFS的目录文件等元数据，客户端每次对文件的增删改等操作，Namenode都会记录一条日志，叫做editlog，而元数据存储在fsimage中。为了保持Stadnby与active的状态一致，standby需要尽量实时获取每条editlog日志，并应用到FsImage中。这时需要一个共享存储，存放editlog，standby能实时获取日志。这有两个关键点需要保证， 共享存储是高可用的，需要防止两个NameNode同时向共享存储写数据导致数据损坏。 是什么，Qurom Journal Manager，基于Paxos（基于消息传递的一致性算法）。这个算法比较难懂，简单的说，Paxos算法是解决分布式环境中如何就某个值达成一致，（一个典型的场景是，在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个”一致性算法”以保证每个节点看到的指令一致） 如何实现 初始化后，Active把editlog日志写到2N+1上JN上，每个editlog有一个编号，每次写editlog只要其中大多数JN返回成功（即大于等于N+1）即认定写成功。 Standby定期从JN读取一批editlog，并应用到内存中的FsImage中。 如何fencing： NameNode每次写Editlog都需要传递一个编号Epoch给JN，JN会对比Epoch，如果比自己保存的Epoch大或相同，则可以写，JN更新自己的Epoch到最新，否则拒绝操作。在切换时，Standby转换为Active时，会把Epoch+1，这样就防止即使之前的NameNode向JN写日志，也会失败。 写日志： NN通过RPC向N个JN异步写Editlog，当有N/2+1个写成功，则本次写成功。 写失败的JN下次不再写，直到调用滚动日志操作，若此时JN恢复正常，则继续向其写日志。 每条editlog都有一个编号txid，NN写日志要保证txid是连续的，JN在接收写日志时，会检查txid是否与上次连续，否则写失败。 读日志： 定期遍历所有JN，获取未消化的editlog，按照txid排序。 根据txid消化editlog。 切换时日志恢复机制 主从切换时触发 准备恢复（prepareRecovery），standby向JN发送RPC请求，获取txid信息，并对选出最好的JN。 接受恢复（acceptRecovery），standby向JN发送RPC，JN之间同步Editlog日志。 Finalized日志。即关闭当前editlog输出流时或滚动日志时的操作。 Standby同步editlog到最新 如何选取最好的JN 有Finalized的不用in-progress 多个Finalized的需要判断txid是否相等 没有Finalized的首先看谁的epoch更大 Epoch一样则选txid大的。 参考 https://issues.apache.org/jira/secure/attachment/12480489/NameNode%20HA_v2_1.pdf https://issues.apache.org/jira/secure/attachment/12521279/zkfc-design.pdf https://issues.apache.org/jira/secure/attachment/12547598/qjournal-design.pdf https://issues.apache.org/jira/browse/HDFS-1972 https://issues.apache.org/jira/secure/attachment/12490290/DualBlockReports.pdf http://svn.apache.org/viewvc/hadoop/common/branches/branch-2.2.0/ http://yanbohappy.sinaapp.com/?p=205 转自：http://www.cnblogs.com/shenh062326/p/3870219.html作者：南国故人","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"集群","slug":"集群","permalink":"https://www.cz5h.com/tags/%E9%9B%86%E7%BE%A4/"},{"name":"转载","slug":"转载","permalink":"https://www.cz5h.com/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"HA","slug":"HA","permalink":"https://www.cz5h.com/tags/HA/"},{"name":"脑裂","slug":"脑裂","permalink":"https://www.cz5h.com/tags/%E8%84%91%E8%A3%82/"}]},{"title":"SourceInsight-强大的代码编辑和浏览工具-中文教程","slug":"2018-3-5 SourceInsight-强大的代码编辑和浏览工具-中文教程","date":"2018-03-04T23:00:00.000Z","updated":"2020-02-29T18:43:55.506Z","comments":true,"path":"article/bb72.html","link":"","permalink":"https://www.cz5h.com/article/bb72.html","excerpt":"Source Insight概述 是一个面向项目开发的程序编辑器和代码浏览器，它拥有内置的对C/C++，C#和Java等程序的分析。能分析源代码并在工作的同时动态维护它自己的符号数据库，并自动显示有用的上下文信息。 提供了最快速的对源代码的导航和任何程序编辑器的源信息。 提供了快速的访问源代码和源信息的能力。 自动创建并维护它自己高性能的符号数据库，包括函数、method、全局变量、结构、类和工程源文件里定义的其它类型的符号。其以迅速地更新的文件信息，即使在编辑代码的时候。而且符号数据库的符号可以自动创建到的工程文件中。 总而言之，SI是一款强大的代码阅读、编辑工具。","text":"Source Insight概述 是一个面向项目开发的程序编辑器和代码浏览器，它拥有内置的对C/C++，C#和Java等程序的分析。能分析源代码并在工作的同时动态维护它自己的符号数据库，并自动显示有用的上下文信息。 提供了最快速的对源代码的导航和任何程序编辑器的源信息。 提供了快速的访问源代码和源信息的能力。 自动创建并维护它自己高性能的符号数据库，包括函数、method、全局变量、结构、类和工程源文件里定义的其它类型的符号。其以迅速地更新的文件信息，即使在编辑代码的时候。而且符号数据库的符号可以自动创建到的工程文件中。 总而言之，SI是一款强大的代码阅读、编辑工具。 现有资料的不足Source Insight作为一款功能强大的代码编辑查看软件，网上教程很多，但是都侧重某一方面展开介绍，很少有比较系统的完整软件的介绍，写这篇文档的目的就是尝试完成这项工作。此文档围绕Source Insight实际操作的情景，介绍了该软件的大部分功能，对于使用者来说，其提供了： 快速使用该软件的入门教程 对该软件更进一步的详细介绍（对置顶菜单栏、工具栏以及右键菜单栏的功能进行了解释说明） 以实例的形式介绍了一些Source Insight的高级扩展功能 写在最后由于本人能力有限，加之时间紧迫，虽然做了很大努力但是所涉及的依然能涵盖Source Insight功能的全部，而且有些地方的描述可能有误，有些地方的描述可能并不清楚，种种不足还请读者原谅。如果文中出现了错误描述，还请务必在本文最后留言告知，或者使用邮件进行联系。 文档原文请等待下方插件加载完毕后点击插件右上角下载按钮进行下载，如果加载出错（出现空白）请直接点击查看全文","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"SourceInsight3","slug":"SourceInsight3","permalink":"https://www.cz5h.com/tags/SourceInsight3/"},{"name":"中文教程","slug":"中文教程","permalink":"https://www.cz5h.com/tags/%E4%B8%AD%E6%96%87%E6%95%99%E7%A8%8B/"}]},{"title":"谈一谈本站的来历","slug":"2017-12-12 谈一谈本站的来历","date":"2017-12-11T23:00:00.000Z","updated":"2020-02-29T18:43:55.460Z","comments":true,"path":"article/b1f1.html","link":"","permalink":"https://www.cz5h.com/article/b1f1.html","excerpt":"写在前面许久之前，大概是在13年我接触到了CSDN，知道了有技术博客这么一回事，并且在14年9月20号我在CSDN写了我的第一篇博客文章，当时只能使用富文本编辑，因此文章里有时候还会解析出html的代码，虽然不影响阅读，但是体验很不好，后来CSDN也支持Markdown编辑了，但总归感觉写文章非常不方便，一是费事需要打开浏览器的页面进行编辑，另一个原因是杂乱的东西太多包括csdn的文章推荐等等，这都使得我希望找到一种新的写文章的方式，可以及时、快速、舒服的进行记录，并且还能够发布出去供所有人访问。","text":"写在前面许久之前，大概是在13年我接触到了CSDN，知道了有技术博客这么一回事，并且在14年9月20号我在CSDN写了我的第一篇博客文章，当时只能使用富文本编辑，因此文章里有时候还会解析出html的代码，虽然不影响阅读，但是体验很不好，后来CSDN也支持Markdown编辑了，但总归感觉写文章非常不方便，一是费事需要打开浏览器的页面进行编辑，另一个原因是杂乱的东西太多包括csdn的文章推荐等等，这都使得我希望找到一种新的写文章的方式，可以及时、快速、舒服的进行记录，并且还能够发布出去供所有人访问。 （本人也是小白选手一枚，各方面都还可也但远不及精通境界，对于我这一类的人，估计大部分都是懒得出奇，纵使有豪情壮志，往往做的时候要干什么都忘得一干二净了。所以非常需要及时记录所见所学，当然如果能稍加修饰，与他人分享，那也算是赠人玫瑰手留余香，岂不美哉） 有道云笔记这是我做出改变的第一步，抛弃了CSDN拥抱了有道云笔记，虽然这只是众多笔记软件中的一个，但是也带给我了很多方便，尤其是在16年下半年我开始了研究生生涯，有道云笔记在平时需要记录的时候真的是帮了我大忙了：随手就可以编辑，Windows版，网页版还有App都可以进行编辑，这些都非常给力，当然也可以指定文章进行分享，别人访问链接就可以查看文章，还有也是支持有道云笔记用户的分组共享，这也是非常方便的。但是，不足显而易见，别人访问的只是链接，这是硬伤；另外我一直觉着他的Markdown支持一般，所以因为这一点我也找到了我的下一个归宿。 Cmd Markdown 这应该算是个小众软件了，当然也是众多Markdown编辑器中并不起眼的一个，但是俗话说先入为主，况且他提供的功能也足够我使用了，Markdown的解析超越了标准解析器，虽然这有可能会在文章转移时出现一些问题，但这并不影响其在软件内的展示效果，分屏实时展示、编辑自动同步、发布、更新这些功能自然是必备的，而且非常的简洁，整个客户端非常的轻便，曾经一度非常享受使用这款软件来编写Markdown的文章，但是但是但是。美中不足的是，分享之后仍然是简单的单文章共享，虽然提供了整个目录的入口，但是访问体验还是不好，而且移动端的访问非常丑陋，终于终于，在几个星期前的某一天，我踏上了搭建博客的道路。 WordPress讲真，虽然是博客大户，但是我并没有花很多时间在它上边，很重要的原因是它需要有独立的空间，属于带后台的、有完整用户权限的博客系统，虽然功能强大，但是个人使用，太累赘了，不够轻量，所以放弃了。 Jekyll 说起这个，我差点就上车了，因为GitHub现在直接官方支持用户生成Jekyll博客了，但是由于GitHub上的几款主题实在是太丑啦，当时就让我丢掉了兴趣，又恰逢接触到了Hexo，然后看到了一些比较言论，所以就抛弃之了。 Hexo为什么选择Hexo?概括的说，是因为：安装快、易用、主题多实用又好看、圈子活跃。 安装快除了nodejs的下载需要耗费点时间，其他我找不出需要时间的地方了 易用默认的主题虽然巨丑无比，但是直接就能用，如果node和hexo安装顺利，几乎一小时以内就可以完成你的博客搭建工作，而且更换主题也非常的方便，直接[git clone`然后修改配置文件就大功告成，整个过程非常流畅。 主题实用又好看关于Hexo的主题，真的是太多了，当然这么多主题绝对有你喜欢的，主题大都在Github开源且配有详细的配置说明，让你可以放心的为自己的博客更换不同的主题风格。 圈子活跃确实是的，包括最广为人知的Next主题在内的众多主题，在其Github项目中会发现有很多的issue，都是一些使用时出现的问题等等，当然大部分都是closed已解决状态，如果你在使用某主题时出现了问题，最直接的方式就是去Github询问作者本人。 写在中间大概在几周之前就搭好了博客，但苦于没有好看的主题，找到indigo后，终于安了心，于是乎一口气把之前Cmd Markdown里所有的存货还有些有道云笔记的历史遗留都搬到Hexo上了，当然CSDN还有一堆，但是我懒得动了。搭建好Hexo，换好主题后，终于可以舒舒服服的写文章了，无关乎平台，也不需要登录。此时此刻，我使用Cmd Markdown写的这篇文章，写完后导出到Hexo的 _post文件夹中，然后直接运行我放在桌面的deploy.bat（里面只有一条命令那就是 hexo clean &amp;&amp; hexo g &amp;&amp; hexo d），稍等片刻，全世界的人就可以在本博客访问到这篇文章啦！是不是非常方便 :) 写在最后关于自己的博客，当然需要精心打扮，上面说的都很基础，其实作为一个独立站点，需要考虑的东西还是很多的，主要有下面一些： 域名的绑定这是外界对整个博客最直观的感受之一，当然不是必须的，但现在一般域名也就几块钱一年，而且瞬间使自己的站点显得高大上，何乐而不为。这里要注意的是我们只需要购买域名就行了，空间实名备案什么的跟咱们没关系，因为博客是直接托管在Github或者Coding上的！ 托管的平台：Hexo+Github VS Hexo+Coding曾经一度我放在Github，但是其更新后的部署速度让我吃惊，大概需要几分钟，访问也不是那么快速，权衡了一下最终还是托管到了Coding上，体验什么的感觉丝毫不输Github，最重要的是国内源就是快。 md文章的解析差异我们都知道Markdown有一套标准解析语法，但是其扩展却有千千万，就拿我现在的Cmd Markdown这款软件来说，‘#文字’之间不需要空格，但是发布在Hexo上则必须是‘# 文字’这种形式。其他差异还有很多，比如#不能紧跟在上一段后，需要加空行等等，这些自己实际试用一下就知道了，而且修改也比较容易。 不可或缺的评论关于评论，我觉得是博客称之为博客的原因之一，如果没有评论功能，那就失去了共同探讨的机会，也就失去了博客本身存在的价值，虽然有很多人关闭了评论，但不能否认多数人还是希望听到别人的声音，无论是意见不同的分歧，还是志同道合的点赞，在我看来这都是不可或缺的。要注意，现在多说死了，网易云评论好像也完蛋了，disqus国内太慢，然后现在好像来比力还可以，除此之外就是基于GitApp的gitalk和gitment了，现在我正在使用gitment作为评论，还是很不错的，看到这里，还不抓紧在下面评论一波！ 必不可少的用户统计现在百度、腾讯、CNZZ、谷歌等都有类似站长统计的服务，另外在我使用的indigo主题中自带有‘不蒜子’的统计，其将PV和UV显示在最下方，另外，也支持前面讲的几种统计，试过水之后我认为百度统计大概对于国内站点来说是最准确好用的了。 当然要听歌现在我把high一下放到博客里了，由于放在header里移动端会太挤（因为我的名字太长了），所以放到左侧menu里了。坑爹的是第一次集成时并不好使，原来音乐源是amazon的，果断找了一个网易云的换上。非常完美。 站点的SEO这方面其实是个人博客与CSDN这类博客最大的差距所在了吧，如果我们想将自己的站点更好的暴露在搜索引擎的视野范围内，那要做的工作还是很多的，这方面对一个博客来讲显得尤为重要。现在我按这篇文章的叙述，进行了一些简单操作。 碰见一个坑：添加谷歌的Search Console死活验证失败，使用GoogleAnalysis关联也是失败的，非常崩溃，直到看到了下图，网站预览图里那几行诗真的是惊到我了，那特喵的是CodingPages的跳转页!!真相原来在这儿。 知道真相之后马上去Coding的项目中的Pages设置中按要求把署名加到了主页上，居然审核要2工作日，吐槽一下。添加之后，博客的footer变成这个样子了，还还凑合把。 其他：想到了再添加","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"博客搭建","slug":"博客搭建","permalink":"https://www.cz5h.com/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"}]},{"title":"对SNAP图数据进行度分布统计","slug":"2017-12-8 对SNAP图数据进行度分布统计","date":"2017-12-07T23:00:00.000Z","updated":"2020-02-29T18:43:55.466Z","comments":true,"path":"article/8a40.html","link":"","permalink":"https://www.cz5h.com/article/8a40.html","excerpt":"统计度分布的数据集 Name Type Nodes Edges Description ego-Facebook Undirected 4,039 88,234 Social circles from Facebook (anonymized) ego-Gplus Directed 107,614 13,673,453 Social circles from Google+ ego-Twitter Directed 81,306 1,768,149 Social circles from Twitter soc-Epinions1 Directed 75,879 508,837 Who-trusts-whom network of Epinions.com soc-LiveJournal1 Directed 4,847,571 68,993,773 LiveJournal online social network soc-Pokec Directed 1,632,803 30,622,564 Pokec online social network soc-Slashdot0811 Directed 77,360 905,468 Slashdot social network from November 2008 soc-Slashdot0922 Directed 82,168 948,464 Slashdot social network from February 2009 wiki-Vote Directed 7,115 103,689 Wikipedia who-votes-on-whom network","text":"统计度分布的数据集 Name Type Nodes Edges Description ego-Facebook Undirected 4,039 88,234 Social circles from Facebook (anonymized) ego-Gplus Directed 107,614 13,673,453 Social circles from Google+ ego-Twitter Directed 81,306 1,768,149 Social circles from Twitter soc-Epinions1 Directed 75,879 508,837 Who-trusts-whom network of Epinions.com soc-LiveJournal1 Directed 4,847,571 68,993,773 LiveJournal online social network soc-Pokec Directed 1,632,803 30,622,564 Pokec online social network soc-Slashdot0811 Directed 77,360 905,468 Slashdot social network from November 2008 soc-Slashdot0922 Directed 82,168 948,464 Slashdot social network from February 2009 wiki-Vote Directed 7,115 103,689 Wikipedia who-votes-on-whom network 数据格式数据集格式大多为A-&gt;B格式，部分数据集的数据注释：wiki-Vote 123456789# Directed graph (each unordered pair of nodes is saved once): Wiki-Vote.txt # 有向图，每个节点的无序对只存在一次，即无重复# Wikipedia voting on promotion to administratorship (till January 2008). Directed edge A-&gt;B means user A voted on B becoming Wikipedia administrator.# 维基百科投票晋升到管理人的职位（直到2008年1月）。定向边缘A-B意味着用户A给用户B投票# Nodes: 7115 Edges: 103689# FromNodeId ToNodeId30 141230 3352... soc-xx,eg:soc-Epinions1 12345678# Directed graph (each unordered pair of nodes is saved once): soc-Epinions1.txt # 有向图，每个节点的无序对只存在一次，即无重复# Directed Epinions social network# Nodes: 75879 Edges: 508837# FromNodeId ToNodeId0 40 5... ogo-xxx 123数据文件较复杂，一般分为xxx，xxx-combined，readme三个文件；xxx文件一般格式很复杂，具体意义待研究，这里体现网络的使用的是xxx-combined文件，xxx-combind数据格式即A-&gt;B格式 数据集统计相关的数据集统计都在SNAP对应的数据集页面有详细显示 节点度分布的计算从小规模数据集ego-Facebook和wiki-Vote开始计算 A-&gt;B A-&gt;C A-&gt;D B-&gt;D B-&gt;A ind(A)=1,outd(A)=3, d(A)=4 ind(B)=1,outd(B)=2, d(B)=3 ind(C)=1,outd(C)=0, d(C)=1 ind(D)=2,outd(D)=0, d(D)=2第一阶段：统计全部节点的度构造思想**map阶段：** A-&gt;B拆分成&apos;A-&apos;+&apos;-B&apos;,转换为KeyValuePair：&lt;A-,1&gt;和&lt;-B,1&gt; 即结果为：&lt;A-,1&gt;，&lt;-B,1&gt;，&lt;A-,1&gt;，&lt;-C,1&gt;，&lt;A-,1&gt;，&lt;-D,1&gt;，&lt;B-,1&gt;，&lt;-D,1&gt;，&lt;B-,1&gt;，&lt;-A,1&gt; **reduce阶段：** 规约为&lt;A-,3&gt;，&lt;-B,1&gt;...任务流程 启动集群(三虚拟机)，start-all.sh开启hadoop（hdfs） 将源数据加载到hdfs 使用IDEA进行远程作业(mapreduce)提交 返回结果 实际操作：1.上传数据导hdfs12345[hadoop@hadoop01 DATA0001]$ lsfacebook_combined.txt Slashdot0811.txt soc-Epinions1.txt soc-pokec-relationships.txt Wiki-Vote.txt gplus_combined.txt Slashdot0902.txt soc-LiveJournal1.txt twitter_combined.txt[hadoop@hadoop01 ~]$ hadoop fs -mkdir &#x2F;TVCG&#x2F;SNAP[hadoop@hadoop01 ~]$ hadoop fs -put DATA0001 &#x2F;TVCG&#x2F;SNAP&#x2F; #此过程可能会比较慢，会卡几分钟的时间（副本复制，分块等等） 上述意义是：该文件（400多M）被划分成了四个block，400/3=3.x 应该是4个block，正确无误另外，当前块的所在节点为hadoop01，02，04，即此处是容错的三副本，这里可以优化一下，虚拟机小集群其实可以改为1，即取消副本，减少存储开销。 12345# 修改hdfs-site.xml的配置项，修改完需要重启hdfs！ &lt;property&gt; &lt;name&gt;dfs.replication&lt;&#x2F;name&gt; &lt;value&gt;1&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; 修改完后发现所在节点只剩下hadoop01了，因为关闭了三副本策略，所以四个节点只有hadoop01存有数据，同时作为namenode的01节点负担会很重，而且计算时会有网络传输开销，但是：由于是虚拟机集群，存储是很宝贵的，网络传输开销显得不重要，所以这样没毛病（节省两倍空间）。 实际操作：2.构建mapreduce先前的思路在实施起来有一定的复杂性，那么如果需求是计算节点的度，而且不分出度入度，那么可以简化成WordCount问题！ A-&gt;B A-&gt;C A-&gt;D B-&gt;D B-&gt;A d(A)=4, d(B)=3, d(C)=1, d(D)=2第二阶段：对上述度数结果进行计数构造思想mapreduce结果： A 4 B 3 C 2 D 2 度分布为：零次度1、两次度2、一次度3、一次度4实际操作 将mapred结果的key丢弃，只留下value，即度数 对度数进行统计计数 本步骤结果为 &lt;degree_value,degree_count&gt; 操作结果mapreduce结果中的values再统计的结果： &lt;4,1&gt; &lt;3,1&gt; &lt;2,2&gt; 将上述结果画图即整个图的度分布 为了验证度分布符合幂律的特点，可以分别将XY轴取ln底，那么图像会呈现一条斜率为负的倾斜直线，此斜率的大小正是（Y=cX^(-r)）中的幂r的大小，由上面的叙述可知，可以通过对数底坐标轴的呈现图像是否为一条直线来判断度分布是不是符合幂律分布的特性。下面三组是上述九个数据集中度分布十分符合幂律分布的图像。 另外，由于数据及场景的特殊性，度分布并不一定完全符合幂律分布，或者说不可能完全符合，只是接近幂律的程度有大有小，下面三组就是九中数据集中符合幂律分布程度较差的图像。 度分布的统计学特性（以下转自网络） 泊松分布和幂律分布 自然界与社会生活中，许多科学家感兴趣的事件往往都有一个典型的规模，个体的尺度在这一特征尺度附近变化很小。比如说人的身高，中国成年男子的身高绝大多数都在平均值1.70米左右，当然地域不同，这一数值会有一定的变化，但无论怎样，我们从未在大街上见过身高低于10厘米的“小矮人”，或高于10米的“巨人”。如果我们以身高为横坐标，以取得此身高的人数或概率为纵坐标，可绘出一条钟形分布曲线，这种曲线两边衰减地极快；类似这样以一个平均值就能表征出整个群体特性的分布，我们称之为泊松分布。对于另一些分布，像国家GDP或个人收入的分布，情况就大不一样了，个体的尺度可以在很宽的范围内变化，这种波动往往可以跨越多个数量级。幂律分布的示意图如图1右图所示，其通式可写成y=cx^(−r)，其中x，y是正的随机变量，c，r均为大于零的常数。这种分布(幂律分布)的共性是绝大多数事件的规模很小，而只有少数事件的规模相当大。对上式两边取对数，可知lny与lnx满足线性关系lny=lnc-rlnx，也即在双对数坐标下，幂律分布表现为一条斜率为幂指数的负数的直线，这一线性关系是判断给定的实例中随机变量是否满足幂律的依据。泊松分布(左)与“长尾”/幂律分布(右)。从统计物理学来看，网络是一个包含了大量个体及个体之间相互作用的系统。近年来在对复杂网络的研究过程中，科学家们亦发现了众多的幂律分布，虽然这些网络在结构及功能上是如此的千变万化，相差迥异。复杂网络中节点的度值k相对于它的概率P(k)满足幂律关系，且幂指数多在大于2小于3的范围内；这一现象是如此的普遍，如此地令人惊叹不已，以至于人们给具有这种性质的网络起了一个特别的名字——无标度网络。这里的无标度是指网络缺乏一个特征度值（或平均度值），即节点度值的波动范围相当大。在过去的40多年里，科学家们一直想当然地认为现实中的网络都是随机的，随机图论就是专门为了研究随机网络而发展起来的一门数学学科，但无标度特性的发现打破了这种构想。随机网络的度分布是泊松分布，度值比平均值高许多或低许多的节点，都十分罕见，是一种高度“民主”的网络，而无标度网络的度分布则是幂律分布，节点度值相差悬殊，往往可以跨越几个数量级，是一种极端“专制”的网络，二者之间有本质的区别。这两种网络的一个形象化的比较如图所示。具有相同节点数和边数的随机网络（左）和无标度网络（右）。度分布满足幂律的无标度网络还有一个奇特的性质——“小世界”特性，虽然WWW中的页面数已超过80亿，但平均来说，在WWW上只需点击19次超链接，就可从一个网页到达任一其它页面。“小世界”现象在社会学上也称为“六度分离”，它来源于1967年，美国哈佛大学的社会心理学家Milgram的一个实验，这个实验证实，世界上任何两个人，不论他（她）是中国的藏民，非洲的难民，还是美国的政界高层，好莱坞的明星，甚至北极的爱斯基摩人，美洲的土著印第安人，都可通过熟人找熟人的方式建立联系，而两者之间的平均最少“中介”数是6，如此看来，整个地球确实是一个小小的世界。 幂律分布于的判定 Zipf定律与Pareto定律都是简单的幂函数，我们称之为幂律分布；还有其它形式的幂律分布，像名次——规模分布、规模——概率分布，这四种形式在数学上是等价的，幂律分布的示意图如图1右图所示，其通式可写成y=c*x^(-r)，其中x，y是正的随机变量，c，r均为大于零的常数。这种分布的共性是绝大多数事件的规模很小，而只有少数事件的规模相当大。对上式两边取对数，可知lny与lnx满足线性关系，也即在双对数坐标下，幂律分布表现为一条斜率为幂指数的负数的直线，这一线性关系是判断给定的实例中随机变量是否满足幂律的依据。判断两个随机变量是否满足线性关系,可以求解两者之间的相关系数；利用一元线性回归模型和最小二乘法可得lny对lnx的经验回归直线方程，从而得到y与x之间的幂律关系式。下图显示的是一般幂律分布（上一节第一幅图中的右图）在双对数坐标下的图形，由于某些因素的影响，下图前半部分的线性特性并不是很强，而在后半部分（对应于原图的尾部），则近乎为一直线，其斜率的负数就是幂指数。 幂律分布的形成机制 Barabási与Albert针对复杂网络中普遍存在的幂律分布现象，提出了网络动态演化的BA模型，他们解释，成长性和优先连接性是无标度网络度分布呈现幂律的两个最根本的原因。所谓成长性是指网络节点数的增加，像Internet中自治系统或路由器的添加，以及WWW中网站或网页的增加等，优先连接性是指新加入的节点总是优先选择与度值较高的节点相连，比如，新网站总是优先选择人们经常访问的网站作为超链接。随着时间的演进，网络会逐渐呈现出一种“富者愈富，贫者愈贫”的现象。社会学家所说的“马太效应”，《新约》圣经所说的“凡有的，还要加给他，叫他有余”，同优先连接也有某种相通之处。“优先连接”并不适用于所有出现幂律分布的情况，即便是对于某些无标度网络，用它解释幂律的成因也显得很不合理(其他略)。 幂律分布的动力学影响 幂律特性的度分布对无标度网络的动力学性质有着极其深刻的影响。以疾病或病毒在网络中的传播这一物理过程为例，以前的基于规则网络及随机网络的研究表明,疾病的传染强度存在一个阈值，只有传染强度大于这个阈值时,疾病才能在网络中长期存在,否则感染人数会呈指数衰减。但对无标度网络上传染病模型的研究结果表明，不存在类似的阈值，只要传染病发生，就将长时间存在下去，这一特性表明，要想在Internet这样的无标度网络上彻底消灭病毒，即使是已知的病毒，也是不可能的。//区别规则网络、小世界网络、随机网络和无标度网络另外，度分布的幂律特性对网络的容错性和抗攻击能力也有很大的影响，对网络的攻击分为随机攻击和选择性攻击两种类型，分别称为网络的容错能力与抗攻击能力。研究表明，无标度网络具有很强的容错性，但是对基于节点度值的选择性攻击抗攻击能力相当差。比如对万维网或因特网中集散节点的攻击，有可能造成整个网络的瘫痪，对于某些微生物来说，它们体内度值很高的蛋白质通常掌握着细胞的生死。度分布满足泊松分布的随机网络，其容错性和抗攻击能力则是基本相当的。可见，网络的结构稳定性是与网络的度分布特性紧密联系在一起的。","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"图数据","slug":"图数据","permalink":"https://www.cz5h.com/tags/%E5%9B%BE%E6%95%B0%E6%8D%AE/"},{"name":"Mapreduce","slug":"Mapreduce","permalink":"https://www.cz5h.com/tags/Mapreduce/"}]},{"title":"IDEA远程提交hadoop任务时出现的错误","slug":"2017-12-5 IDEA远程提交hadoop任务时出现的错误","date":"2017-12-04T23:00:00.000Z","updated":"2020-02-29T18:43:55.461Z","comments":true,"path":"article/6aea.html","link":"","permalink":"https://www.cz5h.com/article/6aea.html","excerpt":"远程过程中出现的一些错误1Cannot delete &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;hadoop&#x2F;.staging&#x2F;job_1477796535608_0001. Name node is in safe mode. 上述问题解决：Linux集群中的namenode没有关闭safemode","text":"远程过程中出现的一些错误1Cannot delete &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;hadoop&#x2F;.staging&#x2F;job_1477796535608_0001. Name node is in safe mode. 上述问题解决：Linux集群中的namenode没有关闭safemode 1232017-12-05 18:32:27,979 INFO [main] mapred.ClientServiceDelegate (ClientServiceDelegate.java:getProxy(276)) - Application state is completed. FinalApplicationStatus&#x3D;SUCCEEDED. Redirecting to job history serverRetrying connect to server: 192.168.146.130&#x2F;192.168.146.130:10020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)Exception in thread &quot;main&quot; java.io.IOException: java.net.ConnectException: Call From MSI&#x2F;118.202.43.35 to 192.168.146.130:10020 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see: http:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;ConnectionRefused 上述问题解决：开启historyserver服务 mr-jobhistory-daemon.sh start historyserver 1Exception in thread &quot;main&quot; java.io.IOException: Job status not available 上述问题解决：在mapred-site.xml中添加如下配置： 123456789101112&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;&#x2F;name&gt; &lt;value&gt;$&#123;yarn.app.mapreduce.am.staging-dir&#125;&#x2F;history&#x2F;done_intermediate&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.done-dir&lt;&#x2F;name&gt; &lt;value&gt;$&#123;yarn.app.mapreduce.am.staging-dir&#125;&#x2F;history&#x2F;done&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; **注意**：在本地PC可以访问虚拟机集群的 hdfs 监控：xxx.xxx.xxx.xxx:50070 mr job监控：xxx.xxx.xxx.xxx:1988812WebUI无权访问hdfs文件夹&#x2F;tmpPermission denied: user&#x3D;dr.who, access&#x3D;READ_EXECUTE, inode&#x3D;&quot;&#x2F;tmp&quot;:hadoop:supergroup:drwx------ 上述问题解决：hadoop dfs -chmod -R 755 /tmp注意：其显示是弃用的方法，不过仍然有效 hadoop put 机制特别注意！关于hdfs的底层原理（上传一个文件的整个历程）一定要看这个文章、文章的备用连接","categories":[{"name":"Hadoop相关","slug":"Hadoop相关","permalink":"https://www.cz5h.com/categories/Hadoop%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"https://www.cz5h.com/tags/WebSocket/"},{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"布局算法","slug":"布局算法","permalink":"https://www.cz5h.com/tags/%E5%B8%83%E5%B1%80%E7%AE%97%E6%B3%95/"}]},{"title":"Hexo的安装及重置恢复","slug":"2017-12-2 Hexo的安装及重置恢复","date":"2017-12-01T23:00:00.000Z","updated":"2020-02-29T18:43:55.462Z","comments":true,"path":"article/d6bd.html","link":"","permalink":"https://www.cz5h.com/article/d6bd.html","excerpt":"写在前面Hexo博客已经使用挺长时间了，其出色的静态网页渲染能力深得我的喜欢，然鹅也是因为 Hexo 基本完全依赖渲染模板的原因，如果在整博客的过程中引入了错误的代码段或者和已有代码发生了冲突，会直接影响博客的正常渲染，对于Hexo来说，不能渲染就等于完全废了；针对出现的错误，有时候我们还不一定能找得出来。经常是改一处错两处，那么到了迫不得已的时候，就有必要对博客进行重置了。","text":"写在前面Hexo博客已经使用挺长时间了，其出色的静态网页渲染能力深得我的喜欢，然鹅也是因为 Hexo 基本完全依赖渲染模板的原因，如果在整博客的过程中引入了错误的代码段或者和已有代码发生了冲突，会直接影响博客的正常渲染，对于Hexo来说，不能渲染就等于完全废了；针对出现的错误，有时候我们还不一定能找得出来。经常是改一处错两处，那么到了迫不得已的时候，就有必要对博客进行重置了。 关于 Hexo 博客的重置，一般只需重置主题即可，因为 99% 的错误都是在主题文件中的，Hexo主框架自安装完就不会有什么改动。下面，就从重置主题开始，简单记录下我重置过程的代码。 初始化Hexo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106# 回退时可以跳过初始化部分，从主题部分开始重置**H:\\hexo&gt;hexo init myblog**INFO Cloning hexo-starter to H:\\hexo\\myblogCloning into &#39;H:\\hexo\\myblog&#39;...remote: Counting objects: 59, done.remote: Total 59 (delta 0), reused 0 (delta 0), pack-reused 59Unpacking objects: 100% (59&#x2F;59), done.Submodule &#39;themes&#x2F;landscape&#39; (https:&#x2F;&#x2F;github.com&#x2F;hexojs&#x2F;hexo-theme-landscape.git) registered for path &#39;themes&#x2F;landscape&#39;Cloning into &#39;H:&#x2F;hexo&#x2F;myblog&#x2F;themes&#x2F;landscape&#39;...remote: Counting objects: 785, done.remote: Total 785 (delta 0), reused 0 (delta 0), pack-reused 784Receiving objects: 100% (785&#x2F;785), 2.54 MiB | 476.00 KiB&#x2F;s, done.Resolving deltas: 100% (403&#x2F;403), done.Submodule path &#39;themes&#x2F;landscape&#39;: checked out &#39;decdc2d9956776cbe95420ae94bac87e22468d38&#39;INFO Install dependencies错误: 找不到或无法加载主类 installINFO Start blogging with Hexo!**H:\\hexo&gt;cd hexo****H:\\hexo\\hexo&gt;hexo g**INFO Start processingFATAL Something&#39;s wrong. Maybe you can find the solution here: http:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F;troubleshooting.htmlTemplate render error: (unknown path) [Line 8, Column 23] Error: Unable to call &#96;the return value of (posts[&quot;first&quot;])[&quot;updated&quot;][&quot;toISOString&quot;]&#96;, which is undefined or falsey at Object.exports.prettifyError (H:\\hexo\\hexo\\node_modules\\_nunjucks@3.0.1@nunjucks\\src\\lib.js:34:15) at H:\\hexo\\hexo\\node_modules\\_nunjucks@3.0.1@nunjucks\\src\\environment.js:489:31 at new_cls.root [as rootRenderFunc] (eval at _compile (H:\\hexo\\hexo\\node_modules\\_nunjucks@3.0.1@nunjucks\\src\\environment.js:568:24), &lt;anonymous&gt;210:3) at new_cls.render (H:\\hexo\\hexo\\node_modules\\_nunjucks@3.0.1@nunjucks\\src\\environment.js:482:15) at Hexo.module.exports (H:\\hexo\\hexo\\node_modules\\_hexo-generator-feed@1.2.2@hexo-generator-feed\\lib\\generator.js:40:22) at Hexo.tryCatcher (H:\\hexo\\hexo\\node_modules\\_bluebird@3.5.1@bluebird\\js\\release\\util.js:16:23) at Hexo.&lt;anonymous&gt; (H:\\hexo\\hexo\\node_modules\\_bluebird@3.5.1@bluebird\\js\\release\\method.js:15:34) at H:\\hexo\\hexo\\node_modules\\_hexo@3.4.0@hexo\\lib\\hexo\\index.js:340:24 at tryCatcher (H:\\hexo\\hexo\\node_modules\\_bluebird@3.5.1@bluebird\\js\\release\\util.js:16:23) at MappingPromiseArray._promiseFulfilled (H:\\hexo\\hexo\\node_modules\\_bluebird@3.5.1@bluebird\\js\\release\\map.js:61:38) at MappingPromiseArray.PromiseArray._iterate (H:\\hexo\\hexo\\node_modules\\_bluebird@3.5.1@bluebird\\js\\release\\promise_array.js:114:31) at MappingPromiseArray.init (H:\\hexo\\hexo\\node_modules\\_bluebird@3.5.1@bluebird\\js\\release\\promise_array.js:78:10) at MappingPromiseArray._asyncInit (H:\\hexo\\hexo\\node_modules\\_bluebird@3.5.1@bluebird\\js\\release\\map.js:30:10) at Async._drainQueue (H:\\hexo\\hexo\\node_modules\\_bluebird@3.5.1@bluebird\\js\\release\\async.js:138:12) at Async._drainQueues (H:\\hexo\\hexo\\node_modules\\_bluebird@3.5.1@bluebird\\js\\release\\async.js:143:10) at Immediate.Async.drainQueues (H:\\hexo\\hexo\\node_modules\\_bluebird@3.5.1@bluebird\\js\\release\\async.js:17:14) at runCallback (timers.js:789:20) at tryOnImmediate (timers.js:751:5) at processImmediate [as _immediateCallback] (timers.js:722:5)**H:\\hexo\\hexo&gt;cd ..****H:\\hexo&gt;cd myblog****H:\\hexo\\myblog&gt;hexo g**ERROR Local hexo not found in H:\\hexo\\myblogERROR Try running: &#39;npm install hexo --save&#39;**H:\\hexo\\myblog&gt;cnpm install**| [8&#x2F;9] Installing cssom@0.3.xplatform unsupported hexo@3.4.0 › hexo-fs@0.2.2 › chokidar@1.7.0 › fsevents@^1.0.0 Package require os(darwin) not compaible with your platform(win32)[fsevents@^1.0.0] optional install error: Package require os(darwin) not compatible with your platform(win32)√ Installed 9 packages√ Linked 274 latest versions&gt; hexo-util@0.6.1 build:highlight H:\\hexo\\myblog\\node_modules\\_hexo-util@0.6.1@hexo-util&gt; node scripts&#x2F;build_highlight_alias.js &gt; highlight_alias.json√ Run 1 scriptsdeprecate hexo@3.4.0 › swig@1.4.2 This package is no longer maintainedRecently updated (since 2017-10-29): 1 packages (detail see file H:\\hexo\\myblog\\node_modules\\.recently_updates.txt)√ All packages installed (310 packages installed from npm registry, used 29s, speed 39.09kB&#x2F;s, json 283(524.52kB), tarball 608.11kB)**H:\\hexo\\myblog&gt;hexo g**INFO Start processingINFO Files loaded in 207 msINFO Generated: index.htmlINFO Generated: archives&#x2F;index.htmlINFO Generated: fancybox&#x2F;blank.gifINFO Generated: archives&#x2F;2017&#x2F;11&#x2F;index.htmlINFO Generated: fancybox&#x2F;fancybox_sprite.pngINFO Generated: fancybox&#x2F;jquery.fancybox.cssINFO Generated: fancybox&#x2F;fancybox_sprite@2x.pngINFO Generated: fancybox&#x2F;fancybox_loading.gifINFO Generated: fancybox&#x2F;fancybox_loading@2x.gifINFO Generated: fancybox&#x2F;jquery.fancybox.pack.jsINFO Generated: fancybox&#x2F;jquery.fancybox.jsINFO Generated: archives&#x2F;2017&#x2F;index.htmlINFO Generated: fancybox&#x2F;fancybox_overlay.pngINFO Generated: css&#x2F;fonts&#x2F;FontAwesome.otfINFO Generated: fancybox&#x2F;helpers&#x2F;jquery.fancybox-buttons.cssINFO Generated: fancybox&#x2F;helpers&#x2F;jquery.fancybox-thumbs.cssINFO Generated: js&#x2F;script.jsINFO Generated: css&#x2F;fonts&#x2F;fontawesome-webfont.eotINFO Generated: css&#x2F;fonts&#x2F;fontawesome-webfont.woffINFO Generated: css&#x2F;style.cssINFO Generated: fancybox&#x2F;helpers&#x2F;fancybox_buttons.pngINFO Generated: fancybox&#x2F;helpers&#x2F;jquery.fancybox-thumbs.jsINFO Generated: fancybox&#x2F;helpers&#x2F;jquery.fancybox-buttons.jsINFO Generated: fancybox&#x2F;helpers&#x2F;jquery.fancybox-media.jsINFO Generated: 2017&#x2F;11&#x2F;05&#x2F;hello-world&#x2F;index.htmlINFO Generated: css&#x2F;fonts&#x2F;fontawesome-webfont.svgINFO Generated: css&#x2F;fonts&#x2F;fontawesome-webfont.ttfINFO Generated: css&#x2F;images&#x2F;banner.jpgINFO 28 files generated in 546 ms**H:\\hexo\\myblog&gt;hexo s**INFO Start processingINFO Hexo is running at http:&#x2F;&#x2F;localhost:4000&#x2F;. Press Ctrl+C to stop. 主题部分（初始化/重置）Hexo模块化的结构和生成流程决定了Hexo本身和主题theme是分离的，或者说耦合性不大，如果博客的主题在先使用过程中出现了一些无法修改的问题，那么就需要重置一下主题，比如我是用的indigo主题。再次提醒，大部分错误都是theme中的错误导致的，在替换时只需要修改blog目录中的theme文件夹，替换其中的主题即可，blog根目录的东西一般不会出毛病（一般自定义时都是修改主题内的文件）。注：为什么不用版本控制，因为本身对主题修改的地方不多，且直接重新克隆可以保持主题的最新状态。 重置步骤 blog\\theme\\indigo 将这个文件夹重命名（不要删，还有用） blog\\theme 此目录下重新克隆 git clone https://github.com/yscoder/hexo-theme-indigo.git indogo 从原indigo中将一些基本文件替换到新的indigo中 indigo\\ _config.yaml （注意是indigo目录下的配置文件） indigo\\source （全部内容，除了js和css，这两部分可能包含先前的错误） 基本的替换完成后即可运行了（当然现在的版本少了很多自定义内容） 开始恢复自定义代码，这部分每还原一步都要运行一下(本地部署运行)看是否(编译)正常 自定义代码集中在 indigo\\layout indigo\\layout\\ _partial\\menu.ejs indigo\\layout\\ _partial\\footer.ejs indigo\\layout\\ _partial\\plugins\\site-visit indigo\\layout\\ _partial\\plugins\\page-visit indigo\\layout\\ _partial\\plugins\\google-analytics.ejs indigo\\layout\\ _partial\\plugins\\baidu.ejs indigo\\layout\\tag.ejs indigo\\source\\css\\style.css 执行deploy提交git，与git源不冲突，可以正常提交，因为对git来说上述操作都是不可见的。 整个回退过程结束。 正常来说，上述操作可以发现到底是在修改那个文件时出现错误，如果是一些不知道怎么引起的疑难杂症，通过这样的替换工作也可以将版本回退到正常的版本。","categories":[{"name":"Hexo相关","slug":"Hexo相关","permalink":"https://www.cz5h.com/categories/Hexo%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.cz5h.com/tags/Hexo/"},{"name":"Indigo","slug":"Indigo","permalink":"https://www.cz5h.com/tags/Indigo/"},{"name":"重置版本","slug":"重置版本","permalink":"https://www.cz5h.com/tags/%E9%87%8D%E7%BD%AE%E7%89%88%E6%9C%AC/"}]},{"title":"Spark的RDDs相关内容","slug":"2017-11-28 Spark的RDDs相关内容","date":"2017-11-27T23:00:00.000Z","updated":"2020-02-29T18:43:55.423Z","comments":true,"path":"article/f71d.html","link":"","permalink":"https://www.cz5h.com/article/f71d.html","excerpt":"RDDs的介绍Driver program main()方法，RDDs的定义和操作 管理很多节点，称作executors","text":"RDDs的介绍Driver program main()方法，RDDs的定义和操作 管理很多节点，称作executors SparkContext Driver programs通过SparkContext对象访问Spark SparkContext对象代表和一个集群的连接 在Shell中SparkContext是自动创建好的，即sc 12345//使用一下命令进入shell //cd /usr/local/spark/bin //./spark-shellscala&gt; val lines = sc.textFile(\"/home/hadoop/look.sh\")lines: org.apache.spark.rdd.RDD[String] = /home/hadoop/look.sh MapPartitionsRDD[1] at textFile at &lt;console&gt;:24 RDDs(弹性分布式数据集) 上述的lines就是一个弹性分布式数据集（RDD），其可以分布在集群内，但对使用者透明 RDDs是Spark分发数据和计算的基础抽象类 一个RDD代表的是一个不可改变的分布式集合对象 Spark中所有的计算都是通过对RDD的创建、转换、操作完成的 一个RDD由许多分片（partitions）组成，分片可以再不同节点上进行计算 分片是Spark的并行处理单元。Spark顺序的并行处理分片 RDDs的创建通常使用parallelize()函数可以创建一个简单的RDD，测试用（为了方便观察结果）。 12345678910scala&gt; val rdd = sc.parallelize(Array(1,2,2,4),4) 最后一个4指的是并行度，默认是1rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:24scala&gt; rdd.count()res0: Long = 4 scala&gt; rdd.foreach(print)1224scala&gt; rdd.foreach(print)1422注意：上述parallelize()方法就是为了观察数组的作用还有如果集群节点大于一个，由于rdd的分片计算特性，会使两次的遍历结果并不相同 Scala基本知识：(详见Scala学习笔记)小结 Driver program 包含了程序的main方法，整个程序的入口的地方 SparkContext 代表了和集群的连接，一般在程序的开头就出现 RDDs 弹性分布式数据集，代表的就是一个数据集 RDD基本操作之转换（Transformation）RDD的逐元素转换 map()：将map函数应用到RDD的每一个元素，返回一个新的RDD val line2 = line1.map(word=&gt;(word,1)) //word就代表迭代元素 filter():返回只包含filter函数的值组成的新的RDD val line2 = line1.filter(word。contains(“abc”)) //word就代表迭代元素 flatMap():出入一个复杂元素，输出多个简单元素，类似数据的‘压扁’，按照一定的规则（指定函数） 123456789101112131415161718192021222324252627282930scala&gt; val lines = sc.parallelize(Array(\"home Tom\",\"hadoop Jack\",\"look Kim\"))lines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:24scala&gt; lines.foreach(println)home Tomhadoop Jacklook Kim//注意对RDD本身的操作不影响其本身，因为是val定义的常量scala&gt; lines.flatMap(t=&gt;t.split(\" \")) res20: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at flatMap at &lt;console&gt;:27scala&gt; lines.foreach(println)home Tomhadoop Jacklook Kim//必须使用新的常量来接收scala&gt; val newrdd = lines.flatMap(t=&gt;t.split(\" \"))newrdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[7] at flatMap at &lt;console&gt;:26scala&gt; newrdd.foreach(println)homeTomhadoopJacklookKimscala&gt; RDD的集合运算（交集并集）1234567891011121314151617181920212223242526scala&gt; val rdd1 = sc.parallelize(Array(\"one\",\"two\",\"three\"))rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[9] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(Array(\"two\",\"three\",\"three\"))rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; rdd2.distinct().foreach(println)twothreescala&gt; rdd2.union(rdd1).foreach(println)twothreethreeonetwothreescala&gt; rdd2.intersection(rdd1).foreach(println)twothreescala&gt; rdd2.subtract(rdd1).foreach(println)scala&gt; rdd1.subtract(rdd2).foreach(println)one RDDs的基本操作之Action 在RDD上计算出来的一个结果 并把结果返回给driver program，save等等 reduce() 接收一个函数，作用在RDD两个类型相同的元素上，返回新元素 可以实现RDD中元素的累加、计数、和其他类型的聚集操作。 Collect() 遍历整个RDD，想driver program返回RDD内容 需要单机内存能够容纳下（因为需要拷贝给driver） 大数据处理要使用savaAsText方法12345678scala&gt; val rdd = sc.parallelize(Array(1,2,3,4))rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; rdd.collect()res0: Array[Int] = Array(1, 2, 3, 4)scala&gt; rdd.reduce((x,y)=&gt;x+y)res1: Int = 10 take(n) 返回RDD的n个元素（同时尝试访问最少的partitions） 返回的结果是无序的（在单节点时是有序的）12345scala&gt; rdd.take(2)res2: Array[Int] = Array(1, 2)scala&gt; rdd.take(3)res3: Array[Int] = Array(1, 2, 3) top() 排序，默认使用RDD的比较器，可以自定义比较器12scala&gt; rdd.top(2)res7: Array[Int] = Array(4, 3) foreach() 遍历RDD中的每个元素，并执行一次函数，如果为空则仅仅是遍历数据 一般结合print函数来遍历打印几何数据 RDDs的特性血统关系图 Spark维护着RDDs之间的依赖关系和创建关系，叫做血统关系图 Spark使用血统关系图来计算每个RDD的需求和恢复的数据 上述图示中经过了过个操作最后生成了一个RDD，如果badLinesRDD出错数据丢失，那么由于存在完整的血统关系图，所以可以将其恢复 延迟计算（Lazy Evaluation） Spark对RDDs的计算时 在第一次使用action操作的使用触发的 这种方式可以减少数据的传输 Spark内部记实录metedata信息来完成延迟机制 加载数据本身也是延迟的，数据只有在最后被执行action操作时才会被加载 RDD.persist() 持久化 默认每次在RDDs上面进行action操作时，Spark都会重新计算 如果想重复使用一个RDD，就需要使用persist进行缓存，使用unpersist解除缓存 持久化缓存级别： 级别 空间占用 CPU消耗 是否在内存 是否在硬盘 MEMORY_ONLY 高 低 在 不在 MEMORY_ONLY_SER 低 高 在 不在 DISK_ONLY 低 高 不在 在 MEMORY_AND_DISK 高 中 Some Some MEMORY_AND_DISK_SER 低 高 Some Some MEMORY_AND_DISK 内存中放不下往硬盘放 MEMORY_AND_DISK_SER 内存中放不下往硬盘放(序列化的，故CPU消耗较大) 键值对（KeyValue）RDDs创建键值对RDDs 使用map()函数，返回键值对RDD 例如包含多行数据的RDD，将每行数据的第一个单词作为Key123456789101112131415161718192021222324252627282930313233343536373839scala&gt; val lines = sc.textFile(\"/home/hadoop/look.sh\")//注意这是错的，这样默认是取hdfs文件scala&gt; val lines = sc.textFile(\"file:///home/hadoop/look.sh\")//用file://来指明取的系统文件lines: org.apache.spark.rdd.RDD[String] = file:///home/hadoop/look.sh MapPartitionsRDD[5] at textFile at &lt;console&gt;:24scala&gt; lines.foreach(println)#!/bin/bashJarinfo=$(ps -ef|grep java)echo \"$Jarinfo\" | while read Linedo #echo $Line; #echo $&#123;Line##*:&#125; Jarstr=$&#123;Line##*:&#125; Ishere=$(echo $Jarstr | grep $1 ) if [[ \"$Ishere\" != \"\" ]] then echo YES 1&gt; exit 1 fidonescala&gt; val pairs = lines.map(line=&gt;(line.split(\" \")(0),line))pairs: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[6] at map at &lt;console&gt;:26scala&gt; pairs.foreach(println)(#!/bin/bash,#!/bin/bash)(Jarinfo=$(ps,Jarinfo=$(ps -ef|grep java))(echo,echo \"$Jarinfo\" | while read Line)(do,do )( #echo, #echo $Line;)( #echo, #echo $&#123;Line##*:&#125;)( Jarstr=$&#123;Line##*:&#125;, Jarstr=$&#123;Line##*:&#125;)( Ishere=$(echo, Ishere=$(echo $Jarstr | grep $1 ))( if, if [[ \"$Ishere\" != \"\" ]])( then, then)( echo, echo YES 1&gt;)( exit, exit 1 )( fi, fi)(done,done) 键值对常见操作函数 函数名 作用 reduceByKey(func) 把相同key的value进行结合，key不变，是计算 groupByKey(func) 把相同key的value进行分组，key不变，仅分组 combineByKey(,,,) mapValues(func) 将map操作作用于Values，进对Values进行操作 flatMapValues(func) 将flatMap(扩展)操作作用于Values keys 仅返回键的值（RDD.keys） values 仅返回值的值（RDD.values） sortBtKey() 按照Key来排序 123456789101112131415161718192021222324252627282930scala&gt; var rdd = sc.parallelize(Array((1,2),(3,4),(3,6)))rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; rdd.foreach(println)(1,2)(3,4)(3,6)scala&gt; rdd.reduceByKey((x,y)=&gt;x+y).foreach(println)(1,2)(3,10)scala&gt; rdd.groupByKey().foreach(println)(1,CompactBuffer(2))(3,CompactBuffer(4, 6))scala&gt; rdd.sortByKey().foreach(println)(1,2)(3,4)(3,6)scala&gt; rdd.keys.foreach(println)133scala&gt; rdd.values.foreach(println)246 combineByKey()特点：最常用的基于key的聚合函数，返回的类型可以与输入的类型不一样参数：createCombiner,mergeValue,mergeCombiners,partitioner应用：许多基于key的聚合函数都用到了，例如groupByKey底层就应用到了注意： 遍历分片中的元素，元素的key要么之前见过要么没见过 （某个分区）如果是这个分区中的新key，那么就是用createCombiner()函数 （某个分区）如果是这个分区中已经见过的key，那么就是用mergeValue()函数 （全部分区）合计分区结果时，使用mergeCombiner()函数示例：123456789101112131415161718注意：combineByKey(x=&gt;(1,x),,,) // x 表示的是Valuesscala&gt; var rdd = sc.parallelize(Array((\"Tom\",82),(\"Tom\",78),(\"Mike\",76),(\"Mike\",74)))rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24//可以求每个人的成绩之和+课程数目（即一次计算多个指标）scala&gt; val rst = rdd.combineByKey(x=&gt;(1,x),(item:(Int,Int),new)=&gt;(item._1+1,item._2+new),(a:(Int,Int),b:(Int,Int))=&gt;(a._1+b._1,a._2+b._2))scala&gt; rst.foresch(println)(Tom,(2,160))(Mike,(2,150))附：验证格式可以用casescala&gt; rdd.map&#123;case(name,score)=&gt;(name,score,\"valid ok\")&#125;.foreach(println)(Tom,82,valid ok)(Tom,78,valid ok)(Mike,76,valid ok)(Mike,74,valid ok) 小结 Spark的介绍：重点是即与内存 Spark的安装：重点是开发环境的搭建(sbt打包) RDDs的介绍：重点Transformations，Actions RDDs的特性：重点是血统关系图和延迟[lazy]计算 键值对RDDs 后续 Spark的架构 Spark的运行过程 Spark程序的部署过程","categories":[{"name":"Spark学习笔记","slug":"Spark学习笔记","permalink":"https://www.cz5h.com/categories/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://www.cz5h.com/tags/Spark/"},{"name":"原理","slug":"原理","permalink":"https://www.cz5h.com/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"Spark的运行环境及远程开发环境的搭建","slug":"2017-11-26 Spark的运行环境及远程开发环境的搭建","date":"2017-11-25T23:00:00.000Z","updated":"2020-02-29T18:43:55.433Z","comments":true,"path":"article/b081.html","link":"","permalink":"https://www.cz5h.com/article/b081.html","excerpt":"基本概括概述spark快速 扩充了mapreduce 基于内存计算（中间结果的存储位置）","text":"基本概括概述spark快速 扩充了mapreduce 基于内存计算（中间结果的存储位置） spark通用 批处理hadoop 迭代计算 机器学习系统 交互式查询 hive 流处理 storm spark开放 Python API Java/Scala API SQL API 整合好hadoop/kafka 主要内容 环境搭建 核心概念RDD 架构 重要组件SparkStreaming 发展历史2009 RAD实验室，引入内存存储2010 开源2011 AMP实验室，Spark Streaming2013 Apache顶级项目 主要组件Spark Core： 包括spark的基本功能，任务调度、内存管理、容错机制 内部定义RDDs（弹性分布式数据集） 提供APIs来创建和操作RDDs 为其他组件提供底层服务 Spark SQL： 处理结构化数据的库，类似于HiveSQL、Mysql 用于报表统计等 Spark Streaming： 实时数据流处理组件，类似Storm 提供API来操作实时数据流 使用场景是从Kafka等消息队列中接收数据实时统计 Spark Mlib： 包含通用机器学习功能的包，Machine Learning Lib 包含分类、聚类、回归、模型评估、数据导入等 Mlib所有算法均支持集群的横向扩展（区别于python的单机） GraphX： 处理图数据的库，并行的进行图的计算 类似其他组件，都继承了RDD API 提供各种图操作和常用的图算法，PageRank等 Spark Cluster Managers： 集群管理，Spark自带一个集群管理调度器 其他类似的有Hadoop YARN，Apache Mesos 紧密集成的优点 Spark底层优化后，基于底层的组件也会相应优化 减少组件集成的部署测试 增加新组建时其他组件可以方便使用其功能 hadoop应用场景 离线处理、对时效性要求不高、要落到硬盘上 spark应用场景 时效性要求高、机器学习、迭代计算 Doug Cutting的观点生态系统、各司其职Spark需要借助HDFS进行持久化存储 运行环境搭建基础环境 Spark - scala - JVM - Java7+ Python - Python2.6+/3.4+ Spark1.6.2 - Scala2.10/Spark2.0.0 - Scala2.11 搭建Spark不需要Hadoop，如果存在则需要下载相关版本（不是上述对应关系） 具体步骤详见http://dblab.xmu.edu.cn/blog/spark-quick-start-guide/主要是两个步骤： 安装Hadoop（不做介绍） 解压Spark到对应位置，然后在spark-env.sh中添加SPARK_DIST_CLASSPATH run-example SparkPi已可以正常运行示例 注意几点： Spark版本要严格对照Hadoop版本 Spark运行不依赖Hadoop启动 Spark运行目录bin的内容，要确保有执行权限[+x] Spark目录 bin 包含和Spark交互的可执行文件，如Spark shell core，Streaming，python等 包含主要组件的源代码 examples 包含一些单机的Spark job Spark shell Spark的shell能够处理分布在集群上的数据 Spark把数据加载到节点的内存中，故分布式处理可以秒级完成 快速迭代计算，实时查询，分析等都可以在shell中完成 有Scala shell和Python shell Scala shell:/bin/scala-shell注意： 启动日志级别可以修改为WARN，在目录/conf/log4j.properties 开启Spark-shell要先启动hadoop，否则会出现以下错误12345678910111213141516171819202122232425262728[hadoop@hadoop01 bin]$ .&#x2F;spark-shell ... ...Caused by: java.net.ConnectException: Call From hadoop01&#x2F;192.168.146.130 to hadoop01:9000 failed on connection exception: java.net.ConnectException: 拒绝连接;For more details see: http:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;ConnectionRefused; ... 104 more&lt;console&gt;:14: error: not found: value spark import spark.implicits._ ^&lt;console&gt;:14: error: not found: value spark import spark.sql ^Welcome to ____ __ &#x2F; __&#x2F;__ ___ _____&#x2F; &#x2F;__ _\\ \\&#x2F; _ \\&#x2F; _ &#96;&#x2F; __&#x2F; &#39;_&#x2F; &#x2F;___&#x2F; .__&#x2F;\\_,_&#x2F;_&#x2F; &#x2F;_&#x2F;\\_\\ version 2.2.0 &#x2F;_&#x2F; Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112)Type in expressions to have them evaluated.Type :help for more information.scala&gt; scala&gt; val lines &#x3D; sc.textFile(&quot;&#x2F;home&#x2F;hadoop&#x2F;look.sh&quot;)&lt;console&gt;:17: error: not found: value sc val lines &#x3D; sc.textFile(&quot;&#x2F;home&#x2F;hadoop&#x2F;look.sh&quot;) ^ 其他可能出现的错误： 123456789101112131415161718192021222324252627282930[hadoop@hadoop01 spark]$ .&#x2F;bin&#x2F;spark-shell17&#x2F;07&#x2F;02 13:25:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableError while instantiating &#39;org.apache.spark.sql.hive.HiveSessionStateBuilder&#39; ... 47 elided Caused by: java.lang.RuntimeException: java.lang.RuntimeException: The root scratch dir: &#x2F;tmp&#x2F;hive on HDFS should be writable. Current permissions are: rwxr-xr-x; ... 61 more Caused by:java.lang.RuntimeException: The root scratch dir: &#x2F;tmp&#x2F;hive on HDFS should be writable. Current permissions are: rwxr-xr-x ... 70 more The root scratch dir: **&#x2F;tmp&#x2F;hive on HDFS should be writable. Current permissions are: rwxr-xr-x ... 84 more&lt;console&gt;:14: error: not found: value spark import spark.implicits._ ^&lt;console&gt;:14: error: not found: value spark import spark.sql ^Welcome to ____ __ &#x2F; __&#x2F;__ ___ _____&#x2F; &#x2F;__ _\\ \\&#x2F; _ \\&#x2F; _ &#96;&#x2F; __&#x2F; &#39;_&#x2F; &#x2F;___&#x2F; .__&#x2F;\\_,_&#x2F;_&#x2F; &#x2F;_&#x2F;\\_\\ version 2.2.0 &#x2F;_&#x2F; Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112)Type in expressions to have them evaluated.Type :help for more information. 上述错误出现的原因是/tmp/hive这里，本质上是hdfs中此目录的读写权限出了问题（Spark的运行并不需要Hive的开启，甚至没有Hive也可以），此处只是/tmp/hive这个目录出了问题，使用hadoop dfs -chmod 777 /tmp/hive来修改其权限，如果出现 Name node is in safe mode，那么则需要使用hadoop dfsadmin -safemode leave来退出安全模式，之后便可以正常修改权限，改完之后再执行spark-shell变会出现正常的初始化结果： 12345617/07/02 13:27:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable17/07/02 13:27:54 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectExceptionSpark context Web UI available at http://192.168.146.130:4040Spark context available as 'sc' (master = local[*], app id = local-1498973265138).Spark session available as 'spark'.注意上述的三行初始化信息！ 注意Spark-shell中的textFile(path)，参数path默认为hdfs://，要使用file://显式声明 12345678910111213141516171819202122232425262728293031scala&gt; val lines = sc.textFile(\"/home/hadoop/look.sh\")lines: org.apache.spark.rdd.RDD[String] = /home/hadoop/look.sh MapPartitionsRDD[1] at textFile at &lt;console&gt;:24scala&gt; lines.count()org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop01:9000/home/hadoop/look.sh at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287) at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229) at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315) at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:194) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:250) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:250) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087) at org.apache.spark.rdd.RDD.count(RDD.scala:1158) ... 48 elidedscala&gt; val lines = sc.textFile(\"file:///home/hadoop/look.sh\")lines: org.apache.spark.rdd.RDD[String] = file:///home/hadoop/look.sh MapPartitionsRDD[3] at textFile at &lt;console&gt;:24scala&gt; lines.count()res1: Long = 26scala&gt; lines.first()res2: String = #!/bin/bash 开发环境搭建安装Scala环境注意： Scala环境本身的安装跟Spark无关，Scala本身就是一门类似Java的语言 可以在非集群内的主机安装该开发环境，然后通过ssh提交集群运行即可（Spark版本2.x.x - Scala版本2.11.x以上，在IDEA中新建项目时会在首选项中进行选择） 第一个Scala程序：WordCount注意：类似于Hadoop，如果开发环境不在集群内，例如在自己PC中的IDEA进行开发（使用虚拟机同理），那么就会产生两种运行方式，一是本地运行，二是提交集群运行。本质上两种方式都是先打包，再上传（本地或集群）。即流程是一致的，但是在PC中引入的spark-core的作用是不同的，提交集群运行时，PC中的spark-core内容只是作为语法检查，类方法调用等辅助作用；但是本地运行时，除了上述功能外，其还充当了计算部分，即可以使PC成为一个类似节点的且有计算能力的存在。 全部步骤：PC上安装Scala环境，IDEA，IDEA安装Scala插件 1.本地运行 新建Scala的Project，注意要选对应的scala版本 然后在build.sbt中添加spark-core的依赖，可以去MavenRepositories网站去查，找到sbt（ivy）的依赖格式就行了 然后新建一个scala class，选择object，书写代码，要使用本地模式 最后直接点击运行即可。1234567891011121314import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDDobject WordCount extends App &#123; // 读取本地文件 val path = \"C:\\\\Users\\\\msi\\\\Desktop\\\\xiaomi2.txt\" // 本地调试 val conf = new SparkConf().setAppName(\"SparkDemo\").setMaster(\"local\") val sc = new SparkContext(conf) val lines = sc.textFile(path) val words = lines.flatMap(_.split(\" \")).filter(word =&gt; word != \" \") val pairs = words.map(word =&gt; (word, 1)) val wordscount: RDD[(String, Int)] = pairs.reduceByKey(_ + _) wordscount.collect.foreach(println)&#125; 打印结果：注意下述的IP地址和file路径，确实是在本地运行的，而且就是引入的sparl-core起的作用 123456789101112131415161718192021D:\\Java\\jdk1.8.0_77\\bin\\java \"-javaagent:D:\\JetBrains\\IntelliJ IDEA ...17/11/28 00:40:21 INFO Executor: Starting executor ID driver on host localhost17/11/28 00:40:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58570.17/11/28 00:40:21 INFO NettyBlockTransferService: Server created on 192.168.230.1:5857017/11/28 00:40:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy17/11/28 00:40:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.230.1, 58570, None)17/11/28 00:40:21 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.230.1:58570 with 1992.9 MB RAM, BlockManagerId(driver, 192.168.230.1, 58570, None)...17/11/28 00:40:22 INFO HadoopRDD: Input split: file:/C:/Users/msi/Desktop/xiaomi2.txt:0+90317/11/28 00:40:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1111 bytes result sent to driver17/11/28 00:40:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 183 ms on localhost (executor driver) (1/1)...(小米客服那些事,1)(贤艾森秋t4krP0,1)(北京IHG向,1)17/11/28 00:40:22 INFO SparkContext: Invoking stop() from shutdown hook17/11/28 00:40:22 INFO SparkUI: Stopped Spark web UI at http://192.168.230.1:4040...Process finished with exit code 0 2.提交集群运行 第一步同本地模式 第二步同本地模式 然后新建一个scala class，选择object，书写代码，要使集群模式 最后直接点击运行即可。1234567891011121314151617import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDDobject WordCount extends App &#123; // 读取hdfs文件 val path = \"hdfs://192.168.146.130:9000/spark/look.sh\" //远程调试 val conf = new SparkConf() .setAppName(\"scalasparktest\") .setMaster(\"spark://192.168.146.130:7077\") .setJars(List(\"I:\\\\IDEA_PROJ\\\\ScalaSparkTest\\\\out\\\\scalasparktest_jar\\\\scalasparktest.jar\")) val sc = new SparkContext(conf) val lines = sc.textFile(path) val words = lines.flatMap(_.split(\" \")).filter(word =&gt; word != \" \") val pairs = words.map(word =&gt; (word, 1)) val wordscount: RDD[(String, Int)] = pairs.reduceByKey(_ + _) wordscount.collect.foreach(println)&#125; 此处一定要选择对Module（不是默认）和要运行的MainClass 点击OK后，选择Jar打包后的路径 使用命令：启动master: ./sbin/start-master.sh启动worker: ./bin/spark-class org.apache.spark.deploy.worker.Worker spark://192.168.146.130:7077需要配置spark-env.sh中：（下面设为localhost就远程不了了）export SPARK_MASTER_HOST=192.168.146.130export SPARK_LOCAL_IP=192.168.146.130注意更新配置文件后需要把master和worker都重启才可以生效（单机两者都在一个机器上的情况） 出现的错误：错误：java.io.FileNotFoundException: Jar I:\\IDEA_PROJ\\ScalaSparkTest\\out\\scalasparktest.jar not found解决：修改setJar方法参数中的jar路径 错误：Could not connect to spark://192.168.146.130:7077解决：重启worker和master，前提是spark-env.sh中的MASTER_IP和WORKER_IP要设置正确 错误：Exception: Call From msi-PC/192.168.230.1 to 192.168.146.130:8020 failed on connection exception: java.net.ConnectException: Connection refused: no further information;解决：hdfs端口错误，很多教程写的是8020端口，但我hdfs是9000端口，所以要更正 错误：Invalid signature file digest for Manifest main attributes解决：打包的文件很大，把全部依赖都打包了，90多M，但正常应该10多M，删掉无用的依赖，并且把sbt中spark-core的依赖设为provided模式 错误：重复出现如下错误 12317/11/28 20:20:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources17/11/28 20:21:07 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources17/11/28 20:21:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources 解决：Worker失效后被kill了[此时jps应该是没有Worker的]，重启Worker即可，还不行就将hadoop和spark都重启 提交集群运行的结果：(注意IP和端口，确实是提交到集群/虚拟机 上运行后返回的结果)整个过程全部在IDEA中，完全达到了本地调试，自动上传集群，并返回结果的流程 12345678910111213141516171819202122232425262728293031D:\\Java\\jdk1.8.0_77\\bin\\java \"-javaagent:D:\\JetBrains\\IntelliJ IDEA ...17/11/28 02:09:39 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170630223625-0006/0 on worker-20170630215502-192.168.146.130-50762 (192.168.146.130:50762) with 1 cores17/11/28 02:09:39 INFO StandaloneSchedulerBackend: Granted executor ID app-20170630223625-0006/0 on hostPort 192.168.146.130:50762 with 1 cores, 1024.0 MB RAM17/11/28 02:09:39 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170630223625-0006/0 is now RUNNING...17/11/28 02:09:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.146.130:47071 with 413.9 MB RAM, BlockManagerId(0, 192.168.146.130, 47071, None)...17/11/28 02:09:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks17/11/28 02:09:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.146.130, executor 0, partition 0, ANY, 4853 bytes)...17/11/28 02:09:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 192.168.146.130, executor 0, partition 1, ANY, 4853 bytes)...17/11/28 02:09:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks17/11/28 02:09:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 192.168.146.130, executor 0, partition 0, NODE_LOCAL, 4625 bytes)...17/11/28 02:09:56 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 192.168.146.130, executor 0, partition 1, NODE_LOCAL, 4625 bytes)...(-ef|grep,1)($Jarstr,1)([[,1)(do,1)(YES,1)(while,1)(\"$Jarinfo\",1)(echo,1)(#!/bin/bash,1)17/11/28 02:09:56 INFO SparkContext: Invoking stop() from shutdown hook...Process finished with exit code 0","categories":[{"name":"Spark学习笔记","slug":"Spark学习笔记","permalink":"https://www.cz5h.com/categories/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"Spark","slug":"Spark","permalink":"https://www.cz5h.com/tags/Spark/"},{"name":"IDEA","slug":"IDEA","permalink":"https://www.cz5h.com/tags/IDEA/"}]},{"title":"通过局部聚集自适应的解开小世界网络的纠结","slug":"2017-11-23 通过局部聚集自适应的解开小世界网络的纠结","date":"2017-11-22T23:00:00.000Z","updated":"2020-02-29T18:43:55.416Z","comments":true,"path":"article/74eb.html","link":"","permalink":"https://www.cz5h.com/article/74eb.html","excerpt":"全称 IEEE Transactions on Visualization and Computer Graphics (TVCG)，是计算机图形学领域仅次于TOG (ACM Transactions on Graphics) 的顶级期刊","text":"全称 IEEE Transactions on Visualization and Computer Graphics (TVCG)，是计算机图形学领域仅次于TOG (ACM Transactions on Graphics) 的顶级期刊 摘要小世界网络具有典型的低成对的短距路径距离，导致基于距离的布局方法产生毛球图形。因此，最近的研究方法是寻找图像的稀疏表示来放大成对距离的变化。由于在布局上对布局的影响难以进行分析，因此这些方法的组合过滤参数通常必须手动选择，并为每个输入实例分别选择。我们建议使用图不变量来自动确定合适的参数。这使我们能够执行自适应滤波来获得最突出的集群结构。该方法基于实际和合成网络的输入和输出特征之间的经验关系。实验结果表明，该方法能有效地提高强制导向布局方法的鲁棒性。 小世界网络Steven H.Strogatz的文章Exploringcomplexnetworks综述了动力学网络方面的研究。他把网络分成规则网络和复杂网络两种，而复杂网络分为随机网络，小世界网络和自相似网络。小世界网络和自相似网络都介于规则和随机网络之间。小世界网络的特点：在网络理论中，小世界网络是一类特殊的复杂网络结构，在这种网络中大部份的节点彼此并不相连，但绝大部份节点之间经过少数几布就可到达。用数学中图论的语言来说，小世界网络就是一个由大量顶点构成的图，其中任意两点之间的平均路径长度比顶点数量小得多。除了社会人际网络以外，小世界网络的例子在生物学、物理学、计算机科学等领域也有出现。哈佛大学社会心理学家米尔格伦的的“六度分隔”理论，他发现，20%的信件最终送到了目标人物手中。而在成功传递的信件中，平均只需要6.5次转发，就能够到达目标。也就是说，在社会网络中，任意两个人之间的“距离”是6。这就是所谓的“六度分隔”理论。通过上面的论述可以发现社会网络图结构的特点非常具有小世界网络特征 引申-什么是社会网络在学术上来讲，英文文献中出现的”socialnetwork”应被理解为”社会网络”，而非狭义的社交网络。社交网络的分析单位(unit of analysis)通常是”人”,比如我们会说某人的社交网络比较大广；此外，社交网络这个概念中”network tie “的性质也侧重在指代人与人之间的交互、沟通、互动、情感上面。而“社会网络”(social network)既可以指人和人之间的网络，又可以指组织和组织之间的网络，甚至城市和城市之间的关系网络，国家和国家之间的关系网络。这里unitofanalysis不一定是人，而是社会中的实体（entity).此外，社会网络里面networktie可以是指各种类型的关系，比如，可以用来表示两个组织之间有没有相同的组织愿景或服务人群，两个球队之间有没有过交战记录，两个人之间对彼此的评价，或国家之间有没有经济往来等等，不限于“社交”的概念。 社会网络的特征值：三点组：全局集聚系数基于节点的三点组。一个三点组由三个节点组成，其中可以两边连接(为闭三点组)或三边连接(开三点组)，统称连通三点组。全局集聚系数C：是闭三点组个数(或三倍三角形个数)除以全部连通三点组个数。局部集聚系数：对于每个节点i，ni是节点i的邻居节点个数。平均本地聚集系数：示例嵌入性(embeddeness)：两个端点所共同拥有的邻居节点个数（邻居节点重叠(neighborhood overlap)：）网络基序(Network motif)网络的复杂性本质上就是关系的复杂性。但是，研究者通过对真实网络的分析，发现各种关系种类的出现频率是非随机性的。某些特定的关系种类在网络中反复出现，形成网络的典型连接方式不同类型的网络具有不同的典型连接方式“网络基序”，认为它们是一个网络的基本构造单元 小世界网络：高集聚系数低平均最短路径长度示例：每个节点有K&gt;=4最近邻居节点（局部）可调：改变重连接给定边的概率p小p：规则网格大p：经典随机图小世界的起源：隐含的社会网络结构并不包含群体关系，单但是如果将群体关系明确之后，会发现小世界网结构的产生正是由于实体的多个群体归属所导致的，如下图： 回到原文关键点自动参数选择，自适应边缘滤波，图形化简图，力导向布局，小世界网络，网络可视化 Introduction社交网络通常高度互联，并展示所谓的“小世界”属性。具有这一特征的网络一般都有一个小的平均成对的最短路径距离和一个高的局部密度。例如，对于脸书的友谊图，这意味着任何人只要与网络中的其他人有少量的中间连接就可以连接起来。尽管可视化方法如强制导向的布局方法为许多图形提供了高质量的结果，但是它们的能力对于小世界图来说是有限的。最终的图纸通常凌乱,看起来像个毛团,其中固有的图形结构是不可见的。这使得社交网络中最重要的任务之一——工作分析、发现和可视化的内聚亚群以及它们在网络中的相互关系，非常有问题。这些布局方法的主要问题是，它们试图直接将成对的图-理论距离转化为欧氏距离。然而，如果成对的两种形式的变化是很低的，就像发球图一样，结果的布局在欧氏距离上也有较低的变化。 人们提出了各种各样的方法来减少毛球图上的乱画。虽然基于几何图形的可视化技术，比如“边圆面包”，由于在飞机上的节点上的缺失，应用到毛球图纸上，不太可能带来好处，但修改后的布局算法似乎更有希望，因为它们可以补偿图像中的结构性影响。例如，Boitmans等4人提议修改成对的距离，以使相邻的高度节点被进一步分开。这允许放大布局的核心，特别适合于具有极度倾斜程度分布的图形。另一个建议是在人工选择的领域中扭曲布局5。一种不同的一般方法是在图中识别集群，然后使用这些集群来可视化网络6、7。这种方法将可视化问题转移到集群或社区检测方法的选择上。 处理这个问题的另一种方法是图像的简化——阳离子，它的思想是将图中最重要的部分减少到最重要的位置。这个稀疏的子图，也就是所谓的主干，可以用来布局图。简化方法，保持特定的柱状图，如频谱8，cu/连接性9，或者缩短测试路径10，11，倾向于保持小的成对距离，也就是说，产生的后骨仍然是毛球图形。最具前景的简化方法是对内聚亚群进行的最有效的和视觉化的探索，就是根据嵌入的标准7、12、13、14、15来过滤边缘，这是基于边缘周围的局部密度来确定的。按照这种方法，所有的方法都需要输入一个阈值参数，根据这个参数提取主干，然后使用标准的强制方法进行布局。由于参数对最终可视化的非线性影响，找到合适的阈值参数来检索有意义的网络可视化是非常昂贵的。阈值的一个小差异可能已经完全改变了布局。因此，确定最佳的阈值，在布局中最突出的组结构，在试验和错误的基础上是一项非常耗时的任务，特别是对于大型网络来说。 对于这些方法，我们提出了一种预处理技术，它可以自适应地确定组结构在图和布局中最明显的阈值(如我们的实验中所示)。利用聚类系数，我们的技术可以量化每一个可能的阈值值。 我们的贡献是: 一种新颖的方法来量化每一个阈值对主干的组结构的影响。 一种有效的动态算法，保持在边删除下的聚类系数，在O(α(G)m)总时间内运行，其中m是图中的边数，而α(G)是最小的能够覆盖图G边集合的生成森林 对我们的方法在许多真实世界和合成网络中的有效性进行了广泛的评估。 为了评估我们的预处理技术，我们使用了四边的Simmelian脊骨，作为14个具有不同边缘嵌入度指标的实验研究，它非常适合于解开毛球图形，同时保留了组结构。特别是在多中心的小世界网络中，它支持标准的强制导向的布局技术，在最终的图中强调了固有的组结构。 我们首先从下一节的几个方面概述了主干布局方法，并解释了第3部分中自适应滤波的量化度量方法。在第4部分中，我们首先评估它的含义——确定与已知集群的关联，然后研究它对网络布局的影响。第5节总结了我们的结论。 骨干布局方法图1中显示的主干布局方法的总体工作流程如图1所示。在第一步中，计算了仅依赖于图结构的边嵌入度度量。基于这些边权值，过滤步骤将删除低于给定阈值的所有边。由于连接对于强制导向的布局方法是不可缺少的，但是可能会通过过滤步骤被破坏，下一步是将所有的边缘从基于嵌入度的所有树的结合中重新插入，以确保主干保持连接。请注意，为了避免任意的(非结构的)优先的优势，联合是必要的。 图1所示。主干方法的工作流程由一个流程来扩展，该流程分析了所有可能的阈值参数，并对组结构进行了分析。这允许向用户指出有趣的阈值，以及对该参数的完全自动选择。 结构边缘嵌入的计算可以分为两部分。第一部分计算的是最初的边缘重量，即所谓的四边形重量，它测量了一个边在其邻域内的集成程度，并被定义为q(u,v) 是包含边(u,v)的四边形的个数v∈V，N(v)是v的邻居节点 此后，将执行重新附权值。重新调整权值原因是关系(u,v)∈E可能对u和v的重要性是不一样的。关系e可能对u是最重要的，可能是对v无关紧要的,例如因为v可能有其他更重要的关系,因此趋向于摆脱u的束缚. 对于一个边(u，v)，重新加权是根据最初的边权值来对u和v的邻边进行排序的。然后，k被选为Jaccard的系数，u和v的系数是最大化的。这一最大的Jaccard系数可以被看作是在他们的排名上的u和v的协议，并被用作过滤步骤的嵌入度。 为了计算最后一步的强制导向布局，我们使用了在14、18中建议的应力最小化16初始化。这种布局算法发现了节点位置，使得两两匹配的算法与图的理论距离相匹配。为了在算法的每一次迭代中找到这些位置，每个节点都被重新定位为所有其他节点的函数。在下一节中，我们将提出一个仅依赖于图形结构的度量，但是它仍然是最终布局质量的一个适当的指示器。 基于局布局类的自适应过滤在本节中，我们的目标是对网络中可聚类的结构程度进行量化，这应该可以作为网络集群结构的清晰程度的度量，但不需要执行实际的聚类操作。这个量化允许对手动选择和全自动参数提取提供可视化支持.不同于在19、20中的现有方法来执行聚类操作，而是度量聚类在网络中的一个经常观察到的参数，即很高的平均聚类系数。聚类系数可以捕捉到一个顶点的邻域之间的关联程度。 在一系列具有不同的稀疏化参数的后骨中，主要的假设是，具有高聚类系数的主干比具有低聚类系数的主干更有可能包含内聚类。如果使用聚类系数的量化是有效的，那么它的最高值应该将我们指向稀疏化参数，由此产生的主干最类似于一个预定义的集群图22，表示底层的组结构。 图2所示。摘要通过对一个具有隐式群结构的综合网络，对聚类系数的聚类系数的有效性进行了评价。最高集群系数(a)表示参数，其中组刚刚开始分解(d)，这也是产生的主干与地面真相集群图最相似的点。(e)过滤删除了越来越多的集群边缘，破坏了组的相对位置。 更准确地说，我们使用phi系数作为一种相似性度量来评估聚类系数的有效性。phi系数可以被理解为两个矩阵实体之间的相关度量，其中第一个矩阵是主干图的邻接矩阵，第二个矩阵是给定的聚类结构的块矩阵。图2给出了整个过程的概况，并展示了一个带有预先划分的合成网络的聚类和phi系数，以及4个有不同的稀疏化阈值的主干图(图2b到2e)。聚类系数可以作为聚类结构的一个指标，并将我们的聚类参数化，这很有可能强调了团体信息。 聚类系数的有效计算现在，我们研究了如何计算聚类系数对每一个可能的稀疏化参数进行计算的方法。局部聚类系数被定义为一个顶点v的封闭三元组的百分比 λ(v)是顶点v涉及的闭合三元组(三角形)的数目。τ(v)是v中连接的三元组的数量，对于N(v)≤1（邻居节点个数）我们定义其聚类系数为0，这样便可以过滤掉外围度为1的点。由上述可定义全局聚类系数或者叫做平均聚类系数： 当Sun调查网络统计数据的有效更新模式时，我们还不清楚他们更新方案的聚集系数。它们提供了一个用于更新聚类系数的O()时间复杂度，是网络的平均度。考虑到时间复杂度O()远小于n=|V|,目前尚不清楚如何在聚集系数为n(最坏的情况下)时更新删除下一个边缘的数量可以在O()时间内执行完毕。当删除的边被包含在图中每个顶点的三角形中时，就会给出这样的情况(例如，算法1的例子)。 ### 算法1 对全部可能的过滤变量计算聚集系数输入：Graph G = (V,E) 其中 n = |V|,m = |E|,边权重 w：E -&gt; R(非零实数)，随E变化而变化数据：Tr[e]:包含边e的三角形集合 d[v]:顶点v的度，λ[v]:顶点v的三角形数目输出：多级(i=0,1,,k)主干结构的聚集系数Ci 1234567891011121314151617181920Tr[e],e∈E 列出三角形集合（triangle listing algorithm）for v∈V do C[v]&lt;-λ[v]&#x2F;τ(v) 计算全部节点的聚集系数由全部节点的聚集系数得到平均聚集系数 C0按边的权重划分到不同的桶B1..Bk中按权重的降序将桶进行排序for i to k foreach e &#x3D; (u,v)∈Bi do 循环每个桶中的边 &#x2F;&#x2F;remove来自三角形中的边e的贡献值 平均聚集系数 &#x3D; 平均聚集系数 - u和v的聚集系数影响 u的邻接点 &#x3D; 邻接点 - 边e涉及的三角形的顶点 v的邻接点 &#x3D; 邻接点 - 边e涉及的三角形的顶点 顶点u的度-1 顶点v的度-1 此时重新根据λ和τ计算顶点uv的聚集系数 最后再讲uv聚集系数的影响加回平均聚集系数 foreach Tr[e]结合中的 (u,v,w) 类似上述过程 完成后就删除Tr() Ci&lt;-C 迭代计算定义 w:E-&gt;R 为边缘的权重，反映了结构上的边嵌入，W={w(e)|e∈E}是可能的边权重的集合，Gz，z∈W是 主干结构的结果。为了计算一个图的聚类系数,我们只需要知道每个顶点的三角形数量,时间复杂度为O(α(G)m),α(G)是图的荫度,或是图g所需的能覆盖所有的边的最小生成森林。实际上,α(G)可以被认为是社交网络的一个小常数。对每个主干结构执行这个计算将花费O(α(G)m)时间，此时可能的主干结构数为O(m)。算法1描述了如何通过计算原始图的聚类系数来提高效率，并迭代地更新正在删除的每条边的三角统计数据。当边缘e被删除(第7行)时，所有的三角形(Tr)都会被销毁。因此，正如已经观察到的，对于其中一个三角形中的每个顶点来说，它的本地聚集系数和它对C的贡献值需要更新。 算法1的正确性根据聚集系数的定义，C平均、对初始图是正确的。它显示在每条边被删除后C平均、的适当的更新。因此，在删除边e=(u,v)之后，就可以显示每个局部聚集系数C[v]被正确地更新了。局部系数只改变了通过e创建了三角形的顶点，而e的所有三角形(Tr[e])也都是u和v的三角形(见算法1下面的数字)。因为它们都被破坏了，所以我们需要通过|Tr[e]|来减少λ[v]和λ[u]。对于e所在的三角形中一个带有权重w的边e，实际上只有一个三角形受到影响(算法1的第17行)，通过从三角形集合中删除另外两条边（(u,w)(v,w)）的每个三角形移除每个三角形t，我们确保这个三角形永远不会再被考虑。因此，C[w]被正确地更新了。u和v的度也被正确地减少了一个单位，这样就可以更新C[u]和C[v]了。 算法1的运行时间算法1的第一部分的运行时间主要由列出三角形(O(α(G)m))和排序(O(mlogm))组成，在第二个for循环中，每个三角形都处理一次，所需的更新需要常数时间。因为最多有O(α(G)m)个三角形，所以第二个for循环的运行时间是O(α(G)m)。 评估有效性根据所选的稀疏化参数，原图的散化会产生不同的主干结构。对于每一个这样的结构，我们想要量化它的结构是如何被一组固有的群集所组成的。模块化通常用于聚类质量评估，但我们不使用它，这是因为它的反直觉行为:即使是对图的完美划分，也只包含有连接的组件，而模块化具有多样性，并且与1的最优值有很大的不同。我们将读者介绍到引述27和28，就这一行为进行更广泛的讨论。相反，我们测量了主干结构的相似性，其反映了一个完美的划分，由非连接的小结组成，使用对应的邻接矩阵的phi系数。当应用于二进制变量时，phi系数是皮尔逊相关系数的一种变化。弗里曼也称其为Borgatti的参数。直观的解释是，如果图形与给定的完美划分相似则它的值很大，如果不相似，则很小(接近0)。 由于成对的缩短路径距离通过力导向布局被转换成欧氏距离，我们计算了平均成对的最短路径距离来量化特定参数的图的扩展。在此基础上，利用欧氏空间内的平均邻居距离作为一种局部紧度测度，对阈值参数对最终布局的影响进行了评价。如果聚集结构对于某一特定的主干来说是突出的，那么局部的布局紧凑性应该是很高的。 我们现在更精确地定义phi系数，并给出一个具体的例子。在此之后，我们将解释数据集和图形模型，以最终讨论结果和局限。 phi相关系数对于主干图G’={V,E’∈E}和顶点集V的划分C={C1,…,Ck} ，令C(v)∈C表示顶点v∈V的聚类。进一步，令X作为G’的邻接矩阵Y作为在这个划分上的完美图的邻接矩阵在这里，循环并不重要，只要它们的存在或不存在被定义为X和Y。由于我们只对一个顶点对的布尔值感兴趣，所以皮尔逊相关系数会降低到phi值。a，b，c，d代表观测的频率，从2 x 2个偶然事件表中得到 下图3给出了一个图和一个完美的分区之间的相似性的例子。左边的图G与一个完美的分区(顶点颜色)相似，正如G的邻接矩阵X和完美的划分Y的矩阵结构之间的高度相似性所表明的那样。此时，x=1,y=1是23，x=1,y=0是2，x=0,y=1是2，x=0,y=0是22φ(X,Y)=套公式=0.84 数据和模型对于评估，我们使用来自facebook100数据集的网络。这些网络最初来自Facebook，包含了美国100所高等教育机构的学生的社会关系。网络大小不同，从762到41K个顶点，从16K到160M条边。其他的属性，如性别，预期的毕业年，宿舍等，都被作为顶点属性。Traud等30人认为，宿舍对社会关系的形成很重要。因此，我们使用寝室属性作为分区C，从而用phi值进行评估。当主干结构和聚类系数计算考虑到图的所有顶点时，在计算phi值时则会忽略一个缺失宿舍值的顶点。因此，大量缺失的值可能会将phi值作为评估准则。请注意，尽管经验上许多社交相关的属性与同型属性值相关联，但在Facebook的网络中，没有一个真实的群体结构是可用的。出于这个原因，我们还使用了一种基础的地面真相组结构，使用了14个分区模型(PPM)，并由Lancichinetti等人(LFR)的模型来生成人工网络。 结论和讨论图2a 剩余的边缘聚集效应（上）以聚类参数为基础，和真实的情况（phi系数）十分相似（下），沿着稀疏化参数的减小。 首先，我们讨论了聚集系数作为phi系数的代理的有效性。然后通过查看局部布局的紧凑性，来评估这种行为是否也反映在最终的布局中。实验的结果是每个网络的两条曲线，类似于图2a。从左到右，根据嵌入的测量，越来越多的边被移走。这些曲线通常有一个顶点。图4显示了这些apexes的聚类和phi系数值。由于空间限制，我们无法显示所有facebook100网络的结果。因此，我们选择了11个在图4中标注他们名字的Facebook网络。选择准则是对不同区域的覆盖，反映了网络的各种属性。 图4 聚集系数和phi值的关系facebook 100网络和PPM500的最大聚集系数，以及所有可能的稀疏化比例。为进一步的分析选择了被标记的网络。图5 聚集系数和phi值的关系在不同网络(Facebook100+PPM500)上的散化比和聚类系数。如果phi值很高，那么峰值位置就非常接近。 图5显示了选定网络的聚类和phi系数的曲线。最大值处使用虚线突出显示。对于PPM500网络，聚集系数的峰值非常接近phi值的峰值，在这种情况下，对边缘的进一步过滤只会使集群变得稀疏。如果phi系数很高，例如，对于奥伯恩71、Caltech36、Lehigh96或史密斯60，那么这两个最大值就会接近于彼此。这意味着最大的聚集系数将我们指向过滤参数，其中的密度对于固有的聚类来说是最高的。与其他网络相比，Auburn71略有不同，因为它的phi值比上半年的聚类系数要大。看图4，我们可以看到80%的边都缺少相应顶点的宿舍属性值。当集群系数考虑到网络的所有顶点时，phi系数必须忽略那些缺少值的值，因为它们不为它们所知。对这些缺失值的认识可能会改变曲线的形状。 我们可以看到，对于许多其他的网络，phi系数并没有明显上升，这可能表明，宿舍并不是固有群体的解释变量。然而，聚集系数却有一个明显的峰值，这使我们能够识别出固有的结构的一个重要的全局方面。我们还试验了许多关于集群系数的加权变量，如Panzarasa和Opsahl所讨论的，但我们没有看到对非加权的准确性的提高。结果具有可比性。我们期望其他的变量，如传递比，网络中三角形的数目除以三元组的数目，也可以工作。利用Lancichinetti等人的图形模型，我们生成了一个真实社区结构的网络。我们将混合参数m从0.1增加到0.8，这增加了潜在的噪音，并逐渐模糊了群体结构。我们使用这个模型的实际经验是，如果混合参数大于0.6，那么这个组结构就几乎不存在了。图6显示了最大的聚类系数与最大的phi系数，使用地真信息的最大程度相同。这意味着最大的聚类系数是一个很好的代理，可以用来识别在生成的主干中最显著的组结构的稀疏化参数。 图6 phi值代替稀疏化参数基于LFR模型的真实社区结构和不同噪音水平的图表。选择基于最大聚类系数的稀疏比，给出的结果与使用phi系数的结果几乎相同。 图8 阈值和最大聚集系数的关系最大群集系数和它的过滤值(四边形Simmelian)为facebook100+PPM500。在三个阈值值之间进行分组，表明在不同的固有社区中存在类似的密度衰减。 过滤值我们可以观察到一个有趣的观察值，当观察到集群系数最大(图8)的四边Simmelian主干的阈值过滤值时，可以清楚地看到facebook100网络组大约有三个不同的阈值值。这表明，在这些网络中，本地社区结构是相似的，网络中的不同社区之间的密度衰减是一致的(在这些群体中)。找出社区的规模，看看它们是否与这些群体相关联也会很有趣。使用更多的机构信息进行额外的分析，例如，基础设施的属性，可能会揭示更多的解释。图7平均最短路径在增长聚集系数指明了甜蜜点局部布局更紧凑，组织变得明显网络通常有一个甜蜜的点，在这个点上，成对的距离(灰色的曲线)增加，但是组(蓝色曲线)是紧凑的，正如聚类系数(红色曲线)所显示的那样。 布局质量对于较小的图形，我们计算了各种增加滤波器参数的力导向布局，并对该布局的局部紧化度进行了评估(图7)，可以观察到局部紧实曲线与聚类系数非常相似。高紧度表示图中的顶点的邻边与布局中的这个顶点非常接近。考虑到布局实际上是在扩展，用单调递增的平均短时间曲线(图7顶部)表示，这意味着基于最优聚类系数参数，在最终的布局中，底层集群变得越来越紧凑。为了查看我们的自适应过滤对图表的影响，我们使用了我们的方法来处理PPM500(25%)图(图2d)。图7所示的聚类系数曲线表明，如果去掉许多边，该群结构就会被破坏。这表明我们的方法甚至可以应用于非发球图上，使图更清晰地显示出一个已经清晰的组结构。 图9来自facebook 100数据集的网络图(Rice31，|V|=4k，|E|=9.7w)。左:原始的力导向布局，右:力导向的布局，我们的自动过滤在四边形的Simmelian主结构上。图10facebook 100数据集的一个较大的网络(Auburn71，|V|=18k，|E|=100万)。左:原力导向的布局，右:力导向的布局，我们的自动过滤在四边形的Simmelian主结构上。图11来自facebook100数据集(Smith60，|V|=3k，|E|=100k。左:原始的力导向布局，右:力导向的布局，我们的自动过滤在四边形的Simmelian主结构上。 绘制看一看图9、10,11中所产生的主干图，可以看到不同的集群，分别为Caltech36、Rice31、Auburn71和smith 60。强调网络的局部密度和主干布局让我们能够在全局环境中获得关于局部图结构的信息的图。主干显示: 许多强大的社区存在。 社区常常是高度重叠的。 许多参与者(或顶点)没有很好地集成在强社区内。 根据这些建议，一个图形聚类(或社区检测)方法应该关注于强大的、可能是重叠的社区，以及一组参与者，它们在那里，但不是这些强大社区的一部分。当然，这些参与者可以根据需要分配到他们最接近的社区，例如，如果应用程序需要的话。在分析的网络中，唯一的例外是收获图，在图中没有可见的局部集群，图9b。图8中Facebook网络的边界位置表明，与其他选择的网络相比，它具有不同的结构属性。这可能是哈佛大学住房政策的结果，该政策仅在第一年就为学生提供宿舍。注意，具有最大集群系数的主干强调了全局上下文中的组结构。可能需要过滤出更多的边来观察固有组的精细结构。 局限我们提出的技术在运行时可以很好地扩展到大的图形。然而，我们对集群结构的量化是整个网络的聚合，因此对单个局部细节不敏感。将这种量化以一种更精细的方式进行扩展将是很有趣的，例如，只对交互选择的顶点的子集进行计算。这将允许在图中更详细地分析和可视化特定的兴趣区域。 正如在14个方面所注意到的，四边形的Simmelian主干对于多中心网络特别有用，而对于一个单一中心的网络则更少，例如中心-外围结构。我们的量化指标只能和潜在的边缘嵌入度一样有效，因此当应用于单一核心网络时，它也有类似的缺点。 结论我们提出了聚类系数的使用方法，在聚类结构的基础上，定量地分析了在主链结构上对主链结构的影响。使用真实世界和合成网络进行的实验评估，证实了其在四边形Simmelian脊骨上的有效性，结果也可能扩展到其他密度的基础上。此外，我们还展示了如何有效地计算每一个可能的阈值参数的聚类系数。这在探索和可视化大型网络时尤其有用，因为在大量的网络空间中，由于时间密集型的布局重新计算，在一个错误的基础上选择适当的sparsi虚拟化级别是非常麻烦的。虽然图表的布局度量和视觉检查都表明集群确实是明显的，但是在2行中详细的用户研究必须显示出这种视觉效果对于特定任务的重要性，例如类簇的选择。","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"力导向","slug":"力导向","permalink":"https://www.cz5h.com/tags/%E5%8A%9B%E5%AF%BC%E5%90%91/"},{"name":"图简化","slug":"图简化","permalink":"https://www.cz5h.com/tags/%E5%9B%BE%E7%AE%80%E5%8C%96/"},{"name":"小世界网络","slug":"小世界网络","permalink":"https://www.cz5h.com/tags/%E5%B0%8F%E4%B8%96%E7%95%8C%E7%BD%91%E7%BB%9C/"},{"name":"边过滤","slug":"边过滤","permalink":"https://www.cz5h.com/tags/%E8%BE%B9%E8%BF%87%E6%BB%A4/"}]},{"title":"基于Spark的大数据热图可视化方法","slug":"2017-11-20 基于Spark的大数据热图可视化方法","date":"2017-11-19T23:00:00.000Z","updated":"2020-02-29T18:43:55.412Z","comments":true,"path":"article/84c8.html","link":"","permalink":"https://www.cz5h.com/article/84c8.html","excerpt":"计算机辅助设计与图形学学报 2016-11 浙江工业大学&amp;浙江大学概述针对普通客户端浏览和分析大数据困难的问题, 结合 Spark 和 LOD 技术, 以热图为例提出一种面向大数据可视化技术框架. 首先利用 Spark 平台分层并以瓦片为单位并行计算, 然后将结果分布式存储在 HDFS 上, 最后通过web 服务器应用Ajax技术结合地理信息提供各种时空分析服务.文中重点解决了数据点位置和地图之间的映射, 以及由于并行计算导致的热图瓦片之间边缘偏差这2个问题.实验结果表明,该方法将数据交互操作与数据绘制和计算任务分离, 为浏览器端大数据可视化提供了一个新的思路.","text":"计算机辅助设计与图形学学报 2016-11 浙江工业大学&amp;浙江大学概述针对普通客户端浏览和分析大数据困难的问题, 结合 Spark 和 LOD 技术, 以热图为例提出一种面向大数据可视化技术框架. 首先利用 Spark 平台分层并以瓦片为单位并行计算, 然后将结果分布式存储在 HDFS 上, 最后通过web 服务器应用Ajax技术结合地理信息提供各种时空分析服务.文中重点解决了数据点位置和地图之间的映射, 以及由于并行计算导致的热图瓦片之间边缘偏差这2个问题.实验结果表明,该方法将数据交互操作与数据绘制和计算任务分离, 为浏览器端大数据可视化提供了一个新的思路. 目前大数据可视化面临的主要问题包括:1) 数据复杂散乱. 经常发生数据缺失、数据值不对、结构化程度不高.2) 迭代式分析成本高. 在初次查询后如果发现结果不对, 改变查询条件重新查询代价高.3) 构建复杂工作流困难. 从多数据源取得包含各种不同特征的原始数据,然后执行机器学习算法或者复杂查询, 探索过程漫长.4) 受到原有技术限制, 对小规模数据分析很难直接扩展到大数据分析.5) 数据点的规模超过普通显示器可能提供的有效像素点.Hadoop和Spark先后成为大数据分析工业界的研究热点,前者是一个能够对大量数据提供分布式处理的软件框架和文件系统(hadoopdistrib-utedfilesystem,HDFS);后者是一个通用大数据计算平台,可以解决大数据计算中的批处理、 交互查询及流式计算等核心问题.Zeppelin可以作为Spark的解释器,进一步提供基于 Web 页面的数据分析和可视化协作可以输出表格、柱状图、折线图、饼状图、点图等,但是无法提供更为复杂的交互分析手段. 相关工作面向 web 的轻量级数据可视化工具主要是一些JavaScript库，利用canvas或者svg画散点，svg不能支持十亿以上的节点，使用 canvas 画布绘图的heatmap.js 在面对大数据量时也无能为力.热图是一种常用的基本数据可视化技术,通常用颜色编码数值大小，并以矩阵或方格形式整齐排列,在二维平面或者地图上呈现数据空间分布,被广泛应用在许多领域.近年来,许多研究者成功地将热图应用在眼动数据可视分析上, 有效地概括并表达用户视觉注意力的累计分布LOD针对数据可视化绘制速度慢、效率低等问题,孙敏等提出基于格网划分的LOD(levelsofdetail)分层方法, 实现对大数据集 DEM 数据的实时漫游. 并行计算大数据热图 经纬度换算 并行计算在 Spark 平台上实现热图的绘制,首先将经纬度坐标转换为对应不同瓦片上的像素坐标.每个基站的辐射范围可近似认为相同, 即每个基站（收集数据的基站坐标）的初始影响力近似相同,因此可采用影响力叠加法将数据点绘制到画布上,然后做径向渐变,叠加出每个位置的影响大小,得到初始灰度图,如图2a所示.然后将每一个像素点着色,根据每个像素的灰度值大小,以及调色板将灰度值映射成相对应的颜色. 图 2b 是一个透明的PNG 格式图片, 调色板如图 2c 所示. 本文中出现的热图均采用图 2c 调色板.将计算出的热图结果存储在HDFS上,并与经纬度以及层级建立索引关系方便以后读取,拼接后的热图绘制效果如图 3 所示. 瓦片边缘问题边缘热点可能处于2片或者4片瓦片之间,因此需要通过2次或者4次重复计算.通过本文提出的重叠计算方法可以解决热图分片计算的边缘问题。 实验 总结本文提出的大数据热图可视化方法能够有效地解决前端绘制计算量大的问题,通过在Spark平台上以瓦片为单位分层次并行计算热图, 将生成的热图存储在HDFS上,然后通过web服务器提供浏览器交互服务, 用户可以通过在地图上拖动鼠标或放大/缩小等操作选择感兴趣区域,再分析不同时间点用户行为差异或渐变过程. 通过解决热图数据点和地图映射关系问题以及瓦片热图之间的边缘问题,提供大数据热图绘方法, 以满足用户交互、协同和共享等多方面需求.该方法可以拓展到其他常用可视化方法,如ScatterPlot, Bar Chart,平行坐标等.但绘制过程是基于Spark计算后得到的离线数据,在实时性上还不能得到保证, 在下一步工作中, 我们将着手利用 Spark Streaming 库来解决这一问题.","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"热图","slug":"热图","permalink":"https://www.cz5h.com/tags/%E7%83%AD%E5%9B%BE/"},{"name":"并行计算","slug":"并行计算","permalink":"https://www.cz5h.com/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"},{"name":"大数据","slug":"大数据","permalink":"https://www.cz5h.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"细节层次","slug":"细节层次","permalink":"https://www.cz5h.com/tags/%E7%BB%86%E8%8A%82%E5%B1%82%E6%AC%A1/"}]},{"title":"一种基于力导向布局的层次结构可视化方法","slug":"2017-11-18 一种基于力导向布局的层次结构可视化方法","date":"2017-11-17T23:00:00.000Z","updated":"2020-02-29T18:43:55.415Z","comments":true,"path":"article/3bfa.html","link":"","permalink":"https://www.cz5h.com/article/3bfa.html","excerpt":"计算机仿真 2014-3 北京工商大学 在数据结构优化管理的研究中，传统的力导向方法应用于层次结构数据的展示时，会存在树形布局展示不清楚的问题。为解决上述问题，通过层次数据特征分析，提出了一种面向层次数据的力导向布局算法，将力导向布局中不同层次的边赋予不同初始弹簧长度，以解决层次数据中结构信息展示不清楚的问题，然后结合层次上下行、Overview+Detail等交互技术，通过与气泡图的协同，清晰展示层次数据的内容信息，从结构和内容角度对层次数据进行可视化和可视分析。实验表明，能够有效提高层次结构数据的展示能力，最后应用于农产品中农残检测结果数据的分析和观察，取得良好效果。","text":"计算机仿真 2014-3 北京工商大学 在数据结构优化管理的研究中，传统的力导向方法应用于层次结构数据的展示时，会存在树形布局展示不清楚的问题。为解决上述问题，通过层次数据特征分析，提出了一种面向层次数据的力导向布局算法，将力导向布局中不同层次的边赋予不同初始弹簧长度，以解决层次数据中结构信息展示不清楚的问题，然后结合层次上下行、Overview+Detail等交互技术，通过与气泡图的协同，清晰展示层次数据的内容信息，从结构和内容角度对层次数据进行可视化和可视分析。实验表明，能够有效提高层次结构数据的展示能力，最后应用于农产品中农残检测结果数据的分析和观察，取得良好效果。 引言层次数据的节点链接可视化方法主要包括双曲树(Hyperbolic Tree)和径向树(Radial Tree)等。 双曲树是一种fbcus+context技术来显示大型层次数据的可视化方法。该技术的本质是使用统一的算法将层次结构布局到双曲平面上，然后再将该双曲平面映射到圆形显示区域。 力导向算法是一种常用的绘制一般网状结构的方法。最初由PeterEades提出，它仿真物理力学的概念，对网络图中任意一个节点，都受到引力及斥力。引力是由与此节点有边相连接的每个邻居所提供的合力，其意义是节点与其邻居之间的距离不会太远；斥力是由整个网络图中除了本身以外的节点所提供，为了让任意两节点不会因距离太近而产生重叠的现象，也因此整个网络图不会太密集而影响视觉。 算法的终止通过给定节点的初始位置，放人此力导向的物理模型中，系统自动调整节点位置，直至达到稳定条件才结束。 对于簇状数据，出现了一种与力导向结合的方法，此方法是一种基于将一个图划分为多个子图的多级技术．开始先构建最小的子图，使用力导向布局调整节点的位置．然后在下一层级的子图划分中使用调整好后的结果。 但是，力导向算法应用于树型数据的研究还较少。本文将力导向算法应用于层次数据的布局，提出一种可变弹簧的力导向布局算法，能够较好展示结构信息，并结合气泡图展示其中的内容信息，设计完成了多视图协同可视分析系统. 向层次数据的力导向布局算法传统力导向布局会产生边长度相对一致的情况，而这种情况对层次结构的展示是不明显的，尤其是在节点普遍具有较大出度的情况下，在布局中表现为中心节点即根节点处有节点混杂，在叶节点处子树的交叉遮挡现象严重．子树的成团效果不明显，因而影响对层次数据的观察。 本文考虑调整不同层次的边的长度以示区分，以不同的边长表现不同的层次，使得根节点处的边长较长而叶节点的边长较短。并依树形结构的结构特征达到一定的边长比值，从而实现中心节点处发散而叶节点处收敛的目的。 对于不同层次的边赋以不同的长度，以区别其向指父节点和其子节点的边。并使相邻层次满足边长比例为C，C为某常数,将此布局算法称为可变弹簧力导向布局算法VSFDP。 斥力计算： 引力计算： 确定不同层次弹簧长度的比值常数C对于一棵规则的满n叉树来说，节点所处的层次越高，其子树包含的节点数目越多。这篇文章将N层节点与N+1层节点的比值定为常数n，则C=n，但是如果每层节点的度的比值不是常数，那么就使用数的平均度来作为常数C： VSFDP算法步骤： 将输入数据转化为树型结构存储 计算树型结构的源数据：非叶子节点的度，树的平均度 根据期望距离（默认长度）+比例常数C来规定每层的边长度 建立模型，代入数据 交互方式 层次放大和层次缩小 Overview+Detail 气泡图 系统设计可视化流水线是将原始数据转换成计算机内存中数据结构。并用一种可视化方法将数据在屏幕上展现出来的过程，如图下图所示，在这个过程中包括用户的交互反馈以及各种工具方法等的使用。 实验结果 总结这篇论文第一个创新点是对传统力导向算法的改进，加入了分层布局，使用整个图的平均度作为常数C，这样的结果就是度大的节点被分为一层，这里他们在布局时会被赋予较长的边距离，然后依次类推，下一层节点拥有较小的度，但是赋予的边的距离也同样变短。另外，在交互手段上使用气泡图辅助，气泡图的圆心在当前层的节点位置，半径同样是按层间的比例，在将布局展示的同时通过气泡来辅助交互。","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"节点连接","slug":"节点连接","permalink":"https://www.cz5h.com/tags/%E8%8A%82%E7%82%B9%E8%BF%9E%E6%8E%A5/"},{"name":"层次数据","slug":"层次数据","permalink":"https://www.cz5h.com/tags/%E5%B1%82%E6%AC%A1%E6%95%B0%E6%8D%AE/"},{"name":"力导向布局","slug":"力导向布局","permalink":"https://www.cz5h.com/tags/%E5%8A%9B%E5%AF%BC%E5%90%91%E5%B8%83%E5%B1%80/"}]},{"title":"WebSocket的初次使用及调试","slug":"2017-11-17 WebSocket的初次使用及调试","date":"2017-11-16T23:00:00.000Z","updated":"2020-02-29T18:43:55.411Z","comments":true,"path":"article/c4.html","link":"","permalink":"https://www.cz5h.com/article/c4.html","excerpt":"出现的错误1[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.3:compile (default-compile) on project JavaWebSocket: Fatal error compiling: 无效的目标发行版: 1.8 -&gt; [Help 1]","text":"出现的错误1[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.3:compile (default-compile) on project JavaWebSocket: Fatal error compiling: 无效的目标发行版: 1.8 -&gt; [Help 1] 修改： 1234567891011121314151617181920212223242526272829 &lt;plugin&gt; &lt;artifactId&gt;maven-war-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;2.6&lt;&#x2F;version&gt; &lt;configuration&gt; &lt;warSourceDirectory&gt;WebRoot&lt;&#x2F;warSourceDirectory&gt; &lt;failOnMissingWebXml&gt;false&lt;&#x2F;failOnMissingWebXml&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;3.3&lt;&#x2F;version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;&#x2F;source&gt; &lt;target&gt;1.8&lt;&#x2F;target&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt;&lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-war-plugin&lt;&#x2F;artifactId&gt; &lt;&#x2F;plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.6&lt;&#x2F;source&gt; &lt;target&gt;1.6&lt;&#x2F;target&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt;&lt;&#x2F;plugins&gt; 此处使用maven自动打包上传的插件 123456789101112131415 &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;&#x2F;groupId&gt; &lt;artifactId&gt;tomcat-maven-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;1.1&lt;&#x2F;version&gt; &lt;configuration&gt; &lt;url&gt;http:&#x2F;&#x2F;localhost:9080&#x2F;manager&#x2F;text&lt;&#x2F;url&gt; &lt;server&gt;tomcat8&lt;&#x2F;server&gt; &lt;username&gt;admin&lt;&#x2F;username&gt; &lt;password&gt;admin&lt;&#x2F;password&gt; &lt;ignorePackaging&gt;true&lt;&#x2F;ignorePackaging&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt;&#96;&#96;&#96; **tomcat-users.xml（修改后重启Tomcat）** 12**部署的名字与pom.xml开头的配置是一致的** 4.0.0 WeeeebSkt WeeeebSkt 0.0.1-SNAPSHOT war 12**注意下面的路径+项目名** [INFO] Deploying war to http://localhost:8080/WeeeebSkt [INFO] OK - Deployed application at context path /WeeeebSkt [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 2.045 s [INFO] Finished at: 2017-11-13T13:29:02+08:00 [INFO] Final Memory: 15M/308M [INFO] ------------------------------------------------------------------------ 12**果然报错** (index):20 WebSocket connection to 'ws://localhost:8080/websocket' failed: Error during WebSocket handshake: Unexpected response code: 404 1234567891011121314151617181920212223242526272829303132 **定位前台的出错位置：** ![image_1bupu08q5hn82aepo61qp31dr09.png-36.3kB][1] **确认后台的EndPoint：** ![image_1bupu1ltd8muej0nmn1o6u1cv8m.png-12.5kB][2] **完全对应，怎么会出错？？？** **当前环境：** ![image_1bupu4rg71ee81cps6rod41qs39.png-62.4kB][3] **项目环境，依赖：** ![image_1bupu6m1h1ou567jq8k19blcpdm.png-25.3kB][4] **查找错误：** 更换 Tomcat8.5 + JavaSE-1.8 结果完全一样的错误 &gt; j2ee 项目里面是不是设置了拦截器之类的东西，在web.xml中配置的。可能ws协议的请求被拦截器拦截下来了。这方面的问题网络上有很多。我犯过的一个弱智的错误：在 ws 请求路径中没有写 j2ee 的项目名称。比如我的工程项目叫BoardGame，var webSocket &#x3D; new WebSocket(&#39;ws:&#x2F;&#x2F;localhost:8080&#x2F;BoardGame&#x2F;websocket&#39;); &#x2F;&#x2F;没有 BoardGame 所有一定是找不到的。 **修改Tomcat7版本的代码：**前台代码修改： if ('WebSocket' in window) { //websocket = new WebSocket(\"ws://localhost:8080/websocket\"); websocket = new WebSocket(\"ws://localhost:8080/websocket\"); } 12**错误还是依旧：** WebSocket connection to 'ws://localhost:8080/WeeeebSkt/websocket' failed: Error during WebSocket handshake: Unexpected response code: 404 1234**修改Tomcat8版本的代码：**前台代码修改： if ('WebSocket' in window) { //websocket = new WebSocket(\"ws://localhost:8080/websocket\"); websocket = new WebSocket(\"ws://localhost:8080/websocket\"); } 12**已经连接成功：** [CodeLive] HTTP detected: Connecting using WS VM37:109 [CodeLive] Connected to CodeLive at ws://127.0.0.1:8123 123456789101112131415161718通过比较Tomcat7的Webapps中部署的文件来看，和Tomcat8.5中的完全一样，但是，结果是失败的。之后又尝试了很多修改都是报错。#### 结论如果你使用Tomcat7.05以上的版本+J2SE-1.7来尝试websocket时出现404，别找错误了，坑太多，直接更换环境吧，使用Tomcat8以上+J2SE-1.8是完全可行的，已亲测。 #### 运行效果![image_1bupvpea51j8va86v82154jase9.png-142.1kB][5] #### 下面是相关代码前台index.jsp中核心的javascript调用： var websocket = null; //判断当前浏览器是否支持WebSocket if ('WebSocket' in window) { websocket = new WebSocket(\"ws://localhost:8080/websocket\"); } else { alert('当前浏览器 Not support websocket') } //连接发生错误的回调方法 websocket.onerror = function () { setMessageInnerHTML(&quot;WebSocket连接发生错误&quot;); }; //连接成功建立的回调方法 websocket.onopen = function () { setMessageInnerHTML(&quot;WebSocket连接成功&quot;); } //接收到消息的回调方法 websocket.onmessage = function (event) { setMessageInnerHTML(event.data); } //连接关闭的回调方法 websocket.onclose = function () { setMessageInnerHTML(&quot;WebSocket连接关闭&quot;); } //监听窗口关闭事件，当窗口关闭时，主动去关闭websocket连接，防止连接还没断开就关闭窗口，server端会抛异常。 window.onbeforeunload = function () { closeWebSocket(); } //将消息显示在网页上 function setMessageInnerHTML(innerHTML) { document.getElementById(&apos;message&apos;).innerHTML += innerHTML + &apos;&lt;br/&gt;&apos;; } //关闭WebSocket连接 function closeWebSocket() { websocket.close(); } //发送消息 function send() { var message = document.getElementById(&apos;text&apos;).value; websocket.send(message); } 12#### 后台WebSocket的实现 import java.io.IOException;import java.util.concurrent.CopyOnWriteArraySet;import javax.websocket.*;import javax.websocket.server.ServerEndpoint; /** @ServerEndpoint 注解是一个类层次的注解，它的功能主要是将目前的类定义成一个websocket服务器端, 注解的值将被用于监听用户连接的终端访问URL地址,客户端可以通过这个URL来连接到WebSocket服务器端 /@ServerEndpoint(“/websocket”)public class Ws { //静态变量，用来记录当前在线连接数。应该把它设计成线程安全的。 private static int onlineCount = 0; //concurrent包的线程安全Set，用来存放每个客户端对应的MyWebSocket对象。若要实现服务端与单一客户端通信的话，可以使用Map来存放，其中Key可以为用户标识 private static CopyOnWriteArraySet webSocketSet = new CopyOnWriteArraySet(); //与某个客户端的连接会话，需要通过它来给客户端发送数据 private Session session; /** 连接建立成功调用的方法 @param session 可选的参数。session为与某个客户端的连接会话，需要通过它来给客户端发送数据 /@OnOpenpublic void onOpen(Session session){ this.session = session; webSocketSet.add(this); //加入set中 addOnlineCount(); //在线数加1 System.out.println(“有新连接加入！当前在线人数为” + getOnlineCount());} /** 连接关闭调用的方法 /@OnClosepublic void onClose(){ webSocketSet.remove(this); //从set中删除 subOnlineCount(); //在线数减1 System.out.println(“有一连接关闭！当前在线人数为” + getOnlineCount());} /** 收到客户端消息后调用的方法 @param message 客户端发送过来的消息 @param session 可选的参数 /@OnMessagepublic void onMessage(String message, Session session) { System.out.println(“来自客户端的消息:” + message); //群发消息 for(Ws item: webSocketSet){ try { item.sendMessage(message); } catch (IOException e) { e.printStackTrace(); continue; } }} /** 发生错误时调用 @param session @param error /@OnErrorpublic void onError(Session session, Throwable error){ System.out.println(“发生错误”); error.printStackTrace();}} 12#### Maven配置： 4.0.0 JavaWebSocket JavaWebSocket 0.0.1-SNAPSHOT war 3.1 javax.websocket javax.websocket-api 1.0 src maven-compiler-plugin 3.3 1.8 1.8 org.codehaus.mojo tomcat-maven-plugin 1.1 http://localhost:9080/manager/text tomcat8 admin admin true maven-war-plugin 2.6 WebRoot false ```","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"WebSocket","slug":"WebSocket","permalink":"https://www.cz5h.com/tags/WebSocket/"}]},{"title":"用WebSocket给传统Web系统赋能","slug":"2017-11-15 用WebSocket给传统Web系统赋能","date":"2017-11-14T23:00:00.000Z","updated":"2020-02-29T18:43:55.409Z","comments":true,"path":"article/236a.html","link":"","permalink":"https://www.cz5h.com/article/236a.html","excerpt":"现有的机制传统的Web系统的全部交互都采用HTTP请求的方式，另外在可视化领域的一些Web展示系统，也多是以HTTP请求的方式来返回需要展示的结果，虽然使用AJAX等技术可以使展示结果具有很高的交互性，但是本质上仍然是针对某一次返回结果的再加工。","text":"现有的机制传统的Web系统的全部交互都采用HTTP请求的方式，另外在可视化领域的一些Web展示系统，也多是以HTTP请求的方式来返回需要展示的结果，虽然使用AJAX等技术可以使展示结果具有很高的交互性，但是本质上仍然是针对某一次返回结果的再加工。 难以完成的任务对于这样的场景：需要将某个迭代算法中每次迭代的数据进行可视化，传统认知上这样的需求一般在Java语言领域使用JavaSwing进行开发，保证展示效果的同时也保证了整个数据传输的效率（几乎没有传输损耗，数据都在内存中），如果非要以Web方式承载，那么一般方式是使用Ajax长轮询，这种方式核心仍然是一个个的HTTP请求，并不能将算法中迭代的某一中间结果返回到Web端。 解决方式WebSocket技术伴随着HTML5出现后给这种场景提供了一种可行的解决方案，其可以允许前后台创建一种类似Socket的长连接机制，而且可以维持多个连接，这种非常类似Socket的特点也使得其取名为WebSocket，可以允许前后台建立一个稳定的连接，在后台代码中这个连接以Session的方式维护，后台可以无限制的、随时的、随处的向前台传输数据（只需引用这个Session）。总的来说，这种方式赋予了传统Web系统（BS系统）具有CS系统的敏捷性和便捷性（数据传输不限于请求）。 其他解决方案本质上，一切以连接为基础的数据交互都能完成展示中间计算步骤的目的，例如Redis和一些MQ队列等等，但是能在前端支持js客户端的并不多见，阿里现在支持redis的js client，也有类似GoEasy这种黑盒方式的 前后台通讯方式，不过这些方式严重依赖第三方管理（例如上述两者都需要其自家的秘钥等等），在使用过程中并不透明，WebSocket在这方面具有优良的特性，而且也可以轻易的进行诸如队列的扩展。 如何实现在《WebSocket的使用》中已经对其开发方式做了说明，这里只对部分关键内容进行描述，其实区别于示例代码（多人实时聊天），对于一个需要中间过程数据的系统，例如布局算法中迭代结果的过程展现，其区别是很明显的： 不需要多人连接，也就不需要在Server对消息进行群发对于前者，只需在原有在线人数上加判断即可(前文代码中有)对于后者，之前代码中的群发部分就可以删除了 123456789101112131415161718/* * 收到客户端消息后调用的方法 * @param message 客户端发送过来的消息 * @param session */@OnMessagepublic void onMessage(String message, Session session) &#123; System.out.println(\"来自客户端的消息:\" + message); //群发消息,可以删掉 for(WebSocketTest item: webSocketSet)&#123; try &#123; item.sendMessage(message); &#125; catch (IOException e) &#123; e.printStackTrace(); continue; &#125; &#125;&#125; 需要在算法迭代过程中对当前会话进行引用，即迭代时需要Session在场： 123456789101112131415161718192021222324252627282930313233343536373839//下面的核心迭代控制代码中引入Session//然后在每次迭代过程中，将当前步的数据通过session.getBasicRemote().sendText方法发送给前端//当然，也可以将Session传入更深层的算法步中，使前台获取更深层算法步骤的中间值@Overridepublic void doLayout(Session session) &#123; try &#123; this.initAlgo(); if (layoutByTimes) &#123; for (int i = 0; i &lt; times; ++i) &#123; this.goAlgo(); this.temperature=cool(this.temperature); session.getBasicRemote().sendText(Output.outputJson(graph)); Thread.sleep(1000); &#125; &#125; else &#123; //如果没有迭代参数，则会按一次迭代来执行 this.goAlgo(); int times = 1; double force = Math.sqrt(this.resultantForceX * this.resultantForceX + this.resultantForceY + this.resultantForceY); System.out.println(force); while (this.forceThreshold &lt; force) &#123; this.goAlgo(); times++; System.out.println(force); force = Math.sqrt(this.resultantForceX * this.resultantForceX + this.resultantForceY + this.resultantForceY); Thread.sleep(1000); session.getBasicRemote().sendText(Output.outputJson(graph)); &#125; System.out.println(\"times=\" + times); &#125; &#125; catch (Exception e) &#123; // TODO Auto-generated catch block System.out.println(\"暂时没实现这种类型的布局\"); e.printStackTrace(); &#125;&#125; 修改原先的OnMessa方法，将此方法改造为类似Servlet的方法，该方法现在是作为后台接收前端数据的桥头堡，应该在这里对前端的数据进行过滤，并在此方法内发起相应的后台操作。 1234567891011@OnMessagepublic void onMessage(String message, Session session) &#123; System.out.println(\"来自客户端的消息:\" + message); //群发消息 try &#123; dojob(message); //这里做了简单包装，将全部逻辑放到dojob中去做 &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; 实现结果可以将整个布局算法在迭代过程中的每个步骤的布局结构都在前端进行展示，下面的效果只进行了一次请求，每次绘制的数据都是通过WebSocket传输到前端的，效果如下：（由于算法写的还有点问题所以节点运动轨迹有点问题，不过不影响效果展示）","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"https://www.cz5h.com/tags/WebSocket/"},{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"布局算法","slug":"布局算法","permalink":"https://www.cz5h.com/tags/%E5%B8%83%E5%B1%80%E7%AE%97%E6%B3%95/"}]},{"title":"DBLP数据集使用Python解析","slug":"2017-11-12 DBLP数据集使用Python解析","date":"2017-11-11T23:00:00.000Z","updated":"2020-02-29T18:43:55.407Z","comments":true,"path":"article/572b.html","link":"","permalink":"https://www.cz5h.com/article/572b.html","excerpt":"dblp的使用 总的来说，DBLP集成元素不多，只有最基本的论文题目，时间，作者，发表类型及期刊或会议名称等等。可能很多人想要的标签、关键词都没有。但是，基于DBLP数据集这些基本的元素，可以挖掘、利用的也是很多。例如官网给出的统计信息，就能引申出很多东西。","text":"dblp的使用 总的来说，DBLP集成元素不多，只有最基本的论文题目，时间，作者，发表类型及期刊或会议名称等等。可能很多人想要的标签、关键词都没有。但是，基于DBLP数据集这些基本的元素，可以挖掘、利用的也是很多。例如官网给出的统计信息，就能引申出很多东西。 涉及到DBLP，我能一下想到的关键词：经典的复杂网络，小世界，无标度，合作关系网，关系推荐，聚类，连接预测，随机游走，中心作者分析，作者影响力分析，研究热点发展等等，非常多。因此，DBLP是个很丰富宝贵的资源。引述自：http://blog.csdn.net/frontend922/article/details/18552077 dblp下载dblp.dtd 2017-08-29 16:23 13K dblp.xml.gz 2017-11-10 20:26 393M XML下载链接 http://dblp.uni-trier.de/xml/dblp原始数据集示例12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;&lt;!DOCTYPE dblp SYSTEM \"dblp.dtd\"&gt;&lt;dblp&gt; &lt;article mdate=\"2017-05-28\" key=\"journals/acta/Saxena96\"&gt; &lt;author&gt;Sanjeev Saxena&lt;/author&gt; &lt;title&gt;Parallel IntegerSimulation Amongst CRCW Models.&lt;/title&gt; &lt;pages&gt;607-619&lt;/pages&gt; &lt;year&gt;1996&lt;/year&gt; &lt;volume&gt;33&lt;/volume&gt; &lt;journal&gt;Acta Inf.&lt;/journal&gt; &lt;number&gt;7&lt;/number&gt; &lt;url&gt;db/journals/acta/acta33.html#Saxena96&lt;/url&gt; &lt;ee&gt;https://doi.org/10.1007/BF03036466&lt;/ee&gt; &lt;/article&gt; &lt;article mdate=\"2017-05-28\" key=\"journals/acta/Simon83\"&gt; &lt;author&gt;Hans Ulrich Simon&lt;/author&gt; &lt;title&gt;Pattern Matching in Trees and Nets.&lt;/title&gt; &lt;pages&gt;227-248&lt;/pages&gt; &lt;year&gt;1983&lt;/year&gt; &lt;volume&gt;20&lt;/volume&gt; &lt;journal&gt;Acta Inf.&lt;/journal&gt; &lt;url&gt;db/journals/acta/acta20.html#Simon83&lt;/url&gt; &lt;ee&gt;https://doi.org/10.1007/BF01257084&lt;/ee&gt; &lt;/article&gt;&lt;/dblp&gt; dblp数据集建表语句12345678910111213141516171819202122232425262728293031323334353637/*Navicat MySQL Data TransferSource Server : localmysqlSource Server Version : 50540Source Host : localhost:3306Source Database : visual_datasetTarget Server Type : MYSQLTarget Server Version : 50540File Encoding : 65001Date: 2017-11-11 17:44:38*/SET FOREIGN_KEY_CHECKS=0;-- ------------------------------ Table structure for dblp-- ----------------------------DROP TABLE IF EXISTS `dblp`;CREATE TABLE `dblp` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `article_mdate` varchar(255) DEFAULT NULL, `article_key` varchar(255) DEFAULT NULL, `author` varchar(255) DEFAULT NULL, `title` varchar(255) DEFAULT NULL, `pages` varchar(255) DEFAULT NULL, `year` varchar(255) DEFAULT NULL, `volume` varchar(255) DEFAULT NULL, `journal` varchar(255) DEFAULT NULL, `number` varchar(255) DEFAULT NULL, `url` varchar(255) DEFAULT NULL, `ee` varchar(255) DEFAULT NULL, `x2` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=MyISAM DEFAULT CHARSET=gbk; 将dblp.xml解析到文件中的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202# -*- coding: utf-8 -*-\"\"\"原代码只将数据解析到文本，且对重复字段没有进行处理&lt;article&gt; &lt;author&gt;Mr.A&lt;/author&gt; &lt;author&gt;Mr.B&lt;/author&gt;&lt;/article&gt;此代码修正了上述不足，然后将解析后字段导入数据库读取数据：dblp.xml 2.01G导入Mysql：170万+导入表：visual_dataset.dblp生成备份文件：insert.sql@author: Administrator\"\"\"#!/usr/bin/python# -*- coding: UTF-8 -*-from __future__ import print_functionimport xml.saximport sys import ioimport reimport loggingimport traceback import pymysql.cursorssys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='utf8') #改变标准输出的默认编码 logging.basicConfig(level=logging.DEBUG, format='%(message)s', datefmt='%a, %d %b %Y %H:%M:%S', filename='I:\\\\ABC000000000000\\\\Dblp\\\\simple\\\\app.log', filemode='w') class MovieHandler( xml.sax.ContentHandler ): ''' res 类变量，记录解析后的字段值 ''' athr = [] ee = [] res='' sqlval='' def __init__(self): self.CurrentData = \"\" self.author = \"\" self.title = \"\" self.pages = \"\" self.year = \"\" self.volume = \"\" self.journal = \"\" self.number = \"\" self.url = \"\" self.ee = \"\" # 元素开始事件处理,对每个顶级标签内数据的解析都会重复的调用此方法 def startElement(self, tag, attributes): self.CurrentData = tag if tag == \"article\": try: if len(self.__class__.sqlval) : #print(re.sub(\",$\",\"\",self.__class__.sqlval)) lt = re.sub(\",$\",\"\",self.__class__.sqlval).split(\",\") lt2= sorted(set(lt),key=lt.index) insert_mysql( ','.join(lt2),self.__class__.res, ','.join(self.__class__.athr), ','.join(self.__class__.ee) ) except: traceback.print_exc() #清空res变量，由于跨方法拼字符串，所以使用了类变量 self.__class__.res='' self.__class__.sqlval='' self.__class__.athr=[] self.__class__.ee=[] #因为处在if判断后，所以只解析第一个标签内的属性值 mdate = attributes[\"mdate\"] key = attributes[\"key\"] #拼接字符串 self.__class__.res += mdate + SYMBOL + key + SYMBOL self.__class__.sqlval += \"article_mdate,article_key,\" # 经过开始事件-&gt;内容事件的方法之后，调用此结束事件处理， # 对先前内容事件方法中对实例变量的值进行统一过滤处理 def endElement(self, tag): if self.CurrentData == \"author\": self.__class__.sqlval += \"author,\" if '$_author_$' not in self.__class__.res: self.__class__.res += \"$_author_$\" + SYMBOL self.__class__.athr.append(self.author) elif self.CurrentData == \"title\": self.__class__.sqlval += \"title,\" self.__class__.res += self.title + SYMBOL elif self.CurrentData == \"pages\": self.__class__.sqlval += \"pages,\" self.__class__.res += self.pages + SYMBOL elif self.CurrentData == \"year\": self.__class__.sqlval += \"year,\" self.__class__.res += self.year + SYMBOL elif self.CurrentData == \"volume\": self.__class__.sqlval += \"volume,\" self.__class__.res += self.volume + SYMBOL elif self.CurrentData == \"journal\": self.__class__.sqlval += \"journal,\" self.__class__.res += self.journal + SYMBOL elif self.CurrentData == \"number\": self.__class__.sqlval += \"number,\" self.__class__.res += self.number + SYMBOL elif self.CurrentData == \"url\": self.__class__.sqlval += \"url,\" self.__class__.res += self.url + SYMBOL elif self.CurrentData == \"ee\": self.__class__.sqlval += \"ee,\" if '$_ee_$' not in self.__class__.res: self.__class__.res += \"$_ee_$\" + SYMBOL self.__class__.ee.append(self.ee) self.CurrentData = \"\" # 内容事件处理，对每个子元素都执行此方法，并且重置实例变量的值 def characters(self, content): if self.CurrentData == \"author\": self.author = content.replace(\"'\",\"`\") elif self.CurrentData == \"title\": self.title = content.replace(\"'\",\"`\") elif self.CurrentData == \"pages\": self.pages = content.replace(\"'\",\"`\") elif self.CurrentData == \"year\": self.year = content.replace(\"'\",\"`\") elif self.CurrentData == \"volume\": self.volume = content.replace(\"'\",\"`\") elif self.CurrentData == \"journal\": self.journal = content.replace(\"'\",\"`\") elif self.CurrentData == \"number\": self.number = content.replace(\"'\",\"`\") elif self.CurrentData == \"url\": self.url = content.replace(\"'\",\"`\") elif self.CurrentData == \"ee\": self.ee = content.replace(\"'\",\"`\")#class结束'''独立方法：将解析出的字段导入Mysql'''def insert_mysql(names,values,authors,ees): global count if count==100: sys.exit val = re.sub(\",'$\",\"\",values) val = re.sub(\"#\",\"&amp;\",val) val = val.replace(\"$_ee_$\",re.sub(\",\",\",\",ees)) val = val.replace(\"$_author_$\",re.sub(\",\",\",\",authors)) sql = '' if len(names) &amp; len(names): try: #存入Mysql via：github.com/PyMySQL/PyMySQL with connection.cursor() as cursor: sql = \"INSERT INTO `dblp` (\" sql +=names sql +=\" )VALUES ('\" sql +=val sql +=\" )\" count += 1 print('parse items and inserted :'+str(count)) if sql is not None and sql != 'None': logging.info(sql+';') cursor.execute(sql) #创建的connection是非自动提交，需要手动commit connection.commit() a = 1 except: logging.error(traceback.print_exc())#这里直接运行，则本身__name__就是__main__ if ( __name__ == \"__main__\"): count = 0 #定义全局分隔符 SYMBOL = \"','\" XMLFPATH = \"I:\\\\ABC000000000000\\\\Dblp\\\\dblp.xml\" parser = xml.sax.make_parser() parser.setFeature(xml.sax.handler.feature_namespaces, 0) Handler = MovieHandler() parser.setContentHandler( Handler ) connection = pymysql.connect( host='localhost', user='root', password='123', db='visual_dataset', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor) parser.parse(XMLFPATH) connection.close() 原代码来源于网络123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112# -*- coding: utf-8 -*-\"\"\"解析dblp.xml，将结果存入dblp_result.txt内@author: Administrator\"\"\"#!/usr/bin/python# -*- coding: UTF-8 -*-from __future__ import print_functionimport xml.saximport sys import ioimport traceback sys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='utf8') class MovieHandler( xml.sax.ContentHandler ): ''' res 类变量，记录解析后的字段值 ''' res='' def __init__(self): self.CurrentData = \"\" self.author = \"\" self.title = \"\" self.pages = \"\" self.year = \"\" self.volume = \"\" self.journal = \"\" self.number = \"\" self.url = \"\" self.ee = \"\" # 元素开始事件处理 def startElement(self, tag, attributes): self.CurrentData = tag if tag == \"article\": print(\"self.__class__.res=\",self.__class__.res) try: ww.write(self.__class__.res + '\\n') except: traceback.print_exc() #清空res变量，由于跨方法拼字符串，所以使用了类变量 self.__class__.res='' #因为处在if判断后，所以只解析第一个标签内的属性值 mdate = attributes[\"mdate\"] key = attributes[\"key\"] #拼接字符串 self.__class__.res=self.__class__.res + mdate + ';,;' + key + ';,;' # 元素结束事件处理 def endElement(self, tag): if self.CurrentData == \"author\": #print (\"author:\", self.author) self.__class__.res=self.__class__.res + self.author + ';,;' elif self.CurrentData == \"title\": #print (\"title:\", self.title) self.__class__.res=self.__class__.res + self.title + ';,;' elif self.CurrentData == \"pages\": #print (\"pages:\", self.pages) self.__class__.res=self.__class__.res + self.pages + ';,;' elif self.CurrentData == \"year\": #print (\"year:\", self.year) self.__class__.res=self.__class__.res + self.year + ';,;' elif self.CurrentData == \"volume\": #print (\"volume:\", self.volume) self.__class__.res=self.__class__.res + self.volume + ';,;' elif self.CurrentData == \"journal\": #print (\"journal:\", self.journal) self.__class__.res=self.__class__.res + self.journal + ';,;' elif self.CurrentData == \"number\": #print (\"number:\", self.number) self.__class__.res=self.__class__.res + self.number + ';,;' elif self.CurrentData == \"url\": #print (\"url:\", self.url) self.__class__.res=self.__class__.res + self.url + ';,;' elif self.CurrentData == \"ee\": #print (\"ee:\", self.ee) self.__class__.res=self.__class__.res + self.ee + ';,;' self.CurrentData = \"\" # 内容事件处理 def characters(self, content): if self.CurrentData == \"author\": self.author = content elif self.CurrentData == \"title\": self.title = content elif self.CurrentData == \"pages\": self.pages = content elif self.CurrentData == \"year\": self.year = content elif self.CurrentData == \"volume\": self.volume = content elif self.CurrentData == \"journal\": self.journal = content elif self.CurrentData == \"number\": self.number = content elif self.CurrentData == \"url\": self.url = content elif self.CurrentData == \"ee\": self.ee = content #class结束#这里直接运行，则本身__name__就是__main__ if ( __name__ == \"__main__\"): parser = xml.sax.make_parser() parser.setFeature(xml.sax.handler.feature_namespaces, 0) Handler = MovieHandler() parser.setContentHandler( Handler ) ww=open('I:\\\\ABC000000000000\\\\Dblp\\\\simple\\\\dblp_result.txt','w+') parser.parse(\"I:\\\\ABC000000000000\\\\Dblp\\\\simple\\\\dblp.xml\") ww.close() 对于dblp数据的使用(待续)","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"},{"name":"数据集","slug":"数据集","permalink":"https://www.cz5h.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"}]},{"title":"替代Websocket的解决方案：GoEasy","slug":"2017-11-11 替代Websocket的解决方案：GoEasy","date":"2017-11-10T23:00:00.000Z","updated":"2020-02-29T18:43:55.488Z","comments":true,"path":"article/6e50.html","link":"","permalink":"https://www.cz5h.com/article/6e50.html","excerpt":"写在前面GoEasy这个库的适用场景：同Websocket的场景在后台使用例如Java进行逻辑处理后将变量的值传入前台，前台不用发起请求即可接收后台发布的数据，整个流程与Redis的Pub和Sub过程类似整个交互类似Socket的长连接，前台首次调用不需要请求后台。非常适合监控后台参数等场景；","text":"写在前面GoEasy这个库的适用场景：同Websocket的场景在后台使用例如Java进行逻辑处理后将变量的值传入前台，前台不用发起请求即可接收后台发布的数据，整个流程与Redis的Pub和Sub过程类似整个交互类似Socket的长连接，前台首次调用不需要请求后台。非常适合监控后台参数等场景； 但是：GoEasy最大的问题：传输数据大小有限制，大概只有几千字符！！超出大小的传输部分会被丢弃从而会报错。 从GoEasy获取appkeyappkey是验证用户的有效性的唯一标识。 注册账号。 GoEasy官网：http://goeasy.io用注册好的账号登录到GoEasy的后台管理系统，创建您自己应用（application）.Application创建好之后系统会自动为您生成appkey系统会生成两个keys，一个Super key和一个Subscribe key；它们的区别在于前者既可以订阅又可以推送，但后者只能用于订阅。 GoEasy实现向特定用户群推送的原理知道了他们的推送原理，可以更加方便我们了解他们的服务，以及理解我们写的代码。其实原理很简单，只需要确定哪些用户需要接收信息，然后让这些用户都订阅一个相同的channel（频道）。 然后再往这个平台上推送消息即可！所有关键在于channel，channel一致，则可以接收到信息，否则收不到！ 对于订阅必须要的信息有：Appkey, channel 对于推送必须要的信息有：Appkey, channel, content 用GoEasy实现订阅（接收）的实例123456789101112&lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;https:&#x2F;&#x2F;cdn.goeasy.io&#x2F;goeasy.js&quot;&gt;&lt;&#x2F;script&gt;&lt;script type&#x3D;&quot;text&#x2F;javascript&quot;&gt; var goEasy &#x3D; new GoEasy(&#123;appkey: &#39;your appkey&#39;&#125;); goEasy.subscribe(&#123; channel: &#39;your_channel&#39;, onMessage: function(message)&#123; alert(&#39;接收到消息:&#39;+message.content); &#x2F;&#x2F;拿到了信息之后，你可以做你任何想做的事 &#125; &#125;);&lt;&#x2F;script&gt; 有了这几行代码后，只要保证网络畅通的情况下，页面会自动弹出你从任何平台上推送的信息。 用GoEasy实现推送及接收的实例目前GoEasy支持三种推送方式： Java后台推送（它们有提供JAVA SDK和 maven远程仓库）， JS推送，RestAPI推送（有了RestAPI，我们就可以用PHP, .NET, Ruby…来推送信息了，很方便） 说了这么多，来我们看一下怎么用GoEasy的三种方式分别实现推送吧。 用GoEasy SDK推送获取SDKJava SDK的获取方式，方式一，直接在goeasy的官网上进行下载；方式二，用maven远程库直接导入到项目中。尽管官网上已经做了相同的说明了，我这里还是把关键点帖出来，方便大家查看。 GoEasy SDK下载链接：http://maven.goeasy.io/service/local/artifact/maven/redirect?r=releases&amp;g=io.goeasy&amp;a=goeasy-sdk&amp;v=0.3.3&amp;e=jar GoEasy远程maven库的配置： 1234567891011&lt;repository&gt; &lt;id&gt;goeasy&lt;&#x2F;id&gt; &lt;name&gt;goeasy&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;maven.goeasy.io&#x2F;content&#x2F;repositories&#x2F;releases&#x2F;&lt;&#x2F;url&gt;&lt;&#x2F;repository&gt; …&lt;dependency&gt; &lt;groupId&gt;io.goeasy&lt;&#x2F;groupId&gt; &lt;artifactId&gt;goeasy-sdk&lt;&#x2F;artifactId&gt; &lt;version&gt;0.3.3&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt; 需要注意的是：GoEasy需要依赖两个额外的jar 包：gson.jar : http://repo.maven.apache.org/maven2/com/google/code/gson/gson/2.3.1/gson-2.3.1.jarslf4j-api.jar : http://repo.maven.apache.org/maven2/org/slf4j/slf4j-api/1.7.2/slf4j-api-1.7.2.jar 实例化GoEasy对象1234GoEasy goEasy &#x3D; new GoEasy(&quot;your appkey&quot;);&#96;&#96;&#96; ##### 推送消息 goEasy.publish(‘your_channel’, ‘First message’); 1234#### JS推送**Step1.引入goeasy.js** CDN：https://cdn.goeasy.io/goeasy.js 12**Step2.实例化Goeasy对象，并用publish函数进行推送** var goEasy = new GoEasy({appkey: ‘your appkey’}); goEasy. publish ({ channel: ‘your_channel’, message: ‘Second message！’}); #### 用RestAPI进行推送 URL: https://goeasy.io/goeasy/publish Method: Post 参数：appkey, channel, content 例如：https://goeasy.io/goeasy/publish?appkey={your_appkey}&amp;channel={your_channel}&amp;content={your_message} GoEasy官网：http://goeasy.io 快速入门：http://goeasy.io/www/started 文档下载：http://goeasy.io/www/documents","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"https://www.cz5h.com/tags/WebSocket/"},{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"}]},{"title":"可视化布局算法的框架设计","slug":"2017-11-5 可视化布局算法的框架设计","date":"2017-11-04T23:00:00.000Z","updated":"2020-02-29T18:43:55.457Z","comments":true,"path":"article/cb21.html","link":"","permalink":"https://www.cz5h.com/article/cb21.html","excerpt":"写在前面原项目是一个Web项目，采用传统的Servlet方式，后台主要完成的工作是计算节点的坐标，将节点的坐标封装成json格式由与前台进行交互。前期阶段，从前后台的数据传输方面尝试对代码进行理解，但是原始代码运行环境未知，现有的代码在运行时会有各种错误，未果，放弃。现在直接将后台的业务处理代码抽离进行抽离。目的是形成一个最简单的可执行的布局算法效果展示的SDK","text":"写在前面原项目是一个Web项目，采用传统的Servlet方式，后台主要完成的工作是计算节点的坐标，将节点的坐标封装成json格式由与前台进行交互。前期阶段，从前后台的数据传输方面尝试对代码进行理解，但是原始代码运行环境未知，现有的代码在运行时会有各种错误，未果，放弃。现在直接将后台的业务处理代码抽离进行抽离。目的是形成一个最简单的可执行的布局算法效果展示的SDK 整体设计对于布局算法的目的，就是要对给定格式的图数据（如下图）进行节点坐标的计算，计算的规则通过布局算法来实现，整个流程应该包括以下几部分： 格式化数据的读入及数据结构的绑定 通过布局算法对数据的坐标计算 坐标结果的格式化及数据的输出 上述过程中应该涉及的数据结构(类)设计 图结构的设计（基础数据结构）：Graph、Node、Edges 绑定输入数据导上述的结构（配置类）：GraphData、BSPNodeFormatImpl 布局算法设计（布局类）：FRForceLayout 对算法的配置（配置类）：FRLayoutConfig 输入数据的配置：DataConfig 输出数据：Output 整体结构 123456789101112131415161718192021 gvbd.congfig 包含力导向算法的配置类(Getter和Setter) LayoutConifg 所有Config类的父类,公共参数：迭代次数、是否指定迭代次数、布局长宽 ForceLayoutConifg 私有：K值、阈值、是否有向、速度 FRLayoutConifg 私有：K值、阈值、是否有向、冷却值、温度值 gvbd.graph 定义布局数据结构的包,包含图+边+节点等类的定义 gvbd.data 包含图数据的格式化类、 gvbd.layout 包含布局算法的调用类//运行流程 获得输入，之后 gvbd.data 对输入数据进行规整，调用： gvbd.graph 实例化Graph类，Node类等 gvbd.layout 对节点数据进行布局，调用： gvbd.congfig 对布局算法进行配置 gvbd.evaluate 节点价值的计算 布局结束之后获得全部节点的坐标数据，开始可视显示 使用d3/Gephi等等 整个后台代码可大致分为四个部分：基础数据结构、配置类、绑定类、布局类 基础数据结构这里要注意Graph类的成员变量只含一个Node类对象数组，对于Node类，要特别关注，其既包含节点本身的信息，也包含节点涉及的边的信息，对于边Edge类，其包含起始点和目标点(int类型)，以及权重，可以通过不同的构造函数对带权重和不带权重的两种情况进行实例化。 123456789101112131415161718192021public class Graph &#123; Node[] nodes; //节点集 ...&#125;public class Node &#123; private int nodeId; //节点ID private String nodeName; //节点名称 private String nodeLabel; //节点标签 private String nodeValue; //节点权重值 private NodeLayoutData nodeLayoutData; //节点的坐标[x,y]，记录两次，非常重要 private List&lt;Edge&gt; edges; //涉及的所有边，绑定输入数据 private List&lt;Edge&gt; edges2; //备用 private Map&lt;Edge,Float&gt; edge3; //备用，涉及的边并且带边权重的情况 ...&#125;public class Edge &#123; private int resource; //节点关联边的起点 private int target; //节点关联边的终点 private float weight; //节点关联边的权重 ...&#125; 绑定类这部分是将输入的格式化数据初始化为相关数据结构，即使用输入的数据实例化相关对象，主要依靠的方法是graphData.loadNodeData方法。该方法主要是传入输入数据的文件流参数，在GraphData类中默认实例化一个Graph类对象，并通过上述load方法对Graph对象的节点和边进行初始化。 1234567891011121314151617181920//实例化gvbd.data.GraphDatapublic void loadNodeData(BufferedReader nodeDataReader,NodeFormat nodeFormat,int vertexNum) &#123; graph.createNodes(vertexNum); //按输入的节点数创建相关数量节点的图 Node [] nodes=graph.getNodes(); //nodes是构造的 String nodeLine; int lineNo=0; try &#123; nodeLine = nodeDataReader.readLine(); if(lineNo == 0)System.out.println(nodeLine); while (nodeLine != null) &#123; Node node = nodeFormat.stringToNode(nodeLine); //转换节点类型 nodes[lineNo++]=node; nodeLine = nodeDataReader.readLine(); &#125; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;&#125; 配置类这部分主要是调用布局算法前的先导部分，本质是对配置参数的封装，并且规定使用Getter和Setter方法进行参数赋值。另外，在赋值结束后只需在下一步布局算法调用时将该配置类的对象传入即可使布局算法得到相应的参数值。 12345678910111213141516171819202122232425262728293031//不同布局算法具有不同的参数，所以下面是有公共参数的父类，具体算法配置类应该继承此类public class LayoutConfig &#123; private int times=0; private boolean layoutByTimes; private int width; private int height; public int getTimes() &#123; return times; &#125; public void setTimes(int times) &#123; this.times = times; &#125; public boolean isLayoutByTimes() &#123; return layoutByTimes; &#125; public void setLayoutByTimes(boolean layoutByTimes) &#123; this.layoutByTimes = layoutByTimes; &#125; public int getWidth() &#123; return width; &#125; public void setWidth(int width) &#123; this.width = width; &#125; public int getHeight() &#123; return height; &#125; public void setHeight(int height) &#123; this.height = height; &#125;&#125; 执行入口/调用流程123456789101112131415161718192021222324252627282930313233public static void main(String[] args)&#123; DataConfig.setDataPath(\"C:\\\\Users\\\\msi\\\\Desktop\\\\xiaomi.txt\"); //读取此txt文件 BufferedReader br = new BufferedReader( new InputStreamReader(new FileInputStream( DataConfig.getDataPath()), \"utf-8\")); DataConfig.setDataReader(br); DataConfig.setNodeFormat(new BSPNodeFormatImpl());//注意：此处必须和输入数据的个数完全一致，否则会在节点计算初始坐标时出错 DataConfig.setNodeNum(3142); if (layoutMethod.equals(\"FRLayout\")) &#123; //实例化配置对象,有部分方法是继承自父类的 FRLayoutConfig layoutConfig = new FRLayoutConfig(); layoutConfig.setLayoutByTimes(true); layoutConfig.setDirected(false); layoutConfig.setWidth(5000); layoutConfig.setHeight(5000); layoutConfig.setLayoutByTimes(true); layoutConfig.setK(Float.parseFloat(\"0.2\")); layoutConfig.setTimes(Integer.parseInt(\"3\")); layoutConfig.setCool(Float.parseFloat(\"0.8\")); layoutConfig.setTemperature(Integer.parseInt(\"1\")); //按DataConfig的参数输入loadNodeData， GraphData graphData = new GraphData(); //包含全部的数据，点、边、关系等等； graphData.loadNodeData(DataConfig.getDataReader(), DataConfig.getNodeFormat(), DataConfig.getNodeNum()); //传入已建立的图对象 和 布局算法配置对象 Layout layout = new FRForceLayout(graphData.getGraph(), layoutConfig); layout.doLayout();//迭代算法 Output.outputJson(graphData.getGraph(),\"C:\\\\Users\\\\msi\\\\Desktop\\\\xiaomi.json\"); System.out.println(\"FRLayout--end\"); &#125;&#125; ### 结果数据 通过Output类中的转换方法，将存储结果的整个Graph对象的坐标数据转化为Json格式，并输出到文件，最后的数据如下(截取部分)： 1&#123;\"nodes\":[&#123;\"name\":\"1\",\"value\":\"小米微博组 \",\"cy\":\"156.01001534887016\",\"cx\":\"3166.150545142414\"&#125;,&#123;\"name\":\"2\",\"value\":\"神得强Steven \",\"cy\":\"640.4571943091852\",\"cx\":\"2949.154447954751\"&#125;,&#123;\"name\":\"3\",\"value\":\"MIUI论坛 \",\"cy\":\"3570.9778335387005\",\"cx\":\"1698.232117858166\"&#125;,&#123;\"name\":\"4\",\"value\":\"小米平板 \",\"cy\":\"2063.9975461388217\",\"cx\":\"4651.260384562792\"&#125;,&#123;\"name\":\"5\",\"value\":\"小米电视 \",\"cy\":\"4817.615271549201\",\"cx\":\"3685.092338867302\"&#125;,&#123;\"name\":\"6\",\"value\":\"公民大李 \",\"cy\":\"2608.7081901026404\",\"cx\":\"178.94341205320288\"&#125;,&#123;\"name\":\"7\",\"value\":\"小米电视俱乐部 \",\"cy\":\"4989.0703708172905\",\"cx\":\"897.1708897116042\"&#125;,&#123;\"name\":\"8\",\"value\":\"小米客服那些事 \",\"cy\":\"162.15895711734697\",\"cx\":\"1687.8496029706146\"&#125;,&#123;\"name\":\"9\",\"value\":\"小米空气净化器 \",\"cy\":\"1377.3787717779344\",\"cx\":\"1288.6280165113844\"&#125;,&#123;\"name\":\"10\",\"value\":\"肉肉的大飞哥 \",\"cy\":\"4365.703702756339\",\"cx\":\"1768.5797529564923\"&#125;,&#123;\"name\":\"11\",\"value\":\"love魑魅魍魉999\",\"cy\":\"884.4260930361169\",\"cx\":\"1918.0952190492774\"&#125;,&#123;\"name\":\"12\",\"value\":\"金子一言520 \",\"cy\":\"4306.517287882674\",\"cx\":\"857.9229659557374\"&#125;,&#123;\"name\":\"13\",\"value\":\"gasaraki神探2333\",\"cy\":\"2858.0329169946035\",\"cx\":\"3368.2338019639533\"&#125;,&#123;\"name\":\"14\",\"value\":\"hellen1314\",\"cy\":\"1387.16941825711\",\"cx\":\"2564.64775703357\"&#125;,&#123;\"name\":\"15\",\"value\":\"梦的一生9 \",\"cy\":\"2898.153990200826\",\"cx\":\"4351.388549183514\"&#125;,&#123;\"name\":\"16\",\"value\":\"小皮康 \",\"cy\":\"4540.151322415267\",\"cx\":\"4398.402966348725\"&#125;,&#123;\"name\":\"17\",\"value\":\"飞啊飞木有翅膀 \",\"cy\":\"3025.8018395215827\",\"cx\":\"657.2738586083469\"&#125;,&#123;\"name\":\"18\",\"value\":\"小米-惜诺\",\"cy\":\"2601.4091892917922\",\"cx\":\"140.32453861960772\"&#125;,&#123;\"name\":\"19\",\"value\":\"帅得被人侃C\",\"cy\":\"1986.1671743022102\",\"cx\":\"738.5324574182849\"&#125;]&#125; 显示结果这部分主要是按坐标绘制点线的过程，由于大量计算操作已经完成，所以基本上没有什么开销，主要是绘图的开销（渲染和GPU的因素）,总的来说选择很多，如桌面应用形式的Gephi和前端形式的d3js，在这里，主要是使用的d3js对上述结果做了简单的绘制。为什么选择d3js呢，因为其对绘制做了高度的封装，所以代码非常简洁，而且速度也非常两人满意。 核心的坐标计算部分（待完善）第一阶段：读入数据，转化为图结构 涉及的类 第二阶段：坐标的计算 要计算：两节点之间的斥力、引力（斥力和引力与距离的关系如上图所示） 距离越远，引力越大斥力越小。 距离越近，引力越小斥力越大。 上图第一象限的表达式：f = + k/(d*d) 引力为正 第四象限的表达式：f = - (k*k)/d 斥力为负","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"https://www.cz5h.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"布局算法","slug":"布局算法","permalink":"https://www.cz5h.com/tags/%E5%B8%83%E5%B1%80%E7%AE%97%E6%B3%95/"}]},{"title":"Python的一些注意事项","slug":"2017-11-4 Python的一些注意事项","date":"2017-11-03T23:00:00.000Z","updated":"2020-02-29T18:43:55.446Z","comments":true,"path":"article/5dff.html","link":"","permalink":"https://www.cz5h.com/article/5dff.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 使用字典来代替 switch 语句方法： 使用字典的key代替 switch中的case 示例： switcher = { 0 : &apos;Go left&apos;, 1 : &apos;Go right&apos;, 2 : &apos;Go straght&apos; } 使用key选择分支： print(switcher[0]) 缺点： 模拟不了default分支 解决： 使用get方法，并指定匹配不到key时的返回值 print(switcher.get(5, &apos;Unknown&apos;)) #输出 Unknown 仍有缺点： 只能简单赋值，无法书写代码段 完备的示例： 将原有字符串部分均使用函数进行返回 代码： def goleft(): return &apos;goleft&apos; def goright(): return &apos;goright&apos; def gostraght(): return &apos;gostraght&apos; def default(): return &apos;default&apos; 注意：定义顺序必，函数必须先进行定义 switcher1 = { 0 : goleft, 1 : goright, 2 : gostraght } print( switcher1.get(5, default)() ) #输出default 注意： 调用形式，函数式编程 ：switcher1.get(5, default)() 由于get到的是函数名，所以需要加()显式调用列表推导式：（Pythonic）场景： 通过已经存在的列表，生成新的列表 示例： 对[1,2,3,4,5]全部元素进行平方 实现： 其他可以实现的方式：map+filter 推导式实现: a = [1,2,3,4,5,6] b = [i*i for i in a] # [ 关于i的表达式 for i in 列表 ] print(b) #输出：[1, 4, 9, 16, 25, 36] b = [i*i for i in a if i&gt;3] # [ 关于i的表达式 for i in 列表 if 关于i的判断 ] print(b) #输出：[16, 25, 36] 注意： 如果对于set进行集合推导式 代码： a = {1,2,3,4,5,6} b = {i*i for i in a} # [ 关于i的表达式 for i in 列表 ] print(b) #输出：{1, 4, 36, 9, 16, 25} 其他； 元组，列表，字典，集合，都可以被推导 对于字典： stu = { &apos;Tom&apos;:18, &apos;Jack&apos;:13, &apos;Kimmy&apos;:15 } 提取key： stub = { key for key,value in stu } 会报错，字典不能直接被解包遍历，需要使用items()方法 代码： print( { key for key,value in stu.items() } ) #{&apos;Tom&apos;, &apos;Kimmy&apos;, &apos;Jack&apos;} print( { value:key for key,value in stu.items() } ) #{18: &apos;Tom&apos;, 13: &apos;Jack&apos;, 15: &apos;Kimmy&apos;} 键值互换 print( ( key for key,value in stu.items() ) ) #&lt;generator object &lt;genexpr&gt; at 0x0000000000B36B48&gt;None 空类型 type=NoneType注意: 在类型和字符串方面：不是&apos;&apos;,不是[],不是0,不是false 验证： def fun1(): pass #函数没有return，返回None def fun2(): return #return后不跟变量，返回None print(fun1()) #返回None print(fun2()) #返回None 判断a = []为空的方式 正确的方式 if not a: print(&apos;None&apos;) else: print(&apos;NotNone----&apos;) 错误的方式 if a is None: print(&apos;None&apos;) else: print(&apos;NotNone&apos;) #a=[]时，并不能判空 注意： print(bool(None)) #Fasle print(bool([])) #Fasle python中每个类型都和True False有对应关系：None = False 验证一： class Test(): pass test = Test() if test: print(&apos;NotNone&apos;) else: print(&apos;None&apos;) #输出：NotNone print(bool(test)) #True print(bool(Test())) #True 验证二：（添加__len__方法） class Test(): def __len__(self): return 0 test = Test() if test: print(&apos;NotNone&apos;) else: print(&apos;None&apos;) #输出：None print(bool(test)) #False print(bool(Test())) #False 结论： 上述表明，实例化后使用if并不一定能判空，其取决于类内的定义 特别是__len__()和__bool__()方法，会决定实例化后返回的布尔取值 注意： __len__(self)方法返回值：只能返回整形或布尔值 触发函数： bool(Test()),在没有__bool__()时调用对象的__len__() len(Test()),在没有__bool__()时调用对象的__len__() 混合： class Test(): def __bool__(self): return True def __len__(self): return False test = Test() print(bool(Test())) #输出：True 结论： __bool__()方法优先级高于__len__()方法，如果有bool方法存在那么优先使用bool的返回值装饰器的副作用使用自定义的装饰器 import time def decorator(func): def wrapper(): &apos;&apos;&apos; wrapper的描述 &apos;&apos;&apos; print(time.time()) func() return wrapper 验证一： def f1(): &apos;&apos;&apos; 描述f1的作用等等 &apos;&apos;&apos; print(&apos;f1.__name__:&apos;,f1.__name__) f1() # 输出：f1 print(help(f1)) #输出：描述f1的作用等等 验证二： @decorator def f2(): &apos;&apos;&apos; 描述f2的作用等等 &apos;&apos;&apos; print(&apos;f2.__name__:&apos;,f2.__name__) f2() # 输出：wrapper # &gt;&gt;&gt; help(len)可以查看len()函数的说明 print(help(f2)) #输出：“wrapper的描述” 结论： 由上可知，装饰器会盖面原有函数的本身性质，函数名注释等等 定义无副作用的装饰器： 方法： 在自定义装饰器中添加装饰器@wraps 代码： import time from functools import wraps def decorator(func): &apos;&apos;&apos; 装饰器的注释 &apos;&apos;&apos; @wraps(func) #添加装饰器，可以带参数 def wrapper(): print(time.time()) func() return wrapper 验证： @decorator def f1(): &apos;&apos;&apos; 描述f1的作用等等 &apos;&apos;&apos; print(&apos;f1.__name__:&apos;,f1.__name__) f1() # 输出：f1 print(help(f1)) #输出：描述f1的作用等等 @wraps装饰器： 原先的装饰器直接执行的wrapper函数，添加@wraps装饰器之后， 首先执行warps装饰器，其可以获得被装饰函数的全部信息，所以可以将原被修饰的函数的全部信息进行保留","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"用Python实现神经网络(待续)","slug":"2017-11-4 实现神经网络(待续)","date":"2017-11-03T23:00:00.000Z","updated":"2020-06-27T22:26:23.291Z","comments":true,"path":"article/ccc3.html","link":"","permalink":"https://www.cz5h.com/article/ccc3.html","excerpt":"介绍人工智能的基本概念和逻辑体系 研究两种数据的分类算法 使用Python运用分类方法实现只有一层的神经网络","text":"介绍人工智能的基本概念和逻辑体系 研究两种数据的分类算法 使用Python运用分类方法实现只有一层的神经网络 分类两种类型感知器， 适用性的线性神经元使用Python的开发库：Pandas，Numpy，matplotlib：进行读取加工可视化 神经元 从交叉部分即神经末梢进行输入，在细胞核进行统一运算，然后通过轴突传递到末尾，通过末尾的分叉传递到其他神经元 神经元的数学表示：x：电信号w：弱化系数（权重向量），表示神经元分叉部分对信号弱化的向量x1-&gt;w1：从第一个管道传输时伴有的弱化z：细胞核将全部电信号整合在一起激活函数，又称单元步调函数当z的值大于等于阈值时发送1，小于某阈值时发送-1类似一个分类的函数，通常此函数比较复杂 向量的点乘（点积）：矩阵的转置： 有了权重向量w，和训练样本x 把权重向量初始化为0，或把每个分量初始化为【0,1】间得任意小数 把训练样本输入感知器，得到分类结果-1或1 根据分类结果更新权重向量w（反复输入更新，从而准确预测） 简化步调函数添加w0和x0从而将判断直接转换为判断z的正负 权重的更新算法，更新后的权重w： 更新的增量： y指的是输入的正确分类，y’感知器输出的分类即如果分类正确，那么整个增量为零，分类错误才需要调整系数n：模型的学习率，0~1，人为经验参数，需要使用者根据具体情况不断手动调整 权重更新示例：得到了错误分类：进行调整：调整完成：阈值更新： 感知器的适用范围：预测数据可以线性分割，不是A就是B不适用于线性不可分割的数据 感知器分类算法的步骤：初始化训练样本x初始化权重向量w做点积在步调函数/激活函数中进行判断判断正确则输出，错误则更新权重w 做点积 在步调函数/激活函数中进行判断 判断正确则输出，错误则更新权重w … … … … … …","categories":[{"name":"ML/R学习笔记","slug":"ML-R学习笔记","permalink":"https://www.cz5h.com/categories/ML-R%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.cz5h.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"笔记","slug":"笔记","permalink":"https://www.cz5h.com/tags/%E7%AC%94%E8%AE%B0/"}]},{"title":"Python的原生爬虫案例","slug":"2017-11-3 Python的原生爬虫案例","date":"2017-11-02T23:00:00.000Z","updated":"2020-03-02T00:25:28.803Z","comments":true,"path":"article/db81.html","link":"","permalink":"https://www.cz5h.com/article/db81.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 完整的爬虫：反扒机制，自动登录，代理IP等等示例爬虫：简单的数据抓取，简单的数据处理目的：不使用爬虫框架完成数据爬取 巩固知识、合理编程、内部原理示例内容：内容： 爬取直播网站 确定工作： 确定爬取数据：某个分类下各主播人气的数据 确定实现结果：将人气进行排序 准备： 分析网站结构 寻找包含爬取信息的页面 F12检查网页，定位信息（主播姓名，人气数据） 原理： 对html文件进行文本分析并从中提取信息 使用技术 正则表达式 具体步骤： 模拟HTTP请求，向服务器发送请求，获取到服务器返回的HTML 用正则表达式处理网页文本，过滤出有用数据 找到相关常量标签，作为正则的定位边界 定位标签： 尽量选择具有唯一标识的标识的标签 尽量选择与目标数据相近的标签 尽量选择将所有目标数据都包含的标签（闭合的标签），比如包含姓名+人气的标签 上述即尽量选父标签，不选兄弟标签，为了易于构造正则提取内容 注意： 构造正则不是难点，难点是应对反爬虫的措施 整体书写规范 每行代码不要过长 推荐书写一个入口程序 推荐在入口中平行的调用逻辑代码 每个方法中代码尽量少 注意块注释和行级注释的书写格式 代码结构：12345678910111213141516171819202122232425262728293031323334353637383940&#39;&#39;&#39;类注释&#39;&#39;&#39;class spider(): #抓取页面内容（行注释） def __fetch_content(self): &#39;&#39;&#39; 方法注释 &#39;&#39;&#39; pass #数据抽取 def __analysis(self, htmls): pass #数据精炼 def __refine(self, pairs): pass #排序 def __sort(self, pairs): pass #排序的算子 def __seed(self, pairs): pass #数据展现 def __show(self, pairs): pass #函数入口 def go(self):调用s &#x3D; spider()s.go() &#96;&#96;&#96; 书写代码： 抓取页面： url &#x3D; &#39;http:&#x2F;&#x2F;www.huya.com&#x2F;g&#x2F;lol&#39; 分析原网页： &lt;li class=&quot;game-live-item&quot; gid=&quot;1&quot;&gt; &lt;a href=&quot;http://www.huya.com/yanmie&quot; class=&quot;video-info new-clickstat&quot; target=&quot;_blank&quot; report=&apos;{&quot;eid&quot;:&quot;click/position&quot;,&quot;position&quot;:&quot;lol/0/1/5&quot;,&quot;game_id&quot;:&quot;1&quot;,&quot;ayyuid&quot;:&quot;380335691&quot;}&apos;&gt; &lt;img class=&quot;pic&quot; data-original=&quot;//screenshot.msstatic.com/yysnapshot/1711a622dc4d670fe32de2018f78a2d030fcde37cfe8?imageview/4/0/w/338/h/190/blur/1&quot; src=&quot;//a.msstatic.com/huya/main/assets/img/default/338x190.jpg&quot; onerror=&quot;this.onerror=null; this.src=&apos;//a.msstatic.com/huya/main/assets/img/default/338x190.jpg&apos;;&quot; alt=&quot;最强赵信折翼的直播&quot; title=&quot;最强赵信折翼的直播&quot;&gt; &lt;div class=&quot;item-mask&quot;&gt;&lt;/div&gt; &lt;i class=&quot;btn-link__hover_i&quot;&gt;&lt;/i&gt; &lt;em class=&quot;tag tag-blue&quot;&gt;蓝光&lt;/em&gt; &lt;/a&gt; &lt;a href=&quot;http://www.huya.com/yanmie&quot; class=&quot;title new-clickstat&quot; report=&apos;{&quot;eid&quot;:&quot;click/position&quot;,&quot;position&quot;:&quot;lol/0/1/5&quot;,&quot;game_id&quot;:&quot;1&quot;,&quot;ayyuid&quot;:&quot;380335691&quot;}&apos; title=&quot;可以用赵信上王者第一的男人&quot; target=&quot;_blank&quot;&gt;可以用赵信上王者第一的男人&lt;/a&gt; &lt;span class=&quot;txt&quot;&gt; &lt;span class=&quot;avatar fl&quot;&gt; &lt;img data-original=&quot;//huyaimg.msstatic.com/avatar/1081/63/c83bdc0701b64646c86065e273fd05_180_135.jpg&quot; src=&quot;//a.msstatic.com/huya/main/assets/img/default/84x84.jpg&quot; onerror=&quot;this.onerror=null; this.src=&apos;//a.msstatic.com/huya/main/assets/img/default/84x84.jpg&apos;;&quot; alt=&quot;最强赵信折翼&quot; title=&quot;最强赵信折翼&quot;&gt; &lt;i class=&quot;nick&quot; title=&quot;最强赵信折翼&quot;&gt;最强赵信折翼&lt;/i&gt; &lt;/span&gt; &lt;span class=&quot;num&quot;&gt;&lt;i class=&quot;num-icon&quot;&gt;&lt;/i&gt;&lt;i class=&quot;js-num&quot;&gt;1.5万&lt;/i&gt;&lt;/span&gt; &lt;/span&gt; &lt;/li&gt;12找到含目标数据的最小公约子集： &lt;span class=&quot;txt&quot;&gt; &lt;span class=&quot;avatar fl&quot;&gt; &lt;img data-original=&quot;//huyaimg.msstatic.com/avatar/1081/63/c83bdc0701b64646c86065e273fd05_180_135.jpg&quot; src=&quot;//a.msstatic.com/huya/main/assets/img/default/84x84.jpg&quot; onerror=&quot;this.onerror=null; this.src=&apos;//a.msstatic.com/huya/main/assets/img/default/84x84.jpg&apos;;&quot; alt=&quot;最强赵信折翼&quot; title=&quot;最强赵信折翼&quot;&gt; &lt;i class=&quot;nick&quot; title=&quot;最强赵信折翼&quot;&gt;最强赵信折翼&lt;/i&gt; &lt;/span&gt; &lt;span class=&quot;num&quot;&gt;&lt;i class=&quot;num-icon&quot;&gt;&lt;/i&gt;&lt;i class=&quot;js-num&quot;&gt;1.5万&lt;/i&gt;&lt;/span&gt; &lt;/span&gt;12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152目标数据： 最强赵信折翼，1.5万构造正则表达式： 选出上面的最小公约子集： root_pattern &#x3D; &#39;&lt;span class&#x3D;&quot;txt&quot;&gt;([\\s\\S]*?)\\r\\n&lt;&#x2F;li&gt;&#39; 注意： 要加问号，声明是非贪婪的匹配 然后选出，姓名： name_pattern &#x3D; &#39;&lt;i class&#x3D;&quot;nick&quot; title&#x3D;&quot;([\\s\\S]*?)&quot;&gt;&#39; 然后选出，人气： num_pattern &#x3D; &#39;&lt;i class&#x3D;&quot;js-num&quot;&gt;([\\s\\S]*?)&lt;&#x2F;i&gt;&#39; 注意： 上述正则的边界并不一定是完整的html标签，因为使用正则即对字符进行匹配，所以可以随意拆分。 几个重要功能： 获取页面内容： from urllib import request tmp &#x3D; request.urlopen(spider.url) htmls &#x3D; tmp.read() 注意：此处的结果是字节码：bytes 必须要进行转化：bytes-&gt;str htmls &#x3D; str(htmls,encoding&#x3D;&#39;utf-8&#39;) 注意：不推荐打印出页面内容，会出现解码问题，可以加断点调试 循环： 此处的循环需要获取到下标，而直接for i in list，获取不到下标 此时应该使用for index in range(0,len(list))这种形式 代码： for rank in range(0, len(pairs)): print(&#39;第&#39;,rank,&#39;名：&#39;,pairs[rank][&#39;name&#39;],&#39;:&#39;,pairs[rank][&#39;number&#39;]) 排序： 此处使用内置函数sorted(iterable, cmp&#x3D;None, key&#x3D;None, reverse&#x3D;False) 注意： key &#x3D; 函数名，此函数应该返回排序的比较值 cmp &#x3D; 函数名，此函数可以重写排序规则 reverse&#x3D;False，从小到大正序排列 代码： sorted(pairs,key &#x3D; seed,reverse &#x3D; True) def seed(self, pairs): tmp &#x3D; pairs[&#39;number&#39;].replace(&#39;万&#39;,&#39;&#39;) if(&#39;万&#39; in pairs[&#39;number&#39;]): tmp &#x3D; float(tmp) * 10000 return int(tmp)完整的爬虫代码： from urllib import requestimport re class spider(): url = &apos;http://www.huya.com/g/lol&apos; root_pattern = &apos;&lt;span class=&quot;txt&quot;&gt;([\\s\\S]*?)\\r\\n&lt;/li&gt;&apos; #父级目录匹配 # 使用概括字符集 [\\d] [\\w] [\\s] [.] #注意：要加问号，声明是非贪婪的匹配 name_pattern = &apos;&lt;i class=&quot;nick&quot; title=&quot;([\\s\\S]*?)&quot;&gt;&apos; num_pattern = &apos;&lt;i class=&quot;js-num&quot;&gt;([\\s\\S]*?)&lt;/i&gt;&apos; def __fetch_content(self): #加__的函数：私有方法 tmp = request.urlopen(spider.url) htmls = tmp.read() #此处的结果是字节码：bytes # bytes-&gt;str htmls = str(htmls,encoding=&apos;utf-8&apos;) a = 1#如果不添加这一句，htmls赋值发生在最后一句 #那么断点停止时会得不到htmls的值，这时要人为多余的添加一条语句并将断点放到这里即可 #print(htmls)不推荐打印，会出现解码问题 return htmls def __sort(self, pairs): #def sorted(iterable, cmp=None, key=None, reverse=False) #注意要指定key值 return sorted(pairs,key=self.__seed,reverse=True) def __show(self, pairs): #for循环中需要拿到序号，直接使用range形式的for循环 for rank in range(0, len(pairs)): print(&apos;第&apos;,rank,&apos;名：&apos;,pairs[rank][&apos;name&apos;],&apos;:&apos;,pairs[rank][&apos;number&apos;]) def __seed(self, pairs): tmp = pairs[&apos;number&apos;].replace(&apos;万&apos;,&apos;&apos;) if(&apos;万&apos; in pairs[&apos;number&apos;]): tmp = float(tmp) * 10000 return int(tmp) def __refine(self, pairs): f = lambda p: { &apos;name&apos;:p[&apos;name&apos;][0].strip(), &apos;number&apos;:p[&apos;number&apos;][0] } return map(f, pairs) def __analysis(self, htmls): root_htm = re.findall(spider.root_pattern,htmls) pairs = [] for item in root_htm: name = re.findall(spider.name_pattern,item) num = re.findall(spider.num_pattern,item) pair = {&apos;name&apos;:name,&apos;number&apos;:num} pairs.append(pair) return pairs #设置入口函数,这是一个主方法，里面都是平级函数，推荐这种写法 def go(self): htmls = self.__fetch_content() #抓取页面内容 pairs = self.__analysis(htmls) #抽取所需数据 pairs = list(self.__refine(pairs)) #数据精炼 pairs = self.__sort(pairs) #数据排序 self.__show(pairs) #数据显示，或后续处理（入库等）#实例化并调用入口函数s = spider()s.go() 注意事项： 如果需要调试，不推荐站桩print，推荐使用断点调试 调试方法： 启动应用程序 F5 单步执行F10 跳到下一个断点 F5 调到函数内部 F11 例如在 html = tmp.read() 处打断点 在当前断点处，悬停鼠标会显示变量值，也可以在vscode左侧的甲壳虫选项中查看变量的值 缺陷： 虽然通过类进行了封装，但是其实最基础的封装 但是，复用性差，抵御需求变化的能力太差，违反开闭原则 进阶： 可以使用更加面向对象的设计来完成功能 借助构造函数__init__来对类进行带参数的实例化： 代码： class Spider(): &apos;&apos;&apos; This is a class &apos;&apos;&apos; url = &apos;&apos; root_pattern = &apos;&apos; name_pattern = &apos;&apos; num_pattern = &apos;&apos; def __init__(self,url,root_pattern,name_pattern,num_pattern): Spider.url = url Spider.root_pattern = root_pattern Spider.name_pattern = name_pattern Spider.num_pattern = num_pattern s = Spider( &apos;http://www.huya.com/g/4&apos;, &apos;&lt;span class=&quot;txt&quot;&gt;([\\s\\S]*?)\\r\\n&lt;/li&gt;&apos;, &apos;&lt;i class=&quot;nick&quot; title=&quot;([\\s\\S]*?)&quot;&gt;&apos;, &apos;&lt;i class=&quot;js-num&quot;&gt;([\\s\\S]*?)&lt;/i&gt;&apos; ) s.go() 类封装的意义： 这样封装可以完成一个主播人气排序的爬虫类，参数有四个： 爬取的直播网站； 爬取的名称人气的父元素的正则 爬取名称的正则 爬取人气的正则 展望： 爬虫模块或框架： BeautifulSoup模块 Scrapy框架（多线程、分布式、较臃肿，看需求谨慎使用） 反反爬虫技术： 频繁爬取会使IP被封，需要使用定时器！切记！！ 寻找代理IP库，应对封IP 整个流程的核心： 爬取的原始数据如何处理，精炼 处理的结果如何存储，分析","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"Scala中的Collection","slug":"2017-10-30 Scala中的Collection","date":"2017-10-29T23:00:00.000Z","updated":"2020-02-29T18:43:55.406Z","comments":true,"path":"article/8cf3.html","link":"","permalink":"https://www.cz5h.com/article/8cf3.html","excerpt":"Scala中的immutable Collection 集合Traversable 遍历Iterable 迭代Set无序集合 Sequence序列 Map映射","text":"Scala中的immutable Collection 集合Traversable 遍历Iterable 迭代Set无序集合 Sequence序列 Map映射 Set SortedSet HashSet BitSet ListSet TreeSet Sequence IndexedSeq LinearSeq IndexedSeq Vector,NumericRange,Range,Array,String LinearSeq List,Stream,Quene,Stack Map Sortedmap HashMap LsitMap TreeMap List[T]T是类型，由于会自动推导类型，所以不必指明类型 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152scala&gt; val a = List(1,2,3,4) //定义方法一a: List[Int] = List(1, 2, 3, 4) //自动推导为Int类型的Listscala&gt; val b = 0::a //定义方法二：连接操作符b: List[Int] = List(0, 1, 2, 3, 4) //将左边的元素添加到右边List的头部scala&gt; var c = \"x\"::\"y\"::\"z\"::Nil //Nil是空Listc: List[String] = List(x, y, z) //上述过程是从右往左连接，步骤如下：scala&gt; \"z\"::Nilres5: List[String] = List(z)scala&gt; \"y\"::res5res6: List[String] = List(y, z)scala&gt; \"x\"::res6res7: List[String] = List(x, y, z)scala&gt; val d = a:::c //定义方法三：使用:::连接两个Listd: List[Any] = List(1, 2, 3, 4, x, y, z) //自动推导为Int,String的父类为Anyscala&gt; a.headres8: Int = 1scala&gt; d.head //.head返回头元素res9: Any = 1scala&gt; c.headres10: String = xscala&gt; a.tail //.tail返回除头元素之外的元素res11: List[Int] = List(2, 3, 4)scala&gt; d.tailres12: List[Any] = List(2, 3, 4, x, y, z)scala&gt; a.isEmpty //.isEmpty返回List是否为空res13: Boolean = falsescala&gt; Nil.isEmptyres14: Boolean = true//利用tail和isEmpty构造循环来实现List的遍历：scala&gt; def scanf(list: List[Int]):String = &#123; | if(list.isEmpty) \"NULL\" | else list.head.toString+\" \"+scanf(list.tail) | &#125;scanf: (list: List[Int])Stringscala&gt; scanf(a)res15: String = 1 2 3 4 NULL List的高阶函数 filter：过滤1234567891011121314151617//将List元素进行过滤//下面filter参数是一个匿名函数，x代表一个元素，filter会遍历List判断每个元素是否满足条件scala&gt; a.filter(x =&gt; x % 2 ==1) res17: List[Int] = List(1, 3)//toList表达式，结果是将当前字符串转为Listscala&gt; \"100 Persons\".toListres18: List[Char] = List(1, 0, 0, , P, e, r, s, o, n, s)//判断是否为数字可以用Character.isDigit(x)方法scala&gt; \"100 Persons\".toList.filter(x =&gt; Character.isDigit(x))res20: List[Char] = List(1, 0, 0)//takeWhile满足条件则取元素，直到！取到某元素才停止//（类似while循环）下面取元素取到字符‘o’终止，并且不会打印‘o’scala&gt; \"100 Persons\".toList.takeWhile(x =&gt; x!='o')res21: List[Char] = List(1, 0, 0, , P, e, r, s) List的高阶函数 map/flatMap：映射1234567891011121314151617181920212223242526272829303132333435363738394041//对于下面的变量a和c应用映射scala&gt; ares22: List[Int] = List(1, 2, 3, 4)scala&gt; cres22: List[String] = List(x, y, z)//map的参数就是一个匿名函数，表明一个转换过程，参数中的匿名函数参数x是List中得每个元素//使用map实现全部字母大写scala&gt; c.map(x =&gt; x.toUpperCase)res23: List[String] = List(X, Y, Z)//参数中的匿名函数参数x可以使用通配符下划线'_'来代替scala&gt; c.map( _.toUpperCase)res24: List[String] = List(X, Y, Z)//同样的filter也可以使用通配符下划线'_'来代替scala&gt; a.filter( _ % 2 ==1)res25: List[Int] = List(1, 3)//通过filter和map来实现对List中过滤后元素的具体操作//下面是将奇数全部加10scala&gt; a.filter( _ % 2 ==1).map( _ + 10)res26: List[Int] = List(11, 13)//下面是嵌套Listscala&gt; val complex = List( a,List(4,5,6))complex: List[List[Int]] = List(List(1, 2, 3, 4), List(4, 5, 6))//对于嵌套List，filter仍然会遍历到最里层的元素并且进行过滤//但是其返回不会去掉外壳，仍然是个嵌套Listscala&gt; complex.map(x =&gt; x.filter( _%2 ==0))res27: List[List[Int]] = List(List(2, 4), List(4, 6))//同样，使用下划线也可以通配参数xscala&gt; complex.map( _.filter( _%2 ==0))res28: List[List[Int]] = List(List(2, 4), List(4, 6))//使用flatMap可以将嵌套List“打平”，将返回元素全部放在同一层//下面就可以取出嵌套List中的偶数，注意，去除了‘外壳’scala&gt; complex.flatMap( _.filter( _%2 ==0))res30: List[Int] = List(2, 4, 4, 6) List的高阶函数 集合的规约操作把集合的元素通过运算和操作规约为一个值 reduceLeft(op: (T, T) =&gt; T )12345x1 x2 x3 ... xn op x3 ... xn op ... xn ... op 特性1：参数为一个匿名函数特性2：规约结果一定是List元素的类型，所以是被经常使用的（相较于foldLeft） 123456789对于List变量ascala&gt; ares33: List[Int] = List(1, 2, 3, 4)使用reduceLeft，参数为匿名函数，表示规约的表达式scala&gt; a.reduceLeft((x,y) =&gt; x+y)res31: Int = 10可以使用下划线通配scala&gt; a.reduceLeft(_+_)res32: Int = 10 foldLeft(z:U)(op: (U, T) =&gt; U )12345z x1 x2 ... xn op x1 ... xn op ... xn ... op 特性1：使用柯里化定义特性2：必须有初始值z特性3：返回值是初始值z的类型，故不太使用 1234567891011scala&gt; ares33: List[Int] = List(1, 2, 3, 4)//使用foldLeft进行元素的求和，并且初值为0scala&gt; a.foldLeft(0)((x,y) =&gt; x+y)res34: Int = 10//使用通配符scala&gt; a.foldLeft(0)(_+_)res35: Int = 10//初值改变后的结果scala&gt; a.foldLeft(1)(_+_)res36: Int = 11 惰性求值的类型：Stream 流123456789101112131415161718192021222324252627//使用to或until来获取range类型scala&gt; 1 to 10 by 2res41: scala.collection.immutable.Range = inexact Range 1 to 10 by 2//until是小于，取不到边界scala&gt; (1 until 10).toListres44: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9)//to是小于等于，可以取边界scala&gt; (1 to 10).toListres45: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)//使用操作符#::来连接定义一个Stream，其中Stream.empty是空流scala&gt; 1 #:: 2#:: 3#:: Stream.emptyres46: scala.collection.immutable.Stream[Int] = Stream(1, ?)//惰性求值的特性：由打印可知，只显示和判断第一个元素是什么，其他的用？表示scala&gt; val s = (1 to 1000).toStreams: scala.collection.immutable.Stream[Int] = Stream(1, ?)//获取Stream的第一个元素scala&gt; s.headres48: Int = 1//获取Stream除首元素以外的元素，其返回结果仍然是Stream类型，所以仍然只显示(2, ?)scala&gt; s.tailres49: scala.collection.immutable.Stream[Int] = Stream(2, ?)scala&gt; s.tail.headres50: Int = 2 Scala中的tuple：元组1234567891011121314151617181920//元组的概念，和Python中的元组类似，可以放不用类型的变量scala&gt; (1,2)res51: (Int, Int) = (1,2)//只有两个元素的元组叫pair，可以使用箭头的方式来定义scala&gt; 1 -&gt; 2res52: (Int, Int) = (1,2)//scala自动识别元素类型scala&gt; val t = (1,'a',\"Tom\",34.5)t: (Int, Char, String, Double) = (1,a,Tom,34.5)//对于一个元组变量，下划线加数字表示第N个元素，t._1表示第一个元素scala&gt; t._1res54: Int = 1//取元素时不能超出下标，否则报错scala&gt; t._5&lt;console&gt;:13: error: value _5 is not a member of (Int, Char, String, Double) t._5 ^ 元组的用处：可以封装函数的返回值，在函数返回多个类型的变量时，可以包装起来一并返回 12345678//下面这个函数通过元组，一并返回输入参数List变量中所有元素的个数、求和、平方和scala&gt; def _3operate(in:List[Int]):(Int,Int,Int) = | in.foldLeft((0,0,0))((t,v) =&gt; (t._1+1,t._2+v,t._3+v*v) | )_3operate: (in: List[Int])(Int, Int, Int)//调用该函数，可以返回三个值scala&gt; _3operate(a)res56: (Int, Int, Int) = (4,10,30) Scala中的Map&lt;K,V&gt;12345678910111213141516171819//使用类似元组的箭头来定义一个键值对scala&gt; val p = Map(1 -&gt; \"Tom\",9-&gt;\"Jack\")p: scala.collection.immutable.Map[Int,String] = Map(1 -&gt; Tom, 9 -&gt; Jack)//按Key取值scala&gt; p(1)res58: String = Tom//判断指定Key是否在Map中scala&gt; p.contains(1)res59: Boolean = true//返回包含全部Key的Set集合scala&gt; p.keysres60: Iterable[Int] = Set(1, 9)//返回包含全部Value的Iterable类型scala&gt; p.valuesres61: Iterable[String] = MapLike.DefaultValuesIterable(Tom, Jack) 涉及的Map相关运算 1234567891011121314151617181920212223242526272829303132333435//使用+号 添加键值对，注意Map不支持混合类型的添加，否则会出错scala&gt; p + (\"name\" -&gt; \"Kim\")&lt;console&gt;:13: error: type mismatch; found : (String, String) required: (Int, ?) p + (\"name\" -&gt; \"Kim\") ^//正确添加键值对，注意会按Key值覆写键值对，即Key冲突时丢弃原来的Value//有冲突的添加scala&gt; p + (1 -&gt; \"Kim\")res63: scala.collection.immutable.Map[Int,String] = Map(1 -&gt; Kim, 9 -&gt; Jack)//正常的添加scala&gt; p + (2 -&gt; \"Kim\")res65: scala.collection.immutable.Map[Int,String] = Map(1 -&gt; Tom, 9 -&gt; Jack, 2 -&gt; Kim)//使用-号来删除键值对，注意减的是Key值scala&gt; p - 1res70: scala.collection.immutable.Map[Int,String] = Map(9 -&gt; Jack)//注意添加或删减的结果不能直接通过= 赋值给自己，会报错scala&gt; p = p -9&lt;console&gt;:12: error: reassignment to val p = p -9 ^//上述的添加和删除都是操作单个元素，下面使用包含键值对的List集合加上++运算符来完成添加拖个键值对scala&gt; p ++ List(2-&gt;\"a\",5-&gt;\"b\")res72: scala.collection.immutable.Map[Int,String] = Map(1 -&gt; Tom, 9 -&gt; Jack, 2 -&gt; a, 5 -&gt; b)//删除多个键值对，注意删除只需要含Key值的List即可scala&gt; p -- List(1,9,2,5)res73: scala.collection.immutable.Map[Int,String] = Map()//可以联合构成表达式scala&gt; p ++ List(2-&gt;\"a\",5-&gt;\"b\") -- List(2,5)res74: scala.collection.immutable.Map[Int,String] = Map(1 -&gt; Tom, 9 -&gt; Jack) 函数式编程示例：快速排序1234567891011def qSort(a:List[Int]):List[Int] = &#123; if(a.length &lt; 2) a else qSort( a.filter( _ &lt; a.head )) ++ a.filter( _ == a.head ) ++ qSort( a.filter( _ &gt; a.head )) &#125; //&gt; qSort: (a: List[Int])List[Int]qSort(List(2,3,5,1,2,8,5,2)) //&gt; res0: List[Int] = List(1, 2, 2, 2, 3, 5, 5, 8)qSort(List(9,4,8,2,5,1,3,0)) //&gt; res1: List[Int] = List(0, 1, 2, 3, 4, 5, 8, 9) 解释： 首先快排需要一个分割变量，这里直接用a.head即输入List的第一个元素来做分割其次是归类，每次递归都要分出小于，大于和等于的元素然后是合并，使用++操作符，把每次的元素拼接起来，即每次调整后的结果最后是判断递归结束条件：如果当前作为输入的分割后的List元素不足2，那么表示无序调整，排序结束 注意： 这里外层递归中含有两个递归，外层递归即函数的返回的是三部分之和，这并不是尾递归这个例子是综合了函数式编程、高阶函数、递归等Scala编程思想的体现。","categories":[{"name":"Spark学习笔记","slug":"Spark学习笔记","permalink":"https://www.cz5h.com/categories/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://www.cz5h.com/tags/Scala/"},{"name":"Collection","slug":"Collection","permalink":"https://www.cz5h.com/tags/Collection/"}]},{"title":"Scala的基础概念","slug":"2017-10-29 Scala基础概念","date":"2017-10-28T22:00:00.000Z","updated":"2020-02-29T18:43:55.405Z","comments":true,"path":"article/bd82.html","link":"","permalink":"https://www.cz5h.com/article/bd82.html","excerpt":"Scala语言的特性 Scalable语言Scala是一门可伸缩的scalable语言，既可以写复杂的服务器端程序，也可以写简单的脚本纯正的面向对象所有的概念最终都会被时限为纯正的对象","text":"Scala语言的特性 Scalable语言Scala是一门可伸缩的scalable语言，既可以写复杂的服务器端程序，也可以写简单的脚本纯正的面向对象所有的概念最终都会被时限为纯正的对象 函数式编程的特性函数式程序思想！！！无缝的Java互操作构建于Jvm之上，Java的包可以在Scala中使用，huo1Scala写好的程序给Java调用编程思路灵活既可以面向对象的思想，也可以函数式编程的思想 Scala之父：Martin Odersky 导读：函数式变成的概念和思想Scala的开发环境搭建Scala语言的基础Scala中的类型和求值策略Scala中函数的概念Immutable Collections如何用函数式思想实现数据结构和其上的一些操作 函数式编程思想只用纯函数编程 定义：函数式编程是一种编程范式，构建计算机程序和结构的方法和风格，把计算当做数学函数求值的过程，并且避免了改变状态和可变的数据 纯函数特点 Pure Function 纯函数，没有副作用的函数 没有副作用：状态的变化例如：调用 def Add(y:Int) = x + y其结果为xy之和，并且调用之后没有引起x值的变换，没有副作用所以，Add函数没有副作用 引用透明性对于上述Add函数，对于同一输入y，返回结果均相同所以，Add具有引用透明性 如何确保引用透明 不变性Immutablity：任何的状态和值都是不变的，才能获得引用透明函数与变量，对象类是同一级的，即函数中可以定义函数，有变量的地方都可以使用函数，都是等同的 高阶函数 函数作为一个函数的输入或另一个函数的输出 闭包 closure表达式求值函数式编程中，一切都是表达式，表达式求值策略： 严格求值：call by value非严格求值：call by name 惰性求值 定义表达式时不会立即求值，只在第一次调用时才求值 递归函数函数式编程中没有循环语句，全部的循环用递归实现调优递归：尾递归 函数式编程的优点Lisp是第一种函数式编程语言 编程代码量少 当构造完含数之后，对于相同输入，输出相同，便于调试 非常适用于并行编程，没有副作用，具备引用透明性，在n个节点运算结果是相同的 传统语言多核编程非常复杂 Scala环境的搭建安装Jdk6以上，并安装Scala包 Scala基础语法变量修饰符 val 定义 immutable variable 常量 var 定义 mutable variable 变量 lazy val 惰性求值的常量 定义时不用显示的说明类型，scala会自己进行变量推导 前两种定义，在定义时表达式就会立即求值lazy val在REPL中，scala会给没有变量名的变量自动取值resN，可以直接引用已有的resN注意： scala中不允许常量定义后被直接改变，而变量var可以 val x = 10 x = 20 //会报错reassignment to val val x = 20 //正确的重新赋值，需要使用val重新定义 对于lazy val，注意没有lazy var，一般是定义惰性求值的表达式 val l = 常量或变量组成的表达式 Scala的类体系 Any 所有类的父类 AnyVal 值类型 NumericTypes 数值类型 Byte，Shot，Int，Long，Float，Double Boolean 布尔类型 Char 字符类型 Unit 空类型，相当于Java的voidAnyRef 所有引用类型的父类 All java.* ref types 所有Java的引用类都是其子类 All scala.* ref types 所有自定义的scala的类都是其子类 Null 所有引用类型的最后一个子类Nothing 所有类型的最后一个子类（既是AnyVal又是AnyRef的子类） NumericTypes对于数值类型：低精度可以赋值给高精度，反之不行，数据会缺失：报类型不匹配错误 Unit往往作为函数的返回值出现，表明函数有副作用 Null表示一个引用类型的值为空。通常不使用 Nothing对于函数而言，如果返回值为nothing，那么则表示函数异常scala&gt; def foo() = throw new Exception(“1111”)foo: ()Nothing String新特性 - 字符串插值(interpolation)scala&gt; val name=”Jack”name: String = Jackscala&gt; s”my name is $name” //使用字符串插值res11: String = my name is Jack 代码块Block代码块用于组织多个表达式：{exp1；exp2}多个表达式在一行2时需要分号分割，代码块本事也是一个表达式最后的求值，是最后一个表达式 函数定义函数的方式： def functionName（param：paramType）：returnType = { //body expressions }示例： 12345678910111213141516171819 object worksheetA &#123; // 完整形式def Hello(name:String):String = &#123; s\"Hello,$name\"&#125; //&gt; Hello: (name: String)StringHello(\"Jack\") //&gt; res0: String = Hello,Jack// 省略返回值，自动推断类型def Hello2(name:String) = &#123; s\"Hello,$name\"&#125; //&gt; Hello2: (name: String)StringHello2(\"Tom\") //&gt; res1: String = Hello,Tom// 单语句的函数体def add(x:Int,y:Int) = &#123; x + y&#125; //&gt; add: (x: Int, y: Int)Int// 可以省略大扩号//def add(x:Int,y:Int) = x + yadd(1,2) //&gt; res2: Int = 3 &#125; scala中的ifif是表达式，而不是语句if(逻辑表达式) valA else valB val a = 1 //&gt; a : Int = 1 if(a!=1) “not none” //&gt; res3: Any = ()返回空 if(a!=1) “not none” else a //&gt; res4: Any = 1 scala中的for comprehension用于实现循环的一种推导式，本身是由map() reduce()组合实现的是scala语法糖(thin text sugar)的一种 12345for&#123; x &lt;- xs y = x + 1 if( y &gt; 0)&#125;yield y 示例： 1234567891011121314151617181920 object worksheetA &#123;//初始化一个Listval list = List(\"alice\",\"bob\",\"cathy\")for ( //遍历list每一个元素给s，generator s &lt;- list)println(s)for &#123; s &lt;- list //串长度大于三才被打印 if( s.length &gt; 3)&#125;println(s)val res_for = for&#123; s &lt;- list //变量绑定,variable binding s1 = s.toUpperCase() if ( s1 != \"\")//yeild导出的意思，如果每次s1不空，则生成新的collection&#125;yield (s1) &#125; scala中的trytry也是一个表达式，返回一个值 1234567try&#123; Integer.praseInt(\"dog\")&#125;catch&#123; case _ =&gt; 0 //下划线是通配符，统配所有异常&#125;finally&#123; print(\"总是会打印\");&#125; scala中的macth类似switch，但也是一个表达式，返回相应的值，主要用在 pattern match 1234567var expression = 1 //&gt; expression : Int = 1expression match&#123; case 1 =&gt; \"dog\" case 2 =&gt; \"cat\" //类似switch的default case _ =&gt; \"others\" &#125; //&gt; res5: String = dog Scala的求值策略scala中所有运算都是基于表达式的，求值会有不同策略 call by value 对函数实参求值，仅求一次，求得的值直接替换函数中的形式参数 call by value 不会对函数实参进行表达式求值，直接把表达式传入函数体内，替换表达式的形参，然后在函数内每次使用到此形参时会被求值 scala通常使用call by value def foo(x: Int) = x //call by Valuedef foo(x: =&gt; Int) = x //call by Name 下面是两种求值策略在不同情况下的运行机制： 1234567891011def add(x: Int,y: Int) = x * x def add(x: =&gt;Int,y: =&gt;Int) = x * xadd(3+4,7) add(3+4,7) =&gt;add(7,7) =&gt;(3+4)*(3+4)=&gt;7*7 =&gt;7*(3+4)=&gt;49 =&gt;7*7 =&gt;49 add(7,3+4) add(7,3+4)=&gt;add(7,7) =&gt;7*7=&gt;7*7 =&gt;49=&gt;49 注意上述运行机制的区别 123456789101112131415scala&gt; def bar(x:Int, y: =&gt; Int) : Int = 1bar: (x: Int, y: =&gt; Int)Intscala&gt; def loop():Int = looploop: ()Intscala&gt; bar(1,loop) //loop函数位于的参数的定义方式是y: =&gt; Int，即call by name，不进行求值，会带到函数体内并且使用时才求值，此处，loop没有机会执行。res0: Int = 1scala&gt; bar(loop,1)//loop函数位于的参数的定义方式是y: Int，即call by value，会直接将表达式求值并代替形参，此处loop首先被执行求值，故而陷入死循环。输出:死循环 进行函数设计和调用时，两种差异要搞清楚 Scala中的函数 支持把函数作为实参传递给另外一个函数 支持把函数作为返回值 支持把函数赋值给变量 支持把函数存储在数据结构里 即，在scala中，函数跟普通变量一样使用，且具有函数的相关类型 函数的类型在scala中，函数类型的格式为 A =&gt; B,表示一个：接受参数类型为A的、并返回类型B的函数eg： Int =&gt; String 是把整型映射为字符串的函数类型 高阶函数 接受的参数为函数123def funcName( f: (Int, Int) =&gt; Int) = &#123; f(4,4)&#125; 参数：f类型：Int =&gt; Int返回：Int类型 返回值为一个函数1def funcName() = ( name: String) =&gt; &#123;\"hello \"+name&#125; 参数：name类型：String =&gt; String返回：String类型注意上述叫做：匿名函数 - 函数常量 - 函数的文字量（相对于def funcName 叫函数变量） 兼具上述情况 匿名函数匿名函数没有函数名定义格式： （形参列表） =&gt; { 函数体 } (x: Int) =&gt; { x * x } (x: Int,y: Int) =&gt; { x + y } var add = (x: Int,y: Int) =&gt; { x + y } def func() = (x: Int,y: Int) =&gt; { x + y }1234567891011121314scala&gt; (x: Int,y: Int) =&gt; &#123; x + y &#125;res0: (Int, Int) =&gt; Int = $$Lambda$1016/2093139281@69feb4d9scala&gt; var add = (x: Int,y: Int) =&gt; &#123; x + y &#125;add: (Int, Int) =&gt; Int = $$Lambda$1025/1152113439@62108cd3scala&gt; add(1,2)res1: Int = 3scala&gt; def funcName() = ( name: String) =&gt; &#123;\"hello \"+name&#125;funcName: ()String =&gt; Stringscala&gt; funcName()(\"Jack\")res4: String = hello Jack 柯里化Scala中的重要的技术，具有多个参数的函数转化成一个函数列，每个函数只有单一参数 1234567def add(x: Int,y: Int) = &#123; x + y &#125; //普通函数定义def add(x: Int)(y: Int) = &#123; x + y &#125; //柯里化函数定义，多个参数单一化，串接起来def curriedAdd(a: Int)(b: Int) = a + bcurriedAdd(2)(2) //4val add = curriedAdd(1)_ //Int =&gt; Intadd(2) //3 解释：curriedAdd(1)_，下划线统配之后的全部参数列表，此处a=1固定，只有b是可变值，下划线通配变量badd(2),传入curriedAdd后a=1，b=2 利用柯里化技术，通过原有通用函数构造一些新的函数 Scala中的递归scala里计算n的阶乘 123def factorial（n: Int): Int = if(n &lt;= 0) 1 else n * factorial(n - 1) 递归优化：变成尾递归，尾递归会复写当前栈，不会导致堆栈溢出尾递归优化：用￥annotation.tailrec显示指明编译时进行尾递归优化 12345@annotation.tailrecdef factorial（n: Int,m: Int): Int &#x3D; if(n &lt;&#x3D; 0) m else factorial(n - 1, m * n)factorial(5,1) 上述引入m，m保留当前运算之前的历史阶乘结果如果退出，则就是递归的值，如果不退出，那么把当前结果传入下一次，这样不需要开辟栈保留计算结果，每次只需m变量记录结果示例：求f(x)在(a,b)上的和 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859def sum(f: Int =&gt; Int)(a: Int)(b: Int): Int = &#123; @annotation.tailrec def loop(n: Int, acc: Int): Int = &#123; //函数中可定义其他函数 //n:循环变量 //acc:累积和 if (n &gt; b) &#123; println(s\"n=$&#123;n&#125;, acc=$&#123;acc&#125;\") acc //if表达式返回acc的值 &#125; else &#123; println(s\"n=$&#123;n&#125;, acc=$&#123;acc&#125;\") loop(n + 1, acc + f(n)) //else表达式返回loop &#125; &#125; loop(a, 0)&#125; //&gt; sum: (f: Int =&gt; Int)(a: Int)(b: Int)Int//函数y=f(x)sum(x =&gt; x)(1)(5) //&gt; n=1, acc=0 //| n=2, acc=1 //| n=3, acc=3 //| n=4, acc=6 //| n=5, acc=10 //| n=6, acc=15 //| res0: Int = 15 sum(x =&gt; x * x)(1)(5) //&gt; n=1, acc=0 //| n=2, acc=1 //| n=3, acc=5 //| n=4, acc=14 //| n=5, acc=30 //| n=6, acc=55 //| res1: Int = 55 sum(x =&gt; x * x * x)(1)(5) //&gt; n=1, acc=0 //| n=2, acc=1 //| n=3, acc=9 //| n=4, acc=36 //| n=5, acc=100 //| n=6, acc=225 //| res2: Int = 225 //利用柯里化技术，通过原有通用函数构造一些新的函数，简化代码 val sumSquare = sum(x =&gt; x * x)_ //&gt; sumSquare : Int =&gt; (Int =&gt; Int) = sumfunc$$$Lambda$13/2054798982@34ce8af7 sumSquare(1)(5) //&gt; n=1, acc=0 //| n=2, acc=1 //| n=3, acc=5 //| n=4, acc=14 //| n=5, acc=30 //| n=6, acc=55 //| res3: Int = 55","categories":[{"name":"Spark学习笔记","slug":"Spark学习笔记","permalink":"https://www.cz5h.com/categories/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://www.cz5h.com/tags/Scala/"},{"name":"概述","slug":"概述","permalink":"https://www.cz5h.com/tags/%E6%A6%82%E8%BF%B0/"}]},{"title":"Python的函数式编程","slug":"2017-10-28 Python的函数式编程","date":"2017-10-27T22:00:00.000Z","updated":"2020-02-29T18:43:55.404Z","comments":true,"path":"article/4e6b.html","link":"","permalink":"https://www.cz5h.com/article/4e6b.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 匿名函数特点： 定义函数时不需要定义函数名 实现： 借助 lambda 关键字 lambda parameter_list: expression 注意：expression只能包含表达式！不能包含语句！ 示例： 一般函数： def add(x, y): return x + y 上述add函数的匿名形式：(匿名函数 或 叫做Lambda表达式) lambda x,y: x+y 匿名函数的调用： 因为没有名字，所以需要赋值给变量，然后调用 f = lambda x,y: x+y print(f(1,2)) #输出 3三元表达式地位： 表达式版本的if else 语句 实现： Java的形式 x &gt; y ? 1 : 1 Python的三元表达式 [条件为真时返回的结果] if [条件判断] else [条件为假时的返回结果] 示例： x = 1 y = 2 print( True if x &gt; y else False) #输出 Falseclass map(func,*iterables)使用场景 对序列中的全部元素执行相同的操作 应用： 求列表中每个数字的平方 示例： list_x = [1,2,3,4,5,6] def func(x): return x * x print(map(func, list_x)) #输出 &lt;map object at 0x0000000000A5D7F0&gt; print(list(map(func, list_x))) #输出 [1, 4, 9, 16, 25, 36] 注意： map会吧集合中的每个元素都传入到第一个参数，func中去 即按照一定的规则，对一族元素进行映射，规则就是func中的逻辑 map+lambda： 注意： map(func,*iterables)结合Lambda表达式 直接替换掉参数func 注意第二个参数 *iterables，带星号，表示可变参数 如果func参数有多个，第二个参数位置可以传入多个list 示例： 完成返回平方的功能： print( list( map( lambda x: x*x, [1,2,3,4,5] ) ) ) #输出 [1, 4, 9, 16, 25] Lambda表达式有两个参数 print( list( map( lambda x,y: x*y, [1,2,3,4,5],[1,2,3,4,5] ) ) ) #输出[1, 4, 9, 16, 25] 注意多个参数时会一一对照，多余会丢弃 print( list( map( lambda x,y: x*y, [1,2,3,4,5],[1] ) ) ) #输出[1] 上述输出只有一个元素，因为输入的两个参数只有第一个元素满足运行条件def reduce注意： map类在全局命名空间，不需要引入模块，reduce函数需要引入functools def reduce(function,sequence,initial=None) 示例： from functools import reduce list_x = [1,2,3,4] #完成求和 print( reduce( lambda x,y:x+y, list_x ) ) #输出 10 运算过程： #1 2 3 4 ... # 3 3 4 ... # 6 4 ... # 10... 应用场景： 连续规约计算，从前向后对数据进行两两规约计算 带initial参数： print( reduce( lambda x,y:x+y, list_x, 10 ) ) #输出 10 运算过程： #10 1 2 3 4 ... # 11 2 3 4 ... # 13 3 4 ... # 16 4 ... # 20 扩展： map/reduce 映射+规约，进行并行计算（大数据处理）class filter(func or None, iterables)注意： 由参数可知，只能对一个序列进行处理，参数func仍然可以被Lambda表达式替换 另：第一个参数必须可以返回True/False 或者代表True/False的值 filter依靠TF来判断是否要剔除，返回false表示保留 示例： print( filter( lambda x:False if x == 0 else True, [0,0,1,2,3,0] )) #输出 &lt;filter object at 0x000000000111D828&gt; print( list( filter( lambda x:False if x == 0 else True, [0,0,1,2,3,0] ))) #输出 [1, 2, 3] 对比： 命令式编程：依靠def，if else，for/while等等 函数式编程：依靠map，reduce，filter + Lambda（三大函数 + 算子） 解释： map,reduce近似循环 filter近似判断 Lambda近似函数 上述集合使用，可以完成流程控制 注意： Python只是支持部分函数式编程的特性，本身并不是函数式编程语言装饰器地位： 非常有用、常用，是一种设计模式，类似Java的注解 示例： 对所有函数追加打印当前时间的功能 代码： import time def func1(): print(time.time()) #输出 1509550613.9346058 Unix时间戳 print(&apos;This is a function&apos;) func1() 追加修改原则： 开闭原则：对修改是封闭的，对扩展是开放的，如果需要打印时间，会迫使修改函数内部 进一步： 使用函数式编程思想的封装,没有违反开闭原则 def func2(): print(&apos;This is a function&apos;) def print_time(func): print(time.time()) func() print_time(func2) 上述缺点：打印时间并没有和原函数绑定 再进一步： 使用装饰器完成上述功能 装饰器的基本结构： def decorator(): def wrapper(): pass#装饰器的逻辑 return wrapper 示例： import time def func1(): print(&apos;This is a function&apos;) #构造装饰器 def decorator(func): def wrapper(): print(time.time()) func() return wrapper decorator(func1)() 完成调用 解释： 真正业务逻辑在wrapper内 缺点： 仍然没有将打印时间和原始函数绑定起来 再进一步： 使用装饰器语法糖，使用@符号 代码： import time #构造装饰器 def decorator(func): def wrapper(): print(time.time()) func() return wrapper @decorator def func1(): print(&apos;This is a function&apos;) func1() #完成调用，而且没有改变原有调用逻辑 评价： 这才是完整的有意义的装饰器的使用方法，只需要在原有函数定义上添加 @decorator就会执行附加操作，体现了AOP面向切面编程对带参数的原函数添加装饰器示例： import time #构造装饰器 def decorator(funcname): def wrapper(*fc): #此处的参数，应该和被修饰函数的参数个数相对于， #但是为了通用性，应该使用*params可变参数形式 print(time.time()) funcname(*fc) return wrapper @decorator def func1(f_param): print(&apos;This is &apos;,f_param) @decorator def func2(param1,param2): print(&apos;This is &apos;,param1,param2) func1(&apos;Tom&apos;) func2(&apos;Tom&apos;,&apos;Jack&apos;) 上述成功完成了对不同参数的函数进行添加装饰器 进一步优化 适应关键字参数： import time #构造装饰器 def decorator(funcname): def wrapper(*fc,**kw): #参数**kw适用于关键字参数 print(time.time()) funcname(*fc,**kw) return wrapper @decorator def func2(param1,**param2): print(&apos;This is &apos;,param1,param2) func2(&apos;Tom&apos;,a = &apos;1&apos;,b = &apos;2&apos;) #正确适应了关键字参数的函数 关键字参数，回顾： 对于含有关键字参数的函数 def func2(param1,**param2): print(&apos;This is &apos;,param1,param2) func2(&apos;Tom&apos;,a = &apos;1&apos;,b = &apos;2&apos;) 输出 This is Tom {&apos;b&apos;: &apos;2&apos;, &apos;a&apos;: &apos;1&apos;} 综上： 此时的装饰器： def decorator(funcname): def wrapper(*fc,**kw): print(time.time()) funcname(*fc,**kw) return wrapper 在待装饰函数具有多个可变参数，或者包含关键字参数的情况均可使用 装饰器小结： 装饰器的思想：对封装的单元追加行为，保证原有单元的稳定性， 不破坏单元内的代码，遵循开闭原则，更加体现了装饰器内代码的复用 应用场景： flask内，添加@api.route可以使函数变为控制器 @api.route(&apos;/get&apos;,methods=[&apos;GET&apos;]) def test_http(): p = request.agrs.get(&apos;name&apos;) return p,200 添加多个装饰器，追加不同的操作 @api.route(&apos;/get&apos;,methods=[&apos;GET&apos;]) @auth.login_required def get_money(): p = request.agrs.get(&apos;psw&apos;) r = generate_hash(p) return &apos;user&apos;,200","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"Python的闭包","slug":"2017-10-27 Python的闭包","date":"2017-10-26T22:00:00.000Z","updated":"2020-02-29T18:43:55.402Z","comments":true,"path":"article/c301.html","link":"","permalink":"https://www.cz5h.com/article/c301.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 地位：闭包 和函数有关系解释：python中一切皆对象: 函数可以赋值给变量，例如 a = def func()， 可以把函数当做参数，传入一个函数 可以把函数当做一个函数的返回结果示例：Python中允许的正确的调用方法： def curve_pre(): def curve(): print(&apos;This is a funcion&apos;) return curve #函数作为返回值 func = curve_pre() func() #产生调用，输出 This is a funcion将上述示例扩展为闭包：注意： 闭包内的变量与闭包外的变量没有关系 示例： def curve_pre(): a = 25 #a在curve外部 def curve(x): return a*x*x return curve #函数curve作为返回值 func = curve_pre() print(func(2)) #打印100 外部变量对一般函数的影响： a = 10 def f(i): return a*i print(f(2)) #打印20 函数外面的a影响到了函数内a的值 外部变量对闭包的影响： a = 10 print(func(2)) #打印100 调用外面的a没有影响到函数内a的值，def curve(x)内的a仍然是def curve_pre()内的a的值 上述就是闭包的现象闭包定义：由函数以及函数定义时外部的变量构成的整体，叫闭包 闭包 = 函数 + 原函数所处环境的变量（原函数外部）注意：上述函数所处环境的变量不能是全局变量，即：至少需要两个结构体嵌套 闭包内的环境变量： 保存在curve_pre().__closure__内 print(func.__closure__) #输出：(&lt;cell at 0x0000000001158CA8: int object at 0x00000000539604D0&gt;,) print(func.__closure__[0].cell_contents) #输出：25注意：单一函数 + 不同的外部变量 = 多种不同的闭包（类似设计模式的工厂模式）闭包的调用方式：正常非闭包函数的调用： 代码： def func1(): a = 10 def func2(): a = 20 print(&quot;func2&apos;s a = &quot;,a) # 20 运行顺序：2 print(&quot;func1&apos;s a = &quot;,a) # 10 运行顺序：1 func2() print(&quot;func1&apos;s a = &quot;,a) # 10 运行顺序：3 func1() 注意： 上述是一个函数的调用，不是一个闭包，可以使用__closure__来判断是否为闭包 测试是否是闭包： def func1(): a = 10 def func2(): a = 20 return a return func2 func2() func1() f = func1() print(f.__closure__) #输出：None 原因： func2中的a被当做了局部变量，此时func2函数内并没有产生对外部变量的引用！ 所以，并没有构成一个闭包 修改为闭包的方式： def func1(): a = 10 def func2(): #a = 20 将局部变量a注释 c = a * 20 return func2 func2() func1() f = func1() print(f.__closure__) #输出：(&lt;cell at 0x0000000001108CA8: int object at 0x00000000539602F0&gt;,) 成为闭包的原因： 将func2中的局部变量a去掉后，只要func2中产生对外部变量a的使用，就可以被作为闭包 闭包一定要引用外部环境的变量闭包的应用：要求： 对于x，y 按顺序x=3,y=3;x=5,y=8;x=6,y=14 本质： 需要对中间变量进行保存 非闭包实现：（失败） origin = 0 def walk(step): new_pos = origin + step #这一步origin是外面的全局变量 origin = new_pos #此处的赋值会出错，因为如果函数内部有赋值操作，那么origin会变成局部变量，从而导致上一句中找不到origin的定义 return origin print(walk(3)) print(walk(5)) print(walk(6)) 上述代码修改为：（借助global，成功） origin = 0 def walk(step): global origin #显式的声明全局变量之后，就不会讲origin作为局部变量 new_pos = origin + step origin = new_pos return origin print(walk(3)) #3 print(walk(5)) #8 print(walk(6)) #14 闭包方式：（失败） origin = 0 def func1(pos): #pos成为了环境的变量 def walk(step): new_pos = pos + step pos = new_pos #此处报错UnboundLocalError: local variable &apos;pos&apos; referenced before assignment return new_pos return walk tour = func1(origin) print(tour(3)) #3 print(tour(5)) #8 print(tour(6)) #14 修改为：（借助nonlocal，成功） origin = 0 def func1(pos): #pos成为了环境的变量 def walk(step): nonlocal pos #显式声明pos是一个本地变量 new_pos = pos + step pos = new_pos #此处报错UnboundLocalError: local variable &apos;pos&apos; referenced before assignment return new_pos return walk tour = func1(origin) print(tour(3)) #3 print(tour.__closure__[0].cell_contents) #3 print(tour(5)) #8 print(tour.__closure__[0].cell_contents) #8 print(tour(6)) #14 print(tour.__closure__[0].cell_contents) #14 使用闭包的优点：（函数式编程） 没有使用全局变量origin，所有的变量操作均在闭包内部 闭包+nonlocal关键字可以完成中间变量的记录，打印__closure__[0].cell_contents也会发现，闭包确实记录了中间变量闭包的扩展：可以实现设计模式中的；工厂模式 闭包内的变量会常驻内存，使用时要注意 闭包不是函数式编程的全部，只是一种体现","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"Python的枚举类型","slug":"2017-10-26 Python的枚举类型","date":"2017-10-25T22:00:00.000Z","updated":"2020-02-29T18:43:55.401Z","comments":true,"path":"article/3d.html","link":"","permalink":"https://www.cz5h.com/article/3d.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 产生：数据类型 dict，list，tuple，set 等等有时并不适用使用：引入模块：form enum import Enum示例：所有的枚举类型都是Enum类的子类，Py中没有enum这个数据类型 枚举类中定义了多组常量，枚举类名和标识名推荐大写 from enum import Enum class EMP(Enum): YELLOW = 1 GREEN = 2 BLACK = 3 RED = 4 上述常量只要赋值为不同量即可，可以为不同的数字或不同的字符串 print(EMP.BLACK) 注意： 这里运行会报错：ImportError: cannot import name &apos;Enum&apos; 原因是此文件本身叫做enum.py，这与引包操作产生冲突，需要修改文件名 上述正确打印结果为：EMP.BLACK 如果按一般的类，打印结果应该为3，但是枚举类会打印出本身继承Enum的枚举类的特点和优势：对于其他实现枚举的方式： 一是直接定义： BLACK = 1 RED = 2 二是使用字典： {&quot;BLACK&quot;:1,&quot;RED&quot;:2} 三是自定义类： class MyEnum(): BLACK = 1 RED = 2 上述的缺点： 上述实现方式，对应关系均是可变的，可以发生赋值 上述均没有防止重复的功能，不同类型对应值可能相同 枚举的要求： 类型一般不能随意更改 类型一般不能出现重复 对于枚举类型： from enum import Enum class EMP(Enum): BLACK = 1 RED = 2 EMP.BLACK = 3 尝试修改时报错：AttributeError: Cannot reassign members. from enum import Enum class EMP1(Enum): BLACK = 1 BLACK = 2 相同标签对应不同类型时报错：TypeError: Attempted to reuse key: &apos;BLACK&apos; 综上： 枚举类型可以保证类型名称的不重复性、并且保护类型不被修改注意：区别于枚举名称的重复（不允许）： class EMP1(Enum): BLACK = 1 RED = 1 上述这种方式是不报错的，因为这表示两个标签对应同一类别，这是允许的，类似别名的概念 此时print(EMP1.RED)会打印出EMP1.BLACK 对于这种类型相同的枚举类型，在遍历时只会遍历EMP1.BLACK 如果想将别名也全部遍历出来，那么使用 for x in EMP1.__members__.items()： print(x) 输出： (&apos;BLACK&apos;,&lt;EMP1.BLACK: 1&gt;) (&apos;RED&apos;,&lt;EMP1.RED: 1&gt;) 或者使用 for x in EMP1.__members__: print(x) 输出： BLACK RED注意区别：枚举类型、枚举名称、枚举值 对于代码： from enum import Enum class EMP1(Enum): BLACK = 1 RED = 2 通过枚举类型，访问枚举名称和枚举值： print( EMP1.BLACK.value ) #输出 1 print( EMP1.BLACK.name ) #输出 BLACK 验证： print( type(EMP1.BLACK) ) #输出 &lt;enum &apos;EMP1&apos;&gt; 枚举类型 print( type(EMP1.BLACK.name) ) #输出 &lt;class &apos;str&apos;&gt; 枚举名称 通过枚举名称访问枚举类型 print( EMP1[&apos;BLACK&apos;] ) #输出 EMP1.BLACK print( type(EMP1[&apos;BLACK&apos;] ))#输出 &lt;enum &apos;EMP1&apos;&gt; 枚举类型注意：枚举类本身可以被遍历 for x in EMP1: print(x) 输出： EMP1.BLACK EMP1.RED 上述将EMP1类中的全部枚举类型都打印出来枚举类型的比较:限定：同一枚举类中的枚举类型 方式： 两个枚举之间的等值比较， 两个枚举之间的身份比较， 不支持大小比较！！！ 示例： from enum import Enum class EMP1(Enum): BLACK = 1 RED = 2 print( EMP1.BLACK == EMP1.RED ) #输出 False ，表明两个枚举类型不相等 print( EMP1.BLACK == 1 ) #输出 False ，表明枚举类型的比较不是单纯的数值比较 print( EMP1.BLACK is EMP1.BLACK ) #输出 True，表明两者身份相同 对于不同枚举类之间的枚举类型 class EMP2(Enum): BLACK = 1 RED = 2 print( EMP1.BLACK == EMP2.BLACK ) #输出 False 注意： 虽然对应类型相同，但是两者属于不同类，无从比较枚举转换将数值形式来转换为枚举类型的操作 示例： from enum import Enum class EMP1(Enum): BLACK = 1 RED = 2 param = 1 print( EMP1(param) ) #输出EMP1.BLACK枚举类型的总结：对于继承的Enum类，允许类型为字符串类型 from enum import Enum class EMP1(Enum): BLACK = &apos;1&apos; RED = &apos;2&apos; 如果继承IntEnum，上述代码会报错，因为不允许非Int类型 from enum import IntEnum class EMP1(IntEnum): BLACK = &apos;1&apos; RED = &apos;2&apos; 引入的unique模块是一个装饰器，其内不允许重复 from enum import Enum,unique @unique class EMP1(Enum): BLACK = 1 RED = 1 #报错：ValueError:duplicate values found in &lt;enum &apos;EMP1&apos;&gt;: RED -&gt; BLACK其他：枚举类型是单例模式，不允许实例化","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"初识机器学习","slug":"2017-10-25 初识机器学习","date":"2017-10-24T22:00:00.000Z","updated":"2020-06-27T22:26:23.228Z","comments":true,"path":"article/7f0b.html","link":"","permalink":"https://www.cz5h.com/article/7f0b.html","excerpt":"几个基础算法关联规则 啤酒和尿布的案例原始的购物篮分析，属于数据挖掘范畴，但也是机器学习的必备算法。","text":"几个基础算法关联规则 啤酒和尿布的案例原始的购物篮分析，属于数据挖掘范畴，但也是机器学习的必备算法。 协同过滤 高级的购物篮分析，是推荐系统常用到的算法之一 聚类 运营商人群分类案例通过聚类分离不同人群，然后再分析人群的特点，制定不同的品牌，其属于机器学习范畴，对于给定的数据，运行给定的算法即可获得相关结果 朴素贝叶斯 原始的垃圾邮件过滤经常被用到 决策树 通过不同的参数指标来获得对事物的评判 CTR预估 类似于PageRank的目的，其对指定对象进行评分排名 机器学习和数据分析的区别处理数据的不同 机器学习：处理行为数据，搜索记录，浏览记录，评论记录等等数据分析：处理交易数据，账单工单等等数据量：海量/行为数据 VS 交易/少量数据 数据特点不同 交易数据的一致性要求非常高，事务保证，确保数据一致性行为数据一致性不高，数据缺失影响不大，对于整体分析结果影响较少对于交易型数据的存储：关系型数据库行为数据：MongoDB等NoSQL数据库 对于数据一致性不高的特点：产生了NoSQL NoSQL特点：保证数据吞吐量的前提下会损失一致性，所以存储上： 分析方法不同 数据分析多采用采样分析机器学习大多是全量分析，数据量越多，分析结果越贴合 解决的问题不一样 过去的历史数据特点：数据分析预测未来的用户特点：机器学习 技术手段不同 分类 特点 数据分析 汇总数据，OLAP，纬度少，属性少，数据量小，用户驱动，交互式分析 机器学习 明细全量数据，纬度多，属性多，数据量大，数据驱动，自动进行知识发现 参与者、受众不同 数据分析取决于分析师的能力视角，目标用户是特定决策者机器学习结果：取决于数据质量，数据驱动，算法影响较小，目标用户是数据用户本身 机器学习算法分类| 依据|类别|| — |— | —| 按训练数据特点|有监督学习，无监督学习，半监督学习|| 按算法解决的问题|分类和回归，聚类，标注|| 按算法本质|生成模型，判别模型| 按训练数据特点对样本数据进行训练，得到一个模型，然后判断Y(输出)-X(输入)关系 有监督学习：分类算法(类别)、回归算法(数字) 例如分类垃圾邮件：训练数据明确给出每个样本属于哪个类别，已经打好标签特点，垃圾邮件已知，通过训练获得垃圾邮件的特征，从而分类出垃圾邮件评判：给出垃圾邮件，要分到垃圾类别 无监督学习：不知道类别，标签未知，数据中没有Y 例如用户聚类：分类之前不知道具体类别，算法结束后才知道具体类别和类别特征 半监督学习、强化学习 可能开始有Y值，但是模型结果不好，但随着训练增多结果变好 按算法解决问题 分类和回归：预测分类，预测Y值 聚类 标注：例如文本，可以切词，打标签，标注算法 按算法的本质 生成模型：告诉属于各个类的概率，模棱两可，陪审团 判别模型：直接给算法，数据丢进去返回哪一类，非黑即白，法庭宣判 通常用来说分类问题例如逻辑回归和朴素贝叶斯的本质区别：是判别和生成模型的区别从算法实现思想出发，非常重要！ 常见算法 类别 名称 特点 分类 C4.5 有监督算法，淘汰 聚类 K-Means 无监督算法 分类 SVM 基于统计，有数学理论支撑（效果好，有理论支撑）-被深度学取代-必考，公式推导 关联分析 Apriori 频繁项集挖掘，代价大，被FP-Growth取代，只需；两次扫描数据库，推荐不用这些算法了 抽象 EM 算法框架，K-Means本质即为EM算法 链接 PageRank 分类框体 AdaBoost 人脸识别，有监督学习 分类 kNN 最简单，有监督学习，类似k-means 分类 NativeBayes 分类 CART 淘汰 其他杂类 名称 特点 FP-Growth 频繁项集挖掘 逻辑回归 搜索结果排序，本质逻辑回归 RF随机森林、GBDT 类似AdaBoost，都是决策树算法改进 推荐算法 — LDA 文本分析，自然语言处理 难度大 Word2Vector 文本挖掘 HMM马尔科夫模型、CRF条件随机场 文本挖掘 深度学习系列算法 — 机器学习的框架机器学习解决的问题无非两类：预测、分类预测：预测所属分类、预测预测数值，区别：预测目标Y是连续的还是离散的 算法概要流程准备工作 首先确定业务需求，确定问题 围绕问题收集数据 特征工程：预处理、提取特征，清洗整合重构，ETL过程，时间占比七成左右 例如预测购买力，要确定收入、学历等数据，筛选出来结构化如果数据准备好了，那么用哪种模型对结果效果影响较小，特征工程的影响非常大数据的好坏基本会决定了整个学习的效果。 训练模型 针对问题定义模型 定义模型的参数是不知道的，通过训练数据求参数，最终产生一个公式 定义损失函数 评估偏差的大小，机器学习没法得到问题的解析解/精确解，找到偏差最小的函数偏差的定义：对于回归问题就是真实与预测的查，对于分类问题偏差定义较困难不直观，必须用数学方式定义之。loglogth，thinge等等 优化算法 在定义损失函数之后，确定损失函数的最小值，往往演变为优化问题，又会用到一些优化算法，纯数学问题：凸优化问题、优化问题，涉及梯度下降、随机梯度下降、坐标法等等 模型评估（在输入数据、计算、得到模型之后） 交叉验证：K值实值等等 效果评估指标：准确率，召回率，方差，ROC曲线，AOC等等 检验模型好不好的标准难度：损失函数，优化算法 示例：将图像按颜色分类 确定问题：按颜色分类收集数据：大量图片文件特征工程：对于图片要根据图像内容，每个像素点由三数字组成；图片大小不一样，即数据维度不一样，如何将图片文件转换为聚类格式，转换为统一维度的向量训练模型：K-Mean聚类评价指标：暂略注意：每次结果可能不一致","categories":[{"name":"ML/R学习笔记","slug":"ML-R学习笔记","permalink":"https://www.cz5h.com/categories/ML-R%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.cz5h.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"笔记","slug":"笔记","permalink":"https://www.cz5h.com/tags/%E7%AC%94%E8%AE%B0/"}]},{"title":"Python的JSON处理","slug":"2017-10-23 Python的JSON处理","date":"2017-10-22T22:00:00.000Z","updated":"2020-02-29T18:43:55.397Z","comments":true,"path":"article/8830.html","link":"","permalink":"https://www.cz5h.com/article/8830.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 什么是JSON：是一种轻量级的（比较于XML格式）数据交换格式表现形式：字符串 不同语言可以将其转换为不同类型：Python(dict),JavaScript(Object)优势：易于阅读，易于解析，网络传输开销小效率高，适合跨语言交换数据应用场景：前后台交互、多语言服务的交互几种定义：JSON字符串：符合JSON格式的字符串。{&quot;name&quot;:&quot;Tom&quot;}操作JSON字符串:处理方式： Python内置模块json，转换为字典dict类型 示例： import json Json_str = &apos;{&quot;name&quot;:&quot;Tom&quot;, &quot;aga&quot;:20, &quot;sex&quot;:&quot;female&quot;}&apos; 注意上述字符串要加引号：双引号，数字不用加，布尔值不用加 整个字符串可以用单引号包装； student = json.loads(Json_str) print(type(student)) #输出 &lt;class &apos;dict&apos;&gt; print(student) #输出 {&apos;name&apos;: &apos;Tom&apos;, &apos;sex&apos;: &apos;female&apos;, &apos;aga&apos;: 20} 访问JSON的成员 print(student[&apos;name&apos;])包含多个对象的Array形式处理方式： Python中&quot;反序列化&quot;为List&lt;dict&gt; 示例： import json Json_str = &apos;[{&quot;name&quot;:&quot;Tom&quot;, &quot;aga&quot;:20},{&quot;name&quot;:&quot;Jack&quot;, &quot;aga&quot;:16}]&apos; student = json.loads(Json_str) print(type(student)) #&lt;class &apos;list&apos;&gt; print(student) #[{&apos;aga&apos;: 20, &apos;name&apos;: &apos;Tom&apos;}, {&apos;aga&apos;: 16, &apos;name&apos;: &apos;Jack&apos;}]反序列化：上述JSON格式转化为Python类型，即为反序列化 反序列化对应的数据类型： object dict array list string str number int/float true/false True/False null None序列化即将现有数据类型转化为JSON格式 处理方式： 使用json模块的json.dumps() 示例： import json Json_str = [ {&quot;name&quot;:&quot;Tom&quot;, &quot;aga&quot;:20}, {&quot;name&quot;:&quot;Jack&quot;, &quot;aga&quot;:16} ] student =json.dumps(Json_str) print(type(student)) #&lt;class &apos;str&apos;&gt; 上述list已经序列化为JSON字符串 print(student) #[{&quot;name&quot;: &quot;Tom&quot;, &quot;aga&quot;: 20}, {&quot;name&quot;: &quot;Jack&quot;, &quot;aga&quot;: 16}]JSON相关概念：JSON JSON对象 JSON字符串 误区一：JSON和JavaScript没有太大关系 遵循ECMASCRIPT的语言：ActionScript，TypeScript，JavaScript，JSON 误区二：JSON就是字符串 JSON具有自己的数据类型，与JavaScript相似","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"Python的正则表达式","slug":"2017-10-23 Python的正则表达式","date":"2017-10-22T22:00:00.000Z","updated":"2020-02-29T18:43:55.398Z","comments":true,"path":"article/d81.html","link":"","permalink":"https://www.cz5h.com/article/d81.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 正则表达式定义： 是一个特殊的字符序列，一个字符串是否符与此字符序列相匹配 作用： 可以快速检索文本、实现一些替换文本的操作 场景： 检查一串数字是否是电话号码 检查字符串是否是Email格式 把文本中指定单词替换为其他单词 示例： 判断下列字符串是否包含Python a = &apos;C|C++|Java|C#|Python|JavaScript&apos; print(a.index(&apos;Python&apos;)&gt;-1) # True print(&apos;Python&apos; in a) # True 上述是使用python内置的函数来实现字符串查找等功能 使用常量正则表达式： 使用re的方法： re.findall(&apos;正则表达式&apos;,&apos;所匹配的字符串&apos;) import re a = &apos;C|C++|Java|C#|Python|JavaScript&apos; print(re.findall(&apos;Python&apos;,a)) #输出：[&apos;Python&apos;] print(re.findall(&apos;C&apos;,a)) #输出：[&apos;C&apos;, &apos;C&apos;, &apos;C&apos;] #判断是否包含字符串： print( len(re.findall(&apos;Python&apos;,a)) &gt; 0 ) #输出True 常量正则表达式：规则单一，没有体现出正则的强大 正则的强大之处就在于构造有意义的规则 构造有规则的正则表达式： 示例: 提取字符串中的所有数字 方法一：循环遍历每个字符 方法二：构造正则，使用findAll方法 :&apos;\\d&apos;表示0~9 &apos;Python&apos; - 普通字符 &apos;\\d&apos; - 元字符之一 &apos;\\D&apos; - 去掉数字 代码： import re a = &apos;C1|C++2|Java3|C#4|Python5|JavaScript6&apos; print(re.findall(&apos;\\d&apos;,a)) #[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;] 找出了全部数字 print(re.findall(&apos;\\D&apos;,a)) 去掉数字 #[&apos;C&apos;, &apos;|&apos;, &apos;C&apos;, &apos;+&apos;, &apos;+&apos;, &apos;|&apos;, &apos;J&apos;, &apos;a&apos;, &apos;v&apos;, &apos;a&apos;, &apos;|&apos;, &apos;C&apos;, &apos;#&apos;, &apos;|&apos;, # &apos;P&apos;, &apos;y&apos;, &apos;t&apos;, &apos;h&apos;, &apos;o&apos;, &apos;n&apos;, &apos;|&apos;, &apos;J&apos;, &apos;a&apos;, &apos;v&apos;, &apos;a&apos;, &apos;S&apos;, &apos;c&apos;, &apos;r&apos;, # &apos;i&apos;, &apos;p&apos;, &apos;t&apos;] 元字符很多，不需要记忆元字符，要学会自由组合 更重要的是：模式 - 任意的、A或B、等等字符集形式： 中括号 [xyz]：x或y或z 示例： 找出中间一个字符是c或者是f的字符串 代码： import re s = &apos;abc,acc,adc,aec,afc,apc&apos; print( re.findall( &apos;a[cf]c&apos;,s ) ) #[&apos;acc&apos;, &apos;afc&apos;] #输出: a [cf] c # 借助a定界 表示a或f 借助c定界 # 普通字符 元字符 普通字符 中括号 [^xyz]：除去x或y或z 代码： print( re.findall( &apos;a[^cf]c&apos;,s ) ) #[&apos;abc&apos;, &apos;adc&apos;, &apos;aec&apos;, &apos;apc&apos;] 中括号 [a-d]：a或b或c或d 代码： print( re.findall( &apos;a[a-c]c&apos;,s ) ) #[&apos;abc&apos;, &apos;acc&apos;]概括字符集形式： 类似&apos;\\d&apos;,表示囊括所有的数字， 其他相同作用的规则： &apos;\\d&apos; = 字符集&apos;[0-9]&apos; &apos;\\D&apos; = 字符集&apos;[^0-9]&apos; 场景： 匹配字符串中的各类字符 示例： import re a = &apos;C1|C++2|Java3( C#4\\n)Python5_JavaScript6&apos; print( re.findall( &apos;\\w&apos;,a ) ) #返回单词字符，字母，数字，和下划线 #[&apos;C&apos;, &apos;1&apos;, &apos;C&apos;, &apos;2&apos;, &apos;J&apos;, &apos;a&apos;, &apos;v&apos;, &apos;a&apos;, &apos;3&apos;, &apos;C&apos;, &apos;4&apos;, &apos;P&apos;, &apos;y&apos;, &apos;t&apos;, # &apos;h&apos;, &apos;o&apos;, &apos;n&apos;, &apos;5&apos;, &apos;_&apos;, &apos;J&apos;, &apos;a&apos;, &apos;v&apos;, &apos;a&apos;, &apos;S&apos;, &apos;c&apos;, &apos;r&apos;, &apos;i&apos;, &apos;p&apos;, # &apos;t&apos;, &apos;6&apos;] print( re.findall( &apos;\\W&apos;,a ) ) #返回非单词字符 #[&apos;|&apos;, &apos;+&apos;, &apos;+&apos;, &apos;|&apos;, &apos;(&apos;, &apos; &apos;, &apos;#&apos;, &apos;\\n&apos;, &apos;)&apos;] 使用中括号的形式匹配字母和数字 print( re.findall( &apos;[0-9a-zA-Z]&apos;,a ) ) print( re.findall( &apos;\\s&apos;,a ) ) &apos;\\s&apos; 匹配空白字符 #[&apos; &apos;, &apos;\\n&apos;] print( re.findall( &apos;\\S&apos;,a ) ) &apos;\\S&apos; 匹配非空白字符 print( re.findall( &apos;.&apos;,a ) ) &apos;.&apos; 匹配除\\n之外的字符第一种数量词形式： {N} 或者 {M,N} 示例： import re a = &apos;tom12,gimmy*77,kit_001,jack,yang&apos; print(re.findall(&apos;[a-z][a-z][a-z]&apos;,a)) #输出[&apos;tom&apos;, &apos;gim&apos;, &apos;kit&apos;, &apos;jac&apos;, &apos;yan&apos;] 这里重复使用三次中括号字符集完成了匹配长度为三的字符 print(re.findall(&apos;[a-z]{3}&apos;,a)) #输出[&apos;tom&apos;, &apos;gim&apos;, &apos;kit&apos;, &apos;jac&apos;, &apos;yan&apos;] 上述使用大括号：{}，使用数量词来完成重复，结果与上述一致 print(re.findall(&apos;[a-z]{3,6}&apos;,a)) #输出[&apos;tom&apos;, &apos;gimmy&apos;, &apos;kit&apos;, &apos;jack&apos;, &apos;yang&apos;] 如果需要匹配出全部英文单词，即匹配出长度为3-5的字符，可以使用{x,y}来限定重复次数 此处的匹配是：贪婪的，检测到gim之后，并不停止，因为没有达到最大限定5，故还会继续匹配，一直到匹配到gimmy之后才停止 python默认是贪婪的匹配 非贪婪的模式匹配正则:{}? print(re.findall(&apos;[a-z]{3,6}?&apos;,a)) #输出[&apos;tom&apos;, &apos;gim&apos;, &apos;kit&apos;, &apos;jac&apos;, &apos;yan&apos;]，此处匹配到3字符后就停止了第二种数量词形式： *, +, ? 含义： * 匹配*号前的字符0次或者无限多次 + 匹配+号前的字符1次或者无限多次 ? 匹配?号前的字符0次或1次 示例： import re a = &apos;to_tom12tomy*77tomyy,kit_001,jack,yang&apos; print(re.findall(&apos;tomy*&apos;,a)) #输出[&apos;tom&apos;, &apos;tomy&apos;, &apos;tomyy&apos;] 解释： tom 成功匹配t,o,m,匹配y不成功，即成功0次 tomy 成功匹配t,o,m,y tomyy 成功匹配t,o,m,y,y又匹配成功，因为可以无限次，故含有y print(re.findall(&apos;tomy+&apos;,a)) #输出[&apos;tomy&apos;, &apos;tomyy&apos;] 解释： to 没出来，因为to之后的my没匹配成功，即成功-1次 tom 没出来，因为tom之后的y没匹配成功，即成功0次 tomy 成功匹配t,o,m,y tomyy 成功匹配t,o,m,y后，后续匹配y，因为y在tomy内，故成功 print(re.findall(&apos;tomy?&apos;,a)) #输出[&apos;tom&apos;, &apos;tomy&apos;, &apos;tomy&apos;] 解释： to 没匹配出来，因为匹配失败2次，即匹配成功-1次 tom 匹配出tom，因为匹配到tom之后再匹配&apos;y&apos;没有匹配到，即匹配成功0次 tomy 成功匹配t,o,m,y tomy 注意tomyy匹配为tomy，因为成功匹配t,o,m,y后，y虽又匹配成功，但已经是第二次，故丢弃 print(re.findall(&apos;tomy{1,2}&apos;,a)) #使用{}作为数量词限定 #输出[&apos;tomy&apos;, &apos;tomyy&apos;] print(re.findall(&apos;tomy{1,2}?&apos;,a)) #非贪婪 #输出[&apos;tomy&apos;, &apos;tomy&apos;]边界匹配符形式： ^ 、 $ 场景： 验证字符串长度：4-8位 示例： import re a = &apos;11345610000&apos; print(re.findall(&apos;\\d{4,8}&apos;,a)) #输出[&apos;12345610&apos;] 这样可以匹配出4-8位字符，但是判断不了原串是否为4-8位 此处应该使用面向字符串的匹配： 边界匹配：^表达式$，将‘表达式’作为整体匹配 示例： print(re.findall(&apos;^\\d{4,8}$&apos;,a)) #输出[] 边界符的作用： 不用边界符时 print(re.findall(&apos;00&apos;,a)) #输出[&apos;00&apos;, &apos;00&apos;] 使用边界附 ^ 从字符串开始的位置开始匹配 print(re.findall(&apos;^00&apos;,a)) #输出[] 因为字符串开头为1，故匹配失败 使用边界附 $ 从字符串末尾的位置开始匹配 print(re.findall(&apos;00$&apos;,a)) #输出[&apos;00&apos;] 字符串结尾为0000，这里只匹配到0,0就结束组形式： () 场景： 判断字符串是否包含连续！N个子串 示例： 例如判断串中是否存在三个&apos;to&apos; 代码： import re a = &apos;to_tom12tomy*77tomyy, kit_001, jack, yang_tototo&apos; print(len(re.findall(&apos;(to){3}&apos;,a))&gt;0) #输出True 表示存在tototo，连续匹配 t且o 三次 中括号[]和小括号()的区别： [xyz]：x或y或z (xyz)：x且y且z匹配模式形式： re.I re.S 示例： import re a = &apos;to_tom\\ntomy*77tomyy, kit_001, jack, yang_tototo&apos; print(re.findall(&apos;Tom&apos;,a)) #[] 匹配不到tom，大小写不符 使用模式：添加第三个参数： print(re.findall(&apos;Tom&apos;, a, re.I )) #[&apos;tom&apos;, &apos;tom&apos;, &apos;tom&apos;] 如何匹配&apos;tom\\n&apos; ? print(re.findall(&apos;Tom.&apos;, a, re.I )) #[&apos;tomy&apos;, &apos;tomy&apos;] 没匹配出来 使用添加模式re.S print(re.findall(&apos;Tom.&apos;, a, re.I | re.S )) #[&apos;tom\\n&apos;, &apos;tomy&apos;, &apos;tomy&apos;] 这里由于匹配了模式re.S，所以匹配出了\\n字符sub()形式： re.sub(表达式,替换成,原串)) 场景： 替换 字符串 示例： 简单替换（与内置方法replace类似） import re a = &apos;to_tomtomy*77tomyy, kit_001&apos; print(re.sub(&apos;tom&apos;,&apos;Tom&apos;,a)) # to_TomTomy*77Tomyy, kit_001 print(re.sub(&apos;tom&apos;,&apos;Tom&apos;,a, 1)) #to_Tomtomy*77tomyy, kit_001 #注意：使用了count=1参数，只替换了第一个tom #简易替换： print(a.replace(&apos;tom&apos;,&apos;Tom&apos;, 1)) #to_Tomtomy*77tomyy, kit_001 #同样是替换了第一个tom 高级替换： sub的强大之处，在于可以附加一个函数参数 示例： def convert(value): #注意需要带一个参数 print(value) print(re.sub(&apos;tom&apos;,convert,a)) #输出 #&lt;_sre.SRE_Match object; span=(3, 6), match=&apos;tom&apos;&gt; #&lt;_sre.SRE_Match object; span=(6, 9), match=&apos;tom&apos;&gt; #&lt;_sre.SRE_Match object; span=(13, 16), match=&apos;tom&apos;&gt; #to_y*77yy, kit_001 注意上述替换后结果中tom消失了，而且传入函数的参数为对象： 三个对象：表示进行了三次字符串匹配 span(3,6)表示第一次tom字符串的起始边界为3，结束后的边界为6 通过函数进行动态替换,替换为return的值 示例： def conver(value): #注意需要带一个参数 match = value.group() #value.group() 表示匹配成功时传进函数的字符串 return &apos;[&apos;+match+&apos;]&apos; print(re.sub(&apos;tom&apos;,conver,a)) #输出：to_[tom][tom]y*77[tom]yy, kit_001 进一步复杂的替换： 示例： 替换所有大于等于50的数字为 [BIG] ，小于的剔除 代码： import re a = &apos;to_to5mtomy*77tomy65y&apos; def conver(value): #注意需要带一个参数 match = value.group() print(match) if( int(match) &gt;= 50): #注意转换类型 return &apos; [BIG] &apos; return &apos;&apos; print(re.sub(&apos;\\d+&apos;,conver,a)) #输出： #5 #77 注意，此处77作为整体传入，不会分两次传入7,7 #65 #to_tomtomy* [BIG] tomy [BIG] y ，完成替换search(),match()地位： 这是re模块正则匹配的三种方法的除findall之外的其余两种 形式： re.match re.search 示例： import re a = &apos;1_to_to5mtomy*77tomy65y&apos; print(re.match(&apos;\\d&apos;,a)) #输出&lt;_sre.SRE_Match object; span=(0, 1), match=&apos;1&apos;&gt; match()特点： 从字符串首字符开始匹配，如果失败，将返回None,object; span=(0, 1)其实就是&apos;1&apos; print(re.search(&apos;\\d&apos;,a)) #输出&lt;_sre.SRE_Match object; span=(5, 6), match=&apos;5&apos;&gt; search()特点： 将尝试搜索字符串，将找到的第一个字符返回，注意object; span=(5, 6)，其实就是字符&apos;5&apos; 对于返回的object，有相关操作方法 group()方法：返回匹配的字符、字符串 示例： print(re.search(&apos;\\d&apos;,a).group()) #输出 1 span()方法：返回匹配结果在原串中的位置 示例： print(re.search(&apos;\\d&apos;,a).span()) #输出 (0, 1) 比较与findall方法，他们只要匹配成功就会停止匹配 比较与findall方法，他们会返回对象 print(re.findall(&apos;\\d&apos;,a)) #输出[&apos;1&apos;, &apos;5&apos;, &apos;7&apos;, &apos;7&apos;, &apos;6&apos;, &apos;5&apos;]匹配返回对象的方法 分组：group()场景： 例如取出下面的时间信息 示例： import re a = &apos;#2002 2017-10-30 11:12:34 #!&apos; 首先使用普通字符 定界，这里的边界是&apos;2002 &apos;和&apos; #!&apos; print(re.search(&apos;2002 \\w #!&apos;,a).group()) # 报错，因为seach无返回值，None.group()会报错 print(re.search(&apos;2002 \\w* #!&apos;,a).group()) # 报错，因为seach无返回值，None.group()会报错 print(re.search(&apos;2002 .* #!&apos;,a).group()) # 输出:2002 2017-10-30 11:12:34 #! 这里并没有只返回中间内容，连边界也返回了 print(re.search(&apos;2002 .+ #!&apos;,a).group()) # 同样输出:2002 2017-10-30 11:12:34 #! 带参数的group() 注意： group(parameter): group() = group(0) group(0)：永远记录完整匹配结果 group(N)：N&gt;0，记录第N个分组 示例： print(re.search(&apos;2002 (.+) #!&apos;,a).group(1)) #输出：2017-10-30 11:12:34 注意上述括号(),表示一个分组，使用group(1)，表示返回第一个分组 使用findall完全可以实现： print(re.findall(&apos;2002 (.+) #!&apos;,a)) #输出：[&apos;2017-10-30 11:12:34&apos;] 另：groups() print(re.search(&apos;2002 (.+) #!&apos;,a).groups()) 直接返回全部分组：(&apos;2017-10-30 11:12:34&apos;,)","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"Python的面向对象","slug":"2017-10-23 Python的面向对象","date":"2017-10-22T22:00:00.000Z","updated":"2020-02-29T18:43:55.399Z","comments":true,"path":"article/ddd3.html","link":"","permalink":"https://www.cz5h.com/article/ddd3.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 面向对象目的： 写出有意义的面向对象的代码，其作用就是封装代码 定义时注意： 命名规范 Student、StudentPages 类体不能什么都不写，要写pass 定义示例： class Student(): # 开始类体的编写 name = &apos;&apos; age = 0 def print_file(): print(&apos;age = &apos;+str(age)) stu = Student() #不需要使用new 来实例化这个类 调用类的方法： stu.print_file() 注意上述调用会报错 # TypeError: print_file() takes 0 positional arguments but 1 was given 做如下修改： def print_file(self): print(&apos;age = &apos;+str(age)) 仍然报错，报错age没有定义 继续修改,改完正确运行 def print_file(self): print(&apos;age = &apos;+str(self.age)) 正确示例： class Student(): name = &apos;&apos; age = 0 def print_file(self): print(&apos;age = &apos;+str(self.age)) stu = Student() stu.print_file() # 或者直接用：Student().print_file() 注意： 上述类体中，对于print_file函数，不能在类体里调用 写类的模块，最好是只写类，然后通过其他模块来实例化调用什么的 from c1 import Student Student().print_file() Student().age 注意： 如果c1.py中同时包含对Student类的实例化和调用，那么上述import时也会执行c1.py中的示例化和调用 所以，最好是模块和类分开，便于调用时的清晰方法 和 函数：区别： 方法是语言设计层面的考量，应用起来没什么区别 类中的函数应该叫‘方法’，模块中的函数就叫‘函数’ 类中的变量应该叫‘数据成员’，模块中变量叫‘变量’ 类和对象 通过实例化联系在一起 什么是类： 就是数据及一些操作的有意义的封装，可以体现出数据特征和行为特征 行为要联系主体，体现在类的设计中要具有现实意义 什么是对象： 表示具体的类对象，类本身可以实例化多种多样的对象 通过实例化来创造对象的多样性，依靠类的构造函数实现 class Student(): name = &apos;&apos; age = 0 def __init__(self): # 至少需要添加self参数 print(&apos;init&apos;) def print_file(self): print(&apos;age = &apos;+str(self.age)) stu = Student() #构造函数在实例化时自动调用 stu.__init__() #构造函数也可以调用，跟普通函数类似，但是不推荐这样用 但是：对于构造函数 只允许返回None，返回其他则报错 为构造函数添加参数： def __init__(self,param1,param2): 此时实例化类时，必须传入两个值：stu = Student(&apos;a&apos;,&apos;b&apos;) 构造函数通常的用法： 来修改类的数据特征，即重置类的成员变量 class Student(): name = &apos;&apos; age = 0 def __init__(self, name, age): # 至少需要添加self参数 name = name age = age 上面的代码不报错，但是不能修改name和age的值，并不是因为变量作用域的问题 注意： 类的变量的作用域 与 模块变量的作用域 完全不同！ 要注意区别类的行为和模块的行为类变量 实例变量：代码示例： class Student(): name = &apos;&apos; name 是类变量：与类相关 age = 0 age 是类变量：与类相关 def __init__(self, name, age): self.name = name self.name 实例变量：与对象相关 self.age = age self.age 实例变量：与对象相关 s1 = Student(&apos;a&apos;,1) 作为实例变量传入 s2 = Student(&apos;b&apos;,8) 作为实例变量传入 注意上述self可以换成任意名称： def __init__(this, name, age): this.name = name this.age = age 换成this也是对的，但是推荐使用默认的 self 使用区别： 对象名.成员变量 取决于实例化时的构造 类名.成员变量 只跟类有关，不可改变 应用场景： 比如定义一个狗类叫做ClassA: 里面有成员变量 动物种类、狗品种、狗毛色 有构造函数，参数为品种、毛色，但动物种类变量就等于“狗”，构造时不修改 实例化时借助构造函数，得到N个不同的狗对象ObjN，可以对应现实世界中不同的狗个体 此时，ObjN.品种，就是此狗对象的对象属性 而ClassA.动物类型，表明此类的特征属性，表示共同特性或者不属于个体特性的变量就可以作为类的成员变量(类的机制) 类变量和实例变量的特性示例代码： class Student(): name = &apos;类变量name&apos; age = 0 def __init__(self, name, age): # 至少需要添加self参数 name = name age = age obj = Student(&apos;实例变量name&apos;,&apos;实例变量age&apos;) print(obj.name) #打印类变量name print(Student.name) #打印类变量name print(obj.__dict__) #打印{} print(Student.__dict__) #打印{&apos;name&apos;: &apos;类变量name&apos;, &apos;__doc__&apos;: None, &apos;__weakref__&apos;: &lt;attribute &apos;__weakref__&apos; of &apos;Student&apos; objects&gt;, &apos;__init__&apos;: &lt;function Student.__init__ at 0x00000000006BFA60&gt;, &apos;__module__&apos;: &apos;__main__&apos;, &apos;__dict__&apos;:&lt;attribute &apos;__dict__&apos; of &apos;Student&apos; objects&gt;, &apos;age&apos;: 0} 寻找相关变量的机制： 如果尝试去访问对象的一个成员变量 首先会在对象的变量列表obj.__dict__里去查找有没有 否则，到类的变量列表Student.__dict__去寻找 否则继续去类的父类中去寻找 示例代码： class Student(): name = &apos;类变量name&apos; age = 0 def __init__(self, name, age): # 至少需要添加self参数 self.name = name self.age = age obj = Student(&apos;实例变量name&apos;,&apos;实例变量age&apos;) print(obj.name) #打印类变量name print(Student.name) #打印实例变量name print(obj.__dict__) #打印{&apos;age&apos;: &apos;实例变量age&apos;, &apos;name&apos;: &apos;实例变量name&apos;} 解释： 修改为self.name之后，则是实例的变量，在构造函数中必须赋值给实例的变量 定义实例方法例如构造函数时，需要self出现，但是调用实例方法时不需要出现self 注意： self和实例、对象绑定，与类无关 实例、对象可以调用的方法叫：实例方法，参数第一个必须为 self保留 self可以换成其他名字，比如this，但位置必须是第一参数 解释： 意思是实例方法第一个参数应该保留，具体叫self还是this或其他无所谓，但推荐用 selfPython的类--- 变量 --- 类变量 --- 实例变量 --- 方法 --- 实例方法 self + .操作符 -&gt; 修改实例变量 --- 类方法 --- 静态方法 --- 构造函数(特殊的实例方法，只是默认调用) 实例方法要操作变量： --- 实例方法 self + .操作符 -&gt; 修改实例变量 例如： class Student(): name = &apos;类变量name&apos; age = 0 def __init__(self, name1, age): # 至少需要添加self参数 self.name = name1 self.age = age print(self.name) #访问的实例变量 print(name) #这也是访问的实例变量，但是访问的是形参name,如果形参不是name，那就会报错 #print(__dict__) obj = Student(&apos;实例变量name&apos;,&apos;实例变量age&apos;) 打印： 实例变量name 实例变量name 注意： 查找变量列表__dict__只能在外部调用时访问，在实例方法内无法打印 实例方法中，方法参数不要和类变量名相同 类变量定义时，不要与类内置变量重名 --- 实例方法 修改类变量 例如： class Student(): name = &apos;类变量name&apos; age = 0 def __init__(self, name1, age): # 至少需要添加self参数 print(&apos;形参name1:&apos;+name1) self.name = name1 print(&apos;修改实例变量:&apos;+self.name) print(&apos;访问类变量法一:&apos;+Student.name) print(&apos;访问类变量法二:&apos;+self.__class__.name) #注意self.__class__的使用 obj = Student(&apos;实例变量name&apos;,&apos;实例变量age&apos;) 输出： 形参name1:实例变量name 修改实例变量:实例变量name 访问类变量法一:类变量name 访问类变量法二:类变量name --- 实例方法操作类变量 完成类变量的变化 class Student(): sum = 0 name = &apos;类变量name&apos; age = 0 def __init__(self, name1, age): # 至少需要添加self参数 self.__class__.sum += 1 print(&apos;类变量sum变为:&apos;+str(self.__class__.sum)) obj1 = Student(&apos;Tom&apos;,13) obj2 = Student(&apos;Kimmy&apos;,24) obj3 = Student(&apos;Jack&apos;,18) 输出： 类变量sum变为:1 类变量sum变为:2 类变量sum变为:3 注意： 实例方法通常是操作实例变量的，但是也可以操作类变量，引出：专门操作类变量的方法类方法：定义规范： @classmethod #使用装饰器@classmethod来定义一个类方法 def plus_sum(cls): #类方法的参数必须含一个cls参数 pass 重新完成上述类变量的修改： class Student(): sum = 0 def __init__(self,param): pass @classmethod def plus_sum(cls): # cls仍然可以改成别的名字，不建议更改 cls.sum += 1 print(cls.sum) 类方法的调用 stu = Student(1) stu.plus_sum() # 打印 1 stu = Student(2) stu.plus_sum() # 打印 2 stu = Student(3) stu.plus_sum() # 打印 3 再次强调： 实例方法关联的是对象，类方法关联的是类本身 另外，两者有时候都可以完成参数修改，但是要是操作有“意义”有时就需要区分类方法和实例方法，例如与对象无关的操作就应该使用类方法 即，对象最好不要调用@classmethod 类方法（虽然不报错，但是缺失实际意义）静态方法定义规范： @staticmethod def add(x,y): print(&quot;这是一个静态方法&quot;) 与其他方法的区别： 静态方法中没有强制参数 实例方法中，self 参数代表对象本身 类方法中 cls 代表类本身 一个对象或以各类都可以调用静态方法 示例： class Student(): sum = 0 name =&apos;Lei&apos; def __init__(self,param): self.name = param @classmethod def plus_sum(cls): cls.sum += 1 print(cls.sum) #print(self.name) # 类方法不可以引用实例变量 @staticmethod def add(x,y): #print(self.name) # 静态方法不可以引用实例变量 print(Student.sum) # 静态方法可以访问类变量 print(&apos;这是一个静态方法&apos;) #调用 stu = Student(1) stu.add(1,1) # 对象调用静态方法 且访问了类变量 不可以引用实例变量 Student.add(1,1) # 类调用静态方法 且访问了类变量 不可以引用实例变量 stu.plus_sum() # 对象调用类方法 且访问了类变量 不可以引用实例变量 Student.plus_sum() # 类调用类方法 且访问了类变量 不可以引用实例变量 注意： 静态方法不要经常使用，与类的关联性不强，与普通函数无区别类成员的可见性对于下面示例： class Student(): sum = 0 def __init__(self,param,param1): self.name = param self.age = param1 self.score = &apos;0&apos; print(&apos;初值为：&apos;+self.score) def marking(self,score): self.score = score print(&apos;修改后:&apos;+str(score)) def do_hmwork(self): pass def do_eng_hmwork(self): pass s = Student(1,2) # 将socre参数隐藏，不暴露score的直接赋值 s.score = -1 #不推荐方式，直接修改参数，这样没法进行相关过滤，不应该通过直接访问的方式修改 print(&apos;修改后:&apos;+str(s.score)) #正确方法：所有访问应该通过方法操作变量，可以在方法中对输入进行判断，进而保护数据 s1 = Student(1,2) s1.marking(-1) 注意： 上述marking方法之外，仍然可以通过 s.score = -1 来直接赋值， 原因： 上述变量和方法全部都是公开的 Python控制变量的可见性（读、写）： 公开public 私有private 方式： 私有变量：__私有变量名 私有函数：__marking() 注意： 对于构造函数，因为__init__右边也有下划线，这样不会被识别为私有 示例： class Student(): sum = 0 def __init__(self,param,param1): self.name = param self.age = param1 self.__score = &apos;0&apos; print(&apos;初值为：&apos;+self.__score) def __marking(self,score): #添加双下划线 self.__score = score print(&apos;修改后:&apos;+str(self.__score)) s1 = Student(1,2) #s1.__marking(-1) # 访问私有方法报错：&apos;Student&apos; object has no attribute &apos;__marking&apos; s1.__score = -1 print(s1.__score) # 访问私有变量，成功修改 #上述原因是：实际是利用py得动态属性，通过点的方式新添加了一个__score变量，原有私有变量并没有修改 #下面直接访问会发现 访问报错 ：&apos;Student&apos; object has no attribute &apos;__score&apos; s2 = Student(1,2) print(s2.__score) 可以利用__dict__内容验证： print(s1.__dict__) 输出： {&apos;name&apos;: 1, &apos;_Student__score&apos;: &apos;0&apos;, &apos;age&apos;: 2,&apos;_score&apos;:-1} print(s2.__dict__) 输出： {&apos;name&apos;: 1, &apos;_Student__score&apos;: &apos;0&apos;, &apos;age&apos;: 2} 分析上述发现： 其实私有变量会被改名，此处由__score变为了_Student__score，所以访问原名是访问不到的 比较两次打印，会发现s1.__score = -1 这句话其实会添加一个__score变量，而没有修改原来的score。因为原来的socre已经被改名了 上述发现： 其实Python没有完善的私有变量机制，其仅仅是通过改名，如果使用_Student__score来操作，仍然可以完成修改面向对象的特性：继承三大特性：继承、封装、多态 封装：类就是从现实世界的角度对变量和方法进行封装，很抽象比较难讲清楚 类的组成：变量和方法 继承作用：避免定义重复的方法和重复的变量 推荐一个模块创建一个类 对于以下示例： c2模块的Human代码如下： class Human(): sum = 0 def __init__(self, name, age): self.name = name self.age = age def get_name(self): print(self.name) 子类Student如下： from c2 import Human class Student(Human): # 标准的继承方法 sum = 0 def __init__(self,name,age): self.name = name self.age = age self.__score = 0 print(&apos;初值为：&apos;+str(self.__score)) s = Student(&apos;Tom&apos;,13) # 实例化子类时，要按照父类的构造函数传参 print(s.sum) print(Student.sum) # 打印 0 表示子类继承了父类的类变量 print(s.name) # 打印 Tom 表示子类继承了父类的实例变量 print(s.age) # 打印 13 表示子类继承了父类的实例变量 s.get_name() # 打印 Tom 表示子类继承了父类的实例方法 注意： 上述只是将Human父类的变量和方法提取到了子类中 Python允许多继承，一个子类可以有多个父类，一般用不到 进一步： 现在子类有自己独有的方法和变量 例如：Student类有school变量，那么其构造函数为school+父类构造参数 在子类里调用父类的函数，示例： from c2 import Human class Student(Human): # 标准的继承方法 def __init__(self,school,name,age): # 父类构造参数是name age self.school = school Human.__init__(self,name,age) #直接调用父类构造函数 传参 #注意此处父类构造参数要加上self，此处是！普通函数的调用！，传参缺一不可，self必不可少 s = Student(&apos;YangTz&apos;,&apos;Tom&apos;,13) print(s.school) # 正确打印YangTz print(s.name) # 正确打印Tom print(s.age)# 正确打印13 开闭原则： 对扩展是开放的，对更改本身是关闭的 注意： Human.__init__(self,name,age) 上述使用类，调用了实例方法，其实不推荐这样做，如果类调用一个实例方法，那么实例方法的 self 参数会成为一个普通参数，调用时应该被传入方法内 现在对于上述代码，如果父类改变，那么代码中涉及的地方全都要改，违反了开闭原则 引出：super() 通用调用方法，修改为： super(Student,self).__init__(name,age) 注意： 这样修改父类时不需要修改这里的代码 super()目的是继承父类的同名方法，如__init__()或一些公共方法 对于一个普通实例方法do_something(self)，如果其和父类方法同名，那么会优先调用子类的此方法 但是如果修改为 def do_homework(self): super(Student,self).do_homework() 那么此时表示子类的该实例方法继承了父类的该方法，此时调用会执行父类的do_something()","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"Python的函数","slug":"2017-10-22 Python的函数","date":"2017-10-21T22:00:00.000Z","updated":"2020-02-29T18:43:55.395Z","comments":true,"path":"article/8bff.html","link":"","permalink":"https://www.cz5h.com/article/8bff.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 Python的内置的函数round() a = 1.12586 #保留小数的函数,四舍五入，参数为保留小数点后的位数 print(round(a,2)) 快速查看内置函数功能：命令行直接输入python,进入RPEL，使用help()函数 &gt;&gt;&gt; help(round) Help on built-in function round in module builtins: round(...) round(number[, ndigits]) -&gt; number Round a number to a given precision in decimal digits (default 0 digits). This returns an int when called with one argument, otherwise the same type as the number. ndi 打印出python之禅 &gt;&gt;&gt; import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren&apos;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you&apos;re Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it&apos;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let&apos;s do more of those!Python函数的特点： 功能性 隐藏细节 避免编写重复代码 组织代码 自定义函数 函数定义def funcname(parameter_list): pass 上述函数定义有以下特点： 1.parameter_list参数列表可以没有 2.可以使用 return 返回函数结果，如果不写，则认为返回 None 3.funcname不推荐与内置函数名相同，因为会同名并产生递归 对于递归而言默认只允许为995，应该使用： #import sys #sys.setrecursionlimit(1000000) 修改最大递归层数 4.对于函数不强制返回类型，可以动态返回类型函数调用：def add(x,y): return x+y def printA(code): print (code) return print(&quot;a&quot;) #return 后的代码不执行 print(add(1,2),printA(&quot;abc&quot;)) #print(A,B,C)可以连续打印，打印在同一行 打印： abc 为什么先打印abc？ 3 None 为什么打印None不换行？ 解释： 首先执行add函数，内部没有打印，但是返回3，暂存，接下来执行printA函数 此时printA内打印出abc，且printA没有return即返回值为None，此时， print(3,None)，然后就在同一行打印出3，None，之前已经打印了abc对于多个参数的函数调用：def re_two_1(skill_1, skill_2): re_1 = skill_1 re_2 = skill_2 return (re_1,re_2) def re_two_2(skill_1, skill_2): re_1 = skill_1 re_2 = skill_2 return re_1,re_2 print(re_two_1(1,2)) #打印 (1, 2) print(re_two_2(1,2)) #打印 (1, 2) 注意： 不用显式的使用元组，直接逗号罗列即可返回一个元组 拆分多个返回结果： 第一种方法：不推荐，只是使用无意义的下标 res = re_two_2(1,2) print(res[0],res[1]) 第二种方法：序列解包，有意义 r1,r2 = re_two_2(1,2) print(r1,r2) 序列解包：必须严格对照，否则报错 a = 1 b = 2 c = 3 a,b,c = 1,2,3 print(a,b,c) 这也是一种解包的形式 a = 1,2,3 c,d,e = a print(c,d,e)小贴士：a = 1 b = 1 等价于 a = b = 1函数的参数类型调用上的区分： 1.必须参数，在参数列表中定义的参数，必须按顺序传入，否则报错 2.关键字参数，可以指定传入参数顺序，参数数量要对应，可以增加可读性 add(y = 3,x = 2)，这种形式调用，明确指定形参和实参的对应 def add(x,y): 形式参数x，y return x+y print(add(1,2)) 实际参数1,2 3.默认参数 如果函数输入过多，应该封装成对象，或者是添加默认参数 如果不是默认参数，那么形参实参必须严格对照 正常的定义和调用： def print_student(name,age,school): print(name) print(age) print(school) print_student(&apos;Jack&apos;,18,&apos;清华&apos;) 带默认参数的定义和调用： def print_student_defalut(name,age,school= &apos;清华&apos;): print(name) print(age) print(school) print_student_defalut(&apos;Tommy&apos;,18) 不带默认值 print_student_defalut(&apos;Tommy&apos;,18,&apos;北大&apos;) 修改默认值 print_student_defalut(&apos;Tommy&apos;,18,school = &apos;北大&apos;) 修改默认值 注意： 如果默认参数在必传参数的的中间，则报错： def print_student_defalut(name,school= &apos;清华&apos;,age) 上述定义不合规则，应该必传在前，默认参数在后 如果有多个默认参数，例如2个，只想修改第二个默认参数时，必须使用关键字参数指明赋值 def add(a,b,c=1,d=&apos;加法&apos;) 上述只修改d时，调用add(2,3,&apos;减法&apos;)，这是错误的，默认参数列表对应错误 同样，多个默认参数在调用时，调用的形参也必须按照默认参数都放在后面的规范，不能夹杂调用 add(2,c=1,3,d=&apos;减法&apos;) 上述虽然使用了关键参数，但是夹杂定义，仍然报错 4.可变参数 print(&apos;a&apos;,&apos;b&apos;,&apos;c&apos;) 具有可变的形参列表 通过一般参数来实现可变输入： def nomal(param): print(param,type(param)) nomal((1,2,3,5)) 输出#(1, 2, 3, 5) &lt;class &apos;tuple&apos;&gt; 可变输入形式： def change(*param): print(param,type(param)) change(1,2,3,5) 输出#(1, 2, 3, 5) &lt;class &apos;tuple&apos;&gt; 注意： 对于有动态参数的change函数，如果change((1,2,3)),传入元组 那么进入函数后会生成二维元组((1,2,3))，此时可以用change(*(1,2,3)) 上述方式采用*号，将传入元组类似解包，去掉元组外壳注意混合参数的调用：def demo(param1,param2 = &apos;默认值&apos;,*param3): print(param1,type(param)) print(param2,type(param)) print(param3,type(param)) 如果想跳过默认值，直接赋值可变参数，如下形式： demo(1,2,3)这样仍然给 默认参数赋值2 ，这是不行的 首先要修改默认参数，都放到最后 def demo(param1,,*param2,param3 = &apos;默认值&apos;): print(param1,type(param)) print(param2,type(param)) print(param3,type(param)) 如果想修改默认值，使用demo(1,2,3,&apos;修改&apos;),并不能达到目的，其会将(2,3,&apos;修改&apos;)作为可变参数 然后： 正确方法是demo(1,2,3,param3 =&apos;修改&apos;) 上述方法，指明默认参数，而且可变参数也正确的识别为(2,3) 综上，混合参数函数的调用比较复杂，在函数中尽量避免使用高级传参：带可变参数的函数在调用时传入参数的解包： def pingfanghe(*param): sum = 0 for i in param: sum += i*i print(sum) pingfanghe(1,2,3) # 14 pingfanghe(*[1,2,3,4]) # 30 注意此处的*号 使用**传入字典： def print_age(**param): print(param) print_age(xiaoming=18,Tom=24,Lina=13) # {&apos;xiaoming&apos;: 18, &apos;Tom&apos;: 24, &apos;Lina&apos;: 13} 遍历字典; def print_age1(**param): for c in param: print(c) print_age1(xiaoming=18,Tom=24,Lina=13) 输出： #Tom #Lina #xiaoming def print_age2(**param): for a,b in param: print(b) print_age2(xiaoming=18,Tom=24,Lina=13) 输出： #Lina : 13 #xiaoming : 18 #Tom : 24 传入字典： dic = {&apos;xiaoming&apos;:18,&apos;Tom&apos;:24,&apos;Lina&apos;:13} print_age2(**dic) 输出： #xiaoming : 18 #Tom : 24 #Lina : 13 传入空值时什么都不做 print_age2() 输出空白 指明传入字典的映射： def print_age2(**param): print(param) for a,b in param: print(a,b) print_age2( Jack = 18,Tom = 24,Lina = 13) #注意使用方式变量作用域示例代码： c = 50 #全局变量 def add(x, y): c = x + y # 局部变量 c 作用域只是在函数内部，相当于新变量，与外部的 c 无关 print(c) add(1,2) # c = 3 print(c) # c = 50 注意： 对于控制语句，python的特性： for while if 内定义的变量，在结束体之外也能访问 for x in range(0,3): a = &apos;a&apos; print(a) # 可以输出a for while if 内接收的变量，与for同级的变量可以直接引用 a = 10 for x in range(0,1): print(a) # 可以打印出10 原因：Python只有函数作用域，没有块作用域函数的作用域：同变量一样示例代码： c = 1 def func1(): c = 2 def func2(): c = 3 print(c) func2() #func2作用域 只限于函数func1内 func1() #调用func1最终会调用func2，然后执行打印作用域链作用域具有逐级寻找的过程： c = 1 def func1(): def func2(): print(c) # 可以打印c，作用域链，逐级寻找 a = 2 func2() func1() print(a) # 不可以打印a，作用域出错global关键字：示例代码： c = 1 def func1(): def func2(): print(c) # 可以打印c，作用域链，逐级寻找 global a # 注意这里使用global关键字 a = 2 # 注意：使用global a = 2 是错误的 func2() func1() print(a) # global修饰后可以打印，同样体现了作用域链 注意： 对于全局变量，包括global，可以在全部模块中都被使用，只需 import相关模块","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"Python的项目代码结构","slug":"2017-10-17 Python的项目代码结构","date":"2017-10-16T22:00:00.000Z","updated":"2020-02-29T18:43:55.394Z","comments":true,"path":"article/7ad3.html","link":"","permalink":"https://www.cz5h.com/article/7ad3.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 Python项目的代码要求：高性能，封装性（可复用）、抽象自己写程序的要求：不单追求简单业务逻辑。更要考虑封装性项目结构：顶级结构： 包 文件夹，类似jar，dll等 二级结构： 模块 .py文件，单文件可包含多个类，也可以不定义类，但最好用类组织起来 三级结构： 类 函数、变量（类的特性）包和模块的命名：与文件夹和文件的名称相同区分不同包的模块：使用命名空间baoA.module baoB.module注意：包可以包含字包 如果想让文件夹成为一个包，那么必须含有一个_init_.py文件 __init__.py叫做init模块，如果需要引用，形式 不是bao.__init__而是bao，用包的名称即 import bao 即可引入他包的相互引入：引入的第一种方式 如果 import 包名，那么会引入执行该包的全部代码 对于包内模块的引入： 模块AB同级： 如果A模块引用B模块定义的变量param 那么需要在A模块内 import module_name 即：在A模块内：import B; print(B.param)，有严格的先后顺序 模块AB不同级： 加上包的命名空间即可: import bao.B; print(bao.B.param)， 精简：import bao.B as m; print(m.param)，//缩短点引用 注意： 在python中一旦发生模块之间的import引用，就会在包下面生成pycache文件夹 __pycacahe__文件夹和期内的.pyc文件是自动生成的，与py虚拟机相关 在VSCODE中的配置页内的&quot;files.exculed&quot;中添加&quot;__pycache__&quot;:true,则会隐藏文件 引入的第二种方式 使用from..import.. 特点：可以引用单变量，也可以同import：form bao import B; print(bao.B.param) 例如 from bao.B import param ; print(param) 全部引入： from bao.B import * ; 可以把B内变量全部引入，但是会混淆当前模块定义 控制需要引入的变量：仅适用本方式 例如在B模块中有三个变量，但只需import变量param，那么需要在 B模块中第一行：添加模块内置属性 __all__ = [&apos;param&apos;] 此时使用 from bao.B import *; 时只能导入param变量规范的引入：首先明确，python内一行代码字符不超过80个 from bao.B import a,b,c,d,e,f 如果需要换行，那么可以加反斜杠换行： from bao.B import a,b,c,\\ d,e,f 或者使用括号，保证其连接性（适用场景很多）： from bao.B import (a,b,c, d,e,f)init.py的作用：其相当于在全部代码前夹上init内的代码，引用包bao内的模块B时，会自动执行该包内的init模块 例如： 引用包bao内的模块B时，自动执行bao内的init模块 __init__的功能： 在init内，通过 __all__ = [&apos;B&apos;]来控制此包内能够被引用的模块 在init内，添加公共 import 的类库， 例如在包common内的init模块中添加公共库之后，在其他代码中只需 import common即可批量添加类库引入的注意事项：包和模块不会重复导入，类似static代码块，只导入一次 避免循环导入 例如在模块A内 import B，在模块B内 import A 会陷入循环引入，要避免！ 多模块间复杂引用时要避免因引用过多产生环链 关注 import 引入的内容 一旦导入的是一个模块，则就会执行模块的全部代码 无论在代码中重复引入多少次，引入的模块都只会执行一次 搞清入口文件：运行test.py时，其实就是将其作为入口文件模块的内置变量dir()函数：返回当前模块内的全部变量 内置变量： __builtins__ __cached__ __doc__ __file__ __loader__ __name__ __package__ __spec__ 用户自定义变量： userA...注意：内置变量跟变量完全一样，内容可以被修改使用打印输出变量信息：print(&apos;name:&apos;+__name__) print(&apos;pakage:&apos;+__package__) print(&apos;doc:&apos;+__doc__) print(&apos;file:&apos;+__file__)错误信息排查：Traceback (most recent call last): File &quot;h:/mooc/test/a1.py&quot;, line 1, in &lt;module&gt; import test1.a1 File &quot;h:\\mooc\\test\\test1\\a1.py&quot;, line 3, in &lt;module &gt; print(&apos;doc:&apos;+__doc__) TypeError: Can&apos;t convert &apos;NoneType&apos; object to str implicitly 解释： 上述Traceback表示错误栈信息，会列出整个执行路径的全部出错信息，最后的Error是错误类型 应该先看最后的错误类型，然后通过错误栈来定位错误。内置变量打印结果：name:test1.a 模块的完整名称，带命名空间 pakage:test1 模块所属的包名 doc: 注释文件的注释信息 我是开头的注释 file:h:\\mooc\\test\\test1\\a.py 当前模块的物理路径对于打印未知字符串，可以使用容错处理print(&quot;可能的NoneType类型：&quot; + param ) 修改为： print(&quot;可能的NoneType类型：&quot; + (param or &apos;空值&apos;))（区别与上面的普通模块输出） 如果一个py文件被当做入口文件：那么此文件的__name__ = &apos;__main__&apos;，其会被强制改变，不在为文件名 __package__ = &apos;NoneType&apos;即，入口文件不属于任何包 __file__ = &apos;执行时的路径&apos;，即使用python 1/2/3.py时，此变量为1/2/3.py值不确定另外：import sys print(dir(sys)) 打印系统内置变量，比模块的内置变量更多内置变量 name 的作用判断当前模块是否是被作为入口文件： if __name__ == &apos;__main__&apos;: print(&apos;是入口文件，单独执行&apos;) else： print(&apos;作为模块被调用&apos;) 上述代码在a.py中，分别用a1调用和单独执行a：注意路径区别 a.py 在包test/test1下 a1.py 在包test下，代码是 import test1.a H:\\mooc\\test\\test1&gt;python a.py 或 H:\\mooc\\test&gt;python test1\\a.py 是入口文件，单独执行 name:__main__ pakage:没有上一层的包 //注意 doc: 我是开头的注释 file:h:/mooc/test/test1/a.py H:\\mooc\\test&gt;python a1.py 作为模块被调用 name:test1.a //注意 pakage:test1 doc: 我是开头的注释 file:h:\\mooc\\test\\test1\\a.py 将可执行的文件当做模块来调用： H:\\mooc\\test&gt;python -m test1.a 是入口文件，单独执行 name:__main__ pakage:test1 //注意 doc: 我是开头的注释 file:H:\\mooc\\test\\test1\\a.py 注意上述三中运行方式的区别！！！！包和模块导入时的绝对和相对路径顶级包，与入口文件的位置有关 顶级包是相对于入口文件以外的文件来说的， 对于一个项目，在入口文件中需要import 包路径.模块名 而包路径就关乎顶级包的正确确定绝对引入：import 包路径.模块名其中的包路径必须从顶级包开始相对引入：使用from .module 表示引入同级的模块 from ..module 表示引入上一层的模块 注意： import 方式不能使用 . 方式 可以使用 from .module import x 的方式 但是，入口文件不能使用 .module 相对引入 另外，如果待引用模块位于入口文件的同一层。那么无法点引入，会报错 错误：尝试引用一个超过顶级包的模块 相对引入的机制： 使用内置变量__name__来找到模块， 因此，入口文件的name是‘_main_’， 所以如果想在‘入口文件这一层引入模块’，那么只能绝对引入 如果在‘入口文件的上一层’，并使用 python -m module 仍然可以使用相对引入， 因为这种方式，文件仍然是入口文件，但是__name__不会改变，故能够使用相对引入","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"Python的控制流","slug":"2017-10-15 Python的控制流","date":"2017-10-14T22:00:00.000Z","updated":"2020-02-29T18:43:55.393Z","comments":true,"path":"article/2582.html","link":"","permalink":"https://www.cz5h.com/article/2582.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 使用分支时注意变量命名规范：用户名：user_name,按下划线而不是驼峰条件控制if else循环控制for while break continue分支控制没有switch 没有gotoPython的if控制判断元素为空：if not [] : print(&apos;该元素为空&apos;) 判断输入用户输入变量是否正确：account = &apos;admin&apos; passwd = &apos;admin&apos; print(&apos;input:&apos;) i_account = input() print(&apos;input:&apos;) i_passwd = input() if (i_account == account) and (i_passwd == passwd): print(&apos;success&apos;) else: print(&apos;error&apos;) 程序规范问题：不合法的变量定义： [pylint] C0103:Invalid constant name &quot;account&quot; python没有常量机制，但是有默认规范： 常量变量名要：全部大写！（包括串常量和输入值！） 缺失模块定义： [pylint] C0103:Invalid module name &quot;Untitled-1&quot; [pylint] C0111:Missing module docstring 每个文件第一行需要进行此代码段的模块说明，使用块注释！ 其他错误： pylint监测 另外，python代码隔离用四个空格或Tab使用snippet片段快捷的定义各种 python代码段，循环、类、函数等等if condition: pass #pass是空语句，占位语句，如果什么都不写，则会报错 else: pass 这均作为结构体，有变量作用域的问题 嵌套控制 多个if嵌套，封装：提取为函数，具体逻辑封装到函数中 单程控制 if elif else，同一级别完成多个判断（python没有开关控制switch！） 替换switch： 多个elif、使用dict字典 参见python.doc.org//程序设计的F&amp;Q对于input()：动态型语言，输入类型不可控，且输入后并不报错 接收到的值为字符串，如果需要整形：则需要int()转换if用法实例，判断输入：ACCOUNT = &apos;admin&apos; PASSWD = &apos;admin&apos; ACCOUNT1 = input() PASSWD2 = input() if (ACCOUNT1 == ACCOUNT) and (PASSWD2 == PASSWD): print(&apos;success&apos;) else: print(&apos;error&apos;) Python的循环控制while循环：直到型循环，直到目标之后才结束循环 适合于递归示例：CONDITION = True while CONDITION: CONDITION = bool(input()) print(1) else: print(2)for循环：适用于序列、集合字典的遍历！ 直接取的就是元素，会去掉外层包装示例：for x in [1,[&apos;a&apos;,&apos;b&apos;],3,(4,5)]: if not isinstance(x,int): for y in x: print(y) else: print(x) 注意：for-else,while-else循环的最后会执行，一般用不到循环的跳出break 跳出循环，终止循环,对于for-else,while-else，不会执行else continue 跳出当前循环，会执行else示例：for x in [1,2,3]: if x == 2: continue print(x) else: print(&apos;EOF&apos;)注意：均作用于当前循环，多层循环要多个break Python的for循环没有类似Java的指定次数的形式 类似for(int i=0;i&lt;10;i++): for x in range(0,10): print(x) 类似for(int i=0;i&lt;10;i+=2):步长为2 for x in range(0,10,-2): print(x,end=&apos;-\\n&apos;) 类似for(int i=10;i&gt;0;i-=2):递减 for x in range(10,0,-2): print(x,end=&apos;\\n&apos;) #print带参数 独有的特性： 用for打印间隔步长的元素： a = [1,2,3,4,5,6,7,8] for x in range(0,len(a),2): print(a[x],end=&apos; &apos;) #输出 1 3 5 7 用切片用法取间隔元素： print(a[0:len(a):2]) #输出 [1, 3, 5, 7]&apos;&apos;&apos;","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"Python的变量","slug":"2017-9-30 Python的变量","date":"2017-09-29T22:00:00.000Z","updated":"2020-02-29T18:43:55.504Z","comments":true,"path":"article/f569.html","link":"","permalink":"https://www.cz5h.com/article/f569.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 变量不说“定义”！使用变量 “ 变量名称 = 变量类型 ”a = {&apos;name&apos;:&apos;tom&apos;,&apos;sex&apos;:&apos;wm&apos;}变量命名规范： 首字母不能是数字，字母数字下划线组合，_1 = 1 是合法的 注意：保留字不能用，但函数名可以用，但不要用！ type = 1;不报错，但是type(1)时则报错！ 变量是动态赋值，num = 1; num = True 是合法的 变量接收的赋值类型，后续操作若操作引用类型，则源头修改，下游皆改对一般赋值·后来者与数据源无关 [值类型] a = 1;b = a;a = 2; print(b) = 1 a = &apos;a&apos;;b = a;a = 2; print(b) = &apos;a&apos; a = (1,2);b = a;a = 2; print(b) = (1,2) 对于传递后修改了数据源的动态类型，那么则会传导!! [引用类型] a = [1,2];b = a;a[0] = 2; print(b) = [2,2] //注意！ a = (1,2);b = a;a[0] = 2; print(b) //会报错 总结：（两类型与是否为序列无关） 值类型： 字符串str、整形int、元组tuple 引用类型： 列表list、集合set，字典dict对于字符串(字符串是值类型，不可改变)相加：对于a = &apos;1&apos;,a += &apos;2&apos; 结果为&apos;12&apos;，其并不是原串的改变 &gt;&gt;&gt; a = &apos;1&apos; id(a) = 10322752 初始赋值的内存地址 &gt;&gt;&gt; a += &apos;2&apos; id(a) = 55621088 相加后a的地址发生改变 上述结果不违背str是值类型，故&apos;string&apos;[0] = &apos;a&apos;,是错误的！tuple list的区别访问： 两者均可通过Array[id]访问元素，并可以多维访问[i][j] a = (1,2,[3,4,[5]]); a[2][2][0] = 5 赋值： 列表list： &gt;&gt;&gt; a = [1,2,3] hex(id(a)) = &apos;0x34d2f48&apos; &gt;&gt;&gt; a[0] = 2 hex(id(a)) = &apos;0x34d2f48&apos; 元组tuple： &gt;&gt;&gt; a = (1,2,3) a[0] = 2 这是错误的，因为元组是值类型 注意：对于元组中的可变类型，依然可以动态修改 &gt;&gt;&gt; a = (1,2,[3,4,[5]]) a[2][0] = 8; &gt;&gt;&gt; a = (1,2,[8,4,[5]]) 正常完成修改 追加元素： 列表list： &gt;&gt;&gt; b = [1,2] b.append(4) b = [1, 2, 4] 元组tuple： 值类型，没有append方法 总结：列表是动态的，元组定义后不可变的，这是针对元素的说法， 与包装外皮无关，比如元组中的列表依然可以修改运算符python特有的运算符 算术运算符 + &apos;a&apos; + &apos;b&apos; = &apos;ab&apos; - {1,2} - {1} = {2} * [1,2] * 2 = [1,2,1,2] / 3/2 = 1.5 3//2 = 1 % 5 % 2 = 1 取余 ** 2 ** 2 = 4 N次方 赋值运算符 *= 算数运算符后跟等号 python没有自增运算符，i++ 是错误的 比较、关系运算符 == 比较两个类型是否相等， 对于元组等复杂类型，元素类型相同时可以比较长度 返回结果是bool类型！ 逻辑运算符 and True and True = True 且 or False or False = False 或 not 单目运算符：not False = True 非 返回值不一定是bool类型的形式！ 成员运算符 in 在，一个元素是否在另外一组元素 not in 不在 返回值为bool类型！ 身份运算符 is (object) is not (object) 返回值为bool类型! 位运算符 &amp; 按位与，双目 | 按位或 ^ 按位异或 ~ 按位取反 &lt;&lt; 左移 &gt;&gt; 右移 注意：均会转化为二进制数运算 运算符结合顺序：从右往左 b = 1, b+=b&gt;=1 即 b = b + True 复杂类型的比较： &apos;a&apos; &gt; &apos;b&apos; = False 因为 ord(&apos;a&apos;) &lt; ord(&apos;b&apos;) &apos;ab&apos;&gt; &apos;aa&apos;= True 字典比较 [1,2] &lt; [2,3] = True 相同的按元素比较 (1,2) &lt; (2,3) = True 同上，注意 (1,2) &lt; (2,1) = True 对于动态类型列表和集合{}，其不能比较，返回值均为false 复杂类型逻辑运算： not 0 = True not &apos;a&apos; = False 本质只能操作bool类型，整形和字符串都能转换为bool类型 转换规则： 对于int，float，0被认为是False，其余均是True 对于str，空串&apos;&apos;被认为是False，其余均是True 对于list，空列表[]被认为是False，其余均为True 其余类似 [] or [1] = [1] 返回的是True值的形式，但不一定是bool型 &apos;&apos; and &apos;a&apos; = &apos;&apos; 返回False值的形式，&apos;&apos;即为False形式 注意： 1 and 2 = 2， 2 and 1 = 1 必须检测后者，就近返回 1 or 2 = 1， 2 or 1 = 2 此处只需看前者，就近返回 复杂成员运算： 对于非数值类型，str、tuple、list、set、dict判断元素在不在 对于除字典以外：&apos;a&apos; in (1,(1,&apos;a&apos;)) = False，只 对于dict：只能检查key值在不在，不能检查value 对于数值类型，包括int、float、bool，不能使用，否则会报错 身份运算符： 与关系运算符 == 无关！ 身份符比较的是身份！身份：暂时理解为内存地址的比较 &gt;&gt;&gt; 1 is 1 = True 值类型 &gt;&gt;&gt; {1} is {1} = False 引用型 &gt;&gt;&gt; (1,2) == (2,1) False 序列，位置固定，按位置比较 &gt;&gt;&gt; (1,2) is (2,1) False 内存不同 &gt;&gt;&gt; {1,2} == {2,1} True 无序集合，表示内容相同！不说‘相等’！ &gt;&gt;&gt; {1,2} is {2,1} False 内存不同 对于两个变量、或对象的三特征：值(==)、身份(is)、类型(isinstance) a = &apos;hello&apos;, type(a)==str是True，这种方式不推荐 注意： 上述方法不能判断a的子类是否是str 下面的方法可以判断a的子类是不是某种类型 判断对象类型： isinstance(a,str) = True isinstance(a,(int,str))) = True 可以一次判断多种类型 位运算符 按位与 2 &amp; 3 = 0b10 &amp; 0b11 = 0b10 = 2 按位或 2 | 3 = 0b10 &amp; 0b11 = 0b11 = 3 按位异或 2 ^ 3 = 0b10 &amp; 0b11 = 0b01 = 1表达式表达式 Expression = 运算符 operator + 操作数 operand 注意： 上述运算符，包括上一节中的七大运算符 运算符都有优先级顺序：1 or 2 and 3 = 1 or (2 and 3) = 1 or 3 = 1 括号优先级最高！然后是次幂运算 详细优先级可查 执行顺序原则： 不同优先级：按优先级进行先后计算 相同优先级：解释器从左向右解释（左结合）， 上述比较的是优先级，而不是运算符类型，自定义顺序推荐用（括号） 对于 = ： 其优先级高于or，但是a = 1 or 0，则会先计算or，不会按优先级顺序 上述叫做基本类型与基本概念 命令行操作 正经编程开发，需要使用文件，在cmd中用 python file.py 运行 常用IDE： pycharm 复杂项目首选，自动化步骤，功能强大 vscode 初学推荐 sublime vscode： ctrl+` 弹出终端，目录可能不对，点击文件，在终端中即可打开 零插件即可运行python代码 智能感知（完整需要安装插件python、terminal）、断点调试 语句不强制要求加分号，不需要加大括号，不建议使用分号 注意： Python是使用‘缩进’来区分代码段，不能压缩，开源友好 #单行注释 三引号：多行注释","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"Python基本的变量类型","slug":"2017-9-27 Python基本的变量类型","date":"2017-09-26T22:00:00.000Z","updated":"2020-02-29T18:43:55.503Z","comments":true,"path":"article/8da4.html","link":"","permalink":"https://www.cz5h.com/article/8da4.html","excerpt":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。","text":"注：此为慕课网Python（不打广告）视频的观看笔记，只做记录之用，并未勘误。 初识： print(&apos;hello world&apos;) print(&quot;hello world&quot;) print(&apos;hello&apos;,&apos;world&apos;) 错误方式： ERROR:print &apos;hello&apos; ERROR:print &quot;hello&quot;Python基本数据类型： 类型： Number包括整数+小数 整数： int 浮点数：float（不分精度） 布尔型：bool 复数： complex // 36j 简单运算判断数据类型 type(param) 类型合并原则： 向精度更高的方向合并，结果为高精度 示例： type(1+1.1111111) 为float类型 type(1*1.1111111) 为float类型 type(2/2) float type(2//2) int 结果为整形 2/2 = 1.0 2//2 = 1 1//2 = 0 1/2 = 0.5 注意： 符号 / 自动转换类型 符号 // 整除，向下取整 注意： Python2中有long类型，3版本后去掉 错误的方式：数字+字符串，这里不同于Java，不能直接相加进制形式： int类型的不同进制 示例： type(0xFFFFF) 均为int类型 但是，其默认转为进制计算，如下： &gt;&gt;&gt; 0xFFFFF 1048575 注意： 逗号运算符，自动拼为元组 &gt;&gt;&gt; 0x0,&apos;1&apos; (0, &apos;1&apos;) 可以混合，可以识别元组进制转换表示： 二进制 [b] 0b0101 = 十进制 5 八进制 [o] 0o10 = 十进制 8 16进制 [x] 0xF = 十进制 15 转换-利用方法 任意进制-&gt;二进制：bin() bin(10) = &apos;0b1010&apos; 任意进制-&gt;十进制：int() 任意进制-&gt;16进制：hex() 任意进制-&gt;八进制：oct()布尔类型形式： True ERROR:true.TRUE 转化： int(True) = 1 bool(1) = True 注意： 无论正负小数，只要非零，皆为True 示例： &gt;&gt;&gt; bool(&apos;&apos;) False &gt;&gt;&gt; bool(&apos;a&apos;) True &gt;&gt;&gt; bool([]) //列表 False &gt;&gt;&gt; bool([1,2,3]) True 注意： False = 0 . [] . {} . &apos;&apos; 。 None 均为假字符串:str形式： &apos;&apos;单引号 &quot;&quot;双引号 &apos;&apos;&apos;-&apos;&apos;&apos;或者&quot;&quot;&quot;-&quot;&quot;&quot;三引号 含换行的字符串输入 示例： 正确的:&quot; It&apos;s me.&quot; 正确的:&apos; It\\&apos;s me.&apos; 可以转义 正确的:&apos; He is &quot;. &apos; 双引号在内 三引号： 可以在代码中换行输入 代码： &gt;&gt;&gt; &apos;&apos;&apos;2&apos; 22&apos; 222222&apos; 2&apos;&apos;&apos;&apos;&apos; &quot;2&apos;\\n22&apos;\\n222222&apos;\\n2&quot; 显示换行： 需要使用print 代码： &gt;&gt;&gt; &quot;2&apos;\\n22&apos;\\n222222&apos;\\n2&quot; &quot;2&apos;\\n22&apos;\\n222222&apos;\\n2&quot; &gt;&gt;&gt; print(&quot;2&apos;\\n22&apos;\\n222222&apos;\\n2&quot;) 2&apos; 22&apos; 222222&apos; 2 IDLE特点： 使用\\也可以换行 代码： &gt;&gt;&gt; &apos;hello\\ world&apos; &apos;helloworld&apos; 注意： 转义字符，与语言本身有冲突 \\n \\&apos; \\t 单个斜杠：\\\\ &apos;\\\\t&apos;= &apos;\\t&apos; 应用： 文件路径不允许单个\\出现 C:\\\\windows print(&apos;C:\\\\windows&apos;) print(r&apos;C:\\windows&apos;) 字符加r之后，可以忽略转义分歧字符串操作方法形式： 运算符：只有 + * 没有 - / 取单值：[index] 取多值：[m:n] 串合并：&apos;a&apos;+&apos;b&apos; = &apos;ab&apos; 串分割：12345678910111213141516171819202122232425 示例：str[index]：直接转为字符数组，&#39;who&#39;[ 0] &#x3D; &#39;w&#39; 参数为正·是数组下标&#39;who&#39;[-1] &#x3D; &#39;o&#39; 参数为负·是从右起第几个str[ m : n ]：截取一段，只有m或n时都不会返回单值str[ 起始 : 长度 ] ： 不写起始 str[__: 长度 ]，默认为str[0 : 长度 ] 不写长度 str[起始 : __ ]，默认为str[起始 : 截取到最右 ]&#39;who&#39;[0:0] &#x3D; &#39;&#39; 注意第一个参数取得不是下标，&#39;who&#39;[0:] &#x3D; &#39;who&#39; 注意第二个参数取得不是下标，&#39;who&#39;[0:0] 不是 &#39;who&#39;[ 0] &#39;who&#39;[0:3] &#x3D; &#39;who&#39;&#39;who&#39;[0:-2] &#x3D; &#39;w&#39; 从右起拿掉2位之后剩余‘w’&#39;who&#39;[0:10] 多余位数自动丢弃，不会报错&#39;who&#39;[3:-2] &#x3D; &#39;&#39; 从左开始，数三位是o，从o开始，往左数两个，为&#39;&#39; &#39;who&#39;[-3:-2] &#x3D; &#39;w&#39; 从右开始，数三位是&#39;&#39;，从&#39;&#39;开始，即等效于&#39;who&#39;[0:-2] 从右往左数两个，为&#39;w&#39; &#39;who&#39;[-2:] &#x3D; &#39;ho&#39; 从w开始，截取到最右：ho 元组tuple多种方式： &lt;class &apos;list&apos;&gt; 列表[] 元素可以混合 &lt;class &apos;tuple&apos;&gt;元组() 元素可以混合 全空类型： () , [] 定义方式： type([1,2,3,&apos;a&apos;,False]) [1,2,3,[True,&apos;b&apos;],&apos;a&apos;] 嵌套列表 [,,,] 不能为空 访问方式： [1,2,3,4][0] = 1 取单值时，取出元素本身 [1,2,3,4][0:3] = [1,2,3] 取多值时，取出仍为列表 操作方式：同串类似 + 操作 ：[1,2]+[3,4] = [1, 2, 3, 4] * 操作 ：[1,2]*2 = [1, 2, 1, 2] [1,2]*2 = [] 上述操作结果均为一层列表包裹列表和元组的区别：对待单元素的处理不同 type((&apos;a&apos;)) -- &lt;class &apos;str&apos;&gt; 作为运算符括号处理，规定 type([&apos;a&apos;]) -- &lt;class &apos;list&apos;&gt; 定义单元素元组 type((&apos;a&apos;,)) = (&apos;a&apos;) 赋值的区别 元组(1,2)[0] = 1 是错误的，不支持赋值 列表[1,2][0] = 1 正确，结果为[2,2]总结：int，float，bool，complex 简单类型 str，list，tuple 称之为“序列” “序列”的共有操作： 取单值 序列[m] 有序号，有序 取多值(切片) 序列[m：n] 高级用法 序列[m：n：p] 共有的高级方法： 判断序列是否在本序列：in &apos;a&apos; in &apos;abc&apos;或者2 in [1,2,3]或者2 in (1,2,3) 2 in [1,2,3] = True 反之：2 not in [1,2,3] = False 得到序列长度： len(()) = 0 len(&apos;1234&apos;) = 4 得到序列最大最小元素： max(),min() 去包装的返回值 max(&apos;abcd&apos;) = &apos;d&apos; 按ASCII（ord(&apos;&apos;)）码值，空格等照样 max([(1,2),(1,2,3)]) = (1,2,3) 元组长度最大无序：集合type({1,2,3,4}) = &lt;class &apos;set&apos;&gt; 特性： 无序 序列操作方式不适用 集合，集合元素没有一个固定下标 不重复 {1,1,2,2,3,4} = {1,2,3,4} 公共 len(), in, not in 集合运算 - &amp; | 运算： 差集 {1,2,3} - {1} = {2,3} 交集 {1,2} &amp; {2,3} = {2} 并集 {1,2} | {2,3} = {1,2,3} 上述运算符中均不能出现{}空集合，另外结果会自动去掉重复 注意： 直接使用{}定义的不是空集合，其类型为字典：&lt;class &apos;dict&apos;&gt; 定义空集合：set()，即为type(set()) = &lt;class &apos;set&apos;&gt; 列表内不能含有列表或者字典元素 {1,{1,2,3}}是错的，可以含有元组，字符串和整形的混合无序：字典不是序列，类似于集合，但不同 定义方式： {&apos;name&apos;:&apos;tom&apos;,&apos;sex&apos;:&apos;wm&apos;} type类型是 &lt;class &apos;dict&apos;&gt; 取值：通过Key访问Value {&apos;name&apos;:&apos;tom&apos;,&apos;sex&apos;:&apos;wm&apos;}[&apos;name&apos;] = &apos;tom&apos; {(1,2):&apos;a&apos;,&apos;name&apos;:&apos;b&apos;,&apos;sex&apos;:&apos;wm&apos;}[(1,2)] = &apos;a&apos; {&apos;name&apos;:&apos;a&apos;,&apos;name&apos;:&apos;b&apos;,&apos;sex&apos;:&apos;wm&apos;}[&apos;name&apos;] = &apos;b&apos; 字典不能有重复键值对，如果重复，运算之前会自动去重 字典的Key必须为不可变类型 int,str,tuple list和set不行 字典的Value类型不限 {2:{2:&apos;a&apos;}}[2][2] = &apos;a&apos; 定义空字典：直接{} type({}) = &lt;class &apos;dict&apos;&gt;基本类型总结int，float，bool，complex 1 1.2 False 20j str，list，tuple &apos;a&apos; [1,2] 列表 (1,2) 元组 set，dict {1,2} 集合 {1:&quot;2&quot;}字典 不可变类型 整形 布尔型 字符串 元组 int,bool,str,tuple 可变类型 列表 集合 字典 [list],{set},{dict} 集合 无序，无索引，不能切片 字典 类似集合，不是序列","categories":[{"name":"Python学习笔记","slug":"Python学习笔记","permalink":"https://www.cz5h.com/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.cz5h.com/tags/Python/"}]},{"title":"R语言的学习路线","slug":"2017-8-27 R语言学习路线","date":"2017-08-26T22:00:00.000Z","updated":"2020-06-27T22:26:23.408Z","comments":true,"path":"article/64af.html","link":"","permalink":"https://www.cz5h.com/article/64af.html","excerpt":"初级入门 《An Introduction to R》，这是官方的入门小册子。其有中文版，由丁国徽翻译，译名为《R导论》。《R4Beginners》，这本小册子有中文版应该叫《R入门》。除此之外，还可以去读刘思喆的《153分钟学会R》。这本书收集了R初学者提问频率最高的153个问题。为什么叫153分钟呢？因为最初作者写了153个问题，阅读一个问题花费1分钟时间，全局下来也就是153分钟了。","text":"初级入门 《An Introduction to R》，这是官方的入门小册子。其有中文版，由丁国徽翻译，译名为《R导论》。《R4Beginners》，这本小册子有中文版应该叫《R入门》。除此之外，还可以去读刘思喆的《153分钟学会R》。这本书收集了R初学者提问频率最高的153个问题。为什么叫153分钟呢？因为最初作者写了153个问题，阅读一个问题花费1分钟时间，全局下来也就是153分钟了。 《R导论》《153分钟学会R》慕课网视频较少,主要有：R语言基础：http://www.imooc.com/learn/446R语言入门：http://www.imooc.com/learn/546R语言入门进阶：http://www.imooc.com/learn/828以下是从Youtube收集的资源：R语言基础视频，总时长130分钟，讲解R语言的语法、数据结构以及数据操作https://www.youtube.com/playlist?list=PLO5e_-yXpYLBWwWiFiMYbeTDBGtRrnfKJR语言基础视频，总时长170分钟，参照《R入门》进行系统讲解https://www.youtube.com/playlist?list=PLBTcf4SwWEI9_kCOJ-1o-Jwr-_Qb6bkegMachine Learning, Data Mining, Statistics with R，英文讲述https://www.youtube.com/playlist?list=PLjPbBibKHH18I0mDb_H4uP3egypHIsvMnR语言在数据科学领域中的应用 ：DATA SCIENCE WORKSHOPhttp://scientistcafe.com/R语言进阶，包括数据的可视、工具包的扩展等https://www.youtube.com/playlist?list=PLduWpQLOo6Y2rmylzNwOPAz2bLcc-6TF5关于一些常用的用法实例和统计分析实例，在W3CSchool有很详细的介绍：https://www.w3cschool.cn/r/另外的易佰教程：http://www.yiibai.com/r/ 高级入门 读了上述书籍之后，你就可以去高级入门阶段了。这时候要读的书有两本很经典的。《Statistics with R》和《The R book》。之所以说这两本书高级，是因为这两本书已经不再限于R基础了，而是结合了数据分析的各种常见方法来写就的，比较系统的介绍了R在线性回归、方差分析、多元统计、R绘图、时间序列分析、数据挖掘等各方面的内容，看完之后你会发现，哇，原来R能做的事情这么多，而且做起来是那么简洁。读到这里已经差不多了，剩下的估计就是你要专门攻读的某个方面内容了。下面大致说一说。 绘图与可视化 亚里斯多德说，“较其他感觉而言，人类更喜欢观看”。因此，绘图和可视化得到很多人的关注和重视。那么，如何学习R画图和数据可视化呢？再简单些，如何画直方图？如何往直方图上添加密度曲线呢？我想读完下面这几本书你就大致会明白了。首先，画图入门可以读《R Graphics》，个人认为这本是比较经典的，全面介绍了R中绘图系统。该书对应的有一个网站，google之就可以了。更深入的可以读《Lattice：Multivariate Data Visualization with R》。上面这些都是比较普通的。当然，有比较文艺和优雅的——ggplot2系统，看《ggplot2：Elegant Graphics for Data Analysis》。还有数据挖掘方面的书：《Data Mining with Rattle and R》，主要是用Rattle软件，个人比较喜欢Rattle!当然，Rattle不是最好的，Rweka也很棒！再有就是交互图形的书了，著名的交互系统是ggobi，这个我已经喜欢两年多了，关于ggobi的书有《Interactive and Dynamic Graphics for Data Analysis With R and GGobi》，不过，也只是适宜入门，更多更全面的还是去ggobi的主页吧，上面有各种资料以及包的更新信息！特别推荐一下，中文版绘图书籍有谢益辉的《现代统计图形》。 计量经济学 关于计量经济学，首先推荐一本很薄的小册子:《Econometrics In R》，做入门用。然后，是《Applied Econometrics with R》，该书对应的R包是AER，可以安装之后配合使用，效果甚佳。计量经济学中很大一部分是关于时间序列分析的，这一块内容在下面的地方说。 时间序列分析 时间序列书籍的书籍分两类，一种是比较普适的书籍，典型的代表是：《Time Series Analysis and Its Applications ：with R examples》。该书介绍了各种时间序列分析的经典方法及实现各种经典方法的R代码，该书有中文版。如果不想买的话，建议去作者主页直接下载，英文版其实读起来很简单。时间序列分析中有一大块儿是关于金融时间序列分析的。这方面比较流行的书有两本《Analysis of financial time series》，这本书的最初是用的S-plus代码，不过新版已经以R代码为主了。这本书适合有时间序列分析基础和金融基础的人来看，因为书中关于时间序列分析的理论以及各种金融知识讲解的不是特别清楚，将极值理论计算VaR的部分就比较难看懂。另外一个比较有意思的是Rmetrics推出的《TimeSeriesFAQ》，这本书是金融时间序列入门的东西，讲的很基础，但是很难懂。对应的中文版有《金融时间序列分析常见问题集》，当然，目前还没有发出来。经济领域的时间序列有一种特殊的情况叫协整，很多人很关注这方面的理论，关心这个的可以看《Analysis of Integrated and Cointegrated Time Series with R》。最后，比较高级的一本书是关于小波分析的，看《Wavelet Methods in Statistics with R》。附加一点，关于时间序列聚类的书籍目前比较少见，是一个处女地，有志之士可以开垦之！ 金融 金融的领域很广泛，如果是大金融的话，保险也要被纳入此间。用R做金融更多地需要掌握的是金融知识，只会数据分析技术意义寥寥。我觉得这些书对于懂金融、不同数据分析技术的人比较有用，只懂数据分析技术而不动金融知识的人看起来肯定如雾里看花，甚至有人会觉得金融分析比较低级。这方面比较经典的书籍有：《Advanced Topics in Analysis of Economic and Financial Data Using R》以及《Modelling Financial Time Series With S-plus》。金融产品定价之类的常常要用到随机微分方程，有一本叫《Simulation Inference Stochastic Differential Equations：with R examples》的书是关于这方面的内容的，有实例，内容还算详实!此外，是风险度量与管理类。比较经典的有《Simulation Techniques in Financial Risk Management》、《Modern Actuarial Risk Theory Using R》和《Quantitative Risk Management：Concepts, Techniques and Tools》。投资组合分析类和期权定价类可以分别看《Portfolio Optimization with R》和《Option Pricing and Estimation of Financial Models with R》。 数据挖掘 这方面的书，最经典的是《Data Mining with R》","categories":[{"name":"ML/R学习笔记","slug":"ML-R学习笔记","permalink":"https://www.cz5h.com/categories/ML-R%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"https://www.cz5h.com/tags/R%E8%AF%AD%E8%A8%80/"}]},{"title":"Web数据接口开发的阶段总结","slug":"2017-7-10 Web数据接口开发的阶段总结","date":"2017-07-09T22:00:00.000Z","updated":"2020-02-29T18:43:55.499Z","comments":true,"path":"article/d44.html","link":"","permalink":"https://www.cz5h.com/article/d44.html","excerpt":"背景及要求需要将数据提供方（对方）的数据同步到本地（我方）目的是在本地维护一个与数据提供方一致的本地数据库（ORACLE）数据提供方提出的方案就是我方开发一个Web接口供其调用数据方给出其发送数据的格式等信息，即给出了接口规范","text":"背景及要求需要将数据提供方（对方）的数据同步到本地（我方）目的是在本地维护一个与数据提供方一致的本地数据库（ORACLE）数据提供方提出的方案就是我方开发一个Web接口供其调用数据方给出其发送数据的格式等信息，即给出了接口规范 交互情景对于实时数据，数据方在收集后每隔十分钟调用一次接口推送过来这些数据我方接口对其推送数据进行解析并入库对于先前历史数据，直接以文件方式提供给我方我方直接用本地程序解析后入库，不通过接口 推送数据的具体规范原始数据采用的是XML格式的文本，先后经过base64编码和DES加密，之后对方推送至我方原始数据是格式化且规范的，XML共分四级，第三级标签开始代表每一次操作的表数据原始数据包含CLOB和BLOB类型的数据 - 插入Oracle时要额外处理原始数据包含的字段每次都是不确定的一次推送包含若干原始数据段，即本地入库时对表的操作也是不确定的推送的数据量可能很大，一次推送纯文本大约20MB - 构造SQL语句时要额外处理，否则SQL语句会过长（超过4000）数据对应表有5张，每个表平均100个字段左右 对于历史数据数据就是一堆编码并加密了的字符串 - 对其处理与处理推送数据的逻辑差不多此字符串非常长，文本大小约500MB - 在解密解码时只要加大内存限制就可以完成，但解析XML时需要SAXReader方式，dom方式的话要爆炸了原始数据包含CLOB和BLOB类型的数据 - 插入Oracle时要额外处理原始数据包含的字段每次都是不确定的一次推送包含若干原始数据段，即本地入库时对表的操作也是不确定的 我方开发接口的情况接口参数包括验证信息，和数据，就这两个参数接口采用JAX-WS实现，原因是其实现比较简单且轻便，可以参照：真正的轻量级WebService框架 - 使用JAX-WS(JWS)发布WebService接口任务一，DES解密并base64解码推送来的数据— 解密方式一定要和对方的一致，这里直接用对方提供的加解密代码，并且对方告知了DES秘钥— 解码就比较随意，base64解码都大同小异接口任务二，用dom4j包来解析XML树，这里采用SAXReader的方式，原因上面说了接口任务三，映射表名字段名，原始推送数据字段均为汉字，Oracle库中存的都是首字母大写接口任务四，构造SQL语句，具体思想就是解析XML到第三级标签，这一级会包含所有&lt;插入字段名&gt;和&lt;插入字段值&gt; 整个流程大致如下 下面是上述叙述中遇到的问题： 如何按原始数据中的汉语字段建立数据表 如何监控接口的情况也是问题，生成日志是解决之道 乱码问题！!服务器？编译时？原数据？UTF-8？GBK？到底是谁的锅 如何将原始字段快速映射成我即将将其插入到表中的对应字段 Oracle字段的符号要求，废了很多时间 大量字段中含少量CLOB字段时，对CLOB类型的数据使用jdbc插入数据库 大量字段中含少量BLOB字段时，对BLOB类型的数据使用jdbc插入数据库，这个费了一番功夫 整明白插入时，涉及大量字段操作时，对数据的增量更新也是问题 SQL语句如何执行，批量还是单独，这是个问题 程序循环过多导致Oracle连接游标数超出限制的问题 以上问题先放着，抽空再逐个分析，如果上述包括了您急需解决的问题，请@我我会尽快回复我的解决办法，或许对您会有帮助。 对于接口的传输性能 开始时错误的以为历史数据也要通过接口方式调用，就对JWS发布的这个接口进行了一下测试，发现接口的极限是200M左右 也就是说调用接口时，传入参数的字符串大小可以为200M没问题 上述都是废话，正常是不允许出现上述情况的","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://www.cz5h.com/tags/Java/"},{"name":"WebService","slug":"WebService","permalink":"https://www.cz5h.com/tags/WebService/"}]},{"title":"Kettle对接Hadoop","slug":"2017-5-12 Kettle对接Hadoop","date":"2017-05-11T22:00:00.000Z","updated":"2020-02-29T18:43:55.498Z","comments":true,"path":"article/53aa.html","link":"","permalink":"https://www.cz5h.com/article/53aa.html","excerpt":"","text":"","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://www.cz5h.com/tags/Hadoop/"},{"name":"Kettle","slug":"Kettle","permalink":"https://www.cz5h.com/tags/Kettle/"},{"name":"整合","slug":"整合","permalink":"https://www.cz5h.com/tags/%E6%95%B4%E5%90%88/"}]},{"title":"Web接口开发的相关事项","slug":"2017-4-29 Web接口开发的相关事项","date":"2017-04-28T22:00:00.000Z","updated":"2020-02-29T18:43:55.518Z","comments":true,"path":"article/7b5b.html","link":"","permalink":"https://www.cz5h.com/article/7b5b.html","excerpt":"本次任务的具体背景及要求： 需要将数据提供方（对方）的数据同步到本地（我方） 目的是在本地维护一个与数据提供方一致的本地数据库（ORACLE） 数据提供方提出的方案就是我方开发一个Web接口供其调用 数据方给出其发送数据的格式等信息，即给出了接口规范","text":"本次任务的具体背景及要求： 需要将数据提供方（对方）的数据同步到本地（我方） 目的是在本地维护一个与数据提供方一致的本地数据库（ORACLE） 数据提供方提出的方案就是我方开发一个Web接口供其调用 数据方给出其发送数据的格式等信息，即给出了接口规范 本次任务的交互情景： 对于实时数据，数据方在收集后每隔十分钟调用一次接口推送过来这些数据 我方接口对其推送数据进行解析并入库 对于先前历史数据，直接以文件方式提供给我方 我方直接用本地程序解析后入库，不通过接口 推送数据的具体规范： 原始数据采用的是XML格式的文本，先后经过base64编码和DES加密，之后对方推送至我方 原始数据是格式化且规范的，XML共分四级，第三级标签开始代表每一次操作的表数据 原始数据包含CLOB和BLOB类型的数据 - 插入Oracle时要额外处理 原始数据包含的字段每次都是不确定的 一次推送包含若干原始数据段，即本地入库时对表的操作也是不确定的 推送的数据量可能很大，一次推送纯文本大约20MB - 构造SQL语句时要额外处理，否则SQL语句会过长（超过4000） 数据对应表有5张，每个表平均100个字段左右 对于历史数据： 数据就是一堆编码并加密了的字符串 - 对其处理与处理推送数据的逻辑差不多 此字符串非常长，文本大小约500MB - 在解密解码时只要加大内存限制就可以完成，但解析XML时需要SAXReader方式，dom方式的话要爆炸了 原始数据包含CLOB和BLOB类型的数据 - 插入Oracle时要额外处理 原始数据包含的字段每次都是不确定的 一次推送包含若干原始数据段，即本地入库时对表的操作也是不确定的 知道上述信息后我方开发接口的情况： 接口参数包括验证信息，和数据，就这两个参数 接口采用JAX-WS实现，原因是其实现比较简单且轻便，可以参照：真正的轻量级WebService框架 - 使用JAX-WS(JWS)发布WebService 接口任务一，DES解密并base64解码推送来的数据 – 解密方式一定要和对方的一致，这里直接用对方提供的加解密代码，并且对方告知了DES秘钥 – 解码就比较随意，base64解码都大同小异 接口任务二，用dom4j包来解析XML树，这里采用SAXReader的方式，原因上面说了 接口任务三，映射表名字段名，原始推送数据字段均为汉字，Oracle库中存的都是首字母大写 接口任务四，构造SQL语句，具体思想就是解析XML到第三级标签，这一级会包含所有&lt;插入字段名&gt;和&lt;插入字段值&gt; 整个流程大致如下：左边：数据推送方，不必关心其逻辑实现右边：本地逻辑接口实现 下面是上述叙述中遇到的问题： 如何按原始数据中的汉语字段建立数据表 如何监控接口的情况也是问题，生成日志是解决之道 乱码问题！!服务器？编译时？原数据？UTF-8？GBK？到底是谁的锅 如何将原始字段快速映射成我即将将其插入到表中的对应字段 Oracle字段的符号要求，浪费了很多时间 大量字段中含少量CLOB字段时，对CLOB类型的数据使用jdbc插入数据库 大量字段中含少量BLOB字段时，对BLOB类型的数据使用jdbc插入数据库，这个费了一番功夫 整明白插入时，涉及大量字段操作时，对数据的增量更新也是问题 SQL语句如何执行，批量还是单独，这是个问题 程序循环过多导致Oracle连接游标数超出限制的问题 以上问题先放着，抽空再逐个分析，如果上述包括了您急需解决的问题，请@我我会尽快回复我的解决办法，或许对您会有帮助。 对于接口的传输性能： 开始时错误的以为历史数据也要通过接口方式调用，就对JWS发布的这个接口进行了一下测试，发现接口的极限是200M左右 也就是说调用接口时，传入参数的字符串大小可以为200M没问题 上述都是废话，正常是不允许接口传输大量数据的。","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"WebService","slug":"WebService","permalink":"https://www.cz5h.com/tags/WebService/"}]},{"title":"Hadoop常用命令","slug":"2017-4-25 Hadoop常用命令","date":"2017-04-24T22:00:00.000Z","updated":"2020-02-29T18:43:55.494Z","comments":true,"path":"article/547d.html","link":"","permalink":"https://www.cz5h.com/article/547d.html","excerpt":"123456启动Hadoop 进入HADOOP_HOME目录。 执行sh bin&#x2F;start-all.sh关闭Hadoop 进入HADOOP_HOME目录。 执行sh bin&#x2F;stop-all.sh","text":"123456启动Hadoop 进入HADOOP_HOME目录。 执行sh bin&#x2F;start-all.sh关闭Hadoop 进入HADOOP_HOME目录。 执行sh bin&#x2F;stop-all.sh 12345678910111213141516171819202122232425262728293031323334353637383940414243441、查看指定目录下内容hadoop dfs –ls [文件目录]eg: [hadoop@hadoop-1 test]$ hadoop fs -ls &#x2F;gsw&#x2F;rs2、打开某个已存在文件hadoop dfs –cat [file_path]eg:[hadoop@hadoop-1 test]$ hadoop dfs -cat &#x2F;gsw&#x2F;rs1&#x2F;part-r-000003、将本地文件存储至hadoophadoop fs –put [本地地址] [hadoop目录]hadoop fs –put &#x2F;home&#x2F;t&#x2F;file.txt &#x2F;user&#x2F;t (file.txt是文件名)4、将本地文件夹存储至hadoophadoop fs –put [本地目录] [hadoop目录] hadoop fs -put &#x2F;data&#x2F;math &#x2F;gsw&#x2F;demo (dir_name是文件夹名)5、将hadoop上某个文件down至本地已有目录下hadoop fs -get [文件目录] [本地目录]hadoop fs –get &#x2F;user&#x2F;t&#x2F;ok.txt &#x2F;home&#x2F;t6、删除hadoop上指定文件hadoop fs –rm [文件地址]hadoop fs –rm &#x2F;user&#x2F;t&#x2F;ok.txt7、删除hadoop上指定文件夹（包含子目录等）hadoop fs –rm [目录地址]hadoop fs –rm &#x2F;gsw&#x2F;demo&#x2F;english8、在hadoop指定目录内创建新目录hadoop fs –mkdir &#x2F;user&#x2F;t9、在hadoop指定目录下新建一个空文件使用touchz命令：hadoop fs -touchz &#x2F;user&#x2F;new.txt10、将hadoop上某个文件重命名使用mv命令：hadoop fs –mv &#x2F;user&#x2F;test.txt &#x2F;user&#x2F;ok.txt （将test.txt重命名为ok.txt）11、将hadoop指定目录下所有内容保存为一个文件，同时down至本地hadoop dfs –getmerge &#x2F;user &#x2F;home&#x2F;t12、将正在运行的hadoop作业kill掉hadoop job –kill [job-id] 12345678910111213141516171819202122231、列出所有Hadoop Shell支持的命令 $ bin&#x2F;hadoop fs -help2、显示关于某个命令的详细信息 $ bin&#x2F;hadoop fs -help command-name3、用户可使用以下命令在指定路径下查看历史日志汇总 $ bin&#x2F;hadoop job -history output-dir这条命令会显示作业的细节信息，失败和终止的任务细节。4、关于作业的更多细节，比如成功的任务，以及对每个任务的所做的尝试次数等可以用下面的命令查看 $ bin&#x2F;hadoop job -history all output-dir5、 格式化一个新的分布式文件系统： $ bin&#x2F;hadoop namenode -format6、在分配的NameNode上，运行下面的命令启动HDFS： $ bin&#x2F;start-dfs.sh bin&#x2F;start-dfs.sh脚本会参照NameNode上$&#123;HADOOP_CONF_DIR&#125;&#x2F;slaves文件的内容，在所有列出的slave上启动DataNode守护进程。7、在分配的JobTracker上，运行下面的命令启动Map&#x2F;Reduce： $ bin&#x2F;start-mapred.sh bin&#x2F;start-mapred.sh脚本会参照JobTracker上$&#123;HADOOP_CONF_DIR&#125;&#x2F;slaves文件的内容，在所有列出的slave上启动TaskTracker守护进程。8、在分配的NameNode上，执行下面的命令停止HDFS： $ bin&#x2F;stop-dfs.sh bin&#x2F;stop-dfs.sh脚本会参照NameNode上$&#123;HADOOP_CONF_DIR&#125;&#x2F;slaves文件的内容，在所有列出的slave上停止DataNode守护进程。9、在分配的JobTracker上，运行下面的命令停止Map&#x2F;Reduce： $ bin&#x2F;stop-mapred.sh bin&#x2F;stop-mapred.sh脚本会参照JobTracker上$&#123;HADOOP_CONF_DIR&#125;&#x2F;slaves文件的内容，在所有列出的slave上停止TaskTracker守护进程。 DFSShell 12345610、创建一个名为 &#x2F;foodir 的目录 $ bin&#x2F;hadoop dfs -mkdir &#x2F;foodir11、创建一个名为 &#x2F;foodir 的目录 $ bin&#x2F;hadoop dfs -mkdir &#x2F;foodir12、查看名为 &#x2F;foodir&#x2F;myfile.txt 的文件内容 $ bin&#x2F;hadoop dfs -cat &#x2F;foodir&#x2F;myfile.txt DFSAdmin 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216313、将集群置于安全模式 $ bin&#x2F;hadoop dfsadmin -safemode enter14、显示Datanode列表 $ bin&#x2F;hadoop dfsadmin -report15、使Datanode节点 datanodename退役 $ bin&#x2F;hadoop dfsadmin -decommission datanodename16、bin&#x2F;hadoop dfsadmin -help 命令能列出所有当前支持的命令。比如： * -report：报告HDFS的基本统计信息。有些信息也可以在NameNode Web服务首页看到。 * -safemode：虽然通常并不需要，但是管理员的确可以手动让NameNode进入或离开安全模式。 * -finalizeUpgrade：删除上一次升级时制作的集群备份。17、显式地将HDFS置于安全模式 $ bin&#x2F;hadoop dfsadmin -safemode18、在升级之前，管理员需要用（升级终结操作）命令删除存在的备份文件 $ bin&#x2F;hadoop dfsadmin -finalizeUpgrade19、能够知道是否需要对一个集群执行升级终结操作。 $ dfsadmin -upgradeProgress status20、使用-upgrade选项运行新的版本 $ bin&#x2F;start-dfs.sh -upgrade21、如果需要退回到老版本,就必须停止集群并且部署老版本的Hadoop，用回滚选项启动集群 $ bin&#x2F;start-dfs.h -rollback22、下面的新命令或新选项是用于支持配额的。 前两个是管理员命令。 * dfsadmin -setquota &lt;N&gt; &lt;directory&gt;...&lt;directory&gt; 把每个目录配额设为N。这个命令会在每个目录上尝试， 如果N不是一个正的长整型数，目录不存在或是文件名， 或者目录超过配额，则会产生错误报告。 * dfsadmin -clrquota &lt;directory&gt;...&lt;director&gt; 为每个目录删除配额。这个命令会在每个目录上尝试，如果目录不存在或者是文件，则会产生错误报告。如果目录原来没有设置配额不会报错。 * fs -count -q &lt;directory&gt;...&lt;directory&gt; 使用-q选项，会报告每个目录设置的配额，以及剩余配额。 如果目录没有设置配额，会报告none和inf。23、创建一个hadoop档案文件 $ hadoop archive -archiveName NAME &lt;src&gt;* &lt;dest&gt; -archiveName NAME 要创建的档案的名字。 src 文件系统的路径名，和通常含正则表达的一样。 dest 保存档案文件的目标目录。24、递归地拷贝文件或目录 $ hadoop distcp &lt;srcurl&gt; &lt;desturl&gt; srcurl 源Url desturl 目标Url25、运行HDFS文件系统检查工具(fsck tools)用法：hadoop fsck [GENERIC_OPTIONS] &lt;path&gt; [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks]]] 命令选项 描述 &lt;path&gt; 检查的起始目录。 -move 移动受损文件到&#x2F;lost+found -delete 删除受损文件。 -openforwrite 打印出写打开的文件。 -files 打印出正被检查的文件。 -blocks 打印出块信息报告。 -locations 打印出每个块的位置信息。 -racks 打印出data-node的网络拓扑结构。26、用于和Map Reduce作业交互和命令(jar)用法：hadoop job [GENERIC_OPTIONS] [-submit &lt;job-file&gt;] | [-status &lt;job-id&gt;] | [-counter &lt;job-id&gt; &lt;group-name&gt; &lt;counter-name&gt;] | [-kill &lt;job-id&gt;] | [-events &lt;job-id&gt; &lt;from-event-#&gt; &lt;#-of-events&gt;] | [-history [all] &lt;jobOutputDir&gt;] | [-list [all]] | [-kill-task &lt;task-id&gt;] | [-fail-task &lt;task-id&gt;] 命令选项 描述 -submit &lt;job-file&gt; 提交作业 -status &lt;job-id&gt; 打印map和reduce完成百分比和所有计数器。 -counter &lt;job-id&gt; &lt;group-name&gt; &lt;counter-name&gt; 打印计数器的值。 -kill &lt;job-id&gt; 杀死指定作业。 -events &lt;job-id&gt; &lt;from-event-#&gt; &lt;#-of-events&gt; 打印给定范围内jobtracker接收到的事件细节。 -history [all] &lt;jobOutputDir&gt; -history &lt;jobOutputDir&gt; 打印作业的细节、失败及被杀死原因的细节。更多的关于一个作业的细节比如成功的任务，做过的任务尝试等信息可以通过指定[all]选项查看。 -list [all] -list all 显示所有作业。-list只显示将要完成的作业。 -kill-task &lt;task-id&gt; 杀死任务。被杀死的任务不会不利于失败尝试。 -fail-task &lt;task-id&gt; 使任务失败。被失败的任务会对失败尝试不利。27、运行pipes作业用法：hadoop pipes [-conf &lt;path&gt;] [-jobconf &lt;key&#x3D;value&gt;, &lt;key&#x3D;value&gt;, ...] [-input &lt;path&gt;] [-output &lt;path&gt;] [-jar &lt;jar file&gt;] [-inputformat &lt;class&gt;] [-map &lt;class&gt;] [-partitioner &lt;class&gt;] [-reduce &lt;class&gt;] [-writer &lt;class&gt;] [-program &lt;executable&gt;] [-reduces &lt;num&gt;] 命令选项 描述 -conf &lt;path&gt; 作业的配置 -jobconf &lt;key&#x3D;value&gt;, &lt;key&#x3D;value&gt;, ... 增加&#x2F;覆盖作业的配置项 -input &lt;path&gt; 输入目录 -output &lt;path&gt; 输出目录 -jar &lt;jar file&gt; Jar文件名 -inputformat &lt;class&gt; InputFormat类 -map &lt;class&gt; Java Map类 -partitioner &lt;class&gt; Java Partitioner -reduce &lt;class&gt; Java Reduce类 -writer &lt;class&gt; Java RecordWriter -program &lt;executable&gt; 可执行程序的URI -reduces &lt;num&gt; reduce个数28、打印版本信息。用法：hadoop version29、hadoop脚本可用于调调用任何类。用法：hadoop CLASSNAME 运行名字为CLASSNAME的类。30、运行集群平衡工具。管理员可以简单的按Ctrl-C来停止平衡过程(balancer)用法：hadoop balancer [-threshold &lt;threshold&gt;] 命令选项 描述 -threshold &lt;threshold&gt; 磁盘容量的百分比。这会覆盖缺省的阀值。 31、获取或设置每个守护进程的日志级别(daemonlog)。用法：hadoop daemonlog -getlevel &lt;host:port&gt; &lt;name&gt;用法：hadoop daemonlog -setlevel &lt;host:port&gt; &lt;name&gt; &lt;level&gt; 命令选项 描述 -getlevel &lt;host:port&gt; &lt;name&gt; 打印运行在&lt;host:port&gt;的守护进程的日志级别。 这个命令内部会连接http:&#x2F;&#x2F;&lt;host:port&gt;&#x2F;logLevel?log&#x3D;&lt;name&gt;-setlevel &lt;host:port&gt; &lt;name&gt; &lt;level&gt; 设置运行在&lt;host:port&gt;的守护进程的日志级别。这个命令内部会连接http:&#x2F;&#x2F;&lt;host:port&gt;&#x2F;logLevel?log&#x3D;&lt;name&gt; 32、运行一个HDFS的datanode。用法：hadoop datanode [-rollback] 命令选项 描述 -rollback 将datanode回滚到前一个版本。这需要在停止datanode，分发老的hadoop版本之后使用。 33、运行一个HDFS的dfsadmin客户端。用法：hadoop dfsadmin [GENERIC_OPTIONS] [-report] [-safemode enter | leave | get | wait] [-refreshNodes] [-finalizeUpgrade] [-upgradeProgress status | details | force] [-metasave filename] [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;] [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;] [-help [cmd]]命令选项 描述-report 报告文件系统的基本信息和统计信息。-safemode enter | leave | get | wait 安全模式维护命令。安全模式是Namenode的一个状态，这种状态下，Namenode 1. 不接受对名字空间的更改(只读) 2. 不复制或删除块 Namenode会在启动时自动进入安全模式，当配置的块最小百分比数满足最小的副本数条件时，会自动离开 安全模式。安全模式可以手动进入，但是这样的话也必须手动关闭安全模式。-refreshNodes 重新读取hosts和exclude文件，更新允许连到Namenode的或那些需要退出或入编的Datanode的集合。-finalizeUpgrade 终结HDFS的升级操作。Datanode删除前一个版本的工作目录，之后Namenode也这样做。这个操作完结整个升级过程。-upgradeProgress status | details | force 请求当前系统的升级状态，状态的细节，或者强制升级操作进行。-metasave filename 保存Namenode的主要数据结构到hadoop.log.dir属性指定的目录下的&lt;filename&gt;文件。对于下面的每一项， &lt;filename&gt;中都会一行内容与之对应 1. Namenode收到的Datanode的心跳信号 2. 等待被复制的块 3. 正在被复制的块 4. 等待被删除的块-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt; 为每个目录&lt;dirname&gt;设定配额&lt;quota&gt;。目录配额是一个长整型整数，强制限定了目录树下的名字个数。 命令会在这个目录上工作良好，以下情况会报错： 1. N不是一个正整数，或者 2. 用户不是管理员，或者 3. 这个目录不存在或是文件，或者 4. 目录会马上超出新设定的配额。-clrQuota &lt;dirname&gt;...&lt;dirname&gt; 为每一个目录&lt;dirname&gt;清除配额设定。 命令会在这个目录上工作良好，以下情况会报错： 1. 这个目录不存在或是文件，或者 2. 用户不是管理员。 如果目录原来没有配额不会报错。-help [cmd] 显示给定命令的帮助信息，如果没有给定命令，则显示所有命令的帮助信息。34、运行MapReduce job Tracker节点(jobtracker)。用法：hadoop jobtracker35、运行namenode。有关升级，回滚，升级终结的更多信息请参考升级和回滚。用法：hadoop namenode [-format] | [-upgrade] | [-rollback] | [-finalize] | [-importCheckpoint] 命令选项 描述 -format 格式化namenode。它启动namenode，格式化namenode，之后关闭namenode。 -upgrade 分发新版本的hadoop后，namenode应以upgrade选项启动。 -rollback 将namenode回滚到前一版本。这个选项要在停止集群，分发老的hadoop版本后使用。 -finalize finalize会删除文件系统的前一状态。最近的升级会被持久化，rollback选项将再不可用，升级终结操作之后，它会停掉namenode。 -importCheckpoint 从检查点目录装载镜像并保存到当前检查点目录，检查点目录由fs.checkpoint.dir指定。36、运行HDFS的secondary namenode。用法：hadoop secondarynamenode [-checkpoint [force]] | [-geteditsize] 命令选项 描述 -checkpoint [force] 如果EditLog的大小 &gt;&#x3D;fs.checkpoint.size，启动Secondarynamenode的检查点过程。如果使用了-force， 将不考虑EditLog的大小。 -geteditsize 打印EditLog大小。 37、运行MapReduce的task Tracker节点。用法：hadoop tasktracker","categories":[{"name":"Hadoop相关","slug":"Hadoop相关","permalink":"https://www.cz5h.com/categories/Hadoop%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://www.cz5h.com/tags/Hadoop/"},{"name":"转载","slug":"转载","permalink":"https://www.cz5h.com/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"常用命令","slug":"常用命令","permalink":"https://www.cz5h.com/tags/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"}]},{"title":"WebService初步使用","slug":"2017-4-21 WebService初步使用","date":"2017-04-20T22:00:00.000Z","updated":"2020-02-29T18:43:55.493Z","comments":true,"path":"article/d296.html","link":"","permalink":"https://www.cz5h.com/article/d296.html","excerpt":"#### 使用中出现的错误 12345678faultDetail: &#123;http:&#x2F;&#x2F;xml.apache.org&#x2F;axis&#x2F;&#125;stackTrace:服务器无法处理请求。 ---&gt; 值不能为空。 call.setOperationName(new QName(&quot;http:&#x2F;&#x2F;WebXml.com.cn&#x2F;&quot;,&quot;getSupportCity&quot;)); &#x2F;&#x2F;一定要加·QName call.addParameter( new QName(&quot;http:&#x2F;&#x2F;WebXml.com.cn&#x2F;&quot;,&quot;byProvinceName&quot;) , org.apache.axis.encoding.XMLType.XSD_STRING, javax.xml.rpc.ParameterMode.IN ); &#x2F;&#x2F;一定要加·QName","text":"#### 使用中出现的错误 12345678faultDetail: &#123;http:&#x2F;&#x2F;xml.apache.org&#x2F;axis&#x2F;&#125;stackTrace:服务器无法处理请求。 ---&gt; 值不能为空。 call.setOperationName(new QName(&quot;http:&#x2F;&#x2F;WebXml.com.cn&#x2F;&quot;,&quot;getSupportCity&quot;)); &#x2F;&#x2F;一定要加·QName call.addParameter( new QName(&quot;http:&#x2F;&#x2F;WebXml.com.cn&#x2F;&quot;,&quot;byProvinceName&quot;) , org.apache.axis.encoding.XMLType.XSD_STRING, javax.xml.rpc.ParameterMode.IN ); &#x2F;&#x2F;一定要加·QName 123faultString: 服务器未能识别 HTTP 头 SOAPAction 的值: 。 call.setUseSOAPAction(true); call.setSOAPActionURI(&quot;http:&#x2F;&#x2F;WebXml.com.cn&#x2F;getRegionCountry&quot;); 12faultString: (405)Method Not Allowed faultDetail: &#123;&#125;:return code: 405 call.setTargetEndpointAddress(&quot;http:&#x2F;&#x2F;ws.webxml.com.cn&#x2F;WebServices&#x2F;WeatherWebService.asmx&quot;); &#x2F;&#x2F;路径要写对 1234567891011关于invoke传参res&#x3D;(String[]) call.invoke(new Object[]&#123;&quot;4544&quot;&#125;); faultString: 404 faultString: 400 bad request faultString: SimpleDeserializer encountered a child element, which is NOT expected, in something it was trying to deserialize. faultString: (403)Forbidden您未被授权查看该页您试图访问的 Web 服务器上有一个不被允许访问该网站的 IP 地址列表，并且您用来浏览的计算机的 IP 地址也在其中。HTTP 错误 403.6 - 禁止访问：客户端的 IP 地址被拒绝。打开浏览器：直接输网址，同样打不开，被禁了- No returnType was specified to the Call object! You must call setReturnType() if you have called addParameter().call.setReturnType(org.apache.axis.encoding.XMLType.XSD_UNSIGNEDBYTE); 1234- Exception:org.xml.sax.SAXException: SimpleDeserializer encountered a child element, which is NOT expected, in something it was trying to deserialize.（应该与下方错误 原因一样） faultString: org.xml.sax.SAXException: Found character data inside an array element while deserializing 12345678byte a &#x3D; (Byte) call.invoke(new Object[]&#123;&quot;山东&quot;&#125;); &#x2F;&#x2F;要强制转换 ~call.setReturnType(org.apache.axis.encoding.XMLType.XSD_UNSIGNEDBYTE); &#x2F;&#x2F;上一句非常重要：规定了返回的值的类型（UsignedByte），直接打印即可 if(call.invoke(new Object[]&#123;&quot;dqw@qq.com&quot;&#125;).toString().equals(&quot;1&quot;)) System.out.println(&quot;合法&quot;); else System.out.println(&quot;非法&quot;); &#x2F;&#x2F;通过返回值来解析出实际结果 删掉bin中的class之后，出现cant find the main class，解决：执行clean project-&gt;clean 书写规范12345678910111213Service service &#x3D; new Service(); Call call &#x3D; (Call) service.createCall(); call.setTargetEndpointAddress(&quot;http:&#x2F;&#x2F;ws.webxml.com.cn&#x2F;WebServices&#x2F;WeatherWebService.asmx&quot;); &#x2F;&#x2F;?wsdlcall.setOperationName(new QName(&quot;http:&#x2F;&#x2F;WebXml.com.cn&#x2F;&quot;,&quot;getWeatherbyCityName&quot;)); call.addParameter(new QName(&quot;http:&#x2F;&#x2F;WebXml.com.cn&#x2F;&quot;,&quot;theCityName&quot;) ,org.apache.axis.encoding.XMLType.XSD_STRING, javax.xml.rpc.ParameterMode.IN); call.setUseSOAPAction(true); call.setSOAPActionURI(&quot;http:&#x2F;&#x2F;WebXml.com.cn&#x2F;getWeatherbyCityName&quot;); call.setReturnType(org.apache.axis.encoding.XMLType.XSD_UNSIGNEDBYTE); call.setReturnClass(java.lang.String[].class); String[] returnConetext &#x3D; (String[])call.invoke(new Object[]&#123;&quot;53772&quot;&#125;); for (int i &#x3D; 0; i &lt; returnConetext.length; i++) &#123; System.out.println(returnConetext[i]); &#125; 定义Service—————-Service service = new Service();定义Call——————–Call call = (Call) service.createCall();设置EndpointURL———使用服务发布的WSDL（结尾是 ?wsdl）或者ENDPOINT（原链接）http://ws.webxml.com.cn/WebServices/WeatherWebService.asmx 或http://ws.webxml.com.cn/WebServices/WeatherWebService.asmx？wsdl 设置Operation名称——–要使用的接口方法，必须 new QName( 高层域名，原方法名称 )；（如下应为getWeatherbyCityName） 添加Parameter参数——–接口方法传入的参数，有几个就写几个，必须 new QName( 高层域名，原参数名称 )；外加 XMLType.XSD_STRING, 和 ParameterMode.IN 注意，此处是“注明”之意，赋值操作不在此处；（如下应为theCityName） 设置SOAPActionURL——高层域名url+接口方法；（如下应为http://WebXml.com.cn/getWeatherbyCityName），其中高层域名为&quot;http://WebXml.com.cn/&quot;; 设置返回类型Type———-关系到invoke的返回值，具体要与发布的服务信息相对应如下应使用为call.setReturnClass(java.lang.String[].class); ，因为字符串数组是封装类，此时应声明返回的类型如果返回值是可以描述的类型，比如String，byte这些基础类型，可以直接使用call.setReturnType(XMLType.XSD_STRING） 触发invoke—————–传入实际的参数，一是参数要为Object对象数组，二是返回类型可转换时(如String) invoke前要加强制转换， 会员先注册，然后点击我的服务 然后点击一个服务的试用，之后返回，再用UserID就可以了 结果很完美 注意","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"WebService","slug":"WebService","permalink":"https://www.cz5h.com/tags/WebService/"},{"name":"初次使用","slug":"初次使用","permalink":"https://www.cz5h.com/tags/%E5%88%9D%E6%AC%A1%E4%BD%BF%E7%94%A8/"}]},{"title":"hive与hbase的联系与区别","slug":"2017-4-20 hive与hbase的联系与区别","date":"2017-04-19T22:00:00.000Z","updated":"2020-02-29T18:43:55.492Z","comments":true,"path":"article/5835.html","link":"","permalink":"https://www.cz5h.com/article/5835.html","excerpt":"hive与hbase的联系与区别：共同点： hbase与hive都是架构在hadoop之上的。都是用hadoop作为底层存储","text":"hive与hbase的联系与区别：共同点： hbase与hive都是架构在hadoop之上的。都是用hadoop作为底层存储 区别： Hive是建立在Hadoop之上为了减少MapReduce jobs编写工作的批处理系统，HBase是为了支持弥补Hadoop对实时操作的缺陷的项目 。 想象你在操作RMDB数据库，如果是全表扫描，就用Hive+Hadoop,如果是索引访问，就用HBase+Hadoop 。 Hive query就是MapReduce jobs可以从5分钟到数小时不止，HBase是非常高效的，肯定比Hive高效的多。 Hive本身不存储和计算数据，它完全依赖于HDFS和MapReduce，Hive中的表纯逻辑。 hive借用hadoop的MapReduce来完成一些hive中的命令的执行 hbase是物理表，不是逻辑表，提供一个超大的内存hash表，搜索引擎通过它来存储索引，方便查询操作。 hbase是列存储。 hdfs作为底层存储，hdfs是存放文件的系统，而Hbase负责组织文件。 hive需要用到hdfs存储文件，需要用到MapReduce计算框架。","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://www.cz5h.com/tags/Hive/"},{"name":"区别","slug":"区别","permalink":"https://www.cz5h.com/tags/%E5%8C%BA%E5%88%AB/"}]},{"title":"美国是如何一步步成为超级大国的","slug":"2017-4-19 美国是如何一步步成为超级大国的","date":"2017-04-18T22:00:00.000Z","updated":"2020-02-29T18:43:55.580Z","comments":true,"path":"article/8996.html","link":"","permalink":"https://www.cz5h.com/article/8996.html","excerpt":"美国如何一步步成为超级大国的 古往今来，从来没有哪个国家像美国这样强大 美国究竟依靠了哪些力量和因素崛起为超级大国? 一、崛起历程","text":"美国如何一步步成为超级大国的 古往今来，从来没有哪个国家像美国这样强大 美国究竟依靠了哪些力量和因素崛起为超级大国? 一、崛起历程 从它在世界上所占的地位看，美国的发展历程大致可分为三个阶段。 1、从独立建国到发展为头号资本主义国家(1776—1945年)为第一阶段。 这一阶段，北美1 3个英属殖民地的人民通过独立战争和1812——1814年第二次独立战争，彻底摆脱了英 国殖民统治，为美国资本主义的发展开辟了道路；又经过南北战争(1861—1865年)，废 除了奴隶制度，大大推进了在自由劳动制度下美国工业化的进程。虽然1929—1933年的 全球经济危机使美国受到严重打击，但两次世界大战都极大地刺激了美国经济，从而使第二次世界大战结束时，美国发展成为世界头号资本主义国家。 2、从二战后初期到1991年苏联解体前为第二阶段。 当时的美国是与苏联并列的两个超级大国之一。 3、 从1991年底苏联解体至今是第三阶段。 此时的美国是世界上唯一的超级大国。20世纪9 0年代美国率先走上发展信息经济的道路，从而使劳动生产率和经济发展速度再次超过所有发达资本主义国家。苏联的解体和美国经济实力的膨胀，使美国以唯一“超级大国 ”身份行事，摆出世界霸主的架势。 二、美国崛起的原因 1、良好的自然条件和地理环境 良好的自然条件：可耕地占国土面积90%！与加拿大共有的五大湖泊所蓄淡水 约占世界淡水总量一半，并具有丰富的矿产资源。 良好的地理位置：北与加 拿大、南与墨西哥两个弱国相邻，东西有大西洋、太平洋两洋相隔，使其在两次大战中 不仅本土未受损伤，反而可以利用战争的机会聚敛财富。世界上几乎没有任何一个其它 大国享有如此的“天赋权利”。 2、有利于创新、发展的民族精神 美国历来被称为多民族的国家，是一个没有传统的(宗教传统除外) 、从民主共和国开始的国家，是一个比任何别的民族都要精力充沛的民族。 清教对美利坚民族精神的形成起了独特的作用。清教徒们想方设法证明，努力工作就能取悦于上帝，发奋经商就会得到升 华，因为“上帝不会让一个人具有五分的才干只得到二分报酬”。教友们由此而“极其勤劳，为聚敛财富而不辞劳苦与艰辛”。这种职业教义使宗教观和资产阶级经济观有机地结合在一起。，美国之所以能够产生出充满活力、发展迅速的市场经济，是和美国新教徒的伦理道德、职业精神分不开的。使得美利坚 民族养成了自强不息、艰苦奋斗的精神和科学的求实态度，以及藐视封建传统、趋向变 动的革新精神和强烈的自治愿望。 3、有一部维护资产阶级长治久安的相对稳定的宪法 美国宪法制定的过程及其权威性和相对稳定性特点，对美国的发展起了重要作 用。 200多年来除增加了“权利法案” 即前十条修正案外，总共仅26条修正案，且至今仍然适用，在世界宪法史上是绝无仅有的。其中重要原因之一是它只列基本原则，宪法自身随时代的演变而演变。而三权分立 与制衡被认为是美国建国先辈们一项得意之作。其最大成就，在于他们成功地将一个软弱无力、宗派林立的各州联盟转变成一个统一国家 。这个成就本身改变了美国的世界地位”。 4、两次革命与多次改革为经济和综合国力发展创造了有利条件 1775年开始的美国独立战争推翻了英国殖民统治，使美国生产力获得第一次大解放；1861—1865年内战废除了奴隶制，使生产力获得第二次大解放，从而使工业革命于19世 纪80年代完成。无论是18世纪的独立战争或是19世纪60年代的内战，这两次革命的主力都是 广大的人民群众。 值得注意的是，美国从建国伊始还经历了各种社会运动和改革，大到关系国计民生的 经济变革和政府改革，小到禁酒运动和涉及民风民俗，几乎无所不包。其中比较突出的 有19世纪末的黑幕揭发者运动，20世纪初影响深远的进步运动，30年代初的罗斯福“新 政”和二次大战后60年代兴起的空前激烈的争取民权、反对种族歧视、争取妇女平等权 利的运动和新左派运动等。这些运动迫使美国当局在一定程度上进行相应的改革。从这 个意义上讲，社会运动和改革已构成美国重要推动力，这也反映了美国社会的自我调节能力，。 5、一贯重视经济、教育、科技和人才的网罗 作为一个纯粹的资本主义国家，出于对资本增殖和扩展的强烈愿望，美国很懂得加强 经济这个根本。 为了解决工业化时期资金不足的问题，曾大量引进外资，并特别注意树立奖励发明和 技术创新意识。早在1790年联邦政府即正式成立专利局，管理发明创造。 美国新教有重视教育的传统。特别是高等教育在总体上一直处于世界领先地位。而且 ，美国当局不断检讨教育中的问题，时时存在一种危机感。敢于正视存在的问题，也反映了对教育的重视。 美国一直鼓励科学和发明，其办法是促进思想自由交流，鼓励“实用知识”的发展和 网罗来自世界各地的有创造力的人才。20世纪90年代美国又 率先进入知识经济时代，依靠信息网络优势，在劳动生产率和经济发展速度上再次超过 其他发达资本主义国家。作为世界头号科技强国，美国的科技投入也是无人可及的。 美国成为科技强国与其广泛招揽科研人才有密切关系。作为一个主要由外来移民组成 的国家，美国不仅吸收了世界上许多国家文化、科技和其它工艺，而且获得了许多优秀 的科学家和技术人才。 6、利用两次世界大战登上世界霸主地位 20世纪上半叶，两次世界大战造成了空前浩劫，世界上各主要大国除美国外几乎无一 例外地遭到严重打击或削弱。美国由于有两洋之隔，远离战争中心而免遭破坏，反而利 用大战发了横财。 经过第一次世界大战，美国由债务国一跃而为债权国，成为全世界金融中心和财政剥 削中心。 二战后，美国不仅摆脱了30年代严重的经济危机与萧条，而且大大发展了生产 力，促进了国家垄断资本主义的发展。战后初期，美国占资本主义世界工业产量的53.4 %(1948年)，出口贸易的32.4%(1947年)，黄金储备的74.5%(1948年)。美国靠战争的输 血而空前强大起来。当时，美国垄断资产阶级宣称，20世纪是“美国世纪”。杜鲁门总 统在1945年12月的国情咨文中说，“胜利已使美国人民有经常而迫切的必要来领导世界 了”。 7、利用美元在二战后资本主义世界货币领域的霸权地位而获利 美国霸权的基础，是美元在国际货币体系中的作用 和它的核威慑力量扩大到了各个盟国。 《布雷顿森林协定》确立了美元在战后资本主义世界货币领域的霸权地位，为美 国的经济扩张创造了极为有利的条件。由于美国在国际贸易中占有绝对优势，1947年10 月于日内瓦签订的《关税和贸易总协定》(“世界贸易组织”的前身)，同样首先适应了 美国的需要。1999年美国以其占世界4.5%的人口利用了世界85%的流动资本和72%的世界 储备。难怪在两极格局解体后，美国一再强调“这个由美国起领导作用的”、上世纪40 年代形成的世界秩序一直存在，而美国“仍然是世界秩序的核心”。 8、利用“冷战”推行联合与壮大资本主义力量的策略 二战后美苏两国几达半个世纪的“冷战”给了美国国力发展以多方面的刺激。由于“ 冷战”这种疑似临战态势的不断持续，刺激了国防工业，从而“造就了美国尖端技术” 。而更重要的是给了美国以拉拢资本主义同盟者的天赐良机。 1947年6月就提出了援助西欧的马歇尔计划。此计划以“复兴欧洲”为名，行控制西欧之实。对美国来说，马歇尔计划达到了 一箭几雕的目的。既稳定了西欧经济，防止社会动乱与革命的爆发，又使受援国沦为美 国的附庸和势力范围，纳入美国全球称霸的战略轨道。对于美国经济而言，它有助于美国商品的输出。这样，美国就利用“ 冷战”的阴影，把“遏制”共产主义的计谋与制造商、出口商的热情融为一体了。 从以上的论述中我们看到，200多年来促使美国迅速崛起的因素是多方面的，是这些因素综合作用的结果。 How the United States has become a superpower Throughout the past, no country has ever been so powerful as the United States What power and factors do the United States rely on to rise for the superpower? First, the rise of the process From its position in the world, the development of the United States can be divided into three stages. 1, from independent founding to the development of the number one capitalist countries (1776-1945) for the first stage. At this stage, the people of the 13 British colonies in North America opened the way for the development of American capitalism through the War of Independence and the Second War of Independence of 1812 - 1814, completely freeing the British colonial rule, and through the Civil War 1861-1865), abolished the slavery system, greatly promoted the process of industrialization in the United States under the free labor system. Although the global economic crisis of 1929-1933 had dealt a serious blow to the United States, the two world wars had greatly stimulated the American economy, so that at the end of the Second World War, the United States developed into the world’s top capitalist state. 2, from the early post-World War II to 1991 before the disintegration of the Soviet Union for the second stage. At that time the United States was one of the two superpowers tied to the Soviet Union. 3, from the end of 1991 the Soviet Union has been the third stage. At this time the United States is the world’s only superpower. In the 1990s, the United States took the lead in embarking on the path of developing the information economy, thus making labor productivity and economic development faster than all developed capitalist countries. The disintegration of the Soviet Union and the expansion of US economic power, so that the United States to act as the only “superpower” identity, put the world’s overlord posture. Second, the reasons for the rise of the United States 1, good natural conditions and geographical environment Good natural conditions: arable land accounted for 90% of land area! The five major lakes shared with Canada account for about half of the world’s fresh water and are rich in mineral resources. Good location: North and Canada, South and Mexico two weak countries adjacent to things, the Atlantic Ocean, the Pacific Ocean separated, so that in the two wars not only local damage, but can use the opportunity to gather the wealth of war. Almost no other big country in the world enjoys such “natural rights”. 2, is conducive to innovation, the development of national spirit The United States, which has traditionally been called a multi-ethnic country, is a country that is not traditional (except religious traditions) and which begins with a democratic republic, a nation that is energetic than any other nation. Puritan played a unique role in the formation of the American national spirit. The Puritans tried to prove that hard work would be able to please God, and that the counselor would be sublimated because “God would not let a man have five points to earn only two points.” The faithful are “extremely industrious and hardworking and struggling to gather wealth.” This professional doctrine makes the religious and bourgeois economic views organically combined. , The United States has been able to produce a vibrant, rapid development of the market economy, and the American Protestant ethics, professional spirit inseparable. So that the American people to develop self-improvement, the spirit of hard work and scientific truth-seeking attitude, as well as contempt for feudal traditions, tend to change the spirit of innovation and strong desire for autonomy. 3, there is a maintenance of the bourgeoisie long-term stability of the relatively stable constitution The development of the US Constitution and its authoritative and relative stability characteristics, the development of the United States played an important role. Over the past two years, in addition to the addition of the Bill of Rights Act, the first ten amendments, a total of only 26 amendments, and still apply, in the history of the world’s history is unique. One of the important reasons is that it only lists the basic principles, the evolution of the Constitution itself with the evolution of the times. And the separation of powers and checks and balances are considered a founding father of the United States. Its greatest achievement is that they succeed in transforming a weak, sectarian state coalition into a unified state. This achievement itself changed the status of the United States. “ The two revolutions and many reforms have created favorable conditions for economic and comprehensive national strength development The American Civil War, which began in 1775, overthrew the British colonial rule and gave American productive power the first major liberation; the 1861-1865 civil society abolished slavery and made the second great liberation of productive forces, bringing the industrial revolution to the 19th century The age is complete. Whether it was the 18th century War of Independence or the civil war of the 1960s, the main forces of the two revolutions were the broad masses of the people. It is noteworthy that the United States from the beginning of the founding of the country has also experienced a variety of social movements and reform, to the people’s livelihood and economic reform and government reform, small to alcohol and folk customs, almost all-encompassing. Which is more prominent in the late 19th century shady actress movement, the early 20th century, far-reaching progressive movement, the early 1930s Roosevelt “New Deal” and the Second World War 60 years after the rise of unprecedented fierce civil rights, against racial discrimination, Women’s equal rights movement and the New Left movement. These movements forced the US authorities to a certain extent, the corresponding reform. In this sense, social movements and reforms have become an important impetus to the United States, which also reflects the self-regulation of American society. 5, has always attached importance to the economy, education, science and technology and talent snare As a purely capitalist country, thanks to the strong desire for capital proliferation and expansion, the United States knows how to strengthen the economy. In order to solve the problem of insufficient funds in the industrial period, a large number of foreign investment, and pay special attention to establish the invention of incentives and technological innovation. As early as 1790 the federal government formally established the patent office, management of inventions. American Protestantism attaches great importance to the tradition of education. In particular, higher education has been a world leader in general. Moreover, the US authorities continue to review the problems in education, there is always a sense of crisis. Dare to face up to the problems, but also reflects the importance of education. The United States has always encouraged science and invention by promoting freedom of thought, encouraging the development of “practical knowledge” and bringing creative talent from all over the world. In the 1990s, the United States took the lead in entering the era of knowledge economy, relying on the advantages of information network, in labor productivity and economic development speed again over other developed capitalist countries. As the world’s number one technology power, the United States is also unparalleled investment in science and technology. The United States has become a strong power of science and technology and its extensive recruitment of scientific research personnel are closely related. As a country mainly composed of immigrants, the United States not only absorbed many countries in the world of culture, technology and other technology, and access to many outstanding scientists and technical personnel. 6, the use of two world wars boarded the world hegemony The first half of the 20th century, the two world wars caused an unprecedented catastrophe, the world’s major powers in addition to the United States almost without exception, was severely hit or weakened. The United States because of the two seas, away from the war center and from the destruction, but the use of war made a windfall. After the First World War, the United States by the debtor countries leap to creditors, as the world’s financial center and financial exploitation center. After World War II, the United States not only out of the 30’s serious economic crisis and depression, but also greatly developed the productive forces, and promote the development of national monopoly capitalism. In the early years after the war, the United States accounted for 53.4% ​​of the industrial output of the capitalist world (1948), 32.4% of the export trade (1947) and 74.5% of the gold reserves (1948). The United States by the war of blood transfusion and unprecedented strength. At that time, the US monopoly bourgeoisie declared that the twentieth century was “American century”. In his December 1945, President Truman said, “victory has made the American people have the urgent need to lead the world.” 7, the use of the dollar in the post-World War II capitalist world currency hegemony and profit The basis of American hegemony is the role of the dollar in the international monetary system and its nuclear deterrent power has been extended to allies. The Bretton Woods Agreement establishes the hegemonic position of the dollar in the postwar capitalist world currency, creating extremely favorable conditions for US economic expansion. As the United States has an absolute advantage in international trade, the General Agreement on Tariffs and Trade (the predecessor of the World Trade Organization), signed in Geneva in October 1947, first adapted to the needs of the United States. In 1999, the United States used 85% of the world’s working capital and 72% of the world’s reserves for 4.5% of the world’s population. It is no wonder that after the disintegration of the bipolar pattern, the United States has repeatedly stressed that “this is the leading role of the United States,” the formation of the last century, 40 years of the world order has always existed, and the United States “is still the core of the world order.” 8, the use of “cold war” to implement the joint and strengthen the capitalist forces strategy After the Second World War, the United States and the Soviet Union for half a century “cold war” to the United States to the development of a multitude of stimulus. As the “cold war” this suspected war situation continues to continue to stimulate the defense industry, thus “creating the US cutting-edge technology.” And more importantly, to the United States to draw the capitalist allies of the heaven-sent opportunity. In June 1947, the Marshall Plan for Western Europe was proposed. This plan to “revive Europe” in the name of the line control of Western Europe. For the United States, Marshall plans to achieve the purpose of a few arrows. Both to stabilize the Western European economy, to prevent social unrest and the outbreak of the revolution, but also to the recipient countries into the United States of the vassal and sphere of influence, into the United States to dominate the global strategic track. For the US economy, it contributes to the export of US goods. In this way, the United States on the use of “cold war” shadow, the “containment” communist strategy and manufacturers, exporters of the enthusiasm of the integration. From the above discussion we have seen that the factors that have contributed to the rapid rise of the United States in the past 200 years are multidimensional and the result of the combined effect of these factors.","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"翻译","slug":"翻译","permalink":"https://www.cz5h.com/tags/%E7%BF%BB%E8%AF%91/"},{"name":"英语","slug":"英语","permalink":"https://www.cz5h.com/tags/%E8%8B%B1%E8%AF%AD/"}]},{"title":"如何开发并发布REST风格的WebService","slug":"2017-4-16 如何开发并发布REST风格的WebService","date":"2017-04-15T22:00:00.000Z","updated":"2020-02-29T18:43:55.489Z","comments":true,"path":"article/1772.html","link":"","permalink":"https://www.cz5h.com/article/1772.html","excerpt":"在《WebService的使用》中已经可以发现其调用方式非常简单，但是也知道一个WebService服务包含非常多的定义和描述（在对WSDL文件的分析中就可以看出），可以说其实现是非常的复杂。不过，在常见开发语言中（JavaC#）都有对实现WebService的封装框架，Java下有AXIS2、CXF、JAX-WS、XFire方式，并且每种方式在常见IDE如Eclipse中都有相关的插件支持或者操作界面化的支持。","text":"在《WebService的使用》中已经可以发现其调用方式非常简单，但是也知道一个WebService服务包含非常多的定义和描述（在对WSDL文件的分析中就可以看出），可以说其实现是非常的复杂。不过，在常见开发语言中（JavaC#）都有对实现WebService的封装框架，Java下有AXIS2、CXF、JAX-WS、XFire方式，并且每种方式在常见IDE如Eclipse中都有相关的插件支持或者操作界面化的支持。 开发和使用的复杂性 AXIS2、CXF、XFire均需要引入其自身的支持，而且有的需要特殊的代码结构支持，这里采用JDK原生支持的方式来对WebService的实现进行简单阐述。 通常来说使用JAX-WS方式是最简单快速的开发方式，是JDK支持的一种编写方法，实现非常简单，但是相对的，在使用时，其并不支持多种方式调用，也不支持直接的Http调用，需要添加相关方法来辅助实现调用的完成，不过这一过程在Eclipse中可以自动根据WSDL文件来生成相关代码。JAX-WS的不足突出体现在调用方式的笨拙上，即不支持REST方式的调用，为此出现了其进阶版JAX-RS（Java API for RESTful Web Services）这是一个Java编程语言的应用程序接口，支持按照表述性状态转移（REST）架构风格创建Web服务。它有好几种实现方式，而Jersey是其实现方式之一。 使用Eclipse开发示例示例完成的功能： 同天气接口一样，在地址栏输入调用链接并返回数据； 在Java代码中调用返回数据； 新建一个Web Service Project，注意要选择JAX-RS选项，并且添加Maven支持。右侧是初始项目结构： 对于上述项目结构，我们要实现发布一个服务，只需要关心三部分：首先，打开pom.xml添加项目依赖；然后，在src/main/java下编写逻辑代码；最后，在web.xml中修改拦截目录以及对应上述代码的位置 打开pom.xml添加项目依赖； 这部分主要添加的是jersey的包，这里使用的是org.glassfish.jersey提供的包，不同组织提供了不同的包，但本质实现的功能都是一样的，这里添加jersey-bom这个包（对应其他组织提供的包可能需要导入的包名会发生变化）。 在src/main/java下编写逻辑代码； 在web.xml中修改上述三部分完成后，在Tomcat中运行即可，然后在浏览器中测试一下。 Java代码中的调用 总结 在JDK原生支持的WebService开发方式中，JAX-WS和JAX-RS是主要的形式，前者在使用上较为不便，所以推荐时候支持REST方式的JAX-RS进行开发，而其本身只是一种规范，Jersey是这种规范的实现之一，上述示例即使用Jersey完成了一个简单的WebService的开发，其调用方式与之前提到的天气接口相同，非常方便，且使用Jersey开发的整个流程也十分简单快速。","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"WebService","slug":"WebService","permalink":"https://www.cz5h.com/tags/WebService/"}]},{"title":"WebService获取数据实例及WSDL文件解读","slug":"2017-4-17 WebService获取数据实例及WSDL文件解读","date":"2017-04-14T22:00:00.000Z","updated":"2020-02-29T18:43:55.490Z","comments":true,"path":"article/c314.html","link":"","permalink":"https://www.cz5h.com/article/c314.html","excerpt":"网址这是一个汇总webservice的网站:http://www.webxml.com.cn里面有非常多可以供调用的WebService","text":"网址这是一个汇总webservice的网站:http://www.webxml.com.cn里面有非常多可以供调用的WebService 概览点进某个服务，针对这个服务的方法都有非常详细的方法说明，完整说明以文件的形式列出：点进某个具体方法是调用接口的具体方式，例如SOAP方式以及Http的Post或者Get方式等等，通常是使用Http进行请求，这种方式返回的结果就是结果本身。（SOAP方式返回结果还有一层soap的标签） 测试使用在这个页面可以进行对方法的调用测试 测试结果对此种调用方式进行分析上述直接在浏览器中显示的是POST方式；调用返回的数据格式为XML，还可以支持JSON格式（这个示例只能返回XML）；上述服务是采用C#写的，并且发布环境是IIS；上述getRegionCountry方法只是该天气服务的一个方法； 接口描述文件：对于全部方法的描述，在没有说明文档时，可以参考接口描述文件(WSDL)，对于规范的WebService服务都可以使用“WebService地址”+“?WSDL”的方式访问到该文件，该文件详细的描述了：服务中包含的所有方法；方法传入的参数类型方法调用的路径等等； 分析整个WSDL文档： 代码中调用本质还是实现一次Http请求,故非常简单,只需要使用代码发送一次请求即可(默认是POST方式)","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"WebService","slug":"WebService","permalink":"https://www.cz5h.com/tags/WebService/"}]},{"title":"Oracle复杂 Merge Into  | no listener | ORA-00001","slug":"2017-4-6 Oracle复杂 Merge Into  - ORA-00001 - no listener","date":"2017-04-05T22:00:00.000Z","updated":"2020-02-29T18:43:55.497Z","comments":true,"path":"article/92b2.html","link":"","permalink":"https://www.cz5h.com/article/92b2.html","excerpt":"使用 Merge Into 进行数据表的增量更新特点：如果数据存在则更新，如果不存在则插入示例：北斗渔船位置的实时数据表","text":"使用 Merge Into 进行数据表的增量更新特点：如果数据存在则更新，如果不存在则插入示例：北斗渔船位置的实时数据表 12345678910111213141516171819202122232425262728293031323334353637383940MERGE INTO A_DATA T1USING ( SELECT &#39;21212958881122&#39; AS MSG_ID , &#39;北斗星通1&#39; AS COMM_TYPE , &#39;20261&#39; AS USER_ID , &#39;2818101&#39;AS TERMINAL_CODE, &#39;201704051&#39; AS MSG_DATE , &#39;定时回传位置1&#39; AS POS_TYPE , &#39;2017-04-05 20:56:311&#39; AS UTC , &#39;126°33′431″&#39; AS LONGITUDE , &#39;30°11′01″&#39; AS LATITUDE , &#39;6801&#39; AS COURSE , &#39;01&#39; AS TRUEHEADING , &#39;8.91&#39; AS SPEED , &#39;正常1&#39; AS STATUS , &#39;01&#39; AS VDESC , &#39;北斗星通1&#39; AS TERMINAL_TYPE , &#39;2017-04-05 21:21:401&#39; AS SYSTEM_TIME FROM dual) T2ON ( T1.TERMINAL_CODE &#x3D; T2.TERMINAL_CODE)WHEN MATCHED THEN UPDATE SET T1.MSG_ID &#x3D; T2.MSG_ID , T1.COMM_TYPE &#x3D; T2.COMM_TYPE , T1.USER_ID &#x3D; T2.USER_ID , T1.MSG_DATE &#x3D; T2.MSG_DATE , T1.POS_TYPE &#x3D; T2.POS_TYPE , T1.UTC &#x3D; T2.UTC , T1.LONGITUDE &#x3D; T2.LONGITUDE , T1.LATITUDE &#x3D; T2.LATITUDE , T1.COURSE &#x3D; T2.COURSE , T1.TRUEHEADING &#x3D; T2.TRUEHEADING , T1.SPEED &#x3D; T2.SPEED , T1.STATUS &#x3D; T2.STATUS , T1.VDESC &#x3D; T2.VDESC , T1.TERMINAL_TYPE &#x3D; T2.TERMINAL_TYPE , T1.SYSTEM_TIME &#x3D; T2.SYSTEM_TIME WHEN NOT MATCHED THEN INSERT (MSG_ID ,COMM_TYPE ,USER_ID ,TERMINAL_CODE ,MSG_DATE ,POS_TYPE ,UTC ,LONGITUDE ,LATITUDE ,COURSE ,TRUEHEADING ,SPEED ,STATUS ,VDESC ,TERMINAL_TYPE ,SYSTEM_TIME ) VALUES(T2.MSG_ID ,T2.COMM_TYPE ,T2.USER_ID ,T2.TERMINAL_CODE ,T2.MSG_DATE ,T2.POS_TYPE ,T2.UTC ,T2.LONGITUDE ,T2.LATITUDE ,T2.COURSE ,T2.TRUEHEADING ,T2.SPEED ,T2.STATUS ,T2.VDESC ,T2.TERMINAL_TYPE ,T2.SYSTEM_TIME); SQL 错误: ORA-00001: 违反唯一约束条件 (SYSTEM.SYS_C0010160) 00000 - “unique constraint (%s.%s) violated” 原因：在select语句中的AS部分全部是取值于已存在记录的值，在id相同并执行update时，相当于更新一条完全相同的语句，即便是各个值没有违反唯一性约束，此时也会报: ORA-00001错误，不要全都一样就可以了； 在Java中使用Oracle的MERGE INTO语句时，老师报错：sql语句未正常结束，但在Navcat中完全正常解决：Navcat中执行时语句的最后有个分号；但在Java中prepareStatement构造时，要去掉这个分号！！！！ 连接错误:no listener、The Network Adapter could not establish the connection1.打开SQL Developer查看本地SYSTEM(例子)是否能连接（密码：ttzzlll）如不行（报adapter错误等等）：查看Oracle服务启动正常否：如果未启动，则右键属性，转登录栏，使用账号密码重新登录，之后即可启动上述完成，看是否可以本地连接，如果仍不行：执行 lsnrctl start + lsnrctl status 看信息正常否正常，则打开oracle安装目录下：oracle\\product\\12.1.0\\dbhome_1\\network\\admin\\listener.ora打开listener.ora，找到最下方LINSTENER=(…)修改HOST = 主机名（主机名可我的电脑右键属性查看）保存，之后cmd下：lsnrctl stoplsnrctl startlsnrctl status有一定的延迟，在客户机上连接，此时OK！！！！","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://www.cz5h.com/tags/Oracle/"},{"name":"Merge","slug":"Merge","permalink":"https://www.cz5h.com/tags/Merge/"},{"name":"Error","slug":"Error","permalink":"https://www.cz5h.com/tags/Error/"}]},{"title":"VMware虚拟机联网","slug":"2017-4-4 虚拟机联网","date":"2017-04-03T22:00:00.000Z","updated":"2020-02-29T18:43:55.495Z","comments":true,"path":"article/f7b9.html","link":"","permalink":"https://www.cz5h.com/article/f7b9.html","excerpt":"1.复制虚拟机文件2.在VMware中分别打开三个虚拟机3.设置NAT模式，固定IP，可上网4.运行一下","text":"1.复制虚拟机文件2.在VMware中分别打开三个虚拟机3.设置NAT模式，固定IP，可上网4.运行一下 复制虚拟机文件拷到本机，随便放 在VMware中分别打开三个虚拟机第一次打开时一定要选择我已移动此虚拟机！否则需要重新配置IP如果ip变了，要么重新配ip（百度），要么删掉重新拷贝一个；验证：打开hadoop01/02/03 使用ifconfig看ip地址分别应该是192.168.146.130/131/132； 设置NAT模式，固定IP，可上网先改本机（win）的虚拟网卡：修改VMnet8配置如下再改虚拟机的网络设置：虚拟机-&gt;编辑-&gt;虚拟网络设置，如下验证：三个节点打开浏览器看是否能上网，不要用ping，如果不能上网重复第三步； 运行一下三个节点分别有hadoop，zookeeper，storm，kafka，flume，mavenhadoop开：start-all.shhadoop关：stop-all.sh其余开关：先在根目录 cd cmdzookeeper开：sh zookeeperstart.shzookeeper关：sh zookeeperstop.shkafka开：sh kafkastart.shkafka关：sh kafkastop.shstorm开：stormstart.shstorm关：stormstop.sh对于flume的开启：先cd /usr/local/flume再bin/flume-ng agent –conf conf –conf-file conf/flume-conf.properties –name producer -Dflume.root.logger=INFO,console对于flume的关闭：直接关终端或Ctrl+C即可","categories":[{"name":"Hadoop相关","slug":"Hadoop相关","permalink":"https://www.cz5h.com/categories/Hadoop%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"VMware","slug":"VMware","permalink":"https://www.cz5h.com/tags/VMware/"},{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"}]},{"title":"使用flume完成数据的接收","slug":"2017-3-16 使用flume完成数据的接收","date":"2017-03-15T23:00:00.000Z","updated":"2020-02-29T18:43:55.483Z","comments":true,"path":"article/6515.html","link":"","permalink":"https://www.cz5h.com/article/6515.html","excerpt":"使用flume完成数据的接收场景：source是通过tcp发送，chnnel处理过滤字段，sink存在集群中","text":"使用flume完成数据的接收场景：source是通过tcp发送，chnnel处理过滤字段，sink存在集群中 适合①[注意，syslog需要特定环境，也可用telnet发送数据]1234567891011121314151617181920212223242526272829source[syslogtcp],sink[hdfs]a1.sources &#x3D; r1a1.sinks &#x3D; k1a1.channels &#x3D; c1# Describe&#x2F;configure the sourcea1.sources.r1.type &#x3D; syslogtcpa1.sources.r1.port &#x3D; 12345a1.sources.r1.host &#x3D;hadoop01a1.sources.r1.channels &#x3D; c1# Describe the sinka1.sinks.k1.type &#x3D; hdfsa1.sinks.k1.channel &#x3D; c1###HDFS的数目路径a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop01:9000&#x2F;flumea1.sinks.k1.hdfs.filePrefix &#x3D; Sysloga1.sinks.k1.hdfs.round &#x3D; truea1.sinks.k1.hdfs.roundValue &#x3D; 1a1.sinks.k1.hdfs.roundUnit &#x3D; minute# Use a channel which buffers events in memorya1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 100# Bind the source and sink to the channela1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1 123456789101112131415161718[hadoop@hadoop01 flume]$ start-all.sh[hadoop@hadoop01 flume]$ hadoop fs -mkdir flume[hadoop@hadoop01 flume]$ hadoop fs -lsdrwxr-xr-x - hadoop supergroup 0 2017-03-12 17:14 flume接收端：bin&#x2F;flume-ng agent --conf conf --conf-file conf&#x2F;syslog.conf --name a1 -Dflume.root.logger&#x3D;INFO,console发送端：telnet hadoop01 12345，，，结果：[hadoop@hadoop01 flume]$ hadoop fs -ls &#x2F;flume &#x2F;&#x2F;注意在hadoop下面写文件查看时，文件夹要加“&#x2F;”Found 13 items-rw-r--r-- 3 hadoop supergroup 177 2017-03-12 18:09 &#x2F;flume&#x2F;My_netcat_log.1489313346930-rw-r--r-- 3 hadoop supergroup 224 2017-03-12 18:16 &#x2F;flume&#x2F;My_netcat_log.1489313794747-rw-r--r-- 3 hadoop supergroup 185 2017-03-12 17:21 &#x2F;flume&#x2F;Syslog.1489310474526-rw-r--r-- 3 hadoop supergroup 149 2017-03-12 17:21 &#x2F;flume&#x2F;Syslog.1489310474527[hadoop@hadoop01 flume]$ hadoop fs -ls flume &#x2F;&#x2F;没有“&#x2F;”会看不到！！！！[hadoop@hadoop01 flume]$ [hadoop@hadoop01 flume]$ 适合②[使用telnet来发送数据]12345678910111213141516171819202122232425262728293031323334353637source[netcat],sink[hdfs]# Describe&#x2F;configure the sourcea1.sources.r1.type &#x3D; netcata1.sources.r1.port &#x3D; 12321a1.sources.r1.bind &#x3D; hadoop01a1.sources.r1.channels &#x3D; c1# Describe the sinka1.sinks.k1.type &#x3D; hdfsa1.sinks.k1.channel &#x3D; c1###HDFS的数目路径a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop01:9000&#x2F;flumea1.sinks.k1.hdfs.filePrefix &#x3D; My_netcat_loga1.sinks.k1.hdfs.round &#x3D; truea1.sinks.k1.hdfs.roundValue &#x3D; 1a1.sinks.k1.hdfs.roundUnit &#x3D; minute# Use a channel which buffers events in memorya1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 100# Bind the source and sink to the channela1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1接收端：bin&#x2F;flume-ng agent --conf conf --conf-file conf&#x2F;netcat.conf --name a1 -Dflume.root.logger&#x3D;INFO,console发送端：telnet hadoop01 12345，，，结果：[hadoop@hadoop01 flume]$ hadoop fs -ls &#x2F;flumeFound 13 items-rw-r--r-- 3 hadoop supergroup 177 2017-03-12 18:09 &#x2F;flume&#x2F;My_netcat_log.1489313346930-rw-r--r-- 3 hadoop supergroup 224 2017-03-12 18:16 &#x2F;flume&#x2F;My_netcat_log.1489313794747[hadoop@hadoop01 flume]$ hadoop fs -ls flume[hadoop@hadoop01 flume]$ [hadoop@hadoop01 flume]$ 适合③[使用curl来发送数据]1234567891011121314151617181920212223242526272829303132333435363738394041source[http],sink[hdfs]a1.sources &#x3D; r1a1.sinks &#x3D; k1a1.channels &#x3D; c1# Describe&#x2F;configure the sourcea1.sources.r1.type &#x3D; httpa1.sources.r1.port &#x3D; 50000a1.sources.r1.bind &#x3D; hadoop01a1.sources.r1.channels &#x3D; c1# Describe the sinka1.sinks.k1.type &#x3D; hdfsa1.sinks.k1.channel &#x3D; c1###HDFSa1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop01:9000&#x2F;flumea1.sinks.k1.hdfs.filePrefix &#x3D; Http_loga1.sinks.k1.hdfs.round &#x3D; truea1.sinks.k1.hdfs.roundValue &#x3D; 1a1.sinks.k1.hdfs.roundUnit &#x3D; minute# Use a channel which buffers events in memorya1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 100# Bind the source and sink to the channela1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1 接收端：bin&#x2F;flume-ng agent -c conf -f conf&#x2F;http.conf -n a1 -Dflume.root.logger&#x3D;INFO,console发送端：[hadoop@hadoop01 flume]$ curl -X POST -d&#39;[&#123;&quot;headers&quot;:&#123;&quot;h1&quot;:&quot;v1&quot;,&quot;h2&quot;:&quot;v2&quot;&#125;,&quot;body&quot;:&quot;hello body&quot;&#125;]&#39; http:&#x2F;&#x2F;hadoop01:50000[hadoop@hadoop01 flume]$ curl -X POST -d&#39;[&#123;&quot;headers&quot;:&#123;&quot;h1&quot;:&quot;v1&quot;,&quot;h2&quot;:&quot;v2&quot;&#125;,&quot;body&quot;:&quot;asdascfascas&quot;&#125;]&#39; http:&#x2F;&#x2F;hadoop01:50000[hadoop@hadoop01 flume]$ curl -X POST -d&#39;[&#123;&quot;headers&quot;:&#123;&quot;h1&quot;:&quot;v1&quot;,&quot;h2&quot;:&quot;v2&quot;&#125;,&quot;body&quot;:&quot;xxxxxxxxxxx&quot;&#125;]&#39; http:&#x2F;&#x2F;hadoop01:50000结果：[hadoop@hadoop01 flume]$ hadoop fs -ls &#x2F;flumeFound 16 items-rw-r--r-- 3 hadoop supergroup 145 2017-03-12 18:49 &#x2F;flume&#x2F;Http_log.1489315734229-rw-r--r-- 3 hadoop supergroup 147 2017-03-12 18:49 &#x2F;flume&#x2F;Http_log.1489315785602-rw-r--r-- 3 hadoop supergroup 161 2017-03-12 18:49 &#x2F;flume&#x2F;Http_log.1489315785603","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://www.cz5h.com/tags/Flume/"},{"name":"实践","slug":"实践","permalink":"https://www.cz5h.com/tags/%E5%AE%9E%E8%B7%B5/"}]},{"title":"远程IDEA调试Storm","slug":"2017-3-15 远程IDEA调试Storm","date":"2017-03-14T23:00:00.000Z","updated":"2020-02-29T18:43:55.480Z","comments":true,"path":"article/ce1b.html","link":"","permalink":"https://www.cz5h.com/article/ce1b.html","excerpt":"现在IDEA编译成功topo后，使用WinSCP将打包好的包传到主节点注意：打包之前–要讲 [ 本地模式 ] 改为 [ 集群模式 ] 12345678&#x2F;&#x2F;本地测试模式 LocalCluster cluster &#x3D; new LocalCluster(); cluster.submitTopology(&quot;firstTopo&quot;, conf, builder.createTopology()); Utils.sleep(100000); cluster.killTopology(&quot;firstTopo&quot;); cluster.shutdown();&#x2F;&#x2F;集群提交模式 StormSubmitter.submitTopology(&quot;firstTopo&quot;, conf, builder.createTopology());","text":"现在IDEA编译成功topo后，使用WinSCP将打包好的包传到主节点注意：打包之前–要讲 [ 本地模式 ] 改为 [ 集群模式 ] 12345678&#x2F;&#x2F;本地测试模式 LocalCluster cluster &#x3D; new LocalCluster(); cluster.submitTopology(&quot;firstTopo&quot;, conf, builder.createTopology()); Utils.sleep(100000); cluster.killTopology(&quot;firstTopo&quot;); cluster.shutdown();&#x2F;&#x2F;集群提交模式 StormSubmitter.submitTopology(&quot;firstTopo&quot;, conf, builder.createTopology()); storm.yaml文件中的seeds选项不能与host同时存在如果同时存在，则storm运行时会出错：提交任务找不到主类等等 修改storm配置中的UI端口，只能！在nimbus中修改 12345678在supervisor中修改会导致supervisor启动不了，会报一下错误Caused by: while scanning a simple key in &#39;reader&#39;, line 32, column 2: ui.port &#x3D; 8000 ^could not found expected &#39;:&#39; in &#39;reader&#39;, line 33, column 1: # package是把jar打到本项目的target下，install时把target下的jar安装到本地仓库，供其他项目使用此处使用先clean再使用pakage打包 1storm jar simple-1.0.jar Random.FirstTopo Maven配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.test&lt;/groupId&gt; &lt;artifactId&gt;simple&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;simple&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;version&gt;1.0.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; //引入依赖的方式默认[没有scope]为compile，意为最后打包无需包含此依赖， //provided必须显式写出scope[打包时会包含依赖] &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; //可删去 &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; //实测此处的1.8没有什么用处，改成1.6也行，但是最好改成与系统一致的jdk版本 &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; Random包下有三个文件，则再提交时，main入口class应为 Random.FirstTopo提交命令中的jar应为上图中的simple-1.0.jar 123simple-1.0.jar 5 KBsimple-1.0-jar-with-dependencies.jar 24735 KBsrc&#x2F;main&#x2F;java&#x2F;Random : src,main,java都不算路径，Random才对应eclipse中的package 附SenqueceBolt: 1234567891011121314151617181920212223242526package Random; &#x2F;** * Created by hadoop on 2017&#x2F;3&#x2F;1. *&#x2F;import org.apache.storm.topology.BasicOutputCollector;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseBasicBolt;import org.apache.storm.tuple.Tuple;public class SenqueceBolt extends BaseBasicBolt&#123; &#x2F;* (non-Javadoc) * @see backtype.storm.topology.IBasicBolt#execute(backtype.storm.tuple.Tuple, backtype.storm.topology.BasicOutputCollector) *&#x2F; public void execute(Tuple input, BasicOutputCollector collector) &#123; &#x2F;&#x2F; TODO Auto-generated method stub String word &#x3D; (String) input.getValue(0); String out &#x3D; &quot;I&#39;m &quot; + word + &quot;!&quot;; System.out.println(&quot;out&#x3D;&quot; + out); &#125; &#x2F;* (non-Javadoc) * @see backtype.storm.topology.IComponent#declareOutputFields(backtype.storm.topology.OutputFieldsDeclarer) *&#x2F; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#125;&#125; 附RandomSpout: 123456789101112131415161718192021222324252627282930313233343536373839404142package Random; &#x2F;** * Created by hadoop on 2017&#x2F;3&#x2F;1. *&#x2F;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;import java.util.Map;import java.util.Random;public class RandomSpout extends BaseRichSpout&#123; private SpoutOutputCollector collector; private static String[] words &#x3D; &#123;&quot;happy&quot;,&quot;excited&quot;,&quot;angry&quot;&#125;; &#x2F;* (non-Javadoc) * @see backtype.storm.spout.ISpout#open(java.util.Map, backtype.storm.task.TopologyContext, backtype.storm.spout.SpoutOutputCollector) *&#x2F; public void open(Map arg0, TopologyContext arg1, SpoutOutputCollector arg2) &#123; &#x2F;&#x2F; TODO Auto-generated method stub this.collector &#x3D; arg2; &#125; &#x2F;* (non-Javadoc) * @see backtype.storm.spout.ISpout#nextTuple() *&#x2F; public void nextTuple() &#123; &#x2F;&#x2F; TODO Auto-generated method stub String word &#x3D; words[new Random().nextInt(words.length)]; collector.emit(new Values(word)); &#125; &#x2F;* (non-Javadoc) * @see backtype.storm.topology.IComponent#declareOutputFields(backtype.storm.topology.OutputFieldsDeclarer) *&#x2F; public void declareOutputFields(OutputFieldsDeclarer arg0) &#123; &#x2F;&#x2F; TODO Auto-generated method stub arg0.declare(new Fields(&quot;randomstring&quot;)); &#125;&#125; 附FirstTopo: 123456789101112131415161718192021222324252627282930package Random; &#x2F;** * Created by hadoop on 2017&#x2F;3&#x2F;1. *&#x2F;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.StormSubmitter;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.utils.Utils;public class FirstTopo &#123; public static void main(String[] args) throws Exception &#123; TopologyBuilder builder &#x3D; new TopologyBuilder(); builder.setSpout(&quot;spout&quot;, new RandomSpout()); builder.setBolt(&quot;bolt&quot;, new SenqueceBolt()).shuffleGrouping(&quot;spout&quot;); Config conf &#x3D; new Config(); conf.setDebug(false); if (args !&#x3D; null &amp;&amp; args.length &gt; 0) &#123; conf.setNumWorkers(3); StormSubmitter.submitTopology(args[0], conf, builder.createTopology()); &#125; else &#123; LocalCluster cluster &#x3D; new LocalCluster(); cluster.submitTopology(&quot;firstTopo&quot;, conf, builder.createTopology()); Utils.sleep(100000); cluster.killTopology(&quot;firstTopo&quot;); cluster.shutdown(); &#125; &#125;&#125;","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"IDEA","slug":"IDEA","permalink":"https://www.cz5h.com/tags/IDEA/"},{"name":"Storm","slug":"Storm","permalink":"https://www.cz5h.com/tags/Storm/"}]},{"title":"Kafka的应用场景","slug":"2017-3-11 Kafka的应用场景","date":"2017-03-10T23:00:00.000Z","updated":"2020-02-29T18:43:55.477Z","comments":true,"path":"article/dcd4.html","link":"","permalink":"https://www.cz5h.com/article/dcd4.html","excerpt":"Kafka的应用场景1 消息队列 比起大多数的消息系统来说，Kafka有更好的吞吐量，内置的分区，冗余及容错性，这让Kafka成为了一个很好的大规模消息处理应用的解决方案。消息系统 一般吞吐量相对较低，但是需要更小的端到端延时，并尝尝依赖于Kafka提供的强大的持久性保障。在这个领域，Kafka足以媲美传统消息系统，如ActiveMR或RabbitMQ。","text":"Kafka的应用场景1 消息队列 比起大多数的消息系统来说，Kafka有更好的吞吐量，内置的分区，冗余及容错性，这让Kafka成为了一个很好的大规模消息处理应用的解决方案。消息系统 一般吞吐量相对较低，但是需要更小的端到端延时，并尝尝依赖于Kafka提供的强大的持久性保障。在这个领域，Kafka足以媲美传统消息系统，如ActiveMR或RabbitMQ。 2 行为跟踪 Kafka的另一个应用场景是跟踪用户浏览页面、搜索及其他行为，以发布-订阅的模式实时记录到对应的topic里。那么这些结果被订阅者拿到后，就可以做进一步的实时处理，或实时监控，或放到hadoop/离线数据仓库里处理。 3 元信息监控 作为操作记录的监控模块来使用，即汇集记录一些操作信息，可以理解为运维性质的数据监控吧。 4 日志收集 日 志收集方面，其实开源产品有很多，包括Scribe、Apache Flume。很多人使用Kafka代替日志聚合（log aggregation）。日志聚合一般来说是从服务器上收集日志文件，然后放到一个集中的位置（文件服务器或HDFS）进行处理。然而Kafka忽略掉 文件的细节，将其更清晰地抽象成一个个日志或事件的消息流。这就让Kafka处理过程延迟更低，更容易支持多数据源和分布式数据处理。比起以日志为中心的 系统比如Scribe或者Flume来说，Kafka提供同样高效的性能和因为复制导致的更高的耐用性保证，以及更低的端到端延迟。 5 流处理 这 个场景可能比较多，也很好理解。保存收集流数据，以提供之后对接的Storm或其他流式计算框架进行处理。很多用户会将那些从原始topic来的数据进行 阶段性处理，汇总，扩充或者以其他的方式转换到新的topic下再继续后面的处理。例如一个文章推荐的处理流程，可能是先从RSS数据源中抓取文章的内 容，然后将其丢入一个叫做“文章”的topic中；后续操作可能是需要对这个内容进行清理，比如回复正常数据或者删除重复数据，最后再将内容匹配的结果返 还给用户。这就在一个独立的topic之外，产生了一系列的实时数据处理的流程。Strom和Samza是非常著名的实现这种类型数据转换的框架。 6 事件源 事件源是一种应用程序设计的方式，该方式的状态转移被记录为按时间顺序排序的记录序列。Kafka可以存储大量的日志数据，这使得它成为一个对这种方式的应用来说绝佳的后台。比如动态汇总（News feed）。 7 持久性日志（commit log） Kafka可以为一种外部的持久性日志的分布式系统提供服务。这种日志可以在节点间备份数据，并为故障节点数据回复提供一种重新同步的机制。Kafka中日志压缩功能为这种用法提供了条件。在这种用法中，Kafka类似于Apache BookKeeper项目。 8 其他 在大数据系统中，常常会碰到一个问题，整个大数据是由各个子系统组成，数据需要在各个子系统中高性能，低延迟的不停流转。传统的企业消息系统并不是非常适合 大规模的数据处理。为了已在同时搞定在线应用（消息）和离线应用（数据文件，日志）Kafka就出现了。Kafka可以起到两个作用： 降低系统组网复杂度。 降低编程复杂度，各个子系统不在是相互协商接口，各个子系统类似插口插在插座上，Kafka承担高速数据总线的作用。","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"概述","slug":"概述","permalink":"https://www.cz5h.com/tags/%E6%A6%82%E8%BF%B0/"},{"name":"转载","slug":"转载","permalink":"https://www.cz5h.com/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Kafka","slug":"Kafka","permalink":"https://www.cz5h.com/tags/Kafka/"}]},{"title":"Flume+Kafka联通","slug":"2017-3-5 Flume+Kafka","date":"2017-03-04T23:00:00.000Z","updated":"2020-02-29T18:43:55.484Z","comments":true,"path":"article/2b6e.html","link":"","permalink":"https://www.cz5h.com/article/2b6e.html","excerpt":"前提前提是要先把flume和kafka独立的部分先搭建好。下载插件包下载flume-kafka-plus:https://github.com/beyondj2ee/flumeng-kafka-plugin把lib目录下的","text":"前提前提是要先把flume和kafka独立的部分先搭建好。下载插件包下载flume-kafka-plus:https://github.com/beyondj2ee/flumeng-kafka-plugin把lib目录下的 和package下的 都放到flume的lib目录修改原有的flume-conf文件在插件包里有一个flume-conf.properties，把这个文件放到flume的conf文件夹里 修改以下内容1234567producer.sources.s.type &#x3D; execproducer.sources.s.command &#x3D; tail -f -n+1 ~&#x2F;tmp&#x2F;test.logproducer.sources.s.channels &#x3D; c……producer.sinks.r.custom.topic.name&#x3D;test……consumer.sources.s.custom.topic.name&#x3D;test producer.sources.s.type = execproducer.sources.s.command = tail -f -n+1 ~/tmp/test.logproducer.sources.s.channels = c 以上路径处需要注意： 尽量不要使用/tmp/这种目录格式，代表的是当前用户的目录。 使用tail -f filename 就可以了这里 文件目录要对应正确路径，且对应位置要有相应文件！！！！ 与文件权限无关 启动zookeeper zkServer.sh start 启动kafka broker bin/kafka-server-start.sh config/server.properties 创建kafka topic bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic test 启动kafka consumer bin/kafka-console-consumer.sh –zookeeper localhost:2181 –topic test –from-beginning 启动flume bin/flume-ng agent –conf conf –conf-file conf/flume-conf.properties –name producer -Dflume.root.logger=INFO,console 测试 echo “this is a test” &gt;&gt; ~/tmp/test.txt此时只要能在consumer里现“this is a test”就表示成功 测试成功","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://www.cz5h.com/tags/Kafka/"},{"name":"整合","slug":"整合","permalink":"https://www.cz5h.com/tags/%E6%95%B4%E5%90%88/"},{"name":"Flume","slug":"Flume","permalink":"https://www.cz5h.com/tags/Flume/"}]},{"title":"IDEA中运行Topology","slug":"2017-3-4 IDEA中运行Topology","date":"2017-03-03T23:00:00.000Z","updated":"2020-02-29T18:43:55.485Z","comments":true,"path":"article/1fbc.html","link":"","permalink":"https://www.cz5h.com/article/1fbc.html","excerpt":"基本调试过程现在IDEA编译成功topo后，使用WinSCP将打包好的包传到主节点 注意：打包之前–要讲 [ 本地模式 ] 改为 [ 集群模式 ] 12345678&#x2F;&#x2F;本地测试模式 LocalCluster cluster &#x3D; new LocalCluster(); cluster.submitTopology(&quot;firstTopo&quot;, conf, builder.createTopology()); Utils.sleep(100000); cluster.killTopology(&quot;firstTopo&quot;); cluster.shutdown(); &#x2F;&#x2F;集群提交模式 StormSubmitter.submitTopology(&quot;firstTopo&quot;, conf, builder.createTopology());","text":"基本调试过程现在IDEA编译成功topo后，使用WinSCP将打包好的包传到主节点 注意：打包之前–要讲 [ 本地模式 ] 改为 [ 集群模式 ] 12345678&#x2F;&#x2F;本地测试模式 LocalCluster cluster &#x3D; new LocalCluster(); cluster.submitTopology(&quot;firstTopo&quot;, conf, builder.createTopology()); Utils.sleep(100000); cluster.killTopology(&quot;firstTopo&quot;); cluster.shutdown(); &#x2F;&#x2F;集群提交模式 StormSubmitter.submitTopology(&quot;firstTopo&quot;, conf, builder.createTopology()); storm.yaml文件中的seeds选项不能与host同时存在如果同时存在，则storm运行时会出错：提交任务找不到主类等等 修改storm配置中的UI端口，只能！在nimbus中修改在supervisor中修改会导致supervisor启动不了，会报一下错误 1234567Caused by: while scanning a simple key in &#39;reader&#39;, line 32, column 2: ui.port &#x3D; 8000 ^could not found expected &#39;:&#39; in &#39;reader&#39;, line 33, column 1: # package是把jar打到本项目的target下，install时把target下的jar安装到本地仓库，供其他项目使用此处使用先clean再使用pakage打包 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172storm jar simple-1.0.jar Random.FirstTopo&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt; &lt;groupId&gt;com.test&lt;&#x2F;groupId&gt; &lt;artifactId&gt;simple&lt;&#x2F;artifactId&gt; &lt;version&gt;1.0&lt;&#x2F;version&gt; &lt;packaging&gt;jar&lt;&#x2F;packaging&gt; &lt;name&gt;simple&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;maven.apache.org&lt;&#x2F;url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt; &lt;&#x2F;properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;&#x2F;groupId&gt; &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt; &lt;version&gt;3.8.1&lt;&#x2F;version&gt; &lt;scope&gt;test&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;&#x2F;groupId&gt; &lt;artifactId&gt;storm-core&lt;&#x2F;artifactId&gt; &lt;version&gt;1.0.1&lt;&#x2F;version&gt; &lt;scope&gt;compile&lt;&#x2F;scope&gt; &#x2F;&#x2F;引入依赖的方式默认[没有scope]为compile，意为最后打包无需包含此依赖， &#x2F;&#x2F;provided必须显式写出scope[打包时会包含依赖] &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;build&gt; &lt;plugins&gt; &#x2F;&#x2F;可删去 &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;2.3.2&lt;&#x2F;version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;&#x2F;source&gt; &#x2F;&#x2F;实测此处的1.8没有什么用处，改成1.6也行，但是最好改成与系统一致的jdk版本 &lt;target&gt;1.8&lt;&#x2F;target&gt; &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt; &lt;plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;2.4&lt;&#x2F;version&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt; &lt;&#x2F;descriptorRefs&gt; &lt;&#x2F;configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;&#x2F;id&gt; &lt;phase&gt;package&lt;&#x2F;phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;&#x2F;goal&gt; &lt;&#x2F;goals&gt; &lt;&#x2F;execution&gt; &lt;&#x2F;executions&gt; &lt;&#x2F;plugin&gt; &lt;&#x2F;plugins&gt; &lt;&#x2F;build&gt;&lt;&#x2F;project&gt; Random包下有三个文件，则再提交时，main入口class应为 Random.FirstTopo提交命令中的jar应为上图中的simple-1.0.jarsimple-1.0.jar 5 KBsimple-1.0-jar-with-dependencies.jar 24735 KBsrc/main/java/Random : src,main,java都不算路径，Random才对应eclipse中的package 运行组合用例12345678910Object : kafka-storm-demoAssign： [IDEA] 打包的时候要改为 集群 模式[IDEA] 修改topic的名称[IDEA] 验证成功与否需要在console中查看System.out的输出[Attention!] 此实例可以放在集群中提交，但是在集群中无法验证是否执行成功 因为代码中只有 [ 系统输出 ] 在集群中提交并不会将输出写入日志， 也就是说，查看日志等方法无法知道是否执行成功，唯一的方式是 在IDEA的调试窗口看输出。 1234[Prepare] IDEA运行程序[Prepare] CRT_1开启flume[Prepare] CRT_2开启kafka-consumer[Prepare] CRT_3开启shell脚本循环写log文件（模拟生产） 12345[Process-1] CRT_3循环写log，文本 &#x3D;&gt; logg.log [会在界面输出][Process-2] CRT_1监控logg文件，文本 &#x3D;&gt; logg.log &#x3D;&gt; flume [输出同上一致][Process-3] flume充当producer，文本 &#x3D;&gt; logg.log &#x3D;&gt; flume &#x3D;&gt; kafkaproducer[Process-4] storm获取产生数据并处理，文本 &#x3D;&gt; logg.log &#x3D;&gt; flume &#x3D;&gt; kafkaproducer &#x3D;&gt; storm[Process-5] 数据被订阅方consumer接收，文本 &#x3D;&gt; logg.log &#x3D;&gt; flume &#x3D;&gt; kafkaproducer &#x3D;&gt; storm &#x3D;&gt; kafkaconsumer 123456789flume+kafka+storm[do] topic：test 上述各方的topic要一致，不一致要在代码中改过来[do] cd cmd &#x3D;&gt; sh logg.sh 打开脚本生产数据[1][do]应有的终端窗口 flume | consumer | shell-log | idea-console [do] bin&#x2F;flume-ng agent --conf conf --conf-file conf&#x2F;flume-conf.properties --name producer -Dflume.root.logger&#x3D;INFO,console 开启flume[2][do] kafka-console-producer.sh --broker-list hadoop01:9092 --topic test 开启消费者[3][do] storm jar simple-1.0.jar Skafka.KafkaTopologytest 提交任务[4]","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"IDEA","slug":"IDEA","permalink":"https://www.cz5h.com/tags/IDEA/"},{"name":"Storm","slug":"Storm","permalink":"https://www.cz5h.com/tags/Storm/"}]},{"title":"IDEA中调试Topology出现的错误","slug":"2017-3-3 IDEA中调试Topology出现的错误","date":"2017-03-02T23:00:00.000Z","updated":"2020-02-29T18:43:55.486Z","comments":true,"path":"article/e19a.html","link":"","permalink":"https://www.cz5h.com/article/e19a.html","excerpt":"在IDEA的maven项目中编写Topology出错：NoClassFound找不到主类：解决–在pom.xml中，找到中的storm，添加&lt;&gt;compi&lt;&gt; 12345678kafka中的topic不新建也可以使用如果不执行 .&#x2F;kafka-topics.sh --create --zookeeper hadoop01:2181 --replication-factor 1 --partitions 1 --topic test直接执行下述两条命令：kafka生产者客户端命令.&#x2F;kafka-console-producer.sh --broker-list hadoop01:9092 --topic testkafka消费者客户端命令.&#x2F;kafka-console-consumer.sh -zookeeper hadoop01:2181 --from-beginning --topic test也可以起到新建topic的目的","text":"在IDEA的maven项目中编写Topology出错：NoClassFound找不到主类：解决–在pom.xml中，找到中的storm，添加&lt;&gt;compi&lt;&gt; 12345678kafka中的topic不新建也可以使用如果不执行 .&#x2F;kafka-topics.sh --create --zookeeper hadoop01:2181 --replication-factor 1 --partitions 1 --topic test直接执行下述两条命令：kafka生产者客户端命令.&#x2F;kafka-console-producer.sh --broker-list hadoop01:9092 --topic testkafka消费者客户端命令.&#x2F;kafka-console-consumer.sh -zookeeper hadoop01:2181 --from-beginning --topic test也可以起到新建topic的目的 maven有很多插件，在IDEA中调试时需要使用compile插件来执行compile命令、 12mvn compile exec:Java -Dstorm.topology&#x3D;storm.starter.WordCountTopology 错mvn compile exec:java -Dstorm.topology&#x3D;storm.starter.WordCountTopology 对 调试Topology过程： 12345678pom.xml ： 打包出错，jdk版本问题，打包中[显式指定]项目setting加M2_HOME ： -DmultiXXXXX错误[首行出错]import org.apache.storm.. ： 新版storm包 １.０.ｘimport backtype.storm.. ： 旧版storm包版本 ０.９.ｘsrc目录结构 ：待探究本地运行找不到IRichSpout ： 同compile改动打包compile、provided ： ClassNotFoundErrorSLF4J ERROR ： 引入两个dependence - slf4j+log4j kafka启动问题的日志在logs文件夹中的server.log kafka主题的日志才在自己自定义的目录中 122017-03-01 17:23:12.906 o.a.s.u.NimbusClient [WARN] Using deprecated config nimbus.host for backward compatibility. Please update your storm.yaml so it only has config nimbus.seeds2017-03-01 17:23:12.906 o.a.s.u.NimbusClient [WARN] Using deprecated config nimbus.host for backward compatibility. Please update your storm.yaml so it only has config nimbus.seeds 错误原因：更改UI端口只修改了nimbus的，没有修改supervisor的 storm nimbus启动失败：nimbus进程不可用时，storm ui将无法访问查nimbus.log无果，直接使用命令./storm nimbus 则会在下方打印出错误： 1234SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:&#x2F;usr&#x2F;local&#x2F;storm&#x2F;tzl-depend.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:&#x2F;usr&#x2F;local&#x2F;storm&#x2F;lib&#x2F;log4j-slf4j-impl-2.1.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]SLF4J: See http:&#x2F;&#x2F;www.slf4j.org&#x2F;codes.html#multiple_bindings for an explanation. 是因为之前提交的topo有slf4j的错误，再次开启storm时就会自动运行[叙述不恰当]而出错改：删掉之（使用storm kill不行，因为nimbus已经出错启动不起来了，故而直接删除掉相关文件） 下图如是：tzl.jar和tzl-depend.jar是之前提交的错误任务，其有slf4j的错误，在启动时好像storm命令会扫描整个目录文件解决：删掉后，storm nimbus &amp; 完美运行","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"IDEA","slug":"IDEA","permalink":"https://www.cz5h.com/tags/IDEA/"},{"name":"Kafka","slug":"Kafka","permalink":"https://www.cz5h.com/tags/Kafka/"}]},{"title":"Flume、Kafka、Storm如何结合使用","slug":"2017-3-2 Flume,Kafka,Storm如何通信","date":"2017-03-01T23:00:00.000Z","updated":"2020-02-29T18:43:55.481Z","comments":true,"path":"article/f84f.html","link":"","permalink":"https://www.cz5h.com/article/f84f.html","excerpt":"原理如何仔细阅读过关于Flume、Kafka、Storm的介绍，就会知道，在他们各自之间对外交互发送消息的原理。在后面的例子中，主要对Flume的sink进行重构，调用kafka的消费生产者(producer)发送消息;在Storm的spout中继承IRichSpout接口，调用kafka的消息消费者(Consumer)来接收消息，然后经过几个自定义的Bolt，将自定义的内容进行输出。","text":"原理如何仔细阅读过关于Flume、Kafka、Storm的介绍，就会知道，在他们各自之间对外交互发送消息的原理。在后面的例子中，主要对Flume的sink进行重构，调用kafka的消费生产者(producer)发送消息;在Storm的spout中继承IRichSpout接口，调用kafka的消息消费者(Consumer)来接收消息，然后经过几个自定义的Bolt，将自定义的内容进行输出。 flume和kafka的整合 复制flume要用到的kafka相关jar到flume目录下的lib里面。 编写sink.java文件,然后在eclipse导出jar包，放到flume-1.5.1-bin/lib目录中,项目中要引用flume-ng-configuration-1.5.0.jar,flume-ng-sdk-1.5.0.jar,flume-ng-core-1.5.0.jar,zkclient-0.3.jar,commons-logging-1.1.1.jar,在flume目录中，可以找到这几个jar文件，如果找不到就用find命令搜一下。 在m1上配置flume和kafka交互的agent 在m1,m2,s1,s2的机器上,分别启动kafka（如果不会请参考这篇文章介绍了kafka的安装、配置和启动《kafka2.9.2的分布式集群安装和demo(java api)测试》），然后在s1机器上再启动一个消息消费者consumer 在m1启动flume 在m1上再打开一个窗口，测试向flume中发送syslog m1打开的flume窗口中看最后一行的信息，Flume已经向kafka发送了消息 在刚才s1机器上打开的kafka消费端，同样可以看到从Flume中发出的信息，说明flume和kafka已经调试成功了 kafka和storm的整合 我们先在eclipse中写代码，在写代码之前，我们要先对maven进行配置，pom.xml配置文件内容如下： 编写KafkaSpouttest.java文件 编写KafkaTopologytest.java文件 测试kafka和storm的结合 打开两个窗口(也可以在两台机器上分别打开)，分别m2上运行kafka的producer，在s1上运行kafka的consumer(如果刚才打开了就不用再打开),先测试kafka自运行是否正常。 如下所示，我在m2上运行producer，输入“hellowelcomeidoall.org”，在s1的机器上consumer同样收到了消息。说明kafka已经运行正常，并且消息通讯也没有问题。 m2机器输出的消息： s1机器接收的消息： 我们再在Eclipse中运行KafkaTopologytest.java，可以看到在控制台，同样收到了刚才在m2上kafka发送的消息。说明kafka和storm也打通了。 flume、kafka、storm的整合 从上面两个例子我们可以看到，flume和kafka之前已经完成了通讯和部署，kafka和storm之间可以正常通讯，只差把storm的相关文件打包成jar部署到storm中即可完成三者的通讯。 Storm的安装、配置、部署，如果不了解，可以参考这篇文章《ubuntu12.04+storm0.9.2分布式集群的搭建》 复制kafka相关的jar包到storm的lib里面。（因为在上面我们已经说过，kafka和storm的整合，主要是重写storm的spout，调用kafka的Consumer来接收消息并打印，所在需要用到这些jar包） 在m1上启动storm nimbus 在s1,s2上启动storm supervisor 在m1上启动storm ui 将Eclipse中的文件打包成jar复制到做任意目录，然后用storm来运行 在flume中发消息，在storm中看是否有接收到 在flume中发送的消息： storm中显示的内容： 通过以上实例，即完成了flume、kafka、storm之间的通讯，","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://www.cz5h.com/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Kafka","slug":"Kafka","permalink":"https://www.cz5h.com/tags/Kafka/"},{"name":"Storm","slug":"Storm","permalink":"https://www.cz5h.com/tags/Storm/"},{"name":"整合","slug":"整合","permalink":"https://www.cz5h.com/tags/%E6%95%B4%E5%90%88/"},{"name":"Flume","slug":"Flume","permalink":"https://www.cz5h.com/tags/Flume/"}]},{"title":"Kafka初使用","slug":"2017-2-28 Kafka初使用","date":"2017-02-27T23:00:00.000Z","updated":"2020-02-29T18:43:55.473Z","comments":true,"path":"article/45fb.html","link":"","permalink":"https://www.cz5h.com/article/45fb.html","excerpt":"12345678910111213[hadoop@hadoop01 bin]$ kafka-topics.sh --create --zookeeper hadoop01,hadoop02,hadoop03 --replication-factor 1 --partitions 3 --topic hadoop01Created topic &quot;hadoop01&quot;.[hadoop@hadoop01 bin]$ kafka-topics.sh --create --zookeeper hadoop01,hadoop02,hadoop03 --replication-factor 3 --partitions 1 --topic hadoop02Created topic &quot;hadoop02&quot;.[hadoop@hadoop01 bin]$ kafka-topics.sh --list --zookeeper localhost:2181hadoop01hadoop02[hadoop@hadoop01 bin]$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic hadoop01Topic:hadoop01 PartitionCount:3 ReplicationFactor:1 Configs: Topic: hadoop01 Partition: 0 Leader: 3 Replicas: 3 Isr: 3 Topic: hadoop01 Partition: 1 Leader: 1 Replicas: 1 Isr: 1 Topic: hadoop01 Partition: 2 Leader: 2 Replicas: 2 Isr: 2----------------------------------","text":"12345678910111213[hadoop@hadoop01 bin]$ kafka-topics.sh --create --zookeeper hadoop01,hadoop02,hadoop03 --replication-factor 1 --partitions 3 --topic hadoop01Created topic &quot;hadoop01&quot;.[hadoop@hadoop01 bin]$ kafka-topics.sh --create --zookeeper hadoop01,hadoop02,hadoop03 --replication-factor 3 --partitions 1 --topic hadoop02Created topic &quot;hadoop02&quot;.[hadoop@hadoop01 bin]$ kafka-topics.sh --list --zookeeper localhost:2181hadoop01hadoop02[hadoop@hadoop01 bin]$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic hadoop01Topic:hadoop01 PartitionCount:3 ReplicationFactor:1 Configs: Topic: hadoop01 Partition: 0 Leader: 3 Replicas: 3 Isr: 3 Topic: hadoop01 Partition: 1 Leader: 1 Replicas: 1 Isr: 1 Topic: hadoop01 Partition: 2 Leader: 2 Replicas: 2 Isr: 2---------------------------------- 检查log目录，对于topic hadoop01，hadoop01为0号分区，hadoop02为1号分区。而topic hadoop02则复制了3份，都为0号分区hadoop01-1hadoop02-0hadoop01-2hadoop02-0hadoop01-0hadoop02-0即：hadoop01-0,1,2hadoop02-0,0,0","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://www.cz5h.com/tags/Kafka/"},{"name":"常用命令","slug":"常用命令","permalink":"https://www.cz5h.com/tags/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"}]},{"title":"Kafka集群常用命令","slug":"2017-2-28 Kafka集群常用命令","date":"2017-02-27T23:00:00.000Z","updated":"2020-02-29T18:43:55.476Z","comments":true,"path":"article/24fd.html","link":"","permalink":"https://www.cz5h.com/article/24fd.html","excerpt":"","text":"kafka常用命令以下是kafka常用命令行总结： 123456789101112131415161718192021222324252627282930313233343536373839401、kafka服务启动.&#x2F;kafka-server-start.sh -daemon ..&#x2F;config&#x2F;server.properties &lt;!-- more --&gt;2、创建topic.&#x2F;kafka-topics.sh --create --zookeeper hadoop01:2181 --replication-factor 1 --partitions 1 --topic test2.1、为topic增加副本.&#x2F;kafka-reassign-partitions.sh -zookeeper hadoop01:2181 -reassignment-json-file json&#x2F;partitions-to-move.json -execute2.2、为topic增加partition.&#x2F;bin&#x2F;kafka-topics.sh –zookeeper hadoop01:2181 –alter –partitions 20 –topic test3、查看有哪些topic： .&#x2F;kafka-topics.sh --list --zookeeper hadoop01:21814、查看topic的详细信息.&#x2F;kafka-topics.sh -zookeeper hadoop01:2181 -describe -topic test5、kafka生产者客户端命令.&#x2F;kafka-console-producer.sh --broker-list hadoop01:9092 --topic test6、kafka消费者客户端命令.&#x2F;kafka-console-consumer.sh -zookeeper hadoop01:2181 --from-beginning --topic test[成功] 7、下线broker.&#x2F;kafka-run-class.sh kafka.admin.ShutdownBroker --zookeeper hadoop01:2181 --broker #brokerId# --num.retries 3 --retry.interval.ms 60shutdown broker8、删除topic.&#x2F;kafka-run-class.sh kafka.admin.DeleteTopicCommand --topic test --zookeeper hadoop01:2181.&#x2F;kafka-topics.sh --zookeeper hadoop01:2181 --delete --topic test实际不会删除，只会显示marked for deletion9、查看consumer组内消费的offset.&#x2F;kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper hadoop01:2181 --group test --topic test .&#x2F;kafka-consumer-offset-checker.sh --zookeeper hadoop01:2181 --group group1 --topic group1","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://www.cz5h.com/tags/Kafka/"},{"name":"常用命令","slug":"常用命令","permalink":"https://www.cz5h.com/tags/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"}]},{"title":"zkp,kfk,stm联通的问题","slug":"2017-2-27 zkp,kfk,stm联通的问题","date":"2017-02-26T23:00:00.000Z","updated":"2020-02-29T18:43:55.475Z","comments":true,"path":"article/3e6b.html","link":"","permalink":"https://www.cz5h.com/article/3e6b.html","excerpt":"要注意source的使用;在root下使用之后有时需要在hadoop用户下再source一次才可以；","text":"要注意source的使用;在root下使用之后有时需要在hadoop用户下再source一次才可以； 注意各个组件的配置文件zookeeper：zookeeper/conf/zoo.cfg运行参数的配置文件zookeeper/data 自定义工作目录内有myid version-2 zookeeper_server.pid，version-2里有snapshot.xxxxxxxxzookeeper/没配置目录，默认在/bin/zookeeper.outzookeeper/bin:zkServer.sh start[正常的启动/默认后台运行]注意：启动要各个节点分别启动，平等的peer关系，用id区分zkServer.sh start-foreground带日志输出的启动[会不能退出！]zkServer.sh status 显示follow、leader等信息zkServer.sh stop 停止zookeeper注意：zookeeper服务通常要最后关闭… storm :storm/conf/storm.yaml运行参数的配置文件storm/workdir 自定义工作目录storm/logs 默认目录位置storm/bin:./storm nimbus 在nimbus节点使用./storm supervisor 在supervisor节点使用注意：storm的各个节点有主控节点和工作节点之区别！另：在终端运行时要使用nohup ./storm nimbus &amp; 来后台运行 kafka :kafka/logs 自定义的kafka工作目录kafka/config/server.properties运行参数的配置文件kafka/config/zookeeper.propertieskafka所连接的zookeeper的相关配置 nohup kafka-server-start server.properties &amp; 启动注意：kafka也要全部启动，类似平等的关系，用id区分 另：kafka-topics.sh的使用 +–create –zookeeper hadoop01,hadoop02,hadoop03 –replication-factor 1 –partitions 3 –topic hadoop01–list –zookeeper localhost:2181–describe –zookeeper localhost:2181 –topic hadoop01 等等 注意：在关闭kafka时使用kafka-server-stop.sh之后，jps仍然显示有kafka，这时只要关闭zookeeper，就可将kafka彻底关闭，[错误][有时候就是关不掉kafka，kill也不好使？待解决]顺序应为：kafka-&gt;zookeeper","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://www.cz5h.com/tags/Kafka/"},{"name":"Zookeeperp","slug":"Zookeeperp","permalink":"https://www.cz5h.com/tags/Zookeeperp/"},{"name":"Storm","slug":"Storm","permalink":"https://www.cz5h.com/tags/Storm/"},{"name":"整合","slug":"整合","permalink":"https://www.cz5h.com/tags/%E6%95%B4%E5%90%88/"}]},{"title":"在windows下使用IDEA远程连接linux集群进行mapreduce调试","slug":"2017-2-24 在windows下使用IDEA远程连接linux集群进行mapreduce调试","date":"2017-02-23T23:00:00.000Z","updated":"2020-02-29T18:43:55.472Z","comments":true,"path":"article/5dab.html","link":"","permalink":"https://www.cz5h.com/article/5dab.html","excerpt":"在windows下使用IDEA远程连接linux集群进行mapreduce调试 改用户名，administrator改为hadoop，即改为linux集群的用户名，我的为hadoop 将hadoop.tar.gz解压至windows下，添加系统变量跟环境变量 HADOOP_HOME，添加PATH追加上HADOOP_HOME\\bin;HADOOP_HOME\\sbin;","text":"在windows下使用IDEA远程连接linux集群进行mapreduce调试 改用户名，administrator改为hadoop，即改为linux集群的用户名，我的为hadoop 将hadoop.tar.gz解压至windows下，添加系统变量跟环境变量 HADOOP_HOME，添加PATH追加上HADOOP_HOME\\bin;HADOOP_HOME\\sbin; 在Windows下解压winutils包，将其内的七个文件复制到hadoop目录下的bin中和lib中 打开IDEA，新建maven项目，测试maven是否正常打包，正常则跳过，不正常则（maven缺失）：下载maven并解压到自定义目录，配置环境变量和系统变量，MAVEN_HOME(可不要)和M2_HOME，并在PATH路径中加入M2_HOME\\bin和\\sbin。 再次新建maven项目，在idea左下角的运行选项中选择maven，在弹出对话框中填写入运行参数，如-Dmaven……=$M2_HOME 点击运行，出现打包信息，则表示maven正常使用，继续下一步。 开启集群，即 start-all.shmr-jobhistory-daemon.sh start historyserver [一定要开historyserver] linux下开启集群，开启后一定要确定集群开启无误，并且确定\\input目录存在且有数据（因为要运行wordcount必须有输入）之后就不用动了（本人用的是虚拟机） 之后注意要关闭防火墙，一般已经关闭还要离开安全模式，即 hadoop -dfsadmin safemode leave 正式开始IDEA调试： 新建maven项目，之后右键项目出现菜单中选择open module setting ，选择dependencies点绿色＋号添加library，找到hadoop目录下的share中的hadoop文件夹，将其中的除https之外的文件全部添加进来，可以取名为hadoop 打开pom.xml，添加相关依赖，之后右上角会有import导入提示，点击后就会导入相关依赖，同时先前的红色字体（错误）会变为灰色（正确），到此maven配置结束。 在main文件夹中的resources下新建log4j.properties和core-site.xml，即一些配置项文件。 在main文件夹中的java中新建wordcount类，具体代码可从网上得到，注意代码中的conf.()设置，其中内容要跟linux集群相匹配，另外其他xxx:9000等类似地方也要修改为自己的master节点的ip。 从置顶菜单栏中选择run configration，在弹出的配置页中添加运行参数，包括xxxx:9000\\tmp\\input，和xxxx:9000\\tmp\\output，注意此处的input文件夹在运行之前就要存在，而output文件夹在运行之前不能存在。另外端口9000后的路径就是linux下的真实路径，即运行的输入输出均不在Windows本地，而在远程linux。 要去开启或关闭windows功能中，勾选上telnet，在windows系统中telnet是默认关闭的，要手动开启。 最后一步，从IDEA左下角选择maven启动项，填上参数点击run，即可开始运行mapreduce代码，远程调用集群的资源，本地并不涉及存取，相应的运行信息或者日志均在IDEA下打印。 修改用户名为hadoop，首先要去用户管理里修改、然后去，用户组管理里修改，分两步！！！！","categories":[{"name":"Hadoop相关","slug":"Hadoop相关","permalink":"https://www.cz5h.com/categories/Hadoop%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://www.cz5h.com/tags/Hadoop/"},{"name":"IDEA","slug":"IDEA","permalink":"https://www.cz5h.com/tags/IDEA/"}]},{"title":"kafka的使用","slug":"2017-2-22 kafka的使用","date":"2017-02-21T23:00:00.000Z","updated":"2020-02-29T18:43:55.470Z","comments":true,"path":"article/d1cb.html","link":"","permalink":"https://www.cz5h.com/article/d1cb.html","excerpt":"kafka的使用 Kafka是一个消息系统，原本开发自LinkedIn，用作LinkedIn的活动流（Activity Stream） 和运营数据处理 管道（Pipeline）的基础活动流数据是几乎所有站点在对其网站使用情况做报表时都要用到的数据中最常规的部分。活动数据包括页面访问量（PageView）、被查看内容方面的信息以及搜索情况等内容。这种数据通常的处理方式是先把各种活动以日志的形式写入某种文件，然后周期性地对这些文件进行统计分析。运营数据指的是服务器的性能数据（CPU、IO使用率、请求时间、服务日志等等数据)。运营数据的统计方法种类繁多。","text":"kafka的使用 Kafka是一个消息系统，原本开发自LinkedIn，用作LinkedIn的活动流（Activity Stream） 和运营数据处理 管道（Pipeline）的基础活动流数据是几乎所有站点在对其网站使用情况做报表时都要用到的数据中最常规的部分。活动数据包括页面访问量（PageView）、被查看内容方面的信息以及搜索情况等内容。这种数据通常的处理方式是先把各种活动以日志的形式写入某种文件，然后周期性地对这些文件进行统计分析。运营数据指的是服务器的性能数据（CPU、IO使用率、请求时间、服务日志等等数据)。运营数据的统计方法种类繁多。 Kafka是一种分布式的，基于发布/订阅的消息系统。主要设计目标如下： ● 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。 ● 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。 ● 支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。 ● 同时支持离线数据处理和实时数据处理。 ● Scale out：支持在线水平扩展。 为何使用消息系统 ● 解耦这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 ● 冗余有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 ● 扩展性因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改代码、调参数。 ● 灵活性 &amp; 峰值处理能力不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 ● 可恢复性消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 ● 顺序保证大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka保证一个Partition内的消息的有序性。 ● 缓冲在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行———写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。 ● 异步通信很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 Kafka是一个高性能跨语言分布式发布/订阅消息队列系统，而Jafka是在Kafka之上孵化而来的，即Kafka的一个升级版。具有以下特性：快速持久化，可以在O(1)的系统开销下进行消息持久化；高吞吐，在一台普通的服务器上既可以达到10W/s的吞吐速率；完全的分布式系统，Broker、Producer、Consumer都原生自动支持分布式，自动实现负载均衡；支持Hadoop数据并行加载，对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。 Terminology ● BrokerKafka集群包含一个或多个服务器，这种服务器被称为broker ● Topic每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处） ● PartitionParition是物理上的概念，每个Topic包含一个或多个Partition. ● Producer负责发布消息到Kafka broker ● Consumer消息消费者，向Kafka broker读取消息的客户端。 ● Consumer Group每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。 Kafka拓扑结构如上图所示，一个典型的Kafka集群中包含若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。 Topic &amp; Partition Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。若创建topic1和topic2两个topic，且分别有13个和19个分区，则整个集群上会相应会生成共32个文件夹（本文所用集群共8个节点，此处topic1和topic2 replication-factor均为1），如下图所示。每个日志文件都是一个log entrie序列，每个log entrie包含一个4字节整型数值（值为N+5），1个字节的”magic value”，4个字节的CRC校验码，其后跟N个字节的消息体。每条消息都有一个当前Partition下唯一的64字节的offset，它指明了这条消息的起始位置。磁盘上存储的消息格式如下： message length ： 4 bytes (value: 1+4+n) “magic” value ： 1 byte crc ： 4 bytes payload ： n bytes 这个log entries并非由一个文件构成，而是分成多个segment，每个segment以该segment第一条消息的offset命名并以“.kafka”为后缀。另外会有一个索引文件，它标明了每个segment下包含的log entry的offset范围，如下图所示。因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。 对于传统的message queue而言，一般会删除已经被消费的消息，而Kafka集群会保留所有的消息，无论其被消费与否。当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此Kafka提供两种策略删除旧数据。一是基于时间，二是基于Partition文件大小。例如可以通过配$KAFKA_HOME/config/server.properties，让Kafka删除一周前的数据，也可在Partition文件超过1GB时删除旧数据，配置如下所示。 12345678# The minimum age of a log file to be eligible for deletionlog.retention.hours&#x3D;168# The maximum size of a log segment file. When this size is reached a new log segment will be created.log.segment.bytes&#x3D;1073741824# The interval at which log segments are checked to see if they can be deleted according to the retention policieslog.retention.check.interval.ms&#x3D;300000# If log.cleaner.enable&#x3D;true is set the cleaner will be enabled and individual logs can then be marked for log compaction.log.cleaner.enable&#x3D;false 这里要注意，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关。选择怎样的删除策略只与磁盘以及具体的需求有关。另外，Kafka会为每一个Consumer Group保留一些metadata信息——当前消费的消息的position，也即offset。这个offset由Consumer控制。正常情况下Consumer会在消费完一条消息后递增该offset。当然，Consumer也可将offset设成一个较小的值，重新消费一些消息。因为offet由Consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些消费过，也不需要通过broker去保证同一个Consumer Group只有一个Consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。 Producer消息路由 Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。如果Partition机制设置合理，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。如果一个Topic对应一个文件，那这个文件所在的机器I/O将会成为这个Topic的性能瓶颈，而有了Partition后，不同的消息可以并行写入不同broker的不同Partition里，极大的提高了吞吐率。可以在$KAFKA_HOME/config/server.properties中通过配置项num.partitions来指定新建Topic的默认Partition数量，也可在创建Topic时通过参数指定，同时也可以在Topic创建之后通过Kafka提供的工具修改。在发送一条消息时，可以指定这条消息的key，Producer根据这个key和Partition机制来判断应该将这条消息发送到哪个Parition。Paritition机制可以通过指定Producer的paritition. class这一参数来指定，该class必须实现kafka.producer.Partitioner接口。本例中如果key可以被解析为整数则将对应的整数与Partition总数取余，该消息会被发送到该数对应的Partition。（每个Parition都会有个序号,序号从0开始） 1234567891011121314151617181920212223242526import kafka.producer.Partitioner;import kafka.utils.VerifiableProperties;public class JasonPartitioner&lt;T&gt; implements Partitioner &#123; public JasonPartitioner(VerifiableProperties verifiableProperties) &#123;&#125; @Override public int partition(Object key, int numPartitions) &#123; try &#123; int partitionNum &#x3D; Integer.parseInt((String) key); return Math.abs(Integer.parseInt((String) key) % numPartitions); &#125; catch (Exception e) &#123; return Math.abs(key.hashCode() % numPartitions); &#125; &#125;&#125;如果将上例中的类作为partition.class，并通过如下代码发送20条消息（key分别为0，1，2，3）至topic3（包含4个Partition）。public void sendMessage() throws InterruptedException&#123; for(int i &#x3D; 1; i &lt;&#x3D; 5; i++)&#123; List messageList &#x3D; new ArrayList&lt;KeyedMessage&lt;String, String&gt;&gt;(); for(int j &#x3D; 0; j &lt; 4; j++）&#123; messageList.add(new KeyedMessage&lt;String, String&gt;(&quot;topic2&quot;, j+&quot;&quot;, &quot;The &quot; + i + &quot; message for key &quot; + j)); &#125; producer.send(messageList); &#125; producer.close();&#125; 则key相同的消息会被发送并存储到同一个partition里，而且key的序号正好和Partition序号相同。（Partition序号从0开始，本例中的key也从0开始）。下图所示是通过Java程序调用Consumer后打印出的消息列表。 [负载均衡] Consumer Group（本节所有描述都是基于Consumer hight level API而非low level API）。使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。[就是说在同一个group里consumer与topic的m某条消息应该是一对一的，如下；]这是Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。如果需要实现广播，只要每个Consumer有一个独立的Group就可以了。要实现单播只要所有的Consumer在同一个Group里。用Consumer Group还可以将Consumer进行自由的分组而不需要多次发送消息到不同的Topic。实际上，Kafka的设计理念之一就是同时提供离线处理和实时处理。根据这一特性，可以使用Storm这种实时流处理系统对消息进行实时在线处理，同时使用Hadoop这种批处理系统进行离线处理，还可以同时将数据实时备份到另一个数据中心，只需要保证这三个操作所使用的Consumer属于不同的Consumer Group即可。下图是Kafka在Linkedin的一种简化部署示意图。下面这个例子更清晰地展示了Kafka Consumer Group的特性。首先创建一个Topic (名为topic1，包含3个Partition)，然后创建一个属于group1的Consumer实例，并创建三个属于group2的Consumer实例，最后通过Producer向topic1发送key分别为1，2，3的消息。结果发现属于group1的Consumer收到了所有的这三条消息，同时group2中的3个Consumer分别收到了key为1，2，3的消息。如下图所示。 Push vs. Pull作为一个消息系统，Kafka遵循了传统的方式，选择由Producer向broker push消息并由Consumer从broker pull消息。一些logging-centric system，比如Facebook的Scribe和Cloudera的Flume，采用push模式。事实上，push模式和pull模式各有优劣。push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成Consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据Consumer的消费能力以适当的速率消费消息。对于Kafka而言，pull模式更合适。pull模式可简化broker的设计，Consumer可自主控制消费消息的速率，同时Consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。Kafka delivery guarantee有这么几种可能的delivery guarantee： ● At most once 消息可能会丢，但绝不会重复传输 ● At least one 消息绝不会丢，但可能会重复传输 ● Exactly once 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的。 当Producer向broker发送消息时，一旦这条消息被commit，因数replication的存在，它就不会丢。但是如果Producer发送数据给broker后，遇到网络问题而造成通信中断，那Producer就无法判断该条消息是否已经commit。虽然Kafka无法确定网络故障期间发生了什么，但是Producer可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了Exactly once。截止到目前(Kafka 0.8.2版本，2015-03-04)，这一Feature还并未实现，有希望在Kafka未来的版本中实现。（所以目前默认情况下一条消息从Producer到broker是确保了At least once，可通过设置Producer异步发送实现At most once）。 接下来讨论的是消息从broker到Consumer的delivery guarantee语义。（仅针对Kafka consumer high level API）。Consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中保存该Consumer在该Partition中读取的消息的offset。该Consumer下一次再读该Partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。当然可以将Consumer设置为autocommit，即Consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，那Kafka是确保了Exactly once。但实际使用中应用程序并非在Consumer读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的delivery guarantee semantic。 ● 读完消息先commit再处理消息。这种模式下，如果Consumer在commit后还没来得及处理消息就crash了，下次重新开始工作后就无法读到刚刚已提交而未处理的消息，这就对应于At most once ● 读完消息先处理再commit。这种模式下，如果在处理完消息之后commit之前Consumer crash了，下次重新开始工作时还会处理刚刚未commit的消息，实际上该消息已经被处理过了。这就对应于At least once。在很多使用场景下，消息都有一个主键，所以消息的处理往往具有幂等性，即多次处理这一条消息跟只处理一次是等效的，那就可以认为是Exactly once。（笔者认为这种说法比较牵强，毕竟它不是Kafka本身提供的机制，主键本身也并不能完全保证操作的幂等性。而且实际上我们说delivery guarantee 语义是讨论被处理多少次，而非处理结果怎样，因为处理方式多种多样，我们不应该把处理过程的特性——如是否幂等性，当成Kafka本身的Feature） ● 如果一定要做到Exactly once，就需要协调offset和实际操作的输出。精典的做法是引入两阶段提交。如果能让offset和操作输入存在同一个地方，会更简洁和通用。这种方式可能更好，因为许多输出系统可能不支持两阶段提交。比如，Consumer拿到数据后可能把数据放到HDFS，如果把最新的offset和数据本身一起写到HDFS，那就可以保证数据的输出和offset的更新要么都完成，要么都不完成，间接实现Exactly once。（目前就high level API而言，offset是存于Zookeeper中的，无法存于HDFS，而low level API的offset是由自己去维护的，可以将之存于HDFS中）总之，Kafka默认保证At least once，并且允许通过设置Producer异步提交来实现At most once。而Exactly once要求与外部存储系统协作，幸运的是Kafka提供的offset可以非常直接非常容易得使用这种方式。 注：本文转自网络","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"概述","slug":"概述","permalink":"https://www.cz5h.com/tags/%E6%A6%82%E8%BF%B0/"},{"name":"转载","slug":"转载","permalink":"https://www.cz5h.com/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Kafka","slug":"Kafka","permalink":"https://www.cz5h.com/tags/Kafka/"}]},{"title":"ETL工具~Kettle调研","slug":"2017-2-19 ETL工具~Kettle调研","date":"2017-02-18T23:00:00.000Z","updated":"2020-02-29T18:43:55.467Z","comments":true,"path":"article/d21a.html","link":"","permalink":"https://www.cz5h.com/article/d21a.html","excerpt":"ETL工具~Kettle调研 2017.2Kettlekettle是其中Pentaho默认的ETL工具，下图为Pentaho的使用情况","text":"ETL工具~Kettle调研 2017.2Kettlekettle是其中Pentaho默认的ETL工具，下图为Pentaho的使用情况 什么是ETL 抽取（Extract）：需要连接到不同的数据资源，以便为随后的步骤（转换、加载、分析、报表展示等）提供数据。数据抽取实际上是ETL解决方案的成功实施的一个主要障碍。转换（Transform）:任何对数据的处理过程都是转换。通常包括：1、移动数据2、根据规则验证数据3、修改数据的内容或者数据结构4、集成多个数据源的数据5、根据处理后的数据计算派生值或者聚集值加载（Load）:将数据加载到目标系统的所有操作 能解决什么问题？ 适用于将多个应用系统的大批量的、异构的数据进行整合，有强大的数据转换功能。 高效适配多种类型的异构数据库、文件和应用系统。 快速构建复杂数据大集中应用、无需编码。 适合什么场景？ 异构数据库迁移，如将两个SQL Server中的业务数据分别依照特定的逻辑迁移到三个Oracle数据库中。Kettle通过Webservice获取天气信息 http://blog.itpub.net/10009036/viewspace-1398948/kettle学习：JsonInput使用 http://blog.csdn.net/jiesa/article/details/50098601开源ETL工具kettle系列之增量更新设计http://blog.csdn.net/aiynmimi/article/details/52150318用kettle向hdfs复制文件http://www.cnblogs.com/allan00/p/3838256.htmlKETTLE访问HIVE表数据https://ask.hellobi.com/blog/hql15/3450 Kettle的优点？ 插件架构扩展性好Kettle 体系架构http://blog.csdn.net/romaticjun2011/article/details/40680483 流程式设计方便易用 全面的数据访问支持(支持多个数据库, 如果非默认支持,还可以通过插件扩展) 支持多平台 高效稳定：1）每个步骤一个线程或者一个步骤分多个线程处理2）集群，把数据分散在多个机器中，在每个机器中作运算再汇总 商业、社区支持 多种方式应用集成： 1) 把Kettle集成到应用中，通过调用Kettle的API来调用一个作业2) 把自己写的jar包集成到Kettle里面，通过Kettle的javascript来调用自己编写的class3) 通过向web页面提交参数，执行一个kettle作业 是否跨平台？ 底层依靠JVM，且为纯JAVA开发Linux1）进入到Kettle部署的路径2）执行 chmod *.sh，将所有shell文件添加可执行权限3）在Kettle路径下，如果要执行transformation，就运行./pan.sh -file=?.ktr -debug=debug -log=log.log 其中。-file说明你要运行的transformation文件所在的路径；-debug说明日志输出的级别；-log说明日志输出的路径4）同理，对于job的执行，请将./pan.sh更换成./kitchen.sh，其他部分说明不变。Windows执行spoon.bat 是否开源，社区支持如何？ 开源社区http://www.ukettle.org/forum.php 对分布式集群的针对性如何？ Kettle Execution on Storm http://wiki.pentaho.com/display/BAD/Kettle+Execution+on+StormKettle on Spark http://wiki.pentaho.com/display/BAD/Kettle+on+SparkLoading Data into HDFS http://wiki.pentaho.com/display/BAD/Loading+Data+into+HDFSKettle 集群（cluster）在多个服务器（windows、linux）上并发执行 http://blog.csdn.net/lixuemei504/article/details/38271145KETTLE集群搭建 http://www.cnblogs.com/skyrim/p/5104557.html 是否可以进行自定义改进源代码？ kettle插件开发 https://wenku.baidu.com/view/33c46d1459eef8c75fbfb3b5.html?re=viewkettle调用第三方短信平台HTTP接口发送短信 http://www.ukettle.org/thread-1025-1-1.html 结构组成？ Spoon 一个基于swt开发的流式处理客户端，用户开发转换、任务、创建数据库、集群、分区等Pan 独立的命令行程序，支持通过命令行实现界面的功能，如果转换启停,任务启停,状态查看等Kitchen 一个独立的命令行程序，用于执行由Spoon编辑的作业.Carte 一个轻量级的Web容器，用于建立专用、远程的ETL Server。 有什么缺点？ kettle性能及效率提升 http://blog.csdn.net/littlecaesar1234/article/details/18657093kettle中做查询时,遇到大数据时怎么处理 http://www.myexception.cn/database/1294030.html kettle处理数据的速度，相比较？ 让kettle的执行速度飞起来 https://my.oschina.net/sucre/blog/398996 注意事项？开源ETL工具kettle系列之常见问题 http://blog.csdn.net/zftang/article/details/6194124 81个开源大数据处理的工具 http://www.36dsj.com/archives/25042","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"概述","slug":"概述","permalink":"https://www.cz5h.com/tags/%E6%A6%82%E8%BF%B0/"},{"name":"Kettle","slug":"Kettle","permalink":"https://www.cz5h.com/tags/Kettle/"}]},{"title":"Spark安装之问题","slug":"2017-3-13 Spark安装之问题","date":"2017-02-18T23:00:00.000Z","updated":"2020-02-29T18:43:55.478Z","comments":true,"path":"article/b086.html","link":"","permalink":"https://www.cz5h.com/article/b086.html","excerpt":"厦门大学数据库实验室教程有几个坑 SparkSQL context 在执行sql语句时，现在使用spark.sql()替换sqlContext.sal()","text":"厦门大学数据库实验室教程有几个坑 SparkSQL context 在执行sql语句时，现在使用spark.sql()替换sqlContext.sal() sparkapp使用sbt打包simple.sbt直接使用教程中的sbt依赖版本即可，即此处无所谓，不过最好是和本机配置版本一致sbt package打包运行时一定要注意联网！！注意执行此打包命令的位置，要在sparkapp目录下！！即在有simple.sbt的目录位置执行sbt package命令 spark.sql执行时目前需要开启hadoop，原理未知，不开会报错 sbt第一次安装时，直接官网下，现在教程中说的bug已经没有了，会卡住无输出，是正常的，时间非常慢！第一次打包时，下载的依赖非常多，不断输出，但非常慢！ 完成次教程全部都在①台机器上还未进行集群配置","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"Spark","slug":"Spark","permalink":"https://www.cz5h.com/tags/Spark/"}]},{"title":"由主节点配置从节点从而构建集群","slug":"2017-2-18 由主节点配置从节点从而构建集群","date":"2017-02-17T23:00:00.000Z","updated":"2020-02-29T18:43:55.469Z","comments":true,"path":"article/bce3.html","link":"","permalink":"https://www.cz5h.com/article/bce3.html","excerpt":"主节点操作 改名字，改为master，hosts改hadoop的slaves加四个xmlslaves是加一个节点就要改一次","text":"主节点操作 改名字，改为master，hosts改hadoop的slaves加四个xmlslaves是加一个节点就要改一次 从节点操作 新建hadoop用户，修改权限，赋予权限ALL =… 改好主机名，按node01 node02……规则 配好ssh，保证可以通信关防火墙，iptables stop关selinux自ssh可以，主对从可以免密ssh 用scp传jdk和hadoop到从节点的tmp目录下可能会出现权限问题，使用tmp目录则不要紧，文件夹权限问题 配jdk和hadoop的环境变量新建一个java目录，如usr/local/java，将之放入，hadoop同理改./bashrc添加路径改./etc/profile添加路径source文件 重载刷新检查：直接java和hadoop回显相关信息 配置hadoop改五个文件，特别注意，从主节点cp过来的hadoop是已经配好的，无需动slaves仍然要改检测： rebootstart-all.shhadoop dfsadmin -report显示正确jps主从点显示正常进程要关闭安全模式dfsadmin safemode leave 运行实例在hdfs新建一个文件夹构造一个input输入文件夹带数据运行hadoop文件夹中share里的用例在hdfs中ls outputget output到本地再查看内容完成 其他配置zookeeperhbasehive pig mahoutsqoop flumeAmbari此部分配置spark，依附于hadoop集群","categories":[{"name":"Hadoop相关","slug":"Hadoop相关","permalink":"https://www.cz5h.com/categories/Hadoop%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"VMware","slug":"VMware","permalink":"https://www.cz5h.com/tags/VMware/"},{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"集群","slug":"集群","permalink":"https://www.cz5h.com/tags/%E9%9B%86%E7%BE%A4/"}]},{"title":"kettle相关知识","slug":"2017-2-16 kettle相关知识","date":"2017-02-15T23:00:00.000Z","updated":"2020-02-29T18:43:55.464Z","comments":true,"path":"article/31ad.html","link":"","permalink":"https://www.cz5h.com/article/31ad.html","excerpt":"相关链接开源ETL工具(Kettle) V5.1.0 免费Spoon版http://www.cr173.com/soft/30051.htmlETL工具大全，你了解多少http://bbs.csdn.net/topics/390349305Kettle_抽取数据举例http://blog.csdn.net/huangyanlong/article/details/42264543","text":"相关链接开源ETL工具(Kettle) V5.1.0 免费Spoon版http://www.cr173.com/soft/30051.htmlETL工具大全，你了解多少http://bbs.csdn.net/topics/390349305Kettle_抽取数据举例http://blog.csdn.net/huangyanlong/article/details/42264543 文件列表 kettle入门(三) 之kettle连接hadoop&amp;hdfs图文详解http://blog.csdn.net/xiaohai798/article/details/39558939 ETL2004ETL和数据集成工具：ETL和数据集成的工作量占BI项目的40%，但是ETL工具约占BI市场的9%，其中很多应用是采用手工编码方式，ETL工具仍有待普及 从哪里抽取什么样的数据，即抽取规则。要支持增量抽取，即每次抽取只抽取上次抽取后变化的数据。在复杂情况下，还需要检查上次抽取后修改或者删除的数据，并依据数据安全策略进行相应的处理； 数据抽取频率即什么时间抽取，即抽取时间设置，确定每天晚上12点抽取，或者每1小时正点时抽取1次，等等； 数据校验，确定每个抽取的数据是否是有效的，是否是没有缺陷的，是否需要补充内容等； 数据转换规则，即源数据怎样转化成需要的数据的，经过什么样的计算、拆分、合并等等；本数据转换完后，需要触发哪些数据的ETL过程； 数据质量检查，可以采用对账等方式对转换完后的数据进行统一检查，保证数据的抽取质量； 错误处理，如果转换过程中出现错误，需要进行统一的、相应的处理，给出明确的业务描述，记录错误日志，并发到系统信息中心； 记录ETL日志，包括转换的时间，数据源是哪个，转化的数据种类，转换的源数据是哪些，对应的目标数据是哪些，等等。 ETL大致分为两部分 设计期：ETL过程， Extract即是从业务数据库中抽取数据,Transform即是根据业务逻辑规则对数据进行加工的过程，Load即是把数据加载到数据仓库的过程(Extract-Transform-Load )，关键就在T的处理上，这个过程的实现，可 以用可以用perl、shell、存储过程等来实现，也可以用类似Kettle等ETL工具实现 运行期：ETL的调度过程，所谓调度，就是执行定时任务，对以上脚本、job的调度，主要是这其中的依赖如何配置的问题，还有就是对于日增、日全、月增、月全等数据如何加载。 资源库资源库用来保存连接信息和转换信息。用户通过图形界面创建的的数据库连接和数据转换任务可以保存在资源库中。资源库可以是各种常见的数据库，用户通过用户名/密码来访问资源库中的资源，默认的用户名/密码是admin/admin。资源库并不是必须的，如果没有资源库，用户还可以把转换任务保存在 xml 文件中。资源库可以使多用户共享转换任务，转换任务在资源库中是以文件夹形式分组管理的，用户可以自定义文件夹名称。有了资源库,就可以将 transformation/job 保存在数据库里,这样方便共享。 任务Kettle以任务的方式存在，每个任务就是一个转换流程，以流程图的方式表现，支持各种流程处理模式，包括条件跳转、分支、循环等等，每一个节点就是业务处理单元。业务处理单元可以是SQL语句，也可以是存储过程，还可以是Java程序等等。Kettle中有两种脚本文件，transformation和job，transformation完成针对数据的基础转换，job则完成整个工作流的控制。主要描述了从各个数据源中抽取数据、转换数据并加载到数据仓库的各个环节及流程。主要功能有数据校验、数据转换规则、数据质量检查、Bug调试错误处理，定时功能、日志跟踪等。 Kettle使用及练习—安装部署Kettle的下载可以在 http://kettle.pentaho.org/ 网站下载。要求jdk.5以上版本，设置JAVA_HOME 环境变量，.kettle不需要安装,直接解压。运行spoon.bat即能看到kettle的欢迎界面，选择没有资源库，打开kettle主界面，在左边的Main tree,双击transaction，配置DB Connection，点击test,提示成功则表示DB已经配置好了。注： Kettle支持跨平台使用，Spoon.bat 是在windows 平台运行，Spoon.sh 是在Linux、Apple OSX、Solaris 平台运行。 spoon.bat 文件中的set OPT=-Xmx256m 改成 set OPT=-Xmx512m，或者更大也可以，否则会在抽取过程中，可能会出现内存溢出的异常 因为kettle7.0是基于jdk1.8的，所以你用1.7的时候会报错，你可以运行SpoonDebug.bat，把完整的报错输出出来。然后1.8 已经不支持MaxPermSize，所以你要在Spoon.bat中把XX:MaxPermSize修改为MaxMetaspaceSize。 只更新、无更新、无删除只使用【更新操作】，注意，更新操作的查询字段只能包含PRAMARY KEY，其他字段不能被包含，更新字段要包含全部字段。 只增加 增加+更新、无删除上述都可以用【插入\\更新操作】，注意其中的查询字段同样只需要主键。 增加+更新+删除需要【组合】【多种操作】","categories":[{"name":"大数据相关","slug":"大数据相关","permalink":"https://www.cz5h.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"概述","slug":"概述","permalink":"https://www.cz5h.com/tags/%E6%A6%82%E8%BF%B0/"},{"name":"Kettle","slug":"Kettle","permalink":"https://www.cz5h.com/tags/Kettle/"},{"name":"转载","slug":"转载","permalink":"https://www.cz5h.com/tags/%E8%BD%AC%E8%BD%BD/"}]},{"title":"2016研究生数学建模竞赛E题","slug":"2016-11-14 16研究生数学建模竞赛E题","date":"2016-11-13T23:00:00.000Z","updated":"2020-02-29T18:43:55.392Z","comments":true,"path":"article/c035.html","link":"","permalink":"https://www.cz5h.com/article/c035.html","excerpt":"2016年全国研究生数学建模竞赛E题粮食最低收购价政策问题研究 粮食，不仅是人们日常生活的必需食品，而且还是维护国家经济发展和政治稳定的战略物资，具有不可替代的特性。由于耕地减少、人口增加、水资源短缺、气候变化等问题日益凸显，加之国际粮食市场的冲击，我国粮食产业面临着潜在的风险。因此，研究我国的粮食保护政策具有十分重要的作用和意义。","text":"2016年全国研究生数学建模竞赛E题粮食最低收购价政策问题研究 粮食，不仅是人们日常生活的必需食品，而且还是维护国家经济发展和政治稳定的战略物资，具有不可替代的特性。由于耕地减少、人口增加、水资源短缺、气候变化等问题日益凸显，加之国际粮食市场的冲击，我国粮食产业面临着潜在的风险。因此，研究我国的粮食保护政策具有十分重要的作用和意义。 一般而言，粮食保护政策体系主要由三大支持政策组成：粮食生产支持政策、粮食价格支持政策和收入支持政策。粮食最低收购价政策就属于粮食价格支持政策范畴。一般情况下，我国粮食收购价格由市场供需情况决定，国家在充分发挥市场机制作用的基础上实行宏观调控。为保护农民利益、保障粮食市场供应，国家对重点粮食品种，在粮食主产区实行最低收购价格政策，并每年事先公布重点粮食品种的最低收购价。在最低收购价格政策执行期（粮食收获期，一般在2-5个月）内，当市场粮食实际收购价低于国家确定的最低收购价时，国家委托符合一定资质条件的粮食企业，按国家确定的最低收购价格收购农民种植的粮食，以保护粮农的种植积极性。 我国自2005年起开始对粮食主产区实行了最低收购价政策，并连续多年上调最低收购价价格。2016年国家发展与改革委员会公布的小麦（三等）最低收购价格为每50公斤118元，比首次实施小麦最低收购价的2006年提高了66.2%；早籼稻（三等）、中晚籼稻（三等）和粳稻（三等）最低收购价格分别为每50公斤133元、138元和155元，分别比首次实施水稻最低收购价的2005年提高了84.72%、91.67%和106.67%。显而易见，粮食最低收购价政策已经成为了国家保护粮食生产的最为重要的举措之一。 然而，也有学者不认同这项最低收购价政策。他们认为，粮食的实际收购价格（以后称为粮食市场收购价）应该由粮食供需双方通过市场调节来决定。粮食最低收购价政策作为一种粮食种植保护政策，扭曲了粮食市场的供需行为，即该政策的实施很有可能抬高了市场收购价格，导致粮食企业承担了很大的经营风险。对于粮食最低收购价政策实施效果的评价，学者们也是见解不一。部分地区某些粮食品种种植面积、粮食总产量不增反降，导致部分学者质疑粮食最低收购价政策的效果；但也有学者高度肯定了粮食最低收购价政策，认为如果不实施粮食最低收购价政策，这些地区某些粮食品种的种植面积可能会下降得更快，因而认为粮食最低收购价政策在稳定或增加粮食种植面积方面是有着积极的作用。 一般来讲，粮食的种植面积是决定粮食供给的关键因素，也是保障粮食安全的重要前提。衡量粮食最低收购价政策实施的效果，主要是比较政策实施前后粮食种植面积是否有显著性变化。然而，可能影响粮食种植面积的因素有很多，除了粮食最低收购价政策外，还可能有其他很多的影响因素，如农业劳动力人口、粮食进出口贸易、农民受教育程度、城乡收入差距、家庭负担等。因此，要研究粮食最低收购价政策的实施效果，不能仅仅根据种植面积的变化来评定。 与此同时，也有一些学者就粮食最低收购价制定的合理范围进行了探讨。最低收购价并不是实际的市场收购价格，而是一种心理安慰价，是收购粮食的底价。粮农决定是否种植粮食，取决于很多因素，但最主要的还是看种植粮食所获得的纯收益的大小。粮食最低收购价的公布，使得粮农能清楚地算出这笔经济账。因此粮食最低收购价的高低直接影响着当年的粮食生产。中国是一个“以粮为纲”的国家，存储的粮食一般要能够满足全国人民三年的吃饭和需求。同时国家对于粮食的补贴金额也是有限制的，在保持合理库存的前提下，一般不会超出各地粮食市场价格的10%。因此，过高的粮食最低收购价不仅会提高粮食市场价格从而加重消费者负担，同时也会增加粮食的库存压力和国家财政的支出风险。另一方面，过低的粮食最低收购价会打压粮农种植粮食的积极性，造成粮食种植面积的萎缩，这更不是国家所愿意看到的。 请你们查阅相关资料和数据，结合数据特点，回答下列问题： 影响粮食种植面积的因素比较多，它们之间的关系错综复杂而且可能存在着粮食品种和区域差异。请你们建立影响粮食种植面积的指标体系和关于粮食种植面积的数学模型，讨论、评价指标体系的合理性，研究他们之间的关系，并对得出的相应结果的可信度和可靠性给出检验和分析。 对粮食最低收购价政策的作用，学者们褒贬不一。请你们建立粮食最低收购价政策执行效果的评价模型。并运用你们所建立的评价模型，结合粮食品种和区域差异，选择几个省份比较研究粮食主产区粮食最低收购价执行的效果。 粮食市场收购价是粮食企业收购粮食的市场价格，是由粮食供需双方通过市场调节来决定。它与粮食最低收购价一起构成粮食价格体系，是宏观价格调控系统中有一定相对独立性的重要措施。请你们运用数据分析或建立数学模型探讨我国粮食价格所具有的特殊规律性。 结合前面的研究和国家制定粮食最低收购价政策的初衷，请你们建立粮食最低收购价的合理定价模型，进而对“十二五”期间国家发展与改革委员会公布的粮食最低收购价价格的合理性做出评价，并运用你们所建立的模型对2017年的粮食最低收购价的合理范围进行预测。 与2000年相比，2015年我国小麦种植面积略有下降。如果国家想让小麦种植面积增加5%，通过调整粮食最低收购价是否能够达到这一目的？请说明理由。 根据你们的研究结论，请提出调控粮食种植的优化决策和建议。 注：数据收集主要关注（不限于）以下网站或数据库http://www.chinagrain.gov.cn/ 国家粮食局http://www.zzys.moa.gov.cn/ 中华人民共和国农业部种植业管理司http://www.sdpc.gov.cn/ 中华人民共和国国家发展与改革委员会http://faostat.fao.org 世界粮农组织数据库http://apps.fas.usda.gov/ 美国农业部数据库http://www.ncpqh.com/ 布瑞克农产品数据库http://www.stats.gov.cn/tjsj/ndsj/ 中华人民共和国国家统计局http://comtrade.un.org/ 联合国贸易数据库http://www.bjinfobank.com/ 高校财经数据库http://www.soshoo.com.cn/ 搜数数据库http://bbs.pinggu.org/ 人大经济论坛中华人民共和国农业部编，中国农业统计资料，中国农业出版社国家发展与改革委员会编，全国农产品成本收益资料汇编，中国统计出版社","categories":[{"name":"研究方向","slug":"研究方向","permalink":"https://www.cz5h.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"}],"tags":[{"name":"数学建模","slug":"数学建模","permalink":"https://www.cz5h.com/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"}]},{"title":"Hadoop集群运行时问题","slug":"2016-11-05 Hadoop集群运行时问题","date":"2016-11-04T23:00:00.000Z","updated":"2020-02-29T18:43:55.390Z","comments":true,"path":"article/dcd4.html","link":"","permalink":"https://www.cz5h.com/article/dcd4.html","excerpt":"Hadoop Copy -copyFromLocal 相当于复制-moveFromLocal 本地会删除，相当于剪切-getmerge 合并， 源目录 -&gt; 目的目录-mkdir-rmr-ls-copyToLocal 从hadoop下文件，不过通常用 -get（类似于-put）hadoop fs -lsr","text":"Hadoop Copy -copyFromLocal 相当于复制-moveFromLocal 本地会删除，相当于剪切-getmerge 合并， 源目录 -&gt; 目的目录-mkdir-rmr-ls-copyToLocal 从hadoop下文件，不过通常用 -get（类似于-put）hadoop fs -lsr 运行hadoop实例的顺序 首先，开启hadoop start-all.sh之后，创建用户目录 hdfs dfs -mkdir input之后，构造程序输入 hdfs dfs -put ./etc/hadoop/*.xml input之后，确认输入有内容 hdfs dfs -ls input之后，运行jar包 hadoop jar /etc/local/hadoop/etc/…/example-2.7.3 input output ‘[a-z.]+’之后，会输出运行的INFO之后，cat output 看结果，可以选择取回本地完成 hadoop集群节点不全开有hadoop01-04 四个节点，现在只开hadoop01，只用master修改master节点的 /etc/local/hadoop/etc/hadoop/slaves文件将hadoop01加入，即之前没有hadoop01，表明master节点只有namenode，没有datanode，现在将datanode让之启动，就可以使master有双重身份其他配置，其他节点的配置，均不改以上类似伪分布式，但是更灵活，本身为完全分布式状态，只运行hadoop01时即为节点缺省状态，当其他节点运行时，不用任何改动即可以成为一个集群。完。 IDEA报错详解1234Cannot delete &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;hadoop&#x2F;.staging&#x2F;job_1477796535608_0001. Name node is in safe mode. 原因：Linux集群中的namenode没有关闭safemode 123452016-11-01 18:32:27,979 INFO [main] mapred.ClientServiceDelegate (ClientServiceDelegate.java:getProxy(276)) - Application state is completed. FinalApplicationStatus&#x3D;SUCCEEDED. Redirecting to job history serverRetrying connect to server: 192.168.146.130&#x2F;192.168.146.130:10020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)Exception in thread &quot;main&quot; java.io.IOException: java.net.ConnectException: Call From MSI&#x2F;118.202.43.35 to 192.168.146.130:10020 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see: http:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;ConnectionRefused原因：开启historyserver服务 mr-jobhistory-daemon.sh start historyserver 1234567问题出现：使用IDEA运行完分词程序后，在输出界面输出了分词信息；但是去Linux集群下，quer使用find &#x2F; -name &#39;output&#39; 却找不到这个文件，原因：hdfs不是一个实际的路径，如果程序中的代码为&quot;hdfs:&#x2F;&#x2F;192.168.146.130:9000&#x2F;tmp&#x2F;input&quot;&quot;hdfs:&#x2F;&#x2F;192.168.146.130:9000&#x2F;tmp&#x2F;output&quot;那么实际上在master节点上并没有一个tmp文件夹里存放这两个文件 如何将此二文件取回本地，也就是能够实际的打开相应的文件？ 12345678使用hdfs dfs -get 【hdfs的目录】 【本地目录】以上命令即可将hdfs上的文件取回本地如何新建hdfs上的文件，也就是在运行程序的时候需要上传程序的输入到hdfs上使用hdfs dfs -mkdir 【hdfs新建的目录】使用hdfs dfs -put 【本地目录】 【hdfs的目录】以上即可完成hdfs文件的上传","categories":[{"name":"Hadoop相关","slug":"Hadoop相关","permalink":"https://www.cz5h.com/categories/Hadoop%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://www.cz5h.com/tags/Hadoop/"},{"name":"集群","slug":"集群","permalink":"https://www.cz5h.com/tags/%E9%9B%86%E7%BE%A4/"}]},{"title":"CentOS集群相关问题","slug":"2016-11-1 CentOS集群相关问题","date":"2016-10-31T23:00:00.000Z","updated":"2020-02-29T18:43:55.391Z","comments":true,"path":"article/bd8a.html","link":"","permalink":"https://www.cz5h.com/article/bd8a.html","excerpt":"centos ssh连不上 出现22端口拒绝访问等问题 确保安装相关软件可以用yum search 来查找相关ssh的软件包","text":"centos ssh连不上 出现22端口拒绝访问等问题 确保安装相关软件可以用yum search 来查找相关ssh的软件包 要安装 openssh，openssh-servers，openssh-clients然后启动ssh，要用service sshd start 注意要将防火墙关闭，可以直接将iptables stop 现在ssh localhost 就可以登录了 centos ping不通外网很大程度上是因为dns设置错误，如果在默认的dns下不能访问外网就要在Wmware软件中虚拟网络编辑器中的NAT设置中的DNS设置添加上8.8.8.8和8.8.4.4 不需要在centos中修改配置文件（精简版安装的除外可能有部分功能被删减） 此时，centos就可以ping 通外网了 修改hostname主机名需要修改两处：一处是/etc/sysconfig/network，另一处是/etc/hosts，只修改任一处会导致系统启动异常。首先切换到root用户。 12345678910● &#x2F;etc&#x2F;sysconfig&#x2F;network 用任一款你喜爱的编辑器打开该文件，里面有一行 HOSTNAME&#x3D;localhost.localdomain (如果是默认的话），修改 localhost.localdomain 为你的主机名。● &#x2F;etc&#x2F;hosts 打开该文件，会有一行 127.0.0.1 localhost.localdomain localhost 。其中 127.0.0.1 是本地环路地址， localhost.localdomain 是主机名(hostname)，也就是你待修改的。localhost 是主机名的别名（alias），它会出现在Konsole的提示符下。将第二项修改为你的主机名，第三项可选。 将上面两个文件修改完后，并不能立刻生效。如果要立刻生效的话，可以用 hostname your-hostname 作临时修改，它只是临时地修改主机名，系统重启后会恢复原样的。但修改上面两个文件是永久的，重启 系统会得到新的主机名。● uname -n 重启后查看主机名 修改用户密码，等基础操作12345[root@localhost ~]# passwdChanging password for user root.New password:Retype new password:passwd: all authentication tokens updated successfully.","categories":[{"name":"Hadoop相关","slug":"Hadoop相关","permalink":"https://www.cz5h.com/categories/Hadoop%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"},{"name":"CentOS","slug":"CentOS","permalink":"https://www.cz5h.com/tags/CentOS/"},{"name":"联网","slug":"联网","permalink":"https://www.cz5h.com/tags/%E8%81%94%E7%BD%91/"}]},{"title":"VMware虚拟机相关问题","slug":"2016-10-30 虚拟机 VMware相关问题","date":"2016-10-29T22:00:00.000Z","updated":"2020-02-29T18:43:55.389Z","comments":true,"path":"article/7bc.html","link":"","permalink":"https://www.cz5h.com/article/7bc.html","excerpt":"主机连不上VMware，ping不通，分析：与linux能不能上网没有关系，是VMware软件设置出错 可能原因1.服务。打开服务，查看VM的各项服务是否为启动状态【首先要确定】2.同一网段。拿NAT来说，","text":"主机连不上VMware，ping不通，分析：与linux能不能上网没有关系，是VMware软件设置出错 可能原因1.服务。打开服务，查看VM的各项服务是否为启动状态【首先要确定】2.同一网段。拿NAT来说， ①打开Vmware的虚拟网络编辑器，找到NAT对应的NAT设置，记下相应的网关和子网IP地址②使用的是虚拟网卡VMnet8，去网络共享中心的网络适配器中查看这块网卡的属性，记下其ipv4的地址，切记要将此处的地址改为和①中地址所属同一网段的地址！（在ipv4属性中这些地址都可以自己指定） 完成后即可ping通 虚拟机迁移注意事项：直接拷贝虚拟机文件；使用对应版本的VMware虚拟机来打开它；不知道选择“我已复制该虚拟机”;知道选择“我已移动该虚拟机”(免设置网络IP)一定要选这个！！！如果需要修改IP地址，要做以下内容： 要改【虚拟网络编辑器】中的NAT选项页【最下方】的子网IP改成相应IP ifconfig-&gt;看到所显示的信息是哪个网卡【eth0?eth1?eth2?】-&gt;看到HWADDR是什么 cd /etc/sysconfig/network-scripts/中 要将eth中的硬件地址与之相对应； service network restart 新增加一个节点需要的工作 新建用户hadoop 增加权限 修改主机名 安装jdk，需要使用完整的jdk（oracle），使用scp直接远程拷贝 配置profile【root】 配置bashrc【hadoop】 在hadoop用户下测试java 安装hadoop，直接由scp远程拷贝而来 【注意】：拷贝来的hadoop文件夹，其配置文件是配置好的不用改（四个.xml文件完全相同）：slaves文件中要因人而异，主从节点是不同的，就是由此区分 【首次】：对hadoop文件夹进行权限更改！！！！！ 特别重要！！！sudo chown -R hadoop usr/local/hadoop：关闭取消centos的防护机制，iptables，selinux：退出hadoop的安全模式以上完成，即可开始修改配置xml 添加进集群 【主机启动】start-all.sh 使用jps查看进程状态使用hadoop dfsadmin -report查看当前hdfs状态有异常：则查看日志log 新增加节点的问题1）防火墙设置问题：==&gt;解决方案：关闭防火墙：#service iptables stop。出现三行OK，此时防火墙被关闭。重复启动集群后，发现问题没有完全解决。2）SELinux设置问题：==&gt;解决方案：关闭SELinux：可以编辑配置文件达到目的 vi /etc/selinux/conf set SELINUX=disabled3）清除Hadoop集群的缓存目录（在core-sites里面设置） 4）格式化NameNode，同意NameNode的ID号==&gt;解决方案：#hadoop namenode -format 5）关闭Hadoop系统的安全模式：==&gt;解决方案：#hadoop dfsadmin -safemode leave 6）重启集群后，显示正常，且不会逐渐节点衰亡。","categories":[{"name":"Hadoop相关","slug":"Hadoop相关","permalink":"https://www.cz5h.com/categories/Hadoop%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"VMware","slug":"VMware","permalink":"https://www.cz5h.com/tags/VMware/"},{"name":"调试","slug":"调试","permalink":"https://www.cz5h.com/tags/%E8%B0%83%E8%AF%95/"}]},{"title":"CentOS安装JDK","slug":"2016-10-27 CentOS安装JDK","date":"2016-10-26T22:00:00.000Z","updated":"2020-02-29T18:43:55.388Z","comments":true,"path":"article/251a.html","link":"","permalink":"https://www.cz5h.com/article/251a.html","excerpt":"centos 安装jdk 目的是使用hadoop不要使用openjdk，要使用更完整的jdk，Oracle官网 首先卸载当前已有的jdk【root下进行】 12java-version显示当前jdkrpm -qa | grep java将本机全部的jdk查找出来","text":"centos 安装jdk 目的是使用hadoop不要使用openjdk，要使用更完整的jdk，Oracle官网 首先卸载当前已有的jdk【root下进行】 12java-version显示当前jdkrpm -qa | grep java将本机全部的jdk查找出来 然后挨个卸载之 12rpm -e --nodeps java-1.x.x-openjdk-xxx将此jdk卸载yum -y remove java java-1.x.x-openjdk-xxx双管齐下，确保卸载 之后开始安装jdk【hadoop用户下即可】用浏览器去官网下载tar.gz后缀的jdk，64位的要选x64的jdk 1sudo tar -zxf ~&#x2F;下载&#x2F;jdk-x.x.x.tar.gz -C &#x2F;usr&#x2F;local&#x2F;java 【使用浏览器默认下载路径】 以上将下载好的jdk直接解压到/usr/local/java目录中 修改环境变量，双管齐下【root中】修改、etc/profile在profile中末尾添加： 123456JAVA_HOME&#x3D;你的路径export JRE_HOME&#x3D;你的路径&#x2F;jreexport CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib:$JRE_HOME&#x2F;lib:$CLASSPATHexport PATH&#x3D;$JAVA_HOME&#x2F;bin:$JRE_HOME&#x2F;bin:$PATH然后source &#x2F;etc&#x2F;profile 【hadoop中】修改 ~/.bashrc 12在bashrc末尾加上JAVA_HOME&#x3D;你的路径然后source ~&#x2F;.bashrc 【hadoop中】测试java -version输出版本则表示OK 注意：去到hadoop目录下找到hadoop-env.sh此文件在/usr/local/hadoop/etc/hadoop 中其中找到JAVA_HOME，看是否对应正确的当前jdk路径 【hadoop中】终极测试 123456 1. cd &#x2F;usr&#x2F;local&#x2F;hadoop 2. mkdir .&#x2F;input 3. cp .&#x2F;etc&#x2F;hadoop&#x2F;*.xml .&#x2F;input 将配置文件作为输入文件 4. .&#x2F;bin&#x2F;hadoop jar .&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-*.jar grep .&#x2F;input .&#x2F;output &#39;dfs[a-z.]+&#39; 5. cat .&#x2F;output&#x2F;* 运行hadoop的wordcount实例，正常则OK 完","categories":[{"name":"Hadoop相关","slug":"Hadoop相关","permalink":"https://www.cz5h.com/categories/Hadoop%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"CentOS","slug":"CentOS","permalink":"https://www.cz5h.com/tags/CentOS/"},{"name":"JDK","slug":"JDK","permalink":"https://www.cz5h.com/tags/JDK/"}]},{"title":"安装sqlserver并用myeclipse访问之","slug":"2016-4-11 安装sqlserver并用myeclipse访问之","date":"2016-04-10T22:00:00.000Z","updated":"2020-02-29T18:43:55.394Z","comments":true,"path":"article/cad2.html","link":"","permalink":"https://www.cz5h.com/article/cad2.html","excerpt":"之前都是用mysql现在项目要求用sqlserver，现把安装配置连接步骤总结如下： 安装sqlserver数据库网上安装资源很多，我从这里下载的安装版：http://free.zolsky.com/activation/soft/21.htm 下好后按照教程装好，教程可参见：","text":"之前都是用mysql现在项目要求用sqlserver，现把安装配置连接步骤总结如下： 安装sqlserver数据库网上安装资源很多，我从这里下载的安装版：http://free.zolsky.com/activation/soft/21.htm 下好后按照教程装好，教程可参见： 在安装时应该要输入一个秘钥，秘钥在此：http://zhidao.baidu.com/question/873260145955726092.html?fr=iks&amp;word=sqlserver2008r2++empress%C3%D8%D4%BF&amp;ie=gbk 上面链接中的秘钥分标准版企业版等好几个版本，我安装时的版本选项是英文，这里对应尝试一下只要秘钥有效就能继续进行安装工工作了。 使用management studio登录下载安装完成后，在相应安装目录下会发现有sqlserver management studio这歌工具，图标是圆筒和锤子加扳手； 打开之后会填写登录账号密码，一般在安装时你已经设置了密码，这时身份验证选择“sqlserver身份验证”，输入完毕后即可进入； 登录失败的解决方法： 详见链接：http://wenku.baidu.com/link?url=00tLSKLmDSma0-BZLUdteUx2Ij4GOmnAIPpEhgDn_RZdQUn8Gx0luK74dnp_MFcT8R2nfDQvlAys0VsTRhlTeiD9IjdSWDxWqs4HVWxPRKq sqlserver身份验证有时会出现错误（比如忘记密码），这时可以使用windows验证，windows验证一般可以直接进入，无需密码； 进入之后找到左侧“安全性”一栏，点开之后再最低部有个sa项，双击打开之后可以更改密码，之后重新登录，会发现用sqlserver验证也可以成功登录了 注意： 使用sqlserver验证登录是必要的，因为在项目中链接数据库操作是就是使用的此验证，故要确保sqlserver验证可以登陆成功，之后在工程的链接代码中还要提供相应的用户名和密码。 使用myeclipse连接sqlserver·首先下载连接sqlserver的jar包，名为sqljdbc4.jar，下载地址：http://download.csdn.net/download/hgg923/6542847 然后myeclipse中新建一个java工程，把下好的包导入到工程中； 之后在java文件中书写如下，代码转自：http://blog.163.com/jackie_howe/blog/static/19949134720125173539380/ 12345678910111213141516171819202122232425import java.sql.*;public class Main &#123; public static void main(String [] args) &#123; String driverName=\"com.microsoft.sqlserver.jdbc.SQLServerDriver\"; String dbURL=\"jdbc:sqlserver://localhost:1433;DatabaseName=填写你的数据库名\"; String userName=\"填写你的用户名，我的是sa\"; String userPwd=\"填写你的密码\"; try &#123; Class.forName(driverName); System.out.println(\"加载驱动成功！\"); &#125;catch(Exception e)&#123; e.printStackTrace(); System.out.println(\"加载驱动失败！\"); &#125; try&#123; Connection dbConn=DriverManager.getConnection(dbURL,userName,userPwd); System.out.println(\"连接数据库成功！\"); &#125;catch(Exception e) &#123; e.printStackTrace(); System.out.print(\"SQL Server连接失败！\"); &#125; &#125;&#125; 将上述代码中的用户名密码该为自己的信息，然后运行，一般会输出： 加载驱动成功！ 连接数据库成功！如果见到以上内容就代表大功告成！代码访问数据库工作就告一段落了。","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://www.cz5h.com/tags/Java/"},{"name":"Sqlserver","slug":"Sqlserver","permalink":"https://www.cz5h.com/tags/Sqlserver/"}]},{"title":"C语言函数传值的相关问题","slug":"2015-11-29 C语言函数传值的相关问题","date":"2015-11-28T23:00:00.000Z","updated":"2020-02-29T18:43:55.386Z","comments":true,"path":"article/c554.html","link":"","permalink":"https://www.cz5h.com/article/c554.html","excerpt":"（本文年代久远，请谨慎阅读）现有如下程序段： 1234567891011void getmem(char **p,int n)&#123; *p=(char *)malloc(n);&#125;int main(void) &#123; // your code goes here char *str; getmem(&amp;str,100); strcpy(str,\"hello\"); printf(\"%s\",str);free(str); return 0;&#125;","text":"（本文年代久远，请谨慎阅读）现有如下程序段： 1234567891011void getmem(char **p,int n)&#123; *p=(char *)malloc(n);&#125;int main(void) &#123; // your code goes here char *str; getmem(&amp;str,100); strcpy(str,\"hello\"); printf(\"%s\",str);free(str); return 0;&#125; 执行无误，输出hello，没有问题；修改之后如下： 1234567891011void getmem(char *p,int n)&#123; //一开始认为是多余的 p=(char *)malloc(n);&#125;int main(void) &#123; // your code goes here char *str; getmem(str,100); //相应改为str strcpy(str,\"hello\"); printf(\"%s\",str);free(str); return 0;&#125; 代码分析上述输出为null，其实不小心犯了个低级错误，那就是：调用getmem时是值传递，str本身在getmem之后并没有获得相应空间，原因即getmem中的*p 作为局部变量并不能将p返回到main函数，即它只让局部的p指向了一段空间，没有意义。 而如果形参改为开始的 1getmem(char **p,int n) 调用时使用 1getmem(&amp;str,100); 其意思是：char *p即指向指针的指针，意为“p指向一个变量，此变量存放的不是具体数据，而是一个指针的地址”，p 即表示其所指的地址变量，显然，此处被指向的指针即str，那么getmem中的 1*p=(char *)malloc(n); 即表示此“被指向的指针”，即str指向一段空间，而区别于值传递的是此处实参为&amp;str，其结束调用后会改变其指向。 此处会改变的原因：本质仍为值传递，但是传递的不是此指针（不同于前面的getmem(str,100)），而是指针所存放的地址，其被 p所指向，然后在函数中通过p修改了p指向内容的值，即修改了str的地址，即调用后str指向发生改变。 注意char *str中，str是一个地址，printf(str)中str也是个地址，只不过格式控制类型为%s，这样的print即从str地址开始一直输出，直到’\\0’为止（终结符是系统自动加上的），这样便实现打印字符串的工作，好像str真作为一个变量存放了这个串，其实不然。 另外，不用函数的方式来开辟空间确实就不需要**p这么麻烦： 12345678int main(void) &#123; // your code goes here char *str; str=(char *)malloc(100); strcpy(str,\"hello\"); printf(\"%s\",str);free(str); return 0;&#125; 上述即可以完成对char *str开辟空间的工作，此外，除了用malloc手动申请空间，也可以用数组赋值的方式： 12345678910int main(void) &#123; // your code goes here char *str; char a[100];//利用系统自动为数组分配空间的特点 str=a; //这样也可以实现str指向一片连续空间 //str=(char *)malloc(100); strcpy(str,\"hello\"); printf(\"%s\",str);free(str); return 0;&#125;","categories":[{"name":"C/C++","slug":"C-C","permalink":"https://www.cz5h.com/categories/C-C/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://www.cz5h.com/tags/C-C/"},{"name":"malloc","slug":"malloc","permalink":"https://www.cz5h.com/tags/malloc/"}]},{"title":"JQuery选择器和JQuery包装集","slug":"2015-8-24 JQuery选择器和JQuery包装集","date":"2015-08-23T22:00:00.000Z","updated":"2020-02-29T18:43:55.387Z","comments":true,"path":"article/64a8.html","link":"","permalink":"https://www.cz5h.com/article/64a8.html","excerpt":"（本文年代久远，请谨慎阅读）今天学习了JQuery的一些基本用法，包括JQuery选择器和JQuery包装集； 从现在开始，要慎重区分DOM对象和JQuery对象，两种对象的方法不同，属性不同，在使用中要特别注意","text":"（本文年代久远，请谨慎阅读）今天学习了JQuery的一些基本用法，包括JQuery选择器和JQuery包装集； 从现在开始，要慎重区分DOM对象和JQuery对象，两种对象的方法不同，属性不同，在使用中要特别注意 JQuery选择器编写任何javascript程序都需要首先获得对象, jQuery选择器能彻底改变我们平时获取对象的方式, 可以获取几乎任何语意的对象, 比如”拥有title属性并且值中包含test的元素”, 完成这些工作只需要编写一个jQuery选择器字符串. 学习jQuery选择器是学习jQuery最重要的一步. DOM对象获取方法： 单个对象：var objDiv =document.getElementById(“id”); 多个对象：Var arrObj = document.getElementsByTagName(“id”);JQuery对象获取方法： 单个对象：var objDiv = $ (“#Id&quot;); 多个对象：var arrObj = $(&apos;div&apos;); //警告：此处是JQuery语法形式，但依然是dom对象数组！！在DOM编程中我们只能使用有限的函数根据id或者TagName获取DOM对象。 而在JQUERY中则完全不同，JQUERY提供了异常强大的选择器用来帮助我们获取页面上的对象,并且将对象以JQUERY包装集的形式返回。 &quot;$&quot;符号在JQUERY中代表对JQUERY框架集的引用。JQUERY选择器包括以下几种： 1、基础选择器 2、层次选择器 3、基本过滤器 4、内容过滤器 5、可见性过滤器 6、属性过滤器 7、子元素过滤器 8、表单选择器 9、表单过滤器下面列出几种重要的选择器： 基础选择器$(&quot;#Id&quot;) 选择ID为divId的元素（根据元素Id选择） $(&quot;element&quot;) 选择所有元素（根据元素的名称选择） $(&quot;.class&quot;) 选择所用CSS类为bgRed的元素（根据元素的css类选择） $(&quot;*&quot;)选择页面所有元素（选择所有元素） $(&quot;#divId, element, .class&quot;)（可以将几个选择器用&quot;,&quot;分隔开然后再拼成一个选择器字符串.会同时选中这几个选择器匹配的内容.）属性过滤器$(&quot;div[id]&quot;)匹配包含给定属性的元素 $(&quot;input[name=&apos;...&apos;]&quot;) 匹配给定的属性是某个特定值的元素 name=&apos;...&apos; $(&quot;input[name!=&apos;...&apos;]&quot;)匹配给定的属性是不包含某个特定值的元素 name=&apos;...&apos; $(&quot;input[name^=&apos;...&apos;]&quot;)匹配给定的属性是以某些值开始的元素 name^=&apos;...&apos; $(&quot;input[name$=&apos;...&apos;]&quot;)匹配给定的属性是以某些值结尾的元素 name$=&apos;...&apos; $(&quot;input[name*=&apos;...&apos;]&quot;)匹配给定的属性是以包含某些值的元素 name*=&apos;...&apos; $(&quot;input[id][name$=&apos;...&apos;]&quot;)复合属性选择器，需要同时满足多个条件时使用 [id][name$=&apos;...&apos;]表单过滤器$(&quot;input:enabled&quot;)匹配所有可用元素 $(&quot;input:disabled&quot;)匹配所有不可用元素 $(&quot;input:checked&quot;)匹配所有选中的被选中元素(复选框、单选框等，不包括select中的option) $(&quot;select option:selected&quot;)匹配所有选中的option元素注意 DOM转JQUERY包装集：$(arrDiv[i]).html(‘div’+i);//arrDivp[i]是DOM对象，直接用$()转为JQuery对象后调用html方法； JQUERY包装集转DOM对象 通过索引访问到的JQUERY包装集中的单个元素是DOM对象 通过包装集的某些遍历函数，例如each中传递的遍历函数中的this也是DOM元素1234var arrDiv &#x3D; $(&#39;div&#39;);for( var i &#x3D; 0;i &lt; arrDiv.length; i++) &#123; arrDiv[i].innerHTML &#x3D; &#39;div&#39; + i;&#x2F;&#x2F;通过索引访问到的元素不是JQuery对象，而是DOM对象&#125; JQuery包装集在此介绍一些基本的JQuery包装集及使用 ready()方法在使用JQUERY时，当 DOM（文档对象模型） 已经加载完成时，就会发生 ready 事件。由于该事件在文档就绪后发生，因此把所有其他的 JQUERY事件和函数置于该事件中是非常好的做法。 即将JQuery函数及事件全部放到$(document).ready(function(){…all jquery functions..}); 相比较而言，&lt;body onload=&quot;&quot;&gt;&lt;/body&gt;中的onload同ready的区别有： onload是原生的JAVASCRIPT事件方法； onload必须等到页面内包括图片的所有元素加载完毕后才能执行，ready是DOM结构绘制完毕后就执行，不必等到加载完毕； onload不能同时编写多个，如果有多个onload方法，只会执行一个，而ready可以同时编写多个，并且都可以得到执行 ； onload无简化写法，ready有简化的写法，可以简写成$(function(){…})； appendTo()方法在被选元素的结尾（仍然在内部）插入指定内容，可以被用来动态添加若干句HTML语句； 12var testDiv &#x3D; $(&#39;#testDiv&#39;);$(&#39;&lt;select&gt;&lt;option&gt;choose1&lt;&#x2F;option&gt;&lt;option&gt;choose2&lt;&#x2F;option&gt;&lt;&#x2F;select&gt;&#39;).appendTo(testDiv); 意思是将此行html语句先转换成JQuery对象，然后用对象的appendTo方法追加到testDiv这个被选元素的结尾，这个被选元素即某个控件， 如一个div块或者一个文本框，效果是在此控件后显示一个下拉框； 其他一些常用的操作JQUERY包装集的函数$(&quot;p&quot;).eq(1) 获取第N个元素：.eq(Index) $(&quot;p&quot;).filter(&quot;.bgRed&quot;)筛选出与指定表达式匹配的元素集合：.filter(&quot;Express&quot;) $(&quot;div&quot;).filter(function(index) {});筛选出与指定函数返回值匹配的元素集合：.filter(function) $(&quot;input[type=&apos;checkbox&apos;]&quot;).parent().is(&quot;form&quot;)用一个表达式来检查当前选择的元素集合，如果其中至少有一个元素符合这个给定的表达式就返回true $(&quot;p&quot;).parent()查找每个段落的父元素：示例HTML 代码：&lt;div&gt;&lt;p&gt;Hello&lt;/p&gt;&lt;p&gt;Hello&lt;/p&gt;&lt;/div&gt;执行$(&quot;p&quot;).parent()之后结果为：[ &lt;div&gt;&lt;p&gt;Hello&lt;/p&gt;&lt;p&gt;Hello&lt;/p&gt;&lt;/div&gt;] $(&quot;p&quot;).parent(&quot;.selected&quot;)查找段落的父元素中每个类名为selected的父元素： HTML 代码：&lt;div&gt;&lt;p&gt;Hello&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;selected&quot;&gt;&lt;p&gt;Hello Again&lt;/p&gt;&lt;/div&gt;执行$(&quot;p&quot;).parent(&quot;.selected&quot;)之后结果为：[ &lt;div class=&quot;selected&quot;&gt;&lt;p&gt;Hello Again&lt;/p&gt;&lt;/div&gt; ]其他1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253使用is()方法查找段落的父元素中每个类名为selected的父元素（带返回值true&#x2F;false）：使用var flagValue &#x3D; $(&quot;p&quot;).parent().is(&quot;select&quot;) 代替 $(&quot;p&quot;).parent(&quot;.selected&quot;) &#x2F;&#x2F; 使用is()方法会有返回值，如果满足条件，则flagValue会为true;$(&quot;input&quot;).map(function()&#123;&#125;).get().join(&quot;, &quot;)把form中的每个input元素的值建立一个列表&lt;p&gt;&lt;b&gt;Values: &lt;&#x2F;b&gt;&lt;&#x2F;p&gt;&lt;form&gt; &lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;name&quot; value&#x3D;&quot;John&quot;&#x2F;&gt; &lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;password&quot; value&#x3D;&quot;password&quot;&#x2F;&gt; &lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;url&quot; value&#x3D;&quot;http:&#x2F;&#x2F;ejohn.org&#x2F;&quot;&#x2F;&gt;&lt;&#x2F;form&gt;JS：$(&quot;p&quot;).append( $(&quot;input&quot;).map(function()&#123; return $(this).val();&#125;).get().join(&quot;, &quot;) ); 结果：在&lt;p&gt;标签后追加字符串，其内容是取出所有&lt;input&gt;标签的value值，并用“，”分隔，最后结果为：&lt;p&gt;&lt;b&gt;Values:John, password, http:&#x2F;&#x2F;ejohn.org&#x2F;&lt;&#x2F;b&gt;&lt;&#x2F;p&gt;jQuery.map(arr|obj,callback) &#x2F;&#x2F;将一个数组转换为另一个数组将原数组中每个元素加 4 转换为一个新数组：$.map( [0,1,2], function(n)&#123; return n + 4;&#125;); 结果:[4, 5, 6]原数组中大于 0 的元素加 1 ，否则删除：$.map( [0,1,2], function(n)&#123; return n &gt; 0 ? n + 1 : null;&#125;); 结果:[2, 3]原数组中每个元素扩展为一个包含其本身和其值加 1 的数组，并转换为一个新数组：$.map( [0,1,2], function(n)&#123; return [ n, n + 1 ];&#125;); 结果:[0, 1, 1, 2, 2, 3]$(&quot;p&quot;).not($(&quot;#testid&quot;)[0])去除所有与给定选择器匹配的元素&lt;input name&#x3D;&quot;apple&quot; &#x2F;&gt;&lt;input name&#x3D;&quot;flower&quot; checked&#x3D;&quot;checked&quot; &#x2F;&gt;查找所有未选中的 input 元素：$(&quot;input:not(:checked)&quot;) 结果:[ &lt;input name&#x3D;&quot;apple&quot; &#x2F;&gt; ]$(&quot;p&quot;).slice(0, 1);选取一个匹配的子集.slice(start [,end] );第一个参数：开始选取子集的位置。第一个元素是0.如果是负数，则可以从集合的尾部开始选起。第二个参数：结束选取自己的位置，如果不指定，则就是本身的结尾。&lt;p&gt;Hello&lt;&#x2F;p&gt;&lt;p&gt;cruel&lt;&#x2F;p&gt;&lt;p&gt;World&lt;&#x2F;p&gt;选择第一个p元素：$(&quot;p&quot;).slice(0, 1).wrapInner(&quot;&lt;b&gt;&lt;&#x2F;b&gt;&quot;);&#x2F;&#x2F;选择的是value值选择前两个p元素：$(&quot;p&quot;).slice(0, 2).wrapInner(&quot;&lt;b&gt;&lt;&#x2F;b&gt;&quot;);只选取第二个p元素：$(&quot;p&quot;).slice(1, 2).wrapInner(&quot;&lt;b&gt;&lt;&#x2F;b&gt;&quot;);只选取第二第三个p元素$(&quot;p&quot;).slice(1).wrapInner(&quot;&lt;b&gt;&lt;&#x2F;b&gt;&quot;);选取第最后一个p元素：$(&quot;p&quot;).slice(-1).wrapInner(&quot;&lt;b&gt;&lt;&#x2F;b&gt;&quot;);$(&quot;p&quot;).wrapInner(htm|element|fnl)将每一个匹配的元素的子内容(包括文本节点)用一个HTML结构包裹起来，举例如上，将全部字符加粗；","categories":[{"name":"JQuery","slug":"JQuery","permalink":"https://www.cz5h.com/categories/JQuery/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://www.cz5h.com/tags/HTML/"},{"name":"JQuery","slug":"JQuery","permalink":"https://www.cz5h.com/tags/JQuery/"}]},{"title":"JQuery基础概念知识","slug":"2015-8-21 JQuery基础概念知识","date":"2015-08-20T22:00:00.000Z","updated":"2020-02-29T18:43:55.384Z","comments":true,"path":"article/dcf0.html","link":"","permalink":"https://www.cz5h.com/article/dcf0.html","excerpt":"（本文年代久远，请谨慎阅读）JQuery是继prototype之后又一个优秀的Javascript库。它是轻量级的js库 ，它兼容CSS3，还兼容各种浏览器（IE 6.0+, FF 1.5+, Safari 2.0+, Opera 9.0+），jQuery2.0及后续版本将不再支持IE6/7/8浏览器。jQuery使用户能更方便地处理HTML（标准通用标记语言下的一个应用）、events、实现动画效果，并且方便地为网站提供AJAX交互。jQuery还有一个比较大的优势是，它的文档说明很全，而且各种应用也说得很详细，同时还有许多成熟的插件可供选择。jQuery能够使用户的html页面保持代码和html内容分离。jQuery是一个兼容多浏览器的javascript库，核心理念是write less,do more(写得更少,做得更多)。jQuery的语法设计可以使开发者更加便捷，例如操作文档对象、选择DOM元素、制作动画效果、事件处理、使用Ajax以及其他功能。除此以外，jQuery提供API让开发者编写插件。其模块化的使用方式使开发者可以很轻松的开发出功能强大的静态或动态网页。","text":"（本文年代久远，请谨慎阅读）JQuery是继prototype之后又一个优秀的Javascript库。它是轻量级的js库 ，它兼容CSS3，还兼容各种浏览器（IE 6.0+, FF 1.5+, Safari 2.0+, Opera 9.0+），jQuery2.0及后续版本将不再支持IE6/7/8浏览器。jQuery使用户能更方便地处理HTML（标准通用标记语言下的一个应用）、events、实现动画效果，并且方便地为网站提供AJAX交互。jQuery还有一个比较大的优势是，它的文档说明很全，而且各种应用也说得很详细，同时还有许多成熟的插件可供选择。jQuery能够使用户的html页面保持代码和html内容分离。jQuery是一个兼容多浏览器的javascript库，核心理念是write less,do more(写得更少,做得更多)。jQuery的语法设计可以使开发者更加便捷，例如操作文档对象、选择DOM元素、制作动画效果、事件处理、使用Ajax以及其他功能。除此以外，jQuery提供API让开发者编写插件。其模块化的使用方式使开发者可以很轻松的开发出功能强大的静态或动态网页。 jQuery，顾名思议，也就是JavaScript和查询（Query），即是辅助JavaScript开发的库。 AJAX：即“Asynchronous Javascript And XML”（异步JavaScript和XML），可以不刷新页面完成数据库操作（包括查询并返回数据），在验证输入时有很重要的作用。 文档说明：Jquery有完整的API开发文档，比如jQuery1.11.0_20140330.chm，手册查询非常方便，即查即用 代码和html内容分离：使用Jquery不用再在html里面插入一堆js来调用命令了，只需要定义id即可，通过id选择器来选中JQuery对象，之后便可以通过对象方法来操作 总结JQUERY是一个JAVASCRIPT库（子集）、JS文件也是一个工具包； 封装了大量的有用函数，提高了开发效率；强大的DOM、CSS事件处理操作能力；使代码更加简洁；减弱了与浏览器的相关性；提高了运行效率； 下载地址：http://docs.jquery.com/Downloading_jQuery 使用jQuery的Id选择器；jQuery的事件方法；修改样式函数css()；修改属性函数attr()； 新建一个html文档 12345678&lt;!doctype html&gt;&lt;html lang=\"en\"&gt; &lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Document&lt;/title&gt; &lt;/head&gt; &lt;body&gt;&lt;/body&gt;&lt;/html&gt; !DOCTYPE是文档说明，做一个兼容性的网页一般完整的文档说明如下: 为网页提供了一种编码方式,否则页面很可能出现乱码.UTF-8 是没有国家的编码,也就是独立于任何一种语言,任何语言都可以使用 Document是网页的标题名称 要使用JavaScript脚本首先要导入js的库文件，即导入jquery-1.11.1.min.js（此处是压缩版的库文件）：1&lt;script src&#x3D;&quot;js&#x2F;jquery-1.11.1.min.js&quot; type&#x3D;&quot;text&#x2F;javascript&quot;&gt;&lt;&#x2F;script&gt; “src”是js库文件的路径，”type”是代表插入脚本的类型，可以为”text/javascript”或者”text/css”等。 带有min的文件打开后是没有缩进的，不带min的那个是完整格式的，打开后是有良好格式的js代码，方便阅读和修改（一般不要改） 应用 首先在html文档中写一个输入框和两个按钮：1234&lt;form action=\"sub.html\" onsubmit=\"return doCheck();\"&gt; &lt;span&gt;&lt;font color=\"red\"&gt;账号&lt;/font&gt;&lt;/span&gt;&lt;input type=\"text\" id=\"account\" /&gt; &lt;input type=\"submit\" value=\"提交\" /&gt;&lt;input type=\"reset\" value=\"重置\" /&gt;&lt;/form&gt; 以上两句写在html中的&lt;body&gt;&lt;/body&gt;标签中，&lt;input&gt;标签写在&lt;form&gt;标签中，&lt;input&gt;标签中的type类型”submit”是提交form标签之间文本框中输入的数据；&lt;input&gt;标签中的type类型”reset”是重置form标签之间文本框中输入的数据为空；&lt;form&gt;中的action属性标示了按下提交按钮后的跳转，可以是一个简单的网址，也可以是一个action的名字（框架中）；&lt;form&gt;中的onsubmit’属性标示了跳转之前要检查onsubmit的值，false则不跳转，true?false的值来自doCheck()这个JS函数； doCheck函数的实现作为JavaScript脚本要放在&lt;head&gt;&lt;/head&gt;标签中间，并用&lt;script type=&quot;text/javascript&gt;&lt;/script&gt;包含起来，如下所示： 123456789101112131415161718192021&lt;head&gt;&lt;title&gt;Document&lt;/title&gt;&lt;script src=\"js/jquery-1.11.1.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script type=\"text/javascript\"&gt; //public void doCheck(); //在这个方法中写入表单验证的代码。如果此方法返回true，则会执行action中的URL，否则就不执行。 function doCheck() &#123; //alert('docheck done'); var validFlag = true; //获取用户输入的账号文本值 var account = document.getElementById('account').value; //判断账号的长度是6-30个字符之间 //获取账号的长度 var len = account.length; if (len &lt; 6 || len &gt; 30) &#123; validFlag = false; &#125; return validFlag; &#125;&lt;/script&gt;&lt;/head&gt; 从以上代码可以看出，js代码和Java代码非常相似，但： 函数定义以function打头，可以返回true/false或者变量，或者数组等任何值（包括一个对象） 没有数据类型的分别，定义一个变量用var关键字，var parameter = xx； 数组的定义：var Arr = new Array(&apos;a&apos;,&apos;b&apos;,&apos;c&apos;); //new Array(&apos;&apos;)是固定格式以上获取文本框输入值使用的是Dom对象的方法，Dom（documnet），其对象有getElementById()这个方法，可以按html标签内的id名来获取到dom对象 12var account = document.getElementById('account').value;//即取到id为account的控件的value值，并赋值给account， 此处的account值是一个类似Java中的String类型（注意：不要混用Java中的各种方法），Js有其自己的各种函数来操作字符串，使用之前要确定正确 比较字符串可以用if(account==&quot;example&quot;)等方式来比较；其对象还有length属性，直接获取字符串长度； doCheck完成的是判断用户输入内容的长度，在6-30位是正确的，点击提交会跳转到sub.html页面；","categories":[{"name":"JQuery","slug":"JQuery","permalink":"https://www.cz5h.com/categories/JQuery/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://www.cz5h.com/tags/HTML/"},{"name":"JQuery","slug":"JQuery","permalink":"https://www.cz5h.com/tags/JQuery/"}]},{"title":"J2EE前后台传值带中文时乱码","slug":"2014-12-18 J2EE前后台传值带中文时乱码","date":"2014-12-17T23:00:00.000Z","updated":"2020-02-29T18:43:55.378Z","comments":true,"path":"article/421a.html","link":"","permalink":"https://www.cz5h.com/article/421a.html","excerpt":"（本文年代久远，请谨慎阅读）传值乱码问题在Web开发中涉及许多方面：登陆注册时，是否正确得到正确的中文用户名；修改信息时，是否可以显示提交的中文信息； 以上是具体使用，当然包括所有后台想得到值的 文本框 的传值！！可以看到输入中文是我们在做一个网站时必须要考虑的，纯英文的外国网站你输入中文也不会乱码。其实，解决乱码我们只需要注意几个问题，再加之少许操作即可获得想要的中文","text":"（本文年代久远，请谨慎阅读）传值乱码问题在Web开发中涉及许多方面：登陆注册时，是否正确得到正确的中文用户名；修改信息时，是否可以显示提交的中文信息； 以上是具体使用，当然包括所有后台想得到值的 文本框 的传值！！可以看到输入中文是我们在做一个网站时必须要考虑的，纯英文的外国网站你输入中文也不会乱码。其实，解决乱码我们只需要注意几个问题，再加之少许操作即可获得想要的中文 写在前面 一个Web工程项目中代码涉及编码的地方有几处：你的struts.xml配置文件中开头会有一句： 文本框所在的jsp页面的开头也会有一句：&lt;%@ page language=&quot;java&quot; import=&quot;java.util.*&quot; pageEncoding=&quot;GBK&quot;%&gt;好像这就是所涉及到编码的所有部分了，如果我们只是改变一下”GBK”,”UTF-8”等等字眼，估计随便怎么换也是乱码 我们需要进一步的进行“再编码”！！ 下面的解决方式面对两种问题，至今我只遇到了这两种，即两种后台得到值的方式“标准的form表单提交，后台用get和set得到文本框的值；大量重复操作的跳转（比如页面中一个表格里每一行后面有一个删除按钮），直接href=action后挂相应的值进行传递 标准form表单提交1.把显示的jsp页面的第一句话写为：pageEncoding=”UTF-8”，GBK是不行的，对于struts可以忽视 2.在后台Java代码中的业务处理部分， 如果你的get和set没有在.java中，你总要通过getParameter(“xxx”);来获取值即： 如果get与set在其中，则可以直接用，不用getParameter 要做的操作：String xxx = request.getParameter(“xxx”);之后加一句 xxx = URLDecoder.decode(username , “utf-8”); //关键代码,所需包会自动添加 get与set同你的处理代码在一起的，就在excute()里第一次出现xxx的前面添加：xxx = URLDecoder.decode(username , “utf-8”); 用href=action传值首先，要改写，不要用href，改为点击触发一个js的function()，比如： 1234&lt;input name=\"in\" value=\"&lt;%p++(构造了一个id值)%&gt;\"....href=\"xx.action?id=p \"&gt;单传非中文是没有问题的，但是中文就要改一下，先要执行一个js：&lt;input name=\"in\" value=\"&lt;%p++(构造了一个id值)%&gt;\"....onclick=”look('p')“&gt;添加完onclick属性后，再添加代码： 12345function look( str )&#123; //在js中首先进行两侧编码,注意是两次，查阅所有网上资料都可以发现，至于为什么，功力未到还不能搞懂 var faultAddr = encodeURI(encodeURI(str)); //需要通过两次编码 window.location.href=\"xxx.action?id=\"+str; &#125; 这相当于在页面传值的时候进行了编码，在后java中得到的是编码两次的变量，所以要进行一次解码，添加： 12xxx &#x3D; URLDecoder.decode(xxx, &quot;utf-8&quot;); &#x2F;&#x2F;关键代码，添加位置同First所述Second-End 具体实例在“input.jsp”页面中，需要通过js将值传递到后台，后台根据传递的值进行数据查询时， 通过test.jsp的js进行编码（粉色为编码），后台java解码（绿色为解码部分），可以解决 1234567&lt;script type=\"text/javascript\" charset=\"UTF-8\"&gt; function test()&#123; var faultAddr = encodeURI(document.getElementById(\"faultAddr\").value); faultAddr = encodeURI(faultAddr); //需要通过两次编码 window.frames[\"listframe\"].location.href =\"queryorderList.action?faultAddr=\" + faultAddr ; &#125;&lt;/script&gt; 1234&lt;tr&gt; &lt;td height=\"5%\" width=\"50\"&gt;输入内容&lt;/td&gt; &lt;td&gt;&lt;input id=\"faultAddr\" maxlength=\"300\" size=\"10\" name=\"faultAddr\" type=\"text\" value=\"\"/&gt;&lt;/td&gt;&lt;/tr&gt; 1234567import java.net.URLDecoder;String faultAddr =request.getParameter(\"faultAddr\");try&#123; faultAddr = URLDecoder.decode(faultAddr , \"utf-8\");&#125;catch(Exception e)&#123; e.printStackTrace();&#125;","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://www.cz5h.com/tags/JavaScript/"},{"name":"HTML","slug":"HTML","permalink":"https://www.cz5h.com/tags/HTML/"},{"name":"Java","slug":"Java","permalink":"https://www.cz5h.com/tags/Java/"}]},{"title":"select标签添加onclick()事件的兼容写法","slug":"2014-12-7 select标签添加onclick()事件的兼容写法","date":"2014-12-06T23:00:00.000Z","updated":"2020-02-29T18:43:55.380Z","comments":true,"path":"article/166a.html","link":"","permalink":"https://www.cz5h.com/article/166a.html","excerpt":"（本文年代久远，请谨慎阅读） 修改前123456&lt;select style=\"height:25px;width:160px;\"&gt; &lt;option onclick=\"xx('err')\" selected&gt;选择查找方式&lt;/option&gt; &lt;option onclick=\"xx('low')\" &gt;简单查询&lt;/option&gt; &lt;option onclick=\"xx('mid')\" &gt;模糊检索&lt;/option&gt; &lt;option onclick=\"xx('hih')\" &gt;高级搜索&lt;/option&gt;&lt;/select&gt;","text":"（本文年代久远，请谨慎阅读） 修改前123456&lt;select style=\"height:25px;width:160px;\"&gt; &lt;option onclick=\"xx('err')\" selected&gt;选择查找方式&lt;/option&gt; &lt;option onclick=\"xx('low')\" &gt;简单查询&lt;/option&gt; &lt;option onclick=\"xx('mid')\" &gt;模糊检索&lt;/option&gt; &lt;option onclick=\"xx('hih')\" &gt;高级搜索&lt;/option&gt;&lt;/select&gt; javascript如下： 12345678910function xx(value)&#123; alert(value); if(value==\"low\")&#123; ... ... &#125;else if(value==\"mid\")&#123; ... ... &#125;else if(value==\"hih\")&#123; ... ... &#125;&#125; 以上代码片是可以在Firefox和IE9下运行的，但是它在我的360浏览器上就是无效的，究其原因还是IE版本的问题（存在兼容性问题），也就是：老版本只能这样 &lt;select onclick() &gt;&lt;/select&gt; 而高版本和Firefox则支持这样 &lt;option onclick() &gt;&lt;/option&gt; 具体版本我们不去管它，因为我找到了折中的实现办法，即可以兼容的实现触发事件，解决了以上问题 修改后修改后的代码片如下： 123456&lt;select style=\"height:25px;width:160px;\" onclick=\"xx(this)\"&gt; &lt;option value=\"err\" selected&gt;选择查找方式&lt;/option&gt; &lt;option value=\"low\" &gt;简单查询&lt;/option&gt; &lt;option value=\"mid\" &gt;模糊检索&lt;/option&gt; &lt;option value=\"hih\" &gt;高级搜索&lt;/option&gt;&lt;/select&gt; javascript： 1234567891011function xx(value)&#123; var selectedOption=value.options[value.selectedIndex]; //alert(selectedOption.value); if(selectedOption.value==\"low\")&#123;... ... &#125;else if(selectedOption.value==\"mid\")&#123;... ... &#125;else if(selectedOption.value==\"hih\")&#123;... ... &#125;&#125; 修改后的实现其实是用了低版本IE的方法，但是通过获取到选项的value值，来选择要执行的js代码段，从而实现了一种灵活的兼容的触发事件的方法个人认为，此办法非常不错。","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://www.cz5h.com/tags/JavaScript/"},{"name":"HTML","slug":"HTML","permalink":"https://www.cz5h.com/tags/HTML/"}]},{"title":"注册型网站设计的阶段总结","slug":"2014-11-27 注册型网站设计的阶段总结","date":"2014-11-26T23:00:00.000Z","updated":"2020-02-29T18:43:55.376Z","comments":true,"path":"article/ee49.html","link":"","permalink":"https://www.cz5h.com/article/ee49.html","excerpt":"（本文年代久远，请谨慎阅读） 表格属性1&lt;table cellpadding=\"0\" cellspacing=\"0\" width=\"100%\" align=\"center\" style=\"ord-break:break-all;word-wrap:break-word;margin-right:0px;margin-left:0px;border:1px solid #000000; margin-top:10px;\" &gt; align=”center”表格元素在格子内居中 cellpadding=”0” cellspacing=”0”元格子（组成表格的每一个小格子）之间的距离为零，即一般表格样式 border:1px solid #000000;表格线的宽度为1px，颜色是黑色","text":"（本文年代久远，请谨慎阅读） 表格属性1&lt;table cellpadding=\"0\" cellspacing=\"0\" width=\"100%\" align=\"center\" style=\"ord-break:break-all;word-wrap:break-word;margin-right:0px;margin-left:0px;border:1px solid #000000; margin-top:10px;\" &gt; align=”center”表格元素在格子内居中 cellpadding=”0” cellspacing=”0”元格子（组成表格的每一个小格子）之间的距离为零，即一般表格样式 border:1px solid #000000;表格线的宽度为1px，颜色是黑色 表格分色12&lt;%if(i%2!=0)&#123; %&gt;&lt;tr bgcolor=\"#CFEEF8\"&gt;... ...&lt;/tr&gt;&lt;%&#125;else&#123; %&gt;&lt;tr bgcolor=\"#000005\"&gt;... ...&lt;tr&gt;&lt;%&#125; %&gt; 这是我自己想的方法，不知道大众化的方法是怎样实现的，其实分色就是利用bgcolor这个属性给表格上色 点击传值文字/按钮链接属性有form，无form，需要返回值，无需返回值直接简单执行 这是对于“批量按钮”来说的，如果是简单的地址链接，那直接href添加上，无需多说 但是如果是“删除”这种按钮，点击一下，会触发删除的action，后台会在数据库中将相应的id的记录删掉 所以在给“删除”添加链接时，就要传递一个参数id，并且处理一个action，但是单纯的用href=action?id=xx 的方式对于数字英文是可以，不过如果传参不是id，而是一些中文字符，则此处会出现传值乱码现象，具体参见我的： 这篇：http://blog.csdn.net/u012935646/article/details/42007041 直接静态链&lt;span&gt;&lt;/span&gt;以及&lt;a&gt;&lt;/a&gt;标记都可以添href属性 比如&lt;span href=&quot;http:www.baidu.com&quot; &gt;百度&lt;/span&gt; &lt;ahref=&quot;http:www.baidu.com&quot; &gt;百度&lt;/a&gt;都可以为百度二字添加相应的百度链接动态action传数据利用onclick属性转js；同“文字/按钮链接属性有form，无form&gt;…..”具体参见：http://blog.csdn.net/u012935646/article/details/42007041 多项注册型网站要考虑的东西输入检测检测邮箱格式的正确性，电话，手机等等，利用正则表达式+实现正则方法 有多重正则表达式，不能说谁优谁劣，具体有以下等等： 其一： 1234567891011121314&lt;script language=\"javascript\" type=\"text/javascript\" &gt; /** * Check email format */ function emailCheck(obj, labelName) &#123; var objName = eval(\"document.all.\"+obj); var pattern = /^([\\.a-zA-Z0-9_-])+@([a-zA-Z0-9_-])+(\\.[a-zA-Z0-9_-])+/; if (!pattern.test(objName.value)) &#123; alert(\"请输入正确的邮箱地址。\"); objName.focus(); return false; &#125; return true; &#125; &lt;/script&gt; &lt;input type=\"text\" id=\"email\" name=\"email\" maxlength=\"30\" onblur=\"return emailCheck('email', 'email')\" /&gt; //jsp代码 其二： 12345678910111213141516171819&lt;script type=\"text/javascript\"&gt; function isValidMail() &#123; var TextVal = document.getElementById(\"TextBox1\").value; var Regex = /^(?:\\w+\\.?)*\\w+@(?:\\w+\\.)*\\w+$/; if (Regex.test(TextVal))&#123; alert(true); &#125;else &#123; if (TextVal == \"\") &#123; alert(\"请输入电子邮件地址！！\"); return false; &#125;else &#123; alert(\"您好，你输入不正确，请重新输入；\"); document.getElementById(\"TextBox1\").value = \"\"; return false; &#125; &#125; &lt;/script&gt;&lt;asp:TextBox ID=\"TextBox1\" runat=\"server\"&gt;&lt;/asp:TextBox&gt; //asp的代码 &lt;asp:Button ID=\"Button3\" runat=\"server\" Text=\"Button\" OnClientClick=\"return isValidMail()\" OnClick=\"Button3_Click\" /&gt; //asp的代码 其三： 12345678910111213141516171819function test()&#123; var temp = document.getElementById(\"text1\"); //对电子邮件的验证 var myreg = /^([a-zA-Z0-9]+[_|\\_|\\.]?)*[a-zA-Z0-9]+@([a-zA-Z0-9]+[_|\\_|\\.]?)*[a-zA-Z0-9]+\\.[a-zA-Z]&#123;2,3&#125;$/; if(!myreg.test(temp.value))&#123; alert('提示\\n\\n请输入有效的E_mail！'); myreg.focus(); return false; &#125; &#125; //由于方法相同，一下只写出相关的正则表达式 //对于手机号码的验证（提供了两种方法） var mobile=/^((13[0-9]&#123;1&#125;)|159|153)+\\d&#123;8&#125;$/; var mobile1=/^(13+\\d&#123;9&#125;)|(159+\\d&#123;8&#125;)|(153+\\d&#123;8&#125;)$/; //对于区号的验证 var phoneAreaNum = /^\\d&#123;3,4&#125;$/; //对于电话号码的验证 var phone =/^\\d&#123;7,8&#125;$/;&#125; 以上代码的含义说明： 1、/^$/ 这个是个通用的格式。 ^ 匹配输入字符串的开始位置；$匹配输入字符串的结束位置 2、其中输入需要实现的功能。 + 匹配前面的子表达式一次或多次； ？匹配前面的子表达式零次或一次； \\d 匹配一个数字字符，等价于[0-9]； * 匹配前面的子表达式零次或多次；不同选项之间有关联时的动态对应性比较麻烦，以后会单独研究一下 空值检测全空格或回车的定性为空的特性，注册时，更改时 这是对于文本框信息进行检验的一个步骤，当我们进行输入时，应该把空格全部去掉，无论是提交的用户名也好，还是一串查询的信息也好 空格对于后代的数据库处理其实都是无用的，所以要进行过滤，使用的方法： var xxx = inputname.trim()；//假设已经获得了input框的值，并且把它赋值给了inputname 这样可以把输入到文本框里的字符中的空格给去掉 再者，如果对于”输入不为空“这个条件进行检测，那么输入一串空格将是必须的，上述的trim()可以吧字符串去掉空格字符，当然对于一串 空格，处理后就会是空值，即上述方法就可以完成。 必填判断哪个必须输入，哪个不用的提交判断 这是对于有多项注册项目的网站来说的，比如12306的注册，有一二十项，但是必须填的可能也就那几项，我们会在表格后面紧跟红星*等等方法标记 具有注册后更改的页面的实现有一些是需要动态选择的，比如说： 有一个提交文档的选项，是或否，选择是，则会出现提交文档的按钮，禁止提交为空，即不提交；如果选择否，那么不会出现提交选项； 这个实现用js： 12345678function yns()&#123; var publish=$(\"#formxx [name='publish']:checked\").val();//内容可忽略就是获取选择项的值,如果选择是，则写一个*号，如果选择否，则写空 if(publish==\"yes\")&#123; document.getElementById('cnt').innerHTML='&lt;font color=\"red\"&gt; *&lt;/font&gt;'; &#125;else&#123; document.getElementById('cnt').innerHTML=' '; &#125; &#125; 注册名判重用户名的提交判重实现，用户名已存在 这对于一个注册网站来说是很重要的，在后台的数据库中，不会允许有两个相同用户名的账号存在，传统方式是提交完后，然后给注册者一个反馈，这是不合理的 虽然也可以，但是不友好，友好的检测是在用户一旦输入完用户名后就给出相应的弹窗信息，这样用户就会及时的进行修改 如何实现： 利用ajax-Asynchronous Javascript And XML异步JavaScript和XML，ajax是一种编程方式并非一种新语言 可以进行无刷新的检测，即虽然也执行action，也具有后台的处理，但是不会出现跳转，也不会刷新本页面，实现用户名查重的方法如下： 1234567891011121314151617181920212223242526272829&lt;script type=\"text/javascript\"&gt; function checkRegister()&#123; var uname = $(\"#username\").val(); //alert(uname); //检查输入内容格式 if(uname==\"\") &#123; alert(\"用户名不能为空\"); $(\"#username\").focus(); return; &#125; var flag = false; $.ajax(&#123; //编程方式 type : \"post\", async:false, url : \"check.action?uname=\"+uname, success : function(result) &#123; if(result==\"failure\")&#123; alert(\"用户名已存在\"); $(\"#username\").focus(); &#125;else&#123; flag = true; &#125; &#125;, failure:function()&#123;&#125; &#125;); return flag; &#125; &lt;/script&gt; 数据库中表结构格式的设置 action?xx=x直接传值乱码的解决 参见：：http://blog.csdn.net/u012935646/article/details/42007041 Firefox中js function报错xx is not defined的问题火狐的Firebug可以说是给我们web开发提供了许多的便利，但是有些时候一些错误却会误导我们，浪费我们的时间：比如页面中写了一个function do，但是在运行页面时没有执行预期的效果，而且出现了一条错误信息：do is not defined如何看待这个错误：这并不是我们function的问题，有的时候一个变量的定义错误，或许是忘记结尾加分号，或许是function程序体里面的括号匹配有问题上述情况都会引起Firebug的报错，毕竟没法对js中的错误定位到行，所以“尽信工具则不如无工具” 其他使用js进行按键的屏蔽，鼠标键的屏蔽，即屏蔽复制保存等操作 这又是一个在现在web项目中经常遇到的需要解决的一个问题，为何：因为有些数据是不能被复制的 比如说一个会议信息系统可以查询到与会人员的信息，一个普通的注册账户可以正常的查看其它开会人员的信息，这是合情合理的， 但是如果有人将信息复制，或通过网页保存的方式等等，把这个数据获取到了，那么一些提交的重要文档资料，救会被泄露 最真实的应用：360图书馆以及 百度文库的在线预览 如何实现： 1234567891011121314151617181920212223242526272829303132 function key()&#123; //if(event.shiftKey)&#123; //window.close();&#125; //禁止shift if(event.altKey)&#123; alert('禁止CTRL-C复制本贴内容');&#125; //禁止alt if(event.ctrlKey)&#123; alert('禁止CTRL-C复制本贴内容');&#125; //禁止ctrl return false; &#125; document.onkeydown=key; if (window.Event) document.captureEvents(Event.MOUSEUP); function norightclick(e)&#123; if (window.Event)&#123; if (e.which == 2 || e.which == 3) return false;&#125; else if (event.button == 2 || event.button == 3)&#123; event.cancelBubble = true event.returnValue = false; return false;&#125; &#125; function Click()&#123; alert('禁止右键粘贴本贴内容'); window.event.returnValue=false; &#125; document.oncontextmenu=Click; &#125;&#125; 一个更简单的方法就是在中加入如下的代码,这样鼠标的左右键都失效了. 12topmargin=\"0\" oncontextmenu=\"return false\" ondragstart=\"return false\" onselectstart =\"return false\" onselect=\"document.selection.empty()\" oncopy=\"document.selection.empty()\" onbeforecopy=\"return false\" onmouseup=\"document.selection.empty()\" 或 12&lt;body onmousemove=\\HideMenu()\\ oncontextmenu=\"return false\" ondragstart=\"return false\" onselectstart =\"return false\" onselect=\"document.selection.empty()\" oncopy=\"document.selection.empty()\" onbeforecopy=\"return false\" onmouseup=\"document.selection.empty()\"&gt; 禁止网页另存为：在后面加入以下代码：","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://www.cz5h.com/tags/JavaScript/"},{"name":"HTML","slug":"HTML","permalink":"https://www.cz5h.com/tags/HTML/"},{"name":"Java","slug":"Java","permalink":"https://www.cz5h.com/tags/Java/"}]},{"title":"JAVA动态创建表以及动态插入数据","slug":"2014-11-7 JAVA动态创建表以及动态插入数据","date":"2014-11-06T23:00:00.000Z","updated":"2020-02-29T18:43:55.377Z","comments":true,"path":"article/7bfd.html","link":"","permalink":"https://www.cz5h.com/article/7bfd.html","excerpt":"（本文年代久远，请谨慎阅读）首先，连接数据库是必做的工作，在温习一下。 连接数据库利用JDBC驱动链接Mysql数据其实很简单的，第一要下载一个名为 “mysql-connector-java-5.1.20-bin.jar” 驱动包。并解压到相应的目录！5.1.20是版 本号到目前为止这个是最新的版本！","text":"（本文年代久远，请谨慎阅读）首先，连接数据库是必做的工作，在温习一下。 连接数据库利用JDBC驱动链接Mysql数据其实很简单的，第一要下载一个名为 “mysql-connector-java-5.1.20-bin.jar” 驱动包。并解压到相应的目录！5.1.20是版 本号到目前为止这个是最新的版本！ 第一、如果你是在命令行方式下开发，需要把mysql-connector-java-5.1.2.0-bin.jar 添加到系统的CLASSPATH中。怎么加到CLASSPATH中我想不要讲了大家也应懂的吧。 第二、如果你是用Eclipse开发工具的话，还要配置一下 “Java Build Path”、具体的操作“点击Eclipse的Project-&gt;Properties-&gt;Java Build Path-&gt;Libraries” 现在在看以的窗口中点击右边的Add External JARs 然后选择mysql-connector-java-5.1.2.0-bin.jar驱动 点击打开就完成了配置。 下面就是Java利用JDBC连接Mysql数据的实例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.sql.*; public class ConnectMysql &#123; public static void main(String[] args) &#123; String driver = \"com.mysql.jdbc.Driver\"; String url = \"jdbc:mysql://192.168.1.112:3306/linksystem\"; String user = \"root\"; String password = \"blog.micxp.com\"; try &#123; Class.forName(driver); Connection conn = DriverManager.getConnection(url, user, password); if (!conn.isClosed()) &#123; System.out.println(\"Succeeded connecting to the Database!\"); Statement statement = conn.createStatement(); String sql = \"select * from flink_list\"; ResultSet rs = statement.executeQuery(sql); String name; while (rs.next()) &#123; name = rs.getString(\"link_name\"); System.out.println(name); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; ``` 上述实例包含在完整的结构中，如果只是作为项目程序的一部分，也可以将其写为一个类，在主程序中new对象即可，不再赘述。连接时有时会抛出异常，在一般情况下，密码错误是主要原因，通过mysql工具直接修改即可，另外自己的url也容易出错，上述例中在使用 url=\"jdbc:mysql://192.168.1.112:3306/linksystem\" 时，就抛出异常。改正方法：- 把192.168.1.112直接改为localhost。- linksystem是你建表的数据库名称，要换成你自己的。### 动态建表一切就绪后，开始动态建表，建表代码如下： ```javasqlstr = \"create table random_data(\"; sqlstr+= \" id int(32),\"; for(i=0;i&lt;nodenum-1;i++) sqlstr+=\"ND\"+i+\" int(32),\"; sqlstr+= \"ND\"+(nodenum-1)+\" int(32)\"; sqlstr+= \" );\"; 上述代码可输出如下形式字符串： 1create table random_data( id int(32),ND0 int(32),ND1 int(32),ND2 int(32),ND3 int(32),ND4 int(32),ND5 int(32),ND6 int(32) ); 即生成一个列名为id，ND0，ND1……的表格，列名显然是动态生成的。生成表格之后要插入数据，现有一ArrayLst存放着全部的数据，要做的就是逐个放入空表中，显然要用到insert语句。 首先我们可以写出如下代码： 12345678for(i=0;i&lt;nodenum-1;i++)&#123; for(j=0;j&lt;nolist.get(i).get_feature_count();j++)&#123; sqlin = \"insert into random_data( id,\"; for(i=0;i&lt;nodenum-1;i++)sqlin+=\"ND\"+i+\",\"; sqlin+= \"ND\"+(nodenum-1); sqlin+=\") values(\"; &#125; &#125; 可以得到 “ insert into random_data( id,ND0,ND1,ND2,ND3,ND4,ND5,ND6) values( ”的字符串 ，之后的部分必须动态的重构出来，才能拼接完整，令over也是ArrayList类型，是原数据集ArrayList中数据每隔列数个就存入一次得到的，其输出已经形如： &apos;2&apos;,&apos;3&apos;,&apos;1&apos;,&apos;0&apos;,&apos;2&apos;,&apos;1&apos;,&apos;4&apos;, &apos;4&apos;,&apos;2&apos;,&apos;5&apos;,&apos;6&apos;,&apos;2&apos;,&apos;1&apos;,&apos;2&apos;, &apos;3&apos;,&apos;2&apos;,&apos;4&apos;,&apos;0&apos;,&apos;2&apos;,&apos;2&apos;,&apos;0&apos;, 再用 T = T .substring(0,T.length()-1) 这个方法去掉最后重复的逗号，现在，完整的insert语句中values括号内的字符串已经得到，最后过程，有如下代码： 12345for(i=0;i&lt;over.size();i++)&#123; String sqldo = sqlin +\"'\"+(i+1)+\"',\"+ over.get(i) + \");\"; //System.out.println(sqldo); statement.executeUpdate(sqldo); &#125; 输出拼接完成的字符串，可得到若干字符串形如： 1insert into random_data( id,ND0,ND1,ND2,ND3,ND4,ND5,ND6) values('3','2','4','0','2','2','0'); 至此，由以上种种操作，已经得到了以下字符串，显然它们就是是我们想要的sql执行语句： 123create table random_data( id int(32),ND0 int(32),ND1 int(32),ND2 int(32),ND3 int(32),ND4 int(32),ND5 int(32),ND6 int(32) );insert into random_data( id,ND0,ND1,ND2,ND3,ND4,ND5,ND6) values('3','2','4','1','0','3','2'); select * from random_data where id = \"16760\"; //具体查询不做详述 由于在for循环中进行，每次拼接完成后随即执行，完成循环的同时也完成了对数据库中数据的插入操作,所以动态建立的表格中便动态插入了数据。 以上所有内容的关键，就是字符串的拼接，以及所遵循的sql语句的书写格式，在实际调试程序时，最好的方法是在mysql-front中的命令调试器（或命令行）中调试输出的字符串，这个过程应该是个考验细心和耐心的过程。","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://www.cz5h.com/tags/Mysql/"},{"name":"动态建表","slug":"动态建表","permalink":"https://www.cz5h.com/tags/%E5%8A%A8%E6%80%81%E5%BB%BA%E8%A1%A8/"}]},{"title":"关于JAVA中动态创建二维数组的技巧","slug":"2014-10-11 关于JAVA中动态创建二维数组的技巧","date":"2014-10-10T22:00:00.000Z","updated":"2020-02-29T18:43:55.373Z","comments":true,"path":"article/a660.html","link":"","permalink":"https://www.cz5h.com/article/a660.html","excerpt":"（本文年代久远，请谨慎阅读）看似一个非常简单的问题，但是实际却花了我很多时间。我的目的是，创建一个二维数组str[][]，令 str[][] &lt;-- Arraylist&lt;Arraylist&lt;T&gt;&gt;//此处T指的int(Integer)类型 创建二维数组首先JAVA中创建二维数组的方法无非两种:","text":"（本文年代久远，请谨慎阅读）看似一个非常简单的问题，但是实际却花了我很多时间。我的目的是，创建一个二维数组str[][]，令 str[][] &lt;-- Arraylist&lt;Arraylist&lt;T&gt;&gt;//此处T指的int(Integer)类型 创建二维数组首先JAVA中创建二维数组的方法无非两种: 一种是静态的，即已知全部数据，比如要建立3乘3的二维数组，每个数组中的个数，及数组中元素是什么都明确已知，注意，是两者都已知才可以静态赋值，例如 1int a[][] &#x3D; &#123;&#123;1,2,6&#125;,&#123;3,4,5,6&#125;,&#123;7,8,9&#125;&#125; ; 静态赋值比较简单，在实际中用的也不多，因为用到此处时多为不同类型的转化问题，所以大多信息存在于已知的类型数据中，要转化为二维数组中，必然要动态的按照原类型中的信息重构二维数组，所以新的二维数组可能每个数组中元素个数都不确定，需要动态确定。 动态赋值动态赋值，也分两种，因为赋值方式除了直接两类型相等外，绝大多数都是通过两层循环，逐个赋值。于是产生了问题，在所需要的二维数组的要求“不高”时，可以直接用形如 int [][]a = new int[3][3]; 来存储，反之则会出错误。 上述的“要求”高低，就是说在不确定每个数组长度时，直接用较大的空间去存，就好像 变量 a[] 是一个班的成绩，它是未知的，可以直接用int a[100]来存一样，可能结果只用了100个中的30个，但是也完成了储存或输出的任务。 那么，如果要求是”高”的，意思是，结果二维数组不仅仅完成存储的任务，还要保证每个数组的长度，同原信息保持一致。回到正题，要完成 str[][] &lt;-- Arraylist&lt;Arraylist&lt;T&gt;&gt; 这一过程，用str[1000][1000]来存简单情况下是没有问题的，但二维数组却丢失了ArrayList中的每个“小链表”的长度 这一重要信息。其结果第一是浪费了空间，第二个很重要的是这个二维数组不能再利用，可能通过限制可以完成输出的任务，但是用于递归嵌套等对每个数组长度有明确要求的时候，str[1000][1000]完全没用。 其实，二维数组的每一维都可以动态创建，这一点很重要，动态第一维的方法：int [][]a = new a[第一维数][]； 然后，在上面一维创建后，同样可以动态第二维：int a[ i ] = new a[ 第二维数 ]； 实现比如两次循环时，便可以如下操作： 12345678int [][] arr ;arr = new int [ 一维数 ][]; //动态创建第一维for ( i = 0 ; i &lt; 一维数 ; i++ ) &#123; arr [ i ] = new int [ 二维数 ]; //动态创建第二维 for( j=0 ; j &lt; 二维数 ; j++) &#123; arr [i][j] = j; &#125;&#125; 由上可完成赋值，结果每个数组个数可能都不相同，即完成了Arraylist&lt;Arraylist&lt;T&gt;&gt; 给 str[][] 赋值的工作。","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://www.cz5h.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"动态赋值","slug":"动态赋值","permalink":"https://www.cz5h.com/tags/%E5%8A%A8%E6%80%81%E8%B5%8B%E5%80%BC/"}]},{"title":"在VC6.0中连接mysql数据库的方法实例","slug":"2014-10-9 在VC6.0中连接mysql数据库的方法实例","date":"2014-10-08T22:00:00.000Z","updated":"2020-02-29T18:43:55.381Z","comments":true,"path":"article/39cd.html","link":"","permalink":"https://www.cz5h.com/article/39cd.html","excerpt":"（本文年代久远，请谨慎阅读）最近用JAVA写程序，在连接数据库并操作上感觉还是较其他语言简单多了，在这方面C/C++就显得有点繁杂，不过也并非难事。首先就是要清除mysql提供的关于C的API，连接：http://dev.mysql.com/doc/refman/5.1/zh/apis.html","text":"（本文年代久远，请谨慎阅读）最近用JAVA写程序，在连接数据库并操作上感觉还是较其他语言简单多了，在这方面C/C++就显得有点繁杂，不过也并非难事。首先就是要清除mysql提供的关于C的API，连接：http://dev.mysql.com/doc/refman/5.1/zh/apis.html API内容包括以下，用到的大概前几项，主要是数据类型，函数概括，函数描述 25.2. MySQL C API 25.2.1. C API数据类型 25.2.2. C API函数概述 25.2.3. C API函数描述 25.2.4. C API预处理语句 25.2.5. C API预处理语句的数据类型 25.2.6. C API预处理语句函数概述 25.2.7. C API预处理语句函数描述 25.2.8. C API预处理语句方面的问题 25.2.9. 多查询执行的C API处理 25.2.10. 日期和时间值的C API处理 25.2.11. C API线程函数介绍 25.2.12. C API嵌入式服务器函数介绍 25.2.13. 使用C API时的常见问题 25.2.14. 创建客户端程序 25.2.15. 如何生成线程式客户端以上内容非常全面，是mysql官方资料，要自己多看多了解，学会查询即可。知道了上面的内容，那自己写个连接代码也是很容易的，主要就是几个异常的检测以及连接操作，具体的API上面目录里都有详细讲解，在此只贴出代码： 必要的头文件包括以下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;winsock.h&gt; //最好放在首位，位置关系会导致错误,mysql.h#include &lt;mysql.h&gt;#include &lt;stdio.h&gt;#include &lt;cstring&gt;#include &lt;string&gt;#pragma comment(lib, \"ws2_32.lib\")#pragma comment(lib, \"libmysql.lib\")using namespace std;连接及操作的代码实现：MYSQL mydata;if (0 == mysql_library_init(0, NULL, NULL)) &#123;cout &lt;&lt; \"mysql_library_init() succeed\" &lt;&lt; endl;&#125; else &#123; cout &lt;&lt; \"mysql_library_init() failed\" &lt;&lt; endl; return -1;&#125;if (NULL != mysql_init(&amp;mydata)) &#123; cout &lt;&lt; \"mysql_init() succeed\" &lt;&lt; endl;&#125;else &#123; cout &lt;&lt; \"mysql_init() failed\" &lt;&lt; endl; return -1;&#125;if (0 == mysql_options(&amp;mydata, MYSQL_SET_CHARSET_NAME, \"gbk\")) &#123;cout &lt;&lt; \"mysql_options() succeed\" &lt;&lt; endl; &#125; return -1; &#125;if(NULL!=mysql_real_connect(&amp;mydata,\"localhost\",\"root\",\"\",\"cap\",3306,NULL,0)) //更改项&#123;cout &lt;&lt; \"mysql_real_connect() succeed\" &lt;&lt; endl;&#125; else &#123; cout &lt;&lt; \"mysql_real_connect() failed\" &lt;&lt; endl;return -1; &#125;///*****************************/ 生成数据，可忽略不看 /***************/ mysql_query(&amp;mydata,\"truncate table n_e_w;\"); string sqlstr; sqlstr += \"insert into n_e_w \"; sqlstr += \"set input=(SELECT c_name FROM concept ORDER BY RAND() LIMIT 1),\"; sqlstr += \"output=(SELECT c_name FROM concept ORDER BY RAND() LIMIT 1),\"; sqlstr += \"beforer=(SELECT c_name FROM concept ORDER BY RAND() LIMIT 1),\"; sqlstr += \"afterr=(SELECT c_name FROM concept ORDER BY RAND() LIMIT 1),\"; sqlstr += \"s_one=(SELECT s FROM s_table ORDER BY RAND() LIMIT 1),\"; sqlstr += \"s_two=(SELECT s FROM s_table ORDER BY RAND() LIMIT 1),\"; sqlstr += \"s_three=(SELECT s FROM s_table ORDER BY RAND() LIMIT 1);\"; for(int t = 0; t &lt; 30 ; t++)mysql_query(&amp;mydata,sqlstr.c_str()); //******************************/ 获得数据，存到traindata /**************/MYSQL_RES *result = NULL;mysql_query(&amp;mydata,\"SELECT * FROM n_e_w\");result = mysql_store_result(&amp;mydata); //取得并打印行数int rowcount = mysql_num_rows(result);unsigned int fieldcount = mysql_num_fields(result); //取得并打印各字段的名称MYSQL_ROW row = NULL;row = mysql_fetch_row(result); for(int j=0;j&lt;rowcount;j++)&#123; for(int i = 0; i &lt; fieldcount-1; i++) &#123; traindata[j][i] = row[i]; //cout&lt;&lt;\"整个结果集显示：\"&lt;&lt;traindata[j][i] &lt;&lt;\"\\t\"; &#125; row = mysql_fetch_row(result);&#125; //for(int ii=0;ii&lt;DEFINE_COUNT;ii++) cout&lt;&lt;traindata[ii][1]&lt;&lt;endl; mysql_free_result(result) ;mysql_close(&amp;mydata);mysql_server_end(); 上述内容只是简单的建立连接后查询内容，其中只在建立连接部分做了异常判别处理，其实还有很多工作没有做，比如未涉及的创建表，删除表等等操作，其都有对应的异常判别的API函数，通过IF条件判断，可以对创建不成功或删除不成功等异常情况予以显式输出，从而完善代码。 其余配置以上是代码书写的工作，其实在书写代码之前，要用C++连（本人用的VC6.0）数据库，还要在VC中做相应的配置工作： 打开VC6.0 工具栏Tools菜单下的Options选项，在Directories的标签页中右边的“Show directories for:”下拉列表中选中“Includefiles”，然后在中间列表框中添加你本地安装MySQL的include目录路径（X:...\\include）。 在“Show directories for:”下拉列表中选中“Library files”，然后添加本地安装MySQL的Lib目录路径。Lib目录下还有debug和opt两个目录，建议选debug（X:...\\lib\\debug）。 在“Project settings-&gt;Link:Object/library modules”里面添加“libmysql.lib”。 在程序开头的写法，具体参照上文中代码。 注：#include “winsock.h”一定要写在#include “mysql.h”的前面，否则出错。 将“libmySQL.lib、libmySQL.dll”拷到你所建的工程的目录下。 到此，完成配置后，即可进行连接并对数据库进行操作。","categories":[{"name":"C/C++","slug":"C-C","permalink":"https://www.cz5h.com/categories/C-C/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://www.cz5h.com/tags/Mysql/"},{"name":"C/C++","slug":"C-C","permalink":"https://www.cz5h.com/tags/C-C/"}]},{"title":"从N个数组分别取值，穷尽全部情况","slug":"2014-9-23 从N个数组分别取值，穷尽全部情况","date":"2014-09-22T22:00:00.000Z","updated":"2020-02-29T18:43:55.383Z","comments":true,"path":"article/468f.html","link":"","permalink":"https://www.cz5h.com/article/468f.html","excerpt":"（本文年代久远，请谨慎阅读）要求是这样的： 具体要求有N个数组，每个数组元素不定，从每个数组都中取出一个，组成长度为N的序列，求穷尽序列的所有情况。 很明显这区别于给定几个元素并把这些元素全排列的问题，全排列要交换位置，此处则不同，暂称“伪全排列”。 网上资料很多，不过多用指针或者指针数组，这对于JAVA来说没一点用。","text":"（本文年代久远，请谨慎阅读）要求是这样的： 具体要求有N个数组，每个数组元素不定，从每个数组都中取出一个，组成长度为N的序列，求穷尽序列的所有情况。 很明显这区别于给定几个元素并把这些元素全排列的问题，全排列要交换位置，此处则不同，暂称“伪全排列”。 网上资料很多，不过多用指针或者指针数组，这对于JAVA来说没一点用。 在此，我把JAVA中的实现来个详细说明。 首先，思路为递归，将其写为成员函数，方便调用。 具体实现1234567891011121314151617181920public class ABC&#123; private static int[][]str; public ABC() &#123; ... ... //str[][]的赋值语句，得到存储完成的二维数组，见‘创建动态二维数组的技巧’ ... ... int result[]=new int[ N ]; //此处即符合要求的，有N个数组 show(result,0); //进入递归函数，进行测试 &#125; public static void show(int[] result1, int curr)&#123; //curr即代表当前取元素的数组 if (curr == N) &#123; //curr==N即完成一个序列，输出一次结果集 for(int k=0;k&lt;result1.length;k++) System.out.print(result1[k]); System.out.println(); //输出格式控制 &#125;else &#123; for (int i = 0; i &lt;str[curr].length; ++i) &#123; //每次循环此次取元素的数组的长度 result1[curr] = str[curr][i]; //关键赋值处，给到结果集 show(result1, curr+1); //一次递归完成，即从下一个数组中去取元素，curr+1. &#125; &#125; &#125; 以上可完成所有情况的输出，输入集是个二维数组，可方便由多种数据类型转化得到。输出是一维数组，也可方便进行再处理，故代码易改造，简洁明了。以上即是实现的所有代码(测试+方法)。 测试如下,从七个数组中取： [01234]，[012]，[012345]，[0123456]，[0123456]，[012]，[01234]情况序列计数为66150个，结果集输出正确，此处省略。","categories":[{"name":"Java/数据库","slug":"Java-数据库","permalink":"https://www.cz5h.com/categories/Java-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://www.cz5h.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"Java","slug":"Java","permalink":"https://www.cz5h.com/tags/Java/"},{"name":"排列组合","slug":"排列组合","permalink":"https://www.cz5h.com/tags/%E6%8E%92%E5%88%97%E7%BB%84%E5%90%88/"}]},{"title":"关于图中节点间的概率求解问题","slug":"2014-9-20 关于图中节点间的概率求解问题","date":"2014-09-19T22:00:00.000Z","updated":"2020-06-27T22:26:23.163Z","comments":true,"path":"article/4735.html","link":"","permalink":"https://www.cz5h.com/article/4735.html","excerpt":"（本文年代久远，请谨慎阅读）前提：节点是含有若干特征（小节点）的大节点，大节点间连接实际为特征间的连接 在一个网络图中，若干节点之间的概率问题有以下几种： 设现有A,B,C等若干大节点，其内特征为ai,bj,ck;","text":"（本文年代久远，请谨慎阅读）前提：节点是含有若干特征（小节点）的大节点，大节点间连接实际为特征间的连接 在一个网络图中，若干节点之间的概率问题有以下几种： 设现有A,B,C等若干大节点，其内特征为ai,bj,ck; P(A); //数出A节点发散的所有边的数量除以图中出现的总边数 P(AB); //即P(A)*P(B)，原理同上 P(A,B); //此为联合概率，如果AB之间不相联系，则直接为零 P(A | B); //AB间相关联边数/B涉及的边数 P(A | B,C); //在上条基础上求加和，待改进 P(A,C | B); //与AC两节点相关联的边数/B的边数，待改进 P(ai | bj); //该bj特征与ai的边数/bj涉及的边数 P(ai | bj,ck); //在上条基础上求加和，待改进 P(ai,bj | ck); //ck与ai，bj两特征相关联的边数/ai，bj两特征的边数，待改进以上这么多都是区别于传统概率论中的求解方法，因为节点之间表现发生与不发生的 标致就是之间有没有边！！ 求两个节点间的概率此问题的前提是，节点为大节点，内有若干特征，节点间的连接（或称为连线）实际为特征之间的连线。且两节点不是孤立的，而是在一个网络（或称一个图）中。 方法利用已知的特征之间的边，来分别计算边的条数，直接用条数来计算概率。 example： 求条件概率P(A|B)，A内有 a0,a1,a2；B内有b0,b1；现求节点B“发生”的情况下节点A发生的概率，用公式推导P(A|B)=P(AB)/P(B)；或者直接由实际出发， 可得出： 分子是AB间特征的连线条数，分母是B自己特征的全部连线条数，注意B除了与A点的特征相连外还与其他点相连。 由上述可用连线边数来求得概率。 但是，现有一公式如图， 并不是用的节点间数边数的方法，而是进而细化到节点内的特征之间，最底层是数特征的边数，求得是P(ai|bj)的概率，概率最后加和，看似很完美。 但有个致命问题，P(ai|bj)的每一个都是概率值，0~1，对若干项加和后极有可能大于1 !! 说明这个公式是有问题的，目前的解决办法是：求加权平均 这个平均不是所有特征数的和，而是仅仅有概率的数量，即P(ai|bj)=0时，不算入其内。 目前暂且这样处理。 以上两种已java编程实现，结果有较大差异，不过上述思路大体正确，先记于此","categories":[{"name":"ML/R学习笔记","slug":"ML-R学习笔记","permalink":"https://www.cz5h.com/categories/ML-R%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"节点概率","slug":"节点概率","permalink":"https://www.cz5h.com/tags/%E8%8A%82%E7%82%B9%E6%A6%82%E7%8E%87/"},{"name":"贝叶斯公式","slug":"贝叶斯公式","permalink":"https://www.cz5h.com/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F/"}]},{"title":"将博客搬至CSDN","slug":"2000-5-1 搬家","date":"2000-04-30T22:00:00.000Z","updated":"2020-08-07T12:37:09.413Z","comments":true,"path":"article/38f4.html","link":"","permalink":"https://www.cz5h.com/article/38f4.html","excerpt":"","text":"为了不影响文章列表，就放在最最后面吧，这是为了搬迁博客用的验证页面。 ————编辑于2020年8月7日","categories":[{"name":"Daily_Life","slug":"Daily-Life","permalink":"https://www.cz5h.com/categories/Daily-Life/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.cz5h.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"生活","slug":"生活","permalink":"https://www.cz5h.com/tags/%E7%94%9F%E6%B4%BB/"}]}]}